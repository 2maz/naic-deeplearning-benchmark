Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 928k/97.8M [00:00<00:10, 9.49MB/s]  1%|          | 792k/97.8M [00:00<00:12, 8.11MB/s]  5%|▌         | 5.21M/97.8M [00:00<00:07, 12.4MB/s]  7%|▋         | 7.00M/97.8M [00:00<00:08, 11.0MB/s] 10%|▉         | 9.50M/97.8M [00:00<00:05, 15.9MB/s] 13%|█▎        | 12.5M/97.8M [00:00<00:06, 14.5MB/s] 14%|█▍        | 14.1M/97.8M [00:00<00:04, 19.9MB/s] 17%|█▋        | 16.8M/97.8M [00:00<00:04, 18.2MB/s] 20%|█▉        | 19.3M/97.8M [00:00<00:03, 24.5MB/s] 23%|██▎       | 22.3M/97.8M [00:00<00:03, 22.9MB/s] 25%|██▍       | 24.2M/97.8M [00:00<00:02, 29.1MB/s] 27%|██▋       | 26.6M/97.8M [00:00<00:02, 26.9MB/s] 29%|██▉       | 28.4M/97.8M [00:00<00:02, 32.4MB/s] 31%|███▏      | 30.8M/97.8M [00:00<00:02, 29.5MB/s] 33%|███▎      | 32.5M/97.8M [00:00<00:01, 34.4MB/s] 37%|███▋      | 36.0M/97.8M [00:00<00:01, 34.2MB/s] 39%|███▊      | 37.6M/97.8M [00:00<00:01, 38.0MB/s] 41%|████▏     | 40.4M/97.8M [00:00<00:01, 33.2MB/s] 43%|████▎     | 41.9M/97.8M [00:01<00:01, 33.6MB/s] 45%|████▌     | 44.3M/97.8M [00:01<00:01, 31.8MB/s] 47%|████▋     | 45.6M/97.8M [00:01<00:01, 33.7MB/s] 50%|████▉     | 48.8M/97.8M [00:01<00:01, 35.2MB/s] 52%|█████▏    | 50.5M/97.8M [00:01<00:01, 37.5MB/s] 55%|█████▍    | 53.5M/97.8M [00:01<00:01, 38.5MB/s] 56%|█████▌    | 54.5M/97.8M [00:01<00:01, 38.4MB/s] 59%|█████▉    | 57.6M/97.8M [00:01<00:01, 39.0MB/s] 60%|██████    | 58.9M/97.8M [00:01<00:01, 40.3MB/s] 63%|██████▎   | 61.6M/97.8M [00:01<00:01, 37.4MB/s] 64%|██████▍   | 62.9M/97.8M [00:01<00:00, 38.7MB/s] 68%|██████▊   | 66.7M/97.8M [00:01<00:00, 40.9MB/s] 69%|██████▉   | 67.8M/97.8M [00:01<00:00, 41.6MB/s] 72%|███████▏  | 70.8M/97.8M [00:01<00:00, 41.1MB/s] 74%|███████▎  | 71.9M/97.8M [00:01<00:00, 36.8MB/s] 77%|███████▋  | 74.9M/97.8M [00:01<00:00, 30.6MB/s] 77%|███████▋  | 75.6M/97.8M [00:02<00:00, 29.2MB/s] 80%|████████  | 78.3M/97.8M [00:02<00:00, 30.8MB/s] 82%|████████▏ | 80.1M/97.8M [00:02<00:00, 32.9MB/s] 85%|████████▌ | 83.3M/97.8M [00:02<00:00, 35.1MB/s] 87%|████████▋ | 85.0M/97.8M [00:02<00:00, 36.8MB/s] 90%|████████▉ | 87.6M/97.8M [00:02<00:00, 37.7MB/s] 91%|█████████ | 88.9M/97.8M [00:02<00:00, 36.1MB/s] 94%|█████████▎| 91.6M/97.8M [00:02<00:00, 34.7MB/s] 95%|█████████▍| 92.6M/97.8M [00:02<00:00, 34.9MB/s] 97%|█████████▋| 95.2M/97.8M [00:02<00:00, 33.0MB/s] 98%|█████████▊| 96.2M/97.8M [00:02<00:00, 32.3MB/s]100%|██████████| 97.8M/97.8M [00:02<00:00, 38.2MB/s]
100%|██████████| 97.8M/97.8M [00:02<00:00, 38.6MB/s]
DLL 2021-06-26 17:40:47.466446 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 88  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-06-26 17:40:47.544049 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 88  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 2024
Using seed = 7474
loading annotations into memory...
loading annotations into memory...
Done (t=0.36s)
creating index...
Done (t=0.37s)
creating index...
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
DLL 2021-06-26 17:47:09.266801 - () avg_img/sec : 124.46502025780855  med_img/sec : 124.48805825026542  min_img/sec : 123.66057720302348  max_img/sec : 125.12000238647465 
Done benchmarking. Total images: 35200	total time: 282.810	Average images/sec: 124.465	Median images/sec: 124.488
DLL 2021-06-26 17:47:09.266988 - () avg_img/sec : 124.46260999970418  med_img/sec : 124.48963277120913  min_img/sec : 123.64048668490295  max_img/sec : 125.04306801369889 
Done benchmarking. Total images: 35200	total time: 282.816	Average images/sec: 124.463	Median images/sec: 124.490
Training performance = 248.97769165039062 FPS
DLL 2021-06-26 17:47:09.267495 - (0,) time : 358.2052299976349 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-26 17:47:09.267631 - () total time : 358.20509243011475 
DLL 2021-06-26 17:47:09.267662 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-26 17:47:09.267746 - () total time : 358.2052299976349 
DLL 2021-06-26 17:47:09.267765 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
