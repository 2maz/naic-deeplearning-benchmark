:::NVLOGv0.2.2 Tacotron2_PyT 1593678052.141125917 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593678052.168584347 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593678052.187019825 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593678055.198472023 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593678055.206409454 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 36, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593678057.615144730 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593678077.248309135 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593678077.260115147 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678079.590832710 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678091.302809238 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.970909118652344
:::NVLOGv0.2.2 Tacotron2_PyT 1593678092.947529793 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678092.948132753 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 12237.964481656314
:::NVLOGv0.2.2 Tacotron2_PyT 1593678092.948645115 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 13.359084367752075
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678092.958041668 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678094.278613329 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.486202239990234
:::NVLOGv0.2.2 Tacotron2_PyT 1593678096.052070856 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678096.054247856 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 52752.11262659108
:::NVLOGv0.2.2 Tacotron2_PyT 1593678096.055802822 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.0956106185913086
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678096.062089443 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593678097.028874636 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.054969787597656
:::NVLOGv0.2.2 Tacotron2_PyT 1593678098.959262371 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593678098.962200880 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56220.56999822547
:::NVLOGv0.2.2 Tacotron2_PyT 1593678098.964427948 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8980495929718018
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678098.970364571 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593678100.131246805 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.69939422607422
:::NVLOGv0.2.2 Tacotron2_PyT 1593678101.979737520 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593678101.983190060 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 54058.942415445665
:::NVLOGv0.2.2 Tacotron2_PyT 1593678101.985793591 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.0101587772369385
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593678102.097705126 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678102.099001646 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 26267.588379927744
:::NVLOGv0.2.2 Tacotron2_PyT 1593678102.099853277 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 43817.39738047963
:::NVLOGv0.2.2 Tacotron2_PyT 1593678102.100386620 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.80286884307861
:::NVLOGv0.2.2 Tacotron2_PyT 1593678102.100892782 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 24.838366985321045
:::NVLOGv0.2.2 Tacotron2_PyT 1593678102.101404667 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593678103.397808790 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.11589431762695
:::NVLOGv0.2.2 Tacotron2_PyT 1593678103.401499510 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678104.079231501 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678105.070322037 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678106.207189322 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.20756149291992
:::NVLOGv0.2.2 Tacotron2_PyT 1593678108.039820433 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593678108.041843891 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 53675.4555519309
:::NVLOGv0.2.2 Tacotron2_PyT 1593678108.043845654 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.9723827838897705
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678108.052816868 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678109.135147810 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.079429626464844
:::NVLOGv0.2.2 Tacotron2_PyT 1593678111.037203550 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678111.040005684 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 53832.52419049712
:::NVLOGv0.2.2 Tacotron2_PyT 1593678111.043491602 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.9856114387512207
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678111.053253889 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593678112.026693106 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.49160385131836
:::NVLOGv0.2.2 Tacotron2_PyT 1593678113.726432323 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593678113.729698420 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 62361.506178043936
:::NVLOGv0.2.2 Tacotron2_PyT 1593678113.732421398 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6739892959594727
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678113.740615129 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593678114.727194309 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.908687591552734
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.531549454 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.533351421 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59310.532838081854
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.535444498 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.791595220565796
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.658747196 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.661286592 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 51873.92266339768
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.662732363 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 57295.00468963845
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.664069891 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.671820640563965
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.665445805 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 12.580348014831543
:::NVLOGv0.2.2 Tacotron2_PyT 1593678116.666005611 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678118.114915609 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.144866943359375
:::NVLOGv0.2.2 Tacotron2_PyT 1593678118.118600607 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593678118.121639729 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 60.50559115409851
:::NVLOGv0.2.2 Tacotron2_PyT 1593678118.122038841 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 60.50559115409851
:::NVLOGv0.2.2 Tacotron2_PyT 1593678118.122467279 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 66.07549166679382
:::NVLOGv0.2.2 Tacotron2_PyT 1593678118.122807980 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
