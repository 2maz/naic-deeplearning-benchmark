:::NVLOGv0.2.2 Tacotron2_PyT 1593677618.050386190 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593677618.067901134 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593677618.087269783 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593677621.485983133 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593677621.494312763 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 72, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593677624.446644306 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1593677646.132411480 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593677646.133636713 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/2 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677647.945909262 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677659.449840784 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.99517059326172
:::NVLOGv0.2.2 Tacotron2_PyT 1593677661.454592466 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677661.455275774 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24186.60047975521
:::NVLOGv0.2.2 Tacotron2_PyT 1593677661.455792427 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 13.511117458343506
Batch: 1/2 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677661.463893652 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677662.897350550 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.24046325683594
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.223114252 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.224970102 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56537.2707542842
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.226362944 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.760023355484009
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.317373514 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.317794561 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 30798.12109890296
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.318151236 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 40361.935617019706
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.318506002 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.11781692504883
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.318855524 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 21.184539079666138
:::NVLOGv0.2.2 Tacotron2_PyT 1593677667.319201946 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593677669.280254602 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.034095764160156
:::NVLOGv0.2.2 Tacotron2_PyT 1593677669.281667471 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677669.571702719 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/2 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677671.335645437 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677676.899621964 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.6975212097168
:::NVLOGv0.2.2 Tacotron2_PyT 1593677679.720293522 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593677679.724205494 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 38190.270315586546
:::NVLOGv0.2.2 Tacotron2_PyT 1593677679.725635290 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.386088848114014
Batch: 1/2 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677679.734293222 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677684.331780195 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.881893157958984
:::NVLOGv0.2.2 Tacotron2_PyT 1593677687.977229357 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677687.979020596 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 40313.537284565245
:::NVLOGv0.2.2 Tacotron2_PyT 1593677687.982236385 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.24350881576538
:::NVLOGv0.2.2 Tacotron2_PyT 1593677688.130227566 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677688.132437229 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 35162.79290684552
:::NVLOGv0.2.2 Tacotron2_PyT 1593677688.134233236 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 39251.903800075896
:::NVLOGv0.2.2 Tacotron2_PyT 1593677688.134911537 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.28970718383789
:::NVLOGv0.2.2 Tacotron2_PyT 1593677688.135547400 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 18.559162855148315
:::NVLOGv0.2.2 Tacotron2_PyT 1593677688.136173725 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677690.099312305 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.06473922729492
:::NVLOGv0.2.2 Tacotron2_PyT 1593677690.103386879 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593677690.107046127 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 65.65882444381714
:::NVLOGv0.2.2 Tacotron2_PyT 1593677690.108428478 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 65.65882444381714
:::NVLOGv0.2.2 Tacotron2_PyT 1593677690.109764338 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 72.15332770347595
:::NVLOGv0.2.2 Tacotron2_PyT 1593677690.110290766 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
