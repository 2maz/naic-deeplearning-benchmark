0: thread affinity: {0, 32, 64, 8, 40, 72, 16, 48, 80, 24, 56}
0: Collecting environment information...
0: PyTorch version: 1.13.0a0+d0d6b1f
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.2
Libc version: glibc-2.31

Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-52-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB
GPU 4: Tesla V100-SXM2-16GB
GPU 5: Tesla V100-SXM2-16GB
GPU 6: Tesla V100-SXM2-16GB
GPU 7: Tesla V100-SXM2-16GB

Nvidia driver version: 515.65.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] functorch==0.3.0a0
[pip3] numpy==1.22.2
[pip3] pytorch-quantization==2.1.2
[pip3] torch==1.13.0a0+d0d6b1f
[pip3] torch-tensorrt==1.3.0a0
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.14.0a0
[conda] functorch                 0.3.0a0                  pypi_0    pypi
[conda] mkl                       2020.4             h726a3e6_304    conda-forge
[conda] mkl-include               2020.4             h726a3e6_304    conda-forge
[conda] numpy                     1.22.2           py38h6ae9a64_0    conda-forge
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] torch                     1.13.0a0+d0d6b1f          pypi_0    pypi
[conda] torch-tensorrt            1.3.0a0                  pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.14.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=1, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=8, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.002
    maximize: False
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 768
0: Scheduler decay interval: 96
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/1154]	Time 1.196 (0.000)	Data 8.08e-01 (0.00e+00)	Tok/s 6725 (0)	Loss/tok 10.7009 (10.7009)	LR 2.047e-05
0: TRAIN [0][10/1154]	Time 0.270 (0.311)	Data 2.06e-04 (1.88e-04)	Tok/s 20941 (25227)	Loss/tok 9.7171 (10.1369)	LR 2.576e-05
0: TRAIN [0][20/1154]	Time 0.270 (0.283)	Data 2.18e-04 (1.83e-04)	Tok/s 21081 (25118)	Loss/tok 9.2505 (9.8305)	LR 3.244e-05
0: TRAIN [0][30/1154]	Time 0.364 (0.276)	Data 1.27e-04 (2.01e-04)	Tok/s 15463 (24175)	Loss/tok 9.0056 (9.6043)	LR 4.083e-05
0: TRAIN [0][40/1154]	Time 0.267 (0.280)	Data 3.19e-04 (2.27e-04)	Tok/s 21738 (23993)	Loss/tok 8.6616 (9.4169)	LR 5.141e-05
0: TRAIN [0][50/1154]	Time 0.273 (0.290)	Data 3.28e-04 (2.39e-04)	Tok/s 21087 (24100)	Loss/tok 8.5124 (9.2590)	LR 6.472e-05
0: TRAIN [0][60/1154]	Time 0.364 (0.288)	Data 2.89e-04 (2.40e-04)	Tok/s 22232 (24121)	Loss/tok 8.4415 (9.1283)	LR 8.148e-05
0: TRAIN [0][70/1154]	Time 0.372 (0.292)	Data 1.47e-04 (2.49e-04)	Tok/s 15481 (24223)	Loss/tok 8.1036 (8.9903)	LR 1.026e-04
0: TRAIN [0][80/1154]	Time 0.274 (0.296)	Data 1.24e-04 (2.52e-04)	Tok/s 21157 (24048)	Loss/tok 7.9361 (8.8700)	LR 1.291e-04
0: TRAIN [0][90/1154]	Time 0.279 (0.294)	Data 2.80e-04 (2.50e-04)	Tok/s 20685 (23835)	Loss/tok 8.4775 (8.7916)	LR 1.626e-04
0: TRAIN [0][100/1154]	Time 0.516 (0.297)	Data 1.94e-04 (2.56e-04)	Tok/s 20039 (23672)	Loss/tok 8.4194 (8.7162)	LR 2.047e-04
0: TRAIN [0][110/1154]	Time 0.048 (0.295)	Data 4.11e-04 (2.62e-04)	Tok/s 36405 (23790)	Loss/tok 7.1274 (8.6479)	LR 2.576e-04
0: TRAIN [0][120/1154]	Time 0.466 (0.300)	Data 1.79e-04 (2.62e-04)	Tok/s 22742 (23595)	Loss/tok 7.8928 (8.5674)	LR 3.244e-04
0: TRAIN [0][130/1154]	Time 0.365 (0.298)	Data 1.16e-04 (2.55e-04)	Tok/s 16019 (23618)	Loss/tok 7.6274 (8.5118)	LR 4.083e-04
0: TRAIN [0][140/1154]	Time 0.085 (0.298)	Data 1.59e-04 (2.51e-04)	Tok/s 42296 (23726)	Loss/tok 7.5326 (8.4592)	LR 5.141e-04
0: TRAIN [0][150/1154]	Time 0.465 (0.302)	Data 1.58e-04 (2.50e-04)	Tok/s 22390 (23777)	Loss/tok 7.9076 (8.4078)	LR 6.472e-04
0: TRAIN [0][160/1154]	Time 0.099 (0.302)	Data 1.65e-04 (2.50e-04)	Tok/s 35134 (23790)	Loss/tok 7.5110 (8.3650)	LR 8.148e-04
0: TRAIN [0][170/1154]	Time 0.480 (0.303)	Data 1.24e-04 (2.45e-04)	Tok/s 21756 (23695)	Loss/tok 7.8598 (8.3337)	LR 1.026e-03
0: TRAIN [0][180/1154]	Time 0.271 (0.303)	Data 1.65e-04 (2.41e-04)	Tok/s 20885 (23710)	Loss/tok 7.5516 (8.2963)	LR 1.291e-03
0: TRAIN [0][190/1154]	Time 0.272 (0.301)	Data 1.19e-04 (2.38e-04)	Tok/s 20860 (23718)	Loss/tok 7.4395 (8.2631)	LR 1.626e-03
0: TRAIN [0][200/1154]	Time 0.269 (0.305)	Data 1.75e-04 (2.38e-04)	Tok/s 21597 (23631)	Loss/tok 7.4928 (8.2239)	LR 2.000e-03
0: TRAIN [0][210/1154]	Time 0.369 (0.306)	Data 1.63e-04 (2.37e-04)	Tok/s 15929 (23737)	Loss/tok 7.3805 (8.1895)	LR 2.000e-03
0: TRAIN [0][220/1154]	Time 0.087 (0.304)	Data 1.32e-04 (2.36e-04)	Tok/s 41467 (23843)	Loss/tok 7.0176 (8.1533)	LR 2.000e-03
0: TRAIN [0][230/1154]	Time 0.363 (0.301)	Data 1.36e-04 (2.35e-04)	Tok/s 15841 (23750)	Loss/tok 7.2219 (8.1253)	LR 2.000e-03
0: TRAIN [0][240/1154]	Time 0.359 (0.299)	Data 3.92e-04 (2.35e-04)	Tok/s 15942 (23747)	Loss/tok 7.1009 (8.0913)	LR 2.000e-03
0: TRAIN [0][250/1154]	Time 0.089 (0.296)	Data 1.66e-04 (2.34e-04)	Tok/s 38340 (23869)	Loss/tok 6.6955 (8.0605)	LR 2.000e-03
0: TRAIN [0][260/1154]	Time 0.189 (0.297)	Data 1.68e-04 (2.37e-04)	Tok/s 18968 (23947)	Loss/tok 6.7710 (8.0261)	LR 2.000e-03
0: TRAIN [0][270/1154]	Time 0.085 (0.300)	Data 2.68e-04 (2.38e-04)	Tok/s 41606 (23942)	Loss/tok 6.6798 (7.9862)	LR 2.000e-03
0: TRAIN [0][280/1154]	Time 0.380 (0.302)	Data 4.65e-03 (2.69e-04)	Tok/s 21307 (23901)	Loss/tok 7.0348 (7.9499)	LR 2.000e-03
0: TRAIN [0][290/1154]	Time 0.359 (0.303)	Data 3.48e-04 (2.72e-04)	Tok/s 22294 (23825)	Loss/tok 6.9377 (7.9099)	LR 2.000e-03
0: TRAIN [0][300/1154]	Time 0.472 (0.303)	Data 1.68e-04 (2.71e-04)	Tok/s 16817 (23823)	Loss/tok 6.9919 (7.8750)	LR 2.000e-03
0: TRAIN [0][310/1154]	Time 0.262 (0.303)	Data 3.40e-04 (2.70e-04)	Tok/s 22418 (23931)	Loss/tok 6.7161 (7.8415)	LR 2.000e-03
0: TRAIN [0][320/1154]	Time 0.269 (0.301)	Data 3.59e-04 (2.68e-04)	Tok/s 21457 (23908)	Loss/tok 6.6128 (7.8125)	LR 2.000e-03
0: TRAIN [0][330/1154]	Time 0.468 (0.301)	Data 1.97e-04 (2.68e-04)	Tok/s 17290 (23875)	Loss/tok 6.8926 (7.7793)	LR 2.000e-03
0: TRAIN [0][340/1154]	Time 0.365 (0.300)	Data 3.34e-04 (2.68e-04)	Tok/s 22222 (23877)	Loss/tok 6.7052 (7.7479)	LR 2.000e-03
0: TRAIN [0][350/1154]	Time 0.366 (0.300)	Data 3.72e-04 (2.67e-04)	Tok/s 21907 (23900)	Loss/tok 6.6479 (7.7153)	LR 2.000e-03
0: TRAIN [0][360/1154]	Time 0.379 (0.299)	Data 1.63e-04 (2.67e-04)	Tok/s 14787 (23893)	Loss/tok 6.4639 (7.6855)	LR 2.000e-03
0: TRAIN [0][370/1154]	Time 0.100 (0.298)	Data 2.30e-04 (2.66e-04)	Tok/s 34683 (23875)	Loss/tok 6.0309 (7.6550)	LR 2.000e-03
0: TRAIN [0][380/1154]	Time 0.380 (0.299)	Data 1.30e-04 (2.66e-04)	Tok/s 15186 (23837)	Loss/tok 6.2947 (7.6216)	LR 2.000e-03
0: TRAIN [0][390/1154]	Time 0.462 (0.298)	Data 1.52e-04 (2.67e-04)	Tok/s 17330 (23869)	Loss/tok 6.4701 (7.5926)	LR 2.000e-03
0: TRAIN [0][400/1154]	Time 0.089 (0.298)	Data 3.63e-04 (2.67e-04)	Tok/s 37963 (23896)	Loss/tok 6.0398 (7.5641)	LR 2.000e-03
0: TRAIN [0][410/1154]	Time 0.468 (0.299)	Data 4.12e-04 (2.68e-04)	Tok/s 22444 (23829)	Loss/tok 6.5431 (7.5302)	LR 2.000e-03
0: TRAIN [0][420/1154]	Time 0.282 (0.299)	Data 1.40e-04 (2.68e-04)	Tok/s 20873 (23850)	Loss/tok 6.1421 (7.5002)	LR 2.000e-03
0: TRAIN [0][430/1154]	Time 0.362 (0.299)	Data 1.63e-04 (2.69e-04)	Tok/s 22341 (23832)	Loss/tok 6.3071 (7.4696)	LR 2.000e-03
0: TRAIN [0][440/1154]	Time 0.367 (0.299)	Data 3.39e-04 (2.68e-04)	Tok/s 22289 (23879)	Loss/tok 6.1631 (7.4402)	LR 2.000e-03
0: TRAIN [0][450/1154]	Time 0.200 (0.298)	Data 1.17e-04 (2.68e-04)	Tok/s 16952 (23936)	Loss/tok 5.6848 (7.4132)	LR 2.000e-03
0: TRAIN [0][460/1154]	Time 0.149 (0.298)	Data 1.70e-04 (2.69e-04)	Tok/s 22122 (23929)	Loss/tok 5.6955 (7.3871)	LR 2.000e-03
0: TRAIN [0][470/1154]	Time 0.474 (0.300)	Data 2.21e-04 (2.69e-04)	Tok/s 22413 (23903)	Loss/tok 6.2522 (7.3534)	LR 2.000e-03
0: TRAIN [0][480/1154]	Time 0.471 (0.300)	Data 3.46e-04 (2.70e-04)	Tok/s 22308 (23885)	Loss/tok 6.2188 (7.3234)	LR 2.000e-03
0: TRAIN [0][490/1154]	Time 0.081 (0.300)	Data 3.47e-04 (2.70e-04)	Tok/s 42554 (23959)	Loss/tok 5.4196 (7.2993)	LR 2.000e-03
0: TRAIN [0][500/1154]	Time 0.476 (0.300)	Data 3.40e-04 (2.69e-04)	Tok/s 21857 (23996)	Loss/tok 6.1410 (7.2724)	LR 2.000e-03
0: TRAIN [0][510/1154]	Time 0.563 (0.300)	Data 1.23e-04 (2.68e-04)	Tok/s 18459 (24007)	Loss/tok 6.3288 (7.2468)	LR 2.000e-03
0: TRAIN [0][520/1154]	Time 0.086 (0.299)	Data 3.81e-04 (2.68e-04)	Tok/s 40451 (24032)	Loss/tok 5.5236 (7.2225)	LR 2.000e-03
0: TRAIN [0][530/1154]	Time 0.377 (0.300)	Data 1.56e-04 (2.66e-04)	Tok/s 21611 (23972)	Loss/tok 5.7684 (7.1943)	LR 2.000e-03
0: TRAIN [0][540/1154]	Time 0.357 (0.300)	Data 3.60e-04 (2.65e-04)	Tok/s 22583 (23947)	Loss/tok 5.9264 (7.1683)	LR 2.000e-03
0: TRAIN [0][550/1154]	Time 0.370 (0.300)	Data 1.66e-04 (2.64e-04)	Tok/s 15989 (23926)	Loss/tok 5.5941 (7.1414)	LR 2.000e-03
0: TRAIN [0][560/1154]	Time 0.370 (0.299)	Data 1.60e-04 (2.63e-04)	Tok/s 15809 (23945)	Loss/tok 5.7410 (7.1185)	LR 2.000e-03
0: TRAIN [0][570/1154]	Time 0.056 (0.299)	Data 1.52e-04 (2.61e-04)	Tok/s 31750 (24017)	Loss/tok 4.7235 (7.0962)	LR 2.000e-03
0: TRAIN [0][580/1154]	Time 0.267 (0.298)	Data 3.33e-04 (2.60e-04)	Tok/s 21607 (23990)	Loss/tok 5.5345 (7.0731)	LR 2.000e-03
0: TRAIN [0][590/1154]	Time 0.078 (0.298)	Data 3.82e-04 (2.60e-04)	Tok/s 44445 (24007)	Loss/tok 5.1387 (7.0482)	LR 2.000e-03
0: TRAIN [0][600/1154]	Time 0.276 (0.298)	Data 3.22e-04 (2.60e-04)	Tok/s 20867 (23971)	Loss/tok 5.5107 (7.0269)	LR 2.000e-03
0: TRAIN [0][610/1154]	Time 0.367 (0.298)	Data 3.31e-04 (2.60e-04)	Tok/s 21757 (23931)	Loss/tok 5.6235 (7.0021)	LR 2.000e-03
0: TRAIN [0][620/1154]	Time 0.206 (0.297)	Data 1.49e-04 (2.60e-04)	Tok/s 16751 (23950)	Loss/tok 5.0510 (6.9827)	LR 2.000e-03
0: TRAIN [0][630/1154]	Time 0.206 (0.297)	Data 1.53e-04 (2.61e-04)	Tok/s 16969 (23960)	Loss/tok 5.1596 (6.9600)	LR 2.000e-03
0: TRAIN [0][640/1154]	Time 0.462 (0.297)	Data 1.32e-04 (2.60e-04)	Tok/s 17256 (23958)	Loss/tok 5.3839 (6.9361)	LR 2.000e-03
0: TRAIN [0][650/1154]	Time 0.368 (0.297)	Data 1.93e-04 (2.59e-04)	Tok/s 15703 (23934)	Loss/tok 5.3031 (6.9128)	LR 2.000e-03
0: TRAIN [0][660/1154]	Time 0.369 (0.296)	Data 1.61e-04 (2.58e-04)	Tok/s 15522 (23946)	Loss/tok 5.1788 (6.8912)	LR 2.000e-03
0: TRAIN [0][670/1154]	Time 0.483 (0.296)	Data 4.00e-04 (2.59e-04)	Tok/s 21743 (23965)	Loss/tok 5.6763 (6.8698)	LR 2.000e-03
0: TRAIN [0][680/1154]	Time 0.481 (0.296)	Data 1.59e-04 (2.58e-04)	Tok/s 21404 (23971)	Loss/tok 5.6198 (6.8469)	LR 2.000e-03
0: TRAIN [0][690/1154]	Time 0.373 (0.296)	Data 1.52e-04 (2.58e-04)	Tok/s 21931 (23946)	Loss/tok 5.4174 (6.8241)	LR 2.000e-03
0: TRAIN [0][700/1154]	Time 0.089 (0.295)	Data 1.48e-04 (2.57e-04)	Tok/s 39164 (23995)	Loss/tok 4.8349 (6.8041)	LR 2.000e-03
0: TRAIN [0][710/1154]	Time 0.355 (0.296)	Data 3.68e-04 (2.57e-04)	Tok/s 22947 (23994)	Loss/tok 5.2779 (6.7809)	LR 2.000e-03
0: TRAIN [0][720/1154]	Time 0.107 (0.295)	Data 1.25e-04 (2.57e-04)	Tok/s 16285 (24000)	Loss/tok 4.6047 (6.7624)	LR 2.000e-03
0: TRAIN [0][730/1154]	Time 0.270 (0.295)	Data 3.30e-04 (2.56e-04)	Tok/s 21178 (23991)	Loss/tok 4.9729 (6.7410)	LR 2.000e-03
0: TRAIN [0][740/1154]	Time 0.276 (0.295)	Data 3.53e-04 (2.56e-04)	Tok/s 20765 (23964)	Loss/tok 5.0923 (6.7208)	LR 2.000e-03
0: TRAIN [0][750/1154]	Time 0.370 (0.295)	Data 3.60e-04 (2.56e-04)	Tok/s 21768 (23937)	Loss/tok 5.3324 (6.7020)	LR 2.000e-03
0: TRAIN [0][760/1154]	Time 0.090 (0.295)	Data 1.98e-04 (2.56e-04)	Tok/s 40096 (23946)	Loss/tok 4.6588 (6.6814)	LR 2.000e-03
0: TRAIN [0][770/1154]	Time 0.565 (0.295)	Data 1.73e-04 (2.56e-04)	Tok/s 18591 (23943)	Loss/tok 5.4835 (6.6605)	LR 1.000e-03
0: TRAIN [0][780/1154]	Time 0.082 (0.295)	Data 3.29e-04 (2.56e-04)	Tok/s 43427 (23947)	Loss/tok 4.6327 (6.6404)	LR 1.000e-03
0: TRAIN [0][790/1154]	Time 0.268 (0.295)	Data 3.59e-04 (2.56e-04)	Tok/s 21739 (23933)	Loss/tok 4.8977 (6.6216)	LR 1.000e-03
0: TRAIN [0][800/1154]	Time 0.287 (0.294)	Data 1.51e-04 (2.55e-04)	Tok/s 20362 (23959)	Loss/tok 4.7326 (6.6017)	LR 1.000e-03
0: TRAIN [0][810/1154]	Time 0.362 (0.294)	Data 2.17e-04 (2.55e-04)	Tok/s 15907 (23953)	Loss/tok 4.9021 (6.5835)	LR 1.000e-03
0: TRAIN [0][820/1154]	Time 0.481 (0.294)	Data 1.44e-04 (2.54e-04)	Tok/s 21898 (23961)	Loss/tok 5.2083 (6.5622)	LR 1.000e-03
0: TRAIN [0][830/1154]	Time 0.271 (0.294)	Data 1.37e-04 (2.54e-04)	Tok/s 21369 (23971)	Loss/tok 4.7176 (6.5416)	LR 1.000e-03
0: TRAIN [0][840/1154]	Time 0.366 (0.295)	Data 1.74e-04 (2.54e-04)	Tok/s 21987 (23977)	Loss/tok 4.9004 (6.5213)	LR 1.000e-03
0: TRAIN [0][850/1154]	Time 0.099 (0.294)	Data 4.76e-04 (2.54e-04)	Tok/s 34427 (24006)	Loss/tok 4.4123 (6.5032)	LR 1.000e-03
0: TRAIN [0][860/1154]	Time 0.278 (0.294)	Data 5.58e-04 (2.55e-04)	Tok/s 21266 (23974)	Loss/tok 4.7557 (6.4848)	LR 1.000e-03
0: TRAIN [0][870/1154]	Time 0.478 (0.293)	Data 1.56e-04 (2.56e-04)	Tok/s 16898 (23984)	Loss/tok 4.9240 (6.4684)	LR 5.000e-04
0: TRAIN [0][880/1154]	Time 0.088 (0.293)	Data 3.30e-04 (2.56e-04)	Tok/s 40130 (24017)	Loss/tok 4.2979 (6.4503)	LR 5.000e-04
0: TRAIN [0][890/1154]	Time 0.077 (0.293)	Data 3.55e-04 (2.56e-04)	Tok/s 44881 (24031)	Loss/tok 4.3514 (6.4328)	LR 5.000e-04
0: TRAIN [0][900/1154]	Time 0.054 (0.293)	Data 4.08e-04 (2.56e-04)	Tok/s 32018 (24038)	Loss/tok 3.9670 (6.4153)	LR 5.000e-04
0: TRAIN [0][910/1154]	Time 0.368 (0.293)	Data 2.12e-04 (2.56e-04)	Tok/s 22006 (24019)	Loss/tok 4.7788 (6.3955)	LR 5.000e-04
0: TRAIN [0][920/1154]	Time 0.468 (0.293)	Data 3.66e-04 (2.57e-04)	Tok/s 22454 (24048)	Loss/tok 5.0050 (6.3790)	LR 5.000e-04
0: TRAIN [0][930/1154]	Time 0.087 (0.293)	Data 2.61e-04 (2.57e-04)	Tok/s 40064 (24079)	Loss/tok 4.4098 (6.3603)	LR 5.000e-04
0: TRAIN [0][940/1154]	Time 0.577 (0.293)	Data 1.52e-04 (2.57e-04)	Tok/s 18041 (24072)	Loss/tok 5.0461 (6.3441)	LR 5.000e-04
0: TRAIN [0][950/1154]	Time 0.466 (0.293)	Data 1.72e-04 (2.57e-04)	Tok/s 17129 (24082)	Loss/tok 4.7741 (6.3257)	LR 5.000e-04
0: TRAIN [0][960/1154]	Time 0.367 (0.294)	Data 3.20e-04 (2.57e-04)	Tok/s 22099 (24071)	Loss/tok 4.8010 (6.3064)	LR 2.500e-04
0: TRAIN [0][970/1154]	Time 0.466 (0.293)	Data 3.91e-04 (2.57e-04)	Tok/s 22205 (24091)	Loss/tok 4.9154 (6.2922)	LR 2.500e-04
0: TRAIN [0][980/1154]	Time 0.474 (0.294)	Data 3.31e-04 (2.58e-04)	Tok/s 22361 (24084)	Loss/tok 4.9614 (6.2721)	LR 2.500e-04
0: TRAIN [0][990/1154]	Time 0.464 (0.295)	Data 3.69e-04 (2.58e-04)	Tok/s 22432 (24085)	Loss/tok 4.9824 (6.2534)	LR 2.500e-04
0: TRAIN [0][1000/1154]	Time 0.090 (0.294)	Data 3.29e-04 (2.58e-04)	Tok/s 39257 (24116)	Loss/tok 4.1801 (6.2393)	LR 2.500e-04
0: TRAIN [0][1010/1154]	Time 0.182 (0.294)	Data 2.06e-04 (2.58e-04)	Tok/s 19601 (24116)	Loss/tok 4.1225 (6.2255)	LR 2.500e-04
0: TRAIN [0][1020/1154]	Time 0.560 (0.294)	Data 3.05e-04 (2.58e-04)	Tok/s 18399 (24104)	Loss/tok 4.9786 (6.2103)	LR 2.500e-04
0: TRAIN [0][1030/1154]	Time 0.368 (0.294)	Data 2.29e-04 (2.58e-04)	Tok/s 21733 (24099)	Loss/tok 4.6000 (6.1952)	LR 2.500e-04
0: TRAIN [0][1040/1154]	Time 0.197 (0.294)	Data 1.29e-04 (2.58e-04)	Tok/s 18056 (24109)	Loss/tok 4.3544 (6.1814)	LR 2.500e-04
0: TRAIN [0][1050/1154]	Time 0.373 (0.294)	Data 2.78e-04 (2.58e-04)	Tok/s 21591 (24078)	Loss/tok 4.6344 (6.1645)	LR 2.500e-04
0: TRAIN [0][1060/1154]	Time 0.466 (0.294)	Data 1.54e-04 (2.58e-04)	Tok/s 17497 (24095)	Loss/tok 4.7067 (6.1509)	LR 1.250e-04
0: TRAIN [0][1070/1154]	Time 0.473 (0.294)	Data 2.82e-04 (2.58e-04)	Tok/s 22176 (24084)	Loss/tok 4.9361 (6.1352)	LR 1.250e-04
0: TRAIN [0][1080/1154]	Time 0.372 (0.294)	Data 1.55e-04 (2.58e-04)	Tok/s 15716 (24112)	Loss/tok 4.5863 (6.1226)	LR 1.250e-04
0: TRAIN [0][1090/1154]	Time 0.281 (0.294)	Data 4.08e-04 (2.58e-04)	Tok/s 20667 (24101)	Loss/tok 4.4929 (6.1078)	LR 1.250e-04
0: TRAIN [0][1100/1154]	Time 0.054 (0.293)	Data 3.70e-04 (2.58e-04)	Tok/s 31341 (24120)	Loss/tok 3.9090 (6.0965)	LR 1.250e-04
0: TRAIN [0][1110/1154]	Time 0.270 (0.293)	Data 3.45e-04 (2.58e-04)	Tok/s 21296 (24104)	Loss/tok 4.5275 (6.0825)	LR 1.250e-04
0: TRAIN [0][1120/1154]	Time 0.280 (0.293)	Data 3.48e-04 (2.58e-04)	Tok/s 20458 (24082)	Loss/tok 4.4599 (6.0687)	LR 1.250e-04
0: TRAIN [0][1130/1154]	Time 0.469 (0.293)	Data 3.65e-04 (2.59e-04)	Tok/s 22362 (24064)	Loss/tok 4.9058 (6.0564)	LR 1.250e-04
0: TRAIN [0][1140/1154]	Time 0.475 (0.293)	Data 8.39e-05 (2.65e-04)	Tok/s 17108 (24084)	Loss/tok 4.7718 (6.0433)	LR 1.250e-04
0: TRAIN [0][1150/1154]	Time 0.080 (0.293)	Data 2.21e-04 (2.64e-04)	Tok/s 42946 (24107)	Loss/tok 4.2115 (6.0303)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/80]	Time 0.202 (0.000)	Data 2.45e-03 (0.00e+00)	Tok/s 51914 (0)	Loss/tok 6.1274 (6.1274)
0: VALIDATION [0][10/80]	Time 0.084 (0.103)	Data 1.71e-03 (1.87e-03)	Tok/s 69430 (66708)	Loss/tok 5.7766 (5.8843)
0: VALIDATION [0][20/80]	Time 0.068 (0.090)	Data 1.90e-03 (1.86e-03)	Tok/s 68581 (66809)	Loss/tok 5.4824 (5.8029)
0: VALIDATION [0][30/80]	Time 0.060 (0.081)	Data 1.79e-03 (1.83e-03)	Tok/s 65112 (66873)	Loss/tok 5.3091 (5.7238)
0: VALIDATION [0][40/80]	Time 0.049 (0.074)	Data 1.73e-03 (1.81e-03)	Tok/s 65383 (66402)	Loss/tok 5.4447 (5.6798)
0: VALIDATION [0][50/80]	Time 0.040 (0.068)	Data 1.65e-03 (1.78e-03)	Tok/s 66153 (66148)	Loss/tok 5.1721 (5.6442)
0: VALIDATION [0][60/80]	Time 0.034 (0.063)	Data 1.73e-03 (1.77e-03)	Tok/s 62117 (65803)	Loss/tok 5.3350 (5.6127)
0: VALIDATION [0][70/80]	Time 0.027 (0.058)	Data 1.73e-03 (1.76e-03)	Tok/s 59992 (65152)	Loss/tok 4.9584 (5.5801)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/24]	Time 0.9675 (1.4470)	Decoder iters 149.0 (149.0)	Tok/s 9106 (9134)
0: TEST [0][19/24]	Time 0.5572 (1.0812)	Decoder iters 149.0 (149.0)	Tok/s 7700 (8952)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 6.0267	Validation Loss: 5.5509	Test BLEU: 3.76
0: Performance: Epoch: 0	Training: 24109 Tok/s	Validation: 63973 Tok/s
0: Finished epoch 0
0: Total training time 387 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 128|                      3.76|             24108.6411488294|             6.442175241311391|
/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
DONE!
