:::NVLOGv0.2.2 Tacotron2_PyT 1586642481.801928043 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1586642481.814856291 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 64, "name": "Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1586642481.822559118 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "480G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1586642491.198765516 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.64", "num": 8, "name": ["Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB"], "mem": ["16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1586642491.202368736 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 10, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1586642493.076467991 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1586642530.006589651 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1586642530.018414497 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642530.650747299 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642536.539389849 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002154168440029025
:::NVLOGv0.2.2 Tacotron2_PyT 1586642538.352658749 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642538.353202105 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 83071.75870136009
:::NVLOGv0.2.2 Tacotron2_PyT 1586642538.353747368 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 7.7041826248168945
Batch: 1/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642538.356783867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642539.005231619 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002413904992863536
:::NVLOGv0.2.2 Tacotron2_PyT 1586642539.972826481 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642539.973411083 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 395850.1041107526
:::NVLOGv0.2.2 Tacotron2_PyT 1586642539.973839045 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.6167736053466797
Batch: 2/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642539.976326466 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642540.279629230 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020978618413209915
:::NVLOGv0.2.2 Tacotron2_PyT 1586642541.187235832 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642541.187799215 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 528211.2062010574
:::NVLOGv0.2.2 Tacotron2_PyT 1586642541.188230515 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.2116365432739258
Batch: 3/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642541.191249847 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1586642541.468393326 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00238027679733932
:::NVLOGv0.2.2 Tacotron2_PyT 1586642542.371888638 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1586642542.372381210 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 541774.9504969079
:::NVLOGv0.2.2 Tacotron2_PyT 1586642542.372818232 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.1813023090362549
Batch: 4/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642542.375048876 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1586642542.663295984 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023562395945191383
:::NVLOGv0.2.2 Tacotron2_PyT 1586642543.574064016 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1586642543.574769735 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 533392.2345248756
:::NVLOGv0.2.2 Tacotron2_PyT 1586642543.575216532 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.1998674869537354
Batch: 5/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642543.577725887 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1586642543.844152212 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019915609154850245
:::NVLOGv0.2.2 Tacotron2_PyT 1586642544.751406193 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1586642544.751920938 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 545034.2164807622
:::NVLOGv0.2.2 Tacotron2_PyT 1586642544.752455235 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.1742382049560547
Batch: 6/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642544.754601002 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1586642545.052124262 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022831480018794537
:::NVLOGv0.2.2 Tacotron2_PyT 1586642545.948208570 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1586642545.948697090 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 535941.8032898168
:::NVLOGv0.2.2 Tacotron2_PyT 1586642545.949114323 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.1941595077514648
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586642546.158753872 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642546.159254313 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 277542.399237707
:::NVLOGv0.2.2 Tacotron2_PyT 1586642546.159669876 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 451896.6105436475
:::NVLOGv0.2.2 Tacotron2_PyT 1586642546.160107374 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0022395943690623555
:::NVLOGv0.2.2 Tacotron2_PyT 1586642546.160525322 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 16.141677856445312
:::NVLOGv0.2.2 Tacotron2_PyT 1586642546.160929441 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586642547.416119576 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0025533426087349653
:::NVLOGv0.2.2 Tacotron2_PyT 1586642547.416897297 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642550.479975462 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642550.593337297 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642550.875801325 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00226332969032228
:::NVLOGv0.2.2 Tacotron2_PyT 1586642551.839123249 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642551.839614153 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 513219.9899166739
:::NVLOGv0.2.2 Tacotron2_PyT 1586642551.840032578 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.2470285892486572
Batch: 1/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642551.842362642 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642552.158346176 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021980900783091784
:::NVLOGv0.2.2 Tacotron2_PyT 1586642553.072602272 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642553.073182583 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 519882.7048535021
:::NVLOGv0.2.2 Tacotron2_PyT 1586642553.073710680 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.2310469150543213
Batch: 2/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642553.076244116 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642553.379886866 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002015047939494252
:::NVLOGv0.2.2 Tacotron2_PyT 1586642554.277909994 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642554.278496504 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 532253.400846298
:::NVLOGv0.2.2 Tacotron2_PyT 1586642554.279041290 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.202434778213501
Batch: 3/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642554.281741142 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1586642554.574044466 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024899200070649385
:::NVLOGv0.2.2 Tacotron2_PyT 1586642555.468785048 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1586642555.469309092 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 538791.2718511318
:::NVLOGv0.2.2 Tacotron2_PyT 1586642555.469848394 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.1878440380096436
Batch: 4/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642555.472224951 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1586642555.801609993 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002272868761792779
:::NVLOGv0.2.2 Tacotron2_PyT 1586642556.699089050 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1586642556.699577570 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 521394.3631195232
:::NVLOGv0.2.2 Tacotron2_PyT 1586642556.700093508 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.227477788925171
Batch: 5/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642556.702142477 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1586642556.981324196 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002413195325061679
:::NVLOGv0.2.2 Tacotron2_PyT 1586642557.872090816 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1586642557.872631073 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 546768.849780028
:::NVLOGv0.2.2 Tacotron2_PyT 1586642557.873171568 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.1705129146575928
Batch: 6/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642557.875394106 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1586642558.302973509 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019052473362535238
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.202724934 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.203216076 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 481982.20061529
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.203649521 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.3278498649597168
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.249581814 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.250072241 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 510820.28865254746
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.250547409 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 522041.8258546352
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.250972509 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002222528448328376
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.251388311 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 8.770207643508911
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.251797199 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.903292179 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0018582353368401527
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.904171467 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.905461788 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 66.82797241210938
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.905889750 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 66.82797241210938
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.906422377 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 78.22329831123352
:::NVLOGv0.2.2 Tacotron2_PyT 1586642559.906836510 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
