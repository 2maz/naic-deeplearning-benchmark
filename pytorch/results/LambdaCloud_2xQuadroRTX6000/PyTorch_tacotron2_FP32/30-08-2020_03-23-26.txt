:::NVLOGv0.2.2 Tacotron2_PyT 1598757808.560714245 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1598757808.573711157 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 12, "name": "AMD EPYC Processor (with IBPB)"}
:::NVLOGv0.2.2 Tacotron2_PyT 1598757808.593637943 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "90G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1598757808.867630005 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "450.57", "num": 2, "name": ["Quadro RTX 6000", "Quadro RTX 6000"], "mem": ["24220 MiB", "24220 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1598757808.871035099 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 80, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 2, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1598757809.097599506 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1598758423.134135008 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1598758423.134780407 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/3 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758425.310954571 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758427.428579092 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.796142578125
:::NVLOGv0.2.2 Tacotron2_PyT 1598758431.519285202 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758431.519870996 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 14008.108365143311
:::NVLOGv0.2.2 Tacotron2_PyT 1598758431.520350933 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.21011757850647
Batch: 1/3 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758431.530680180 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758432.718512535 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.954017639160156
:::NVLOGv0.2.2 Tacotron2_PyT 1598758436.996264696 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758436.998054504 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 16867.06709931815
:::NVLOGv0.2.2 Tacotron2_PyT 1598758436.998546600 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.466332674026489
Batch: 2/3 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758437.008826256 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1598758438.170830488 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.566001892089844
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.432637930 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.436547041 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 16806.79347889005
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.440313339 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.424532651901245
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.535114288 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.537558794 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 13935.313101760314
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.538385153 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 15893.989647783836
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.538947105 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.105387369791664
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.539564848 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 19.401214599609375
:::NVLOGv0.2.2 Tacotron2_PyT 1598758442.540121794 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1598758445.031639099 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 45.67717361450195
:::NVLOGv0.2.2 Tacotron2_PyT 1598758445.036308765 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758445.674919367 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/3 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758447.995027542 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758449.230906963 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.09780502319336
:::NVLOGv0.2.2 Tacotron2_PyT 1598758453.540322304 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1598758453.542026758 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 15954.524513947554
:::NVLOGv0.2.2 Tacotron2_PyT 1598758453.546272516 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.546827793121338
Batch: 1/3 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758453.557775497 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758454.758366346 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.15190124511719
:::NVLOGv0.2.2 Tacotron2_PyT 1598758459.100554943 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758459.103613377 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 16425.314452294984
:::NVLOGv0.2.2 Tacotron2_PyT 1598758459.108430862 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.543516397476196
Batch: 2/3 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758459.119524240 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1598758460.313194275 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.29507064819336
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.473564386 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.492561340 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 16974.872787982822
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.495972872 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.354797124862671
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.615705013 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.618115187 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 14277.788517481953
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.620054245 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 16451.57058474179
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.620946169 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 46.84825897216797
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.621503115 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 18.941869020462036
:::NVLOGv0.2.2 Tacotron2_PyT 1598758464.622052431 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758467.148611307 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 45.65196228027344
:::NVLOGv0.2.2 Tacotron2_PyT 1598758467.153312206 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1598758467.155459404 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 658.0567984580994
:::NVLOGv0.2.2 Tacotron2_PyT 1598758467.156401634 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 658.0567984580994
:::NVLOGv0.2.2 Tacotron2_PyT 1598758467.180359125 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 658.7163705825806
:::NVLOGv0.2.2 Tacotron2_PyT 1598758467.180966854 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
