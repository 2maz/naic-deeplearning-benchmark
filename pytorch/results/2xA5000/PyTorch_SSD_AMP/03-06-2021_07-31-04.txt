Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  0%|          | 0.00/97.8M [00:00<?, ?B/s]  2%|▏         | 2.41M/97.8M [00:00<00:03, 25.0MB/s]  2%|▏         | 2.20M/97.8M [00:00<00:04, 22.8MB/s]  5%|▍         | 4.75M/97.8M [00:00<00:03, 24.8MB/s]  6%|▌         | 6.02M/97.8M [00:00<00:03, 26.2MB/s]  7%|▋         | 7.00M/97.8M [00:00<00:03, 24.3MB/s]  9%|▉         | 8.72M/97.8M [00:00<00:03, 26.6MB/s] 10%|▉         | 9.31M/97.8M [00:00<00:03, 24.2MB/s] 11%|█▏        | 11.1M/97.8M [00:00<00:03, 26.0MB/s] 12%|█▏        | 11.2M/97.8M [00:00<00:04, 22.7MB/s] 14%|█▎        | 13.3M/97.8M [00:00<00:03, 24.9MB/s] 14%|█▍        | 13.8M/97.8M [00:00<00:03, 23.9MB/s] 16%|█▌        | 15.4M/97.8M [00:00<00:03, 22.7MB/s] 16%|█▌        | 15.9M/97.8M [00:00<00:03, 22.3MB/s] 18%|█▊        | 17.6M/97.8M [00:00<00:03, 22.6MB/s] 18%|█▊        | 18.1M/97.8M [00:00<00:03, 22.4MB/s] 20%|██        | 20.0M/97.8M [00:00<00:03, 23.4MB/s] 21%|██        | 20.4M/97.8M [00:00<00:03, 22.5MB/s] 23%|██▎       | 22.2M/97.8M [00:00<00:03, 23.2MB/s] 23%|██▎       | 22.6M/97.8M [00:01<00:03, 22.5MB/s] 25%|██▌       | 24.6M/97.8M [00:01<00:03, 23.8MB/s] 26%|██▌       | 25.2M/97.8M [00:01<00:03, 23.8MB/s] 28%|██▊       | 27.1M/97.8M [00:01<00:03, 24.1MB/s] 28%|██▊       | 27.4M/97.8M [00:01<00:03, 23.5MB/s] 30%|███       | 29.4M/97.8M [00:01<00:03, 23.7MB/s] 30%|███       | 29.8M/97.8M [00:01<00:03, 23.7MB/s] 32%|███▏      | 31.6M/97.8M [00:01<00:03, 22.1MB/s] 33%|███▎      | 32.0M/97.8M [00:01<00:03, 22.2MB/s] 35%|███▍      | 34.0M/97.8M [00:01<00:02, 22.9MB/s] 35%|███▌      | 34.4M/97.8M [00:01<00:02, 23.0MB/s] 37%|███▋      | 36.5M/97.8M [00:01<00:02, 23.6MB/s] 38%|███▊      | 36.7M/97.8M [00:01<00:02, 22.5MB/s] 40%|███▉      | 38.7M/97.8M [00:01<00:02, 23.6MB/s] 40%|████      | 39.2M/97.8M [00:01<00:02, 23.3MB/s] 42%|████▏     | 41.0M/97.8M [00:01<00:02, 23.4MB/s] 42%|████▏     | 41.5M/97.8M [00:01<00:02, 23.7MB/s] 44%|████▍     | 43.2M/97.8M [00:01<00:02, 23.3MB/s] 45%|████▍     | 43.8M/97.8M [00:01<00:02, 23.3MB/s] 47%|████▋     | 45.7M/97.8M [00:01<00:02, 23.8MB/s] 47%|████▋     | 46.3M/97.8M [00:02<00:02, 24.1MB/s] 49%|████▉     | 48.0M/97.8M [00:02<00:02, 24.1MB/s] 50%|████▉     | 48.8M/97.8M [00:02<00:02, 24.3MB/s] 52%|█████▏    | 50.3M/97.8M [00:02<00:02, 24.1MB/s] 52%|█████▏    | 51.1M/97.8M [00:02<00:02, 24.2MB/s] 54%|█████▍    | 52.6M/97.8M [00:02<00:01, 24.1MB/s] 55%|█████▍    | 53.4M/97.8M [00:02<00:01, 24.2MB/s] 56%|█████▌    | 55.0M/97.8M [00:02<00:01, 23.3MB/s] 57%|█████▋    | 55.7M/97.8M [00:02<00:01, 23.3MB/s] 59%|█████▊    | 57.2M/97.8M [00:02<00:01, 23.3MB/s] 59%|█████▉    | 58.0M/97.8M [00:02<00:01, 23.1MB/s] 61%|██████    | 59.5M/97.8M [00:02<00:01, 23.2MB/s] 62%|██████▏   | 60.3M/97.8M [00:02<00:01, 23.4MB/s] 63%|██████▎   | 61.9M/97.8M [00:02<00:01, 23.7MB/s] 64%|██████▍   | 62.6M/97.8M [00:02<00:01, 23.5MB/s] 66%|██████▌   | 64.1M/97.8M [00:02<00:01, 23.5MB/s] 66%|██████▋   | 64.9M/97.8M [00:02<00:01, 23.8MB/s] 68%|██████▊   | 66.4M/97.8M [00:02<00:01, 23.4MB/s] 69%|██████▉   | 67.2M/97.8M [00:03<00:01, 23.6MB/s] 70%|███████   | 68.6M/97.8M [00:03<00:01, 22.4MB/s] 71%|███████   | 69.5M/97.8M [00:03<00:01, 22.5MB/s] 72%|███████▏  | 70.8M/97.8M [00:03<00:01, 22.4MB/s] 73%|███████▎  | 71.6M/97.8M [00:03<00:01, 22.3MB/s] 75%|███████▍  | 72.9M/97.8M [00:03<00:01, 22.4MB/s] 75%|███████▌  | 73.8M/97.8M [00:03<00:01, 22.2MB/s] 77%|███████▋  | 75.1M/97.8M [00:03<00:01, 22.2MB/s] 78%|███████▊  | 75.9M/97.8M [00:03<00:01, 22.1MB/s] 79%|███████▉  | 77.2M/97.8M [00:03<00:00, 22.2MB/s] 80%|███████▉  | 78.1M/97.8M [00:03<00:00, 22.1MB/s] 81%|████████  | 79.4M/97.8M [00:03<00:00, 22.4MB/s] 82%|████████▏ | 80.2M/97.8M [00:03<00:00, 22.1MB/s] 83%|████████▎ | 81.5M/97.8M [00:03<00:00, 21.5MB/s] 84%|████████▍ | 82.3M/97.8M [00:03<00:00, 21.0MB/s] 86%|████████▌ | 83.6M/97.8M [00:03<00:00, 20.6MB/s] 86%|████████▋ | 84.3M/97.8M [00:03<00:00, 21.0MB/s] 88%|████████▊ | 86.0M/97.8M [00:03<00:00, 21.8MB/s] 89%|████████▉ | 86.9M/97.8M [00:03<00:00, 22.3MB/s] 90%|█████████ | 88.2M/97.8M [00:03<00:00, 21.8MB/s] 91%|█████████ | 89.0M/97.8M [00:04<00:00, 21.8MB/s] 92%|█████████▏| 90.3M/97.8M [00:04<00:00, 21.2MB/s] 93%|█████████▎| 91.1M/97.8M [00:04<00:00, 21.2MB/s] 95%|█████████▍| 92.6M/97.8M [00:04<00:00, 21.9MB/s] 95%|█████████▌| 93.3M/97.8M [00:04<00:00, 21.6MB/s] 97%|█████████▋| 94.7M/97.8M [00:04<00:00, 21.6MB/s] 98%|█████████▊| 95.4M/97.8M [00:04<00:00, 21.7MB/s] 99%|█████████▉| 96.9M/97.8M [00:04<00:00, 22.1MB/s]100%|█████████▉| 97.7M/97.8M [00:04<00:00, 22.2MB/s]100%|██████████| 97.8M/97.8M [00:04<00:00, 23.3MB/s]
100%|██████████| 97.8M/97.8M [00:04<00:00, 22.9MB/s]
DLL 2021-06-03 07:31:10.009608 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-06-03 07:31:10.012976 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 8027
Using seed = 2863
loading annotations into memory...
loading annotations into memory...
Done (t=0.51s)
creating index...
Done (t=0.52s)
creating index...
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0

DLL 2021-06-03 07:34:52.237358 - () avg_img/sec : 198.70722361328893  med_img/sec : 198.58675303661965  min_img/sec : 197.61541958935075  max_img/sec : 200.55924088385035 
Done benchmarking. Total images: 28000	total time: 140.911	Average images/sec: 198.707	Median images/sec: 198.587
DLL 2021-06-03 07:34:52.237962 - () avg_img/sec : 198.72245370478242  med_img/sec : 198.60068977520373  min_img/sec : 197.67389470409975  max_img/sec : 200.65271990546947 
Done benchmarking. Total images: 28000	total time: 140.900	Average images/sec: 198.722	Median images/sec: 198.601
Training performance = 397.18743896484375 FPS
DLL 2021-06-03 07:34:52.238269 - (0,) time : 207.58600759506226 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-03 07:34:52.238387 - () total time : 207.58600759506226 
DLL 2021-06-03 07:34:52.238399 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-03 07:34:52.238424 - () total time : 207.5860526561737 
DLL 2021-06-03 07:34:52.238444 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
