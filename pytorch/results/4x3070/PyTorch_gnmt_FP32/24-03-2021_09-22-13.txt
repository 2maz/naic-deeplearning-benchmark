3: Collecting environment information...
1: Collecting environment information...
2: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
2: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
2: Saving results to: gnmt
0: Using master seed from command line: 2
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
2: Using master seed from command line: 2
3: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
3: Saving results to: gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
3: Using master seed from command line: 2
1: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
3: Worker 3 is using worker seed: 1323436024
2: Worker 2 is using worker seed: 3588440356
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
3: Size of vocabulary: 31794
2: Size of vocabulary: 31794
1: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
3: Filtering data, min len: 0, max len: 125
2: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
3: Pairs before: 5100, after: 5100
2: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
3: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
2: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
2: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1228
0: Scheduler decay interval: 154
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
3: Saving state of the tokenizer
3: Initializing fp32 optimizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 1228
3: Scheduler decay interval: 154
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
3: Starting epoch 0
3: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 1228
1: Scheduler decay interval: 154
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159593523
2: Saving state of the tokenizer
2: Initializing fp32 optimizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 1228
2: Scheduler decay interval: 154
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: Starting epoch 0
2: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
1: Sampler for epoch 0 uses seed 2602510382
0: Sampler for epoch 0 uses seed 2602510382
3: Sampler for epoch 0 uses seed 2602510382
2: Sampler for epoch 0 uses seed 2602510382
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
1: TRAIN [0][0/922]	Time 0.426 (0.000)	Data 1.17e-01 (0.00e+00)	Tok/s 2622 (0)	Loss/tok 10.5954 (10.5954)	LR 2.047e-05
0: TRAIN [0][0/922]	Time 0.424 (0.000)	Data 1.20e-01 (0.00e+00)	Tok/s 2587 (0)	Loss/tok 10.5922 (10.5922)	LR 2.047e-05
3: TRAIN [0][0/922]	Time 0.421 (0.000)	Data 1.30e-01 (0.00e+00)	Tok/s 2572 (0)	Loss/tok 10.6015 (10.6015)	LR 2.047e-05
2: TRAIN [0][0/922]	Time 0.422 (0.000)	Data 1.47e-01 (0.00e+00)	Tok/s 2602 (0)	Loss/tok 10.6130 (10.6130)	LR 2.047e-05
1: TRAIN [0][10/922]	Time 0.268 (0.292)	Data 7.20e-05 (7.73e-05)	Tok/s 3746 (6269)	Loss/tok 9.5780 (10.1157)	LR 2.576e-05
3: TRAIN [0][10/922]	Time 0.268 (0.292)	Data 6.63e-05 (7.77e-05)	Tok/s 3982 (6247)	Loss/tok 9.5442 (10.1084)	LR 2.576e-05
2: TRAIN [0][10/922]	Time 0.268 (0.292)	Data 7.96e-05 (9.41e-05)	Tok/s 3975 (6271)	Loss/tok 9.5453 (10.1017)	LR 2.576e-05
0: TRAIN [0][10/922]	Time 0.268 (0.292)	Data 9.13e-05 (9.63e-05)	Tok/s 4128 (6277)	Loss/tok 9.5751 (10.1091)	LR 2.576e-05
1: TRAIN [0][20/922]	Time 0.287 (0.294)	Data 7.13e-05 (7.69e-05)	Tok/s 6364 (6560)	Loss/tok 9.2350 (9.7718)	LR 3.244e-05
0: TRAIN [0][20/922]	Time 0.287 (0.294)	Data 7.18e-05 (9.93e-05)	Tok/s 6439 (6550)	Loss/tok 9.1661 (9.7682)	LR 3.244e-05
3: TRAIN [0][20/922]	Time 0.287 (0.294)	Data 3.13e-04 (1.27e-04)	Tok/s 6325 (6542)	Loss/tok 9.0772 (9.7613)	LR 3.244e-05
2: TRAIN [0][20/922]	Time 0.287 (0.294)	Data 1.15e-04 (1.11e-04)	Tok/s 6533 (6533)	Loss/tok 9.1823 (9.7596)	LR 3.244e-05
0: TRAIN [0][30/922]	Time 0.287 (0.292)	Data 6.99e-05 (9.33e-05)	Tok/s 6122 (6433)	Loss/tok 8.8506 (9.5347)	LR 4.083e-05
1: TRAIN [0][30/922]	Time 0.287 (0.292)	Data 8.39e-05 (9.82e-05)	Tok/s 6495 (6428)	Loss/tok 8.7958 (9.5439)	LR 4.083e-05
3: TRAIN [0][30/922]	Time 0.287 (0.292)	Data 9.35e-05 (1.25e-04)	Tok/s 6210 (6377)	Loss/tok 8.8228 (9.5359)	LR 4.083e-05
2: TRAIN [0][30/922]	Time 0.287 (0.292)	Data 8.89e-05 (1.22e-04)	Tok/s 6080 (6370)	Loss/tok 8.9122 (9.5418)	LR 4.083e-05
0: TRAIN [0][40/922]	Time 0.266 (0.290)	Data 7.20e-05 (9.00e-05)	Tok/s 4091 (6250)	Loss/tok 8.5997 (9.3597)	LR 5.141e-05
3: TRAIN [0][40/922]	Time 0.266 (0.290)	Data 7.01e-05 (1.13e-04)	Tok/s 4119 (6211)	Loss/tok 8.4445 (9.3573)	LR 5.141e-05
2: TRAIN [0][40/922]	Time 0.266 (0.290)	Data 7.82e-05 (1.12e-04)	Tok/s 4124 (6205)	Loss/tok 8.4429 (9.3762)	LR 5.141e-05
1: TRAIN [0][40/922]	Time 0.268 (0.290)	Data 1.03e-04 (1.02e-04)	Tok/s 4162 (6251)	Loss/tok 8.5179 (9.3650)	LR 5.141e-05
1: TRAIN [0][50/922]	Time 0.311 (0.292)	Data 1.18e-04 (1.07e-04)	Tok/s 8161 (6360)	Loss/tok 8.4782 (9.1949)	LR 6.472e-05
0: TRAIN [0][50/922]	Time 0.312 (0.292)	Data 7.34e-05 (8.81e-05)	Tok/s 8231 (6368)	Loss/tok 8.5627 (9.1945)	LR 6.472e-05
3: TRAIN [0][50/922]	Time 0.312 (0.292)	Data 8.37e-05 (1.10e-04)	Tok/s 8003 (6311)	Loss/tok 8.4878 (9.1995)	LR 6.472e-05
2: TRAIN [0][50/922]	Time 0.312 (0.292)	Data 8.37e-05 (1.20e-04)	Tok/s 8167 (6330)	Loss/tok 8.5247 (9.2132)	LR 6.472e-05
0: TRAIN [0][60/922]	Time 0.290 (0.294)	Data 7.61e-05 (8.69e-05)	Tok/s 6109 (6530)	Loss/tok 8.2036 (9.0382)	LR 8.148e-05
1: TRAIN [0][60/922]	Time 0.287 (0.294)	Data 1.26e-04 (1.06e-04)	Tok/s 6307 (6527)	Loss/tok 8.2387 (9.0408)	LR 8.148e-05
2: TRAIN [0][60/922]	Time 0.290 (0.294)	Data 7.92e-05 (1.13e-04)	Tok/s 6199 (6507)	Loss/tok 8.2406 (9.0613)	LR 8.148e-05
3: TRAIN [0][60/922]	Time 0.290 (0.294)	Data 7.10e-05 (1.06e-04)	Tok/s 6194 (6487)	Loss/tok 8.2026 (9.0403)	LR 8.148e-05
1: TRAIN [0][70/922]	Time 0.287 (0.292)	Data 8.23e-05 (1.03e-04)	Tok/s 6436 (6422)	Loss/tok 8.1066 (8.9429)	LR 1.026e-04
0: TRAIN [0][70/922]	Time 0.287 (0.292)	Data 7.84e-05 (8.55e-05)	Tok/s 6342 (6431)	Loss/tok 8.1840 (8.9364)	LR 1.026e-04
3: TRAIN [0][70/922]	Time 0.287 (0.292)	Data 9.44e-05 (1.05e-04)	Tok/s 6327 (6400)	Loss/tok 8.2198 (8.9412)	LR 1.026e-04
2: TRAIN [0][70/922]	Time 0.286 (0.292)	Data 9.32e-05 (1.13e-04)	Tok/s 6062 (6408)	Loss/tok 7.9934 (8.9586)	LR 1.026e-04
1: TRAIN [0][80/922]	Time 0.344 (0.292)	Data 8.23e-05 (1.01e-04)	Tok/s 9638 (6435)	Loss/tok 8.0408 (8.8262)	LR 1.291e-04
0: TRAIN [0][80/922]	Time 0.344 (0.292)	Data 6.79e-05 (8.44e-05)	Tok/s 9487 (6439)	Loss/tok 8.1464 (8.8321)	LR 1.291e-04
3: TRAIN [0][80/922]	Time 0.344 (0.292)	Data 8.44e-05 (1.03e-04)	Tok/s 9910 (6420)	Loss/tok 8.1022 (8.8302)	LR 1.291e-04
2: TRAIN [0][80/922]	Time 0.344 (0.292)	Data 1.43e-04 (1.14e-04)	Tok/s 9235 (6419)	Loss/tok 8.0955 (8.8428)	LR 1.291e-04
1: TRAIN [0][90/922]	Time 0.287 (0.291)	Data 7.89e-05 (1.00e-04)	Tok/s 6253 (6286)	Loss/tok 7.9095 (8.7383)	LR 1.626e-04
0: TRAIN [0][90/922]	Time 0.287 (0.291)	Data 6.87e-05 (8.44e-05)	Tok/s 6146 (6276)	Loss/tok 8.0077 (8.7547)	LR 1.626e-04
3: TRAIN [0][90/922]	Time 0.287 (0.291)	Data 6.72e-05 (1.01e-04)	Tok/s 6382 (6273)	Loss/tok 7.9489 (8.7433)	LR 1.626e-04
2: TRAIN [0][90/922]	Time 0.287 (0.291)	Data 9.92e-05 (1.18e-04)	Tok/s 6461 (6266)	Loss/tok 7.9376 (8.7578)	LR 1.626e-04
1: TRAIN [0][100/922]	Time 0.286 (0.290)	Data 1.06e-04 (9.89e-05)	Tok/s 6316 (6236)	Loss/tok 7.7193 (8.6519)	LR 2.047e-04
0: TRAIN [0][100/922]	Time 0.287 (0.290)	Data 1.01e-04 (8.39e-05)	Tok/s 6177 (6220)	Loss/tok 7.6582 (8.6649)	LR 2.047e-04
3: TRAIN [0][100/922]	Time 0.287 (0.290)	Data 7.89e-05 (9.90e-05)	Tok/s 6376 (6224)	Loss/tok 7.7032 (8.6576)	LR 2.047e-04
2: TRAIN [0][100/922]	Time 0.287 (0.290)	Data 1.94e-04 (1.21e-04)	Tok/s 5986 (6222)	Loss/tok 7.6770 (8.6669)	LR 2.047e-04
1: TRAIN [0][110/922]	Time 0.288 (0.290)	Data 8.01e-05 (9.71e-05)	Tok/s 6150 (6266)	Loss/tok 7.6167 (8.5699)	LR 2.576e-04
0: TRAIN [0][110/922]	Time 0.288 (0.290)	Data 7.18e-05 (8.51e-05)	Tok/s 6057 (6244)	Loss/tok 7.5593 (8.5759)	LR 2.576e-04
3: TRAIN [0][110/922]	Time 0.288 (0.290)	Data 8.01e-05 (9.77e-05)	Tok/s 6248 (6256)	Loss/tok 7.5847 (8.5740)	LR 2.576e-04
2: TRAIN [0][110/922]	Time 0.288 (0.290)	Data 1.54e-04 (1.25e-04)	Tok/s 6337 (6250)	Loss/tok 7.6949 (8.5852)	LR 2.576e-04
1: TRAIN [0][120/922]	Time 0.311 (0.291)	Data 7.41e-05 (9.55e-05)	Tok/s 8028 (6331)	Loss/tok 7.7459 (8.5010)	LR 3.244e-04
0: TRAIN [0][120/922]	Time 0.311 (0.291)	Data 7.01e-05 (8.40e-05)	Tok/s 7961 (6313)	Loss/tok 7.6994 (8.4993)	LR 3.244e-04
3: TRAIN [0][120/922]	Time 0.311 (0.291)	Data 7.37e-05 (9.69e-05)	Tok/s 8097 (6324)	Loss/tok 7.7921 (8.5013)	LR 3.244e-04
2: TRAIN [0][120/922]	Time 0.311 (0.291)	Data 8.87e-05 (1.24e-04)	Tok/s 8094 (6316)	Loss/tok 7.8564 (8.5135)	LR 3.244e-04
1: TRAIN [0][130/922]	Time 0.339 (0.291)	Data 8.03e-05 (9.46e-05)	Tok/s 9737 (6316)	Loss/tok 7.8970 (8.4421)	LR 4.083e-04
0: TRAIN [0][130/922]	Time 0.339 (0.291)	Data 8.08e-05 (8.34e-05)	Tok/s 9688 (6303)	Loss/tok 7.8562 (8.4377)	LR 4.083e-04
3: TRAIN [0][130/922]	Time 0.339 (0.291)	Data 7.87e-05 (9.60e-05)	Tok/s 9599 (6309)	Loss/tok 7.9386 (8.4430)	LR 4.083e-04
2: TRAIN [0][130/922]	Time 0.338 (0.291)	Data 2.57e-04 (1.26e-04)	Tok/s 9759 (6300)	Loss/tok 7.9695 (8.4520)	LR 4.083e-04
1: TRAIN [0][140/922]	Time 0.308 (0.291)	Data 7.10e-05 (9.37e-05)	Tok/s 8213 (6293)	Loss/tok 7.8102 (8.3959)	LR 5.141e-04
0: TRAIN [0][140/922]	Time 0.309 (0.291)	Data 6.72e-05 (8.32e-05)	Tok/s 8310 (6278)	Loss/tok 7.7716 (8.3924)	LR 5.141e-04
3: TRAIN [0][140/922]	Time 0.309 (0.291)	Data 8.77e-05 (9.58e-05)	Tok/s 8077 (6278)	Loss/tok 7.8131 (8.4009)	LR 5.141e-04
2: TRAIN [0][140/922]	Time 0.308 (0.291)	Data 8.87e-05 (1.25e-04)	Tok/s 8160 (6267)	Loss/tok 7.8508 (8.4065)	LR 5.141e-04
1: TRAIN [0][150/922]	Time 0.265 (0.290)	Data 7.37e-05 (9.30e-05)	Tok/s 4156 (6282)	Loss/tok 7.3915 (8.3479)	LR 6.472e-04
0: TRAIN [0][150/922]	Time 0.266 (0.290)	Data 7.27e-05 (8.26e-05)	Tok/s 4175 (6275)	Loss/tok 7.3930 (8.3443)	LR 6.472e-04
2: TRAIN [0][150/922]	Time 0.266 (0.290)	Data 7.75e-05 (1.23e-04)	Tok/s 4217 (6255)	Loss/tok 7.1614 (8.3570)	LR 6.472e-04
3: TRAIN [0][150/922]	Time 0.266 (0.290)	Data 1.10e-04 (9.62e-05)	Tok/s 4189 (6270)	Loss/tok 7.4431 (8.3529)	LR 6.472e-04
1: TRAIN [0][160/922]	Time 0.310 (0.291)	Data 8.85e-05 (9.24e-05)	Tok/s 8104 (6353)	Loss/tok 7.7055 (8.3048)	LR 8.148e-04
0: TRAIN [0][160/922]	Time 0.310 (0.291)	Data 8.51e-05 (8.24e-05)	Tok/s 8238 (6352)	Loss/tok 7.6599 (8.2996)	LR 8.148e-04
3: TRAIN [0][160/922]	Time 0.310 (0.291)	Data 1.12e-04 (9.75e-05)	Tok/s 8131 (6346)	Loss/tok 7.7924 (8.3092)	LR 8.148e-04
2: TRAIN [0][160/922]	Time 0.310 (0.291)	Data 8.65e-05 (1.21e-04)	Tok/s 7968 (6333)	Loss/tok 7.7390 (8.3144)	LR 8.148e-04
1: TRAIN [0][170/922]	Time 0.286 (0.291)	Data 8.89e-05 (9.19e-05)	Tok/s 6163 (6366)	Loss/tok 7.5632 (8.2650)	LR 1.026e-03
0: TRAIN [0][170/922]	Time 0.286 (0.291)	Data 7.34e-05 (8.21e-05)	Tok/s 6136 (6368)	Loss/tok 7.4118 (8.2614)	LR 1.026e-03
3: TRAIN [0][170/922]	Time 0.285 (0.291)	Data 1.17e-04 (9.79e-05)	Tok/s 6536 (6359)	Loss/tok 7.3823 (8.2679)	LR 1.026e-03
2: TRAIN [0][170/922]	Time 0.286 (0.291)	Data 7.53e-05 (1.19e-04)	Tok/s 6627 (6352)	Loss/tok 7.6265 (8.2723)	LR 1.026e-03
1: TRAIN [0][180/922]	Time 0.246 (0.291)	Data 8.58e-05 (9.27e-05)	Tok/s 2094 (6324)	Loss/tok 7.5295 (8.2331)	LR 1.291e-03
0: TRAIN [0][180/922]	Time 0.246 (0.291)	Data 6.87e-05 (8.18e-05)	Tok/s 2294 (6331)	Loss/tok 7.3109 (8.2287)	LR 1.291e-03
2: TRAIN [0][180/922]	Time 0.246 (0.291)	Data 8.11e-05 (1.17e-04)	Tok/s 2335 (6314)	Loss/tok 7.1857 (8.2397)	LR 1.291e-03
3: TRAIN [0][180/922]	Time 0.247 (0.291)	Data 7.27e-05 (9.77e-05)	Tok/s 2072 (6321)	Loss/tok 7.3814 (8.2376)	LR 1.291e-03
1: TRAIN [0][190/922]	Time 0.262 (0.292)	Data 8.01e-05 (9.20e-05)	Tok/s 4283 (6373)	Loss/tok 7.2861 (8.2008)	LR 1.626e-03
0: TRAIN [0][190/922]	Time 0.263 (0.292)	Data 6.89e-05 (8.15e-05)	Tok/s 4106 (6380)	Loss/tok 6.9311 (8.1946)	LR 1.626e-03
3: TRAIN [0][190/922]	Time 0.263 (0.292)	Data 7.32e-05 (9.72e-05)	Tok/s 4090 (6366)	Loss/tok 7.1283 (8.2031)	LR 1.626e-03
2: TRAIN [0][190/922]	Time 0.262 (0.292)	Data 6.84e-05 (1.16e-04)	Tok/s 3874 (6364)	Loss/tok 7.4164 (8.2053)	LR 1.626e-03
1: TRAIN [0][200/922]	Time 0.266 (0.291)	Data 8.18e-05 (9.17e-05)	Tok/s 4091 (6283)	Loss/tok 7.1555 (8.1724)	LR 2.000e-03
0: TRAIN [0][200/922]	Time 0.266 (0.291)	Data 7.87e-05 (8.16e-05)	Tok/s 4226 (6287)	Loss/tok 7.2861 (8.1683)	LR 2.000e-03
3: TRAIN [0][200/922]	Time 0.266 (0.291)	Data 7.68e-05 (9.65e-05)	Tok/s 4027 (6276)	Loss/tok 7.2497 (8.1762)	LR 2.000e-03
2: TRAIN [0][200/922]	Time 0.266 (0.291)	Data 7.63e-05 (1.14e-04)	Tok/s 3904 (6273)	Loss/tok 7.1871 (8.1786)	LR 2.000e-03
1: TRAIN [0][210/922]	Time 0.287 (0.291)	Data 7.87e-05 (9.13e-05)	Tok/s 6205 (6336)	Loss/tok 7.4423 (8.1412)	LR 2.000e-03
0: TRAIN [0][210/922]	Time 0.287 (0.291)	Data 6.99e-05 (8.13e-05)	Tok/s 6212 (6337)	Loss/tok 7.4106 (8.1398)	LR 2.000e-03
2: TRAIN [0][210/922]	Time 0.287 (0.291)	Data 7.27e-05 (1.13e-04)	Tok/s 6405 (6327)	Loss/tok 7.4496 (8.1465)	LR 2.000e-03
3: TRAIN [0][210/922]	Time 0.287 (0.291)	Data 1.16e-04 (9.61e-05)	Tok/s 6434 (6328)	Loss/tok 7.7572 (8.1469)	LR 2.000e-03
1: TRAIN [0][220/922]	Time 0.287 (0.291)	Data 8.51e-05 (9.10e-05)	Tok/s 6273 (6308)	Loss/tok 7.4462 (8.1099)	LR 2.000e-03
0: TRAIN [0][220/922]	Time 0.287 (0.291)	Data 7.22e-05 (8.11e-05)	Tok/s 6328 (6313)	Loss/tok 7.3576 (8.1122)	LR 2.000e-03
3: TRAIN [0][220/922]	Time 0.287 (0.291)	Data 1.03e-04 (9.55e-05)	Tok/s 6231 (6302)	Loss/tok 7.0215 (8.1128)	LR 2.000e-03
2: TRAIN [0][220/922]	Time 0.287 (0.291)	Data 7.58e-05 (1.12e-04)	Tok/s 6516 (6302)	Loss/tok 7.1002 (8.1180)	LR 2.000e-03
1: TRAIN [0][230/922]	Time 0.264 (0.291)	Data 8.68e-05 (9.05e-05)	Tok/s 4028 (6319)	Loss/tok 6.9243 (8.0725)	LR 2.000e-03
0: TRAIN [0][230/922]	Time 0.264 (0.291)	Data 7.15e-05 (8.08e-05)	Tok/s 4238 (6323)	Loss/tok 6.8721 (8.0734)	LR 2.000e-03
2: TRAIN [0][230/922]	Time 0.264 (0.291)	Data 7.87e-05 (1.10e-04)	Tok/s 4157 (6317)	Loss/tok 6.8134 (8.0786)	LR 2.000e-03
3: TRAIN [0][230/922]	Time 0.264 (0.291)	Data 7.46e-05 (9.46e-05)	Tok/s 4241 (6315)	Loss/tok 7.0314 (8.0732)	LR 2.000e-03
1: TRAIN [0][240/922]	Time 0.268 (0.291)	Data 8.30e-05 (9.02e-05)	Tok/s 3892 (6293)	Loss/tok 6.7830 (8.0401)	LR 2.000e-03
3: TRAIN [0][240/922]	Time 0.268 (0.291)	Data 8.27e-05 (9.46e-05)	Tok/s 3907 (6291)	Loss/tok 6.8875 (8.0408)	LR 2.000e-03
2: TRAIN [0][240/922]	Time 0.268 (0.291)	Data 7.80e-05 (1.09e-04)	Tok/s 4062 (6293)	Loss/tok 6.6985 (8.0442)	LR 2.000e-03
0: TRAIN [0][240/922]	Time 0.269 (0.291)	Data 6.94e-05 (8.07e-05)	Tok/s 4148 (6299)	Loss/tok 6.7657 (8.0388)	LR 2.000e-03
1: TRAIN [0][250/922]	Time 0.313 (0.290)	Data 8.20e-05 (9.00e-05)	Tok/s 8120 (6253)	Loss/tok 7.2280 (8.0084)	LR 2.000e-03
0: TRAIN [0][250/922]	Time 0.313 (0.290)	Data 6.96e-05 (8.08e-05)	Tok/s 7845 (6259)	Loss/tok 7.2479 (8.0075)	LR 2.000e-03
2: TRAIN [0][250/922]	Time 0.313 (0.290)	Data 7.20e-05 (1.08e-04)	Tok/s 8025 (6253)	Loss/tok 7.3729 (8.0139)	LR 2.000e-03
3: TRAIN [0][250/922]	Time 0.313 (0.290)	Data 6.87e-05 (9.42e-05)	Tok/s 8032 (6254)	Loss/tok 7.1752 (8.0078)	LR 2.000e-03
1: TRAIN [0][260/922]	Time 0.246 (0.290)	Data 8.34e-05 (8.97e-05)	Tok/s 2114 (6260)	Loss/tok 6.2590 (7.9700)	LR 2.000e-03
0: TRAIN [0][260/922]	Time 0.246 (0.290)	Data 6.89e-05 (8.05e-05)	Tok/s 2073 (6266)	Loss/tok 6.3098 (7.9730)	LR 2.000e-03
3: TRAIN [0][260/922]	Time 0.245 (0.290)	Data 1.02e-04 (9.35e-05)	Tok/s 2257 (6264)	Loss/tok 6.3543 (7.9708)	LR 2.000e-03
2: TRAIN [0][260/922]	Time 0.245 (0.290)	Data 2.64e-04 (1.08e-04)	Tok/s 2187 (6264)	Loss/tok 6.3626 (7.9780)	LR 2.000e-03
1: TRAIN [0][270/922]	Time 0.307 (0.291)	Data 8.30e-05 (8.95e-05)	Tok/s 8212 (6285)	Loss/tok 7.1728 (7.9375)	LR 2.000e-03
2: TRAIN [0][270/922]	Time 0.307 (0.291)	Data 6.87e-05 (1.07e-04)	Tok/s 8178 (6288)	Loss/tok 7.2665 (7.9440)	LR 2.000e-03
3: TRAIN [0][270/922]	Time 0.307 (0.291)	Data 6.84e-05 (9.31e-05)	Tok/s 8086 (6289)	Loss/tok 7.1740 (7.9368)	LR 2.000e-03
0: TRAIN [0][270/922]	Time 0.307 (0.291)	Data 6.99e-05 (8.05e-05)	Tok/s 8048 (6289)	Loss/tok 7.1386 (7.9399)	LR 2.000e-03
1: TRAIN [0][280/922]	Time 0.282 (0.290)	Data 9.13e-05 (8.92e-05)	Tok/s 6589 (6263)	Loss/tok 6.7932 (7.9073)	LR 2.000e-03
0: TRAIN [0][280/922]	Time 0.282 (0.290)	Data 8.01e-05 (8.06e-05)	Tok/s 6209 (6267)	Loss/tok 6.9947 (7.9091)	LR 2.000e-03
3: TRAIN [0][280/922]	Time 0.282 (0.290)	Data 7.87e-05 (9.26e-05)	Tok/s 6495 (6267)	Loss/tok 6.9802 (7.9060)	LR 2.000e-03
2: TRAIN [0][280/922]	Time 0.282 (0.290)	Data 7.94e-05 (1.06e-04)	Tok/s 6214 (6265)	Loss/tok 7.0156 (7.9135)	LR 2.000e-03
1: TRAIN [0][290/922]	Time 0.247 (0.291)	Data 7.75e-05 (8.90e-05)	Tok/s 2133 (6293)	Loss/tok 5.9225 (7.8701)	LR 2.000e-03
0: TRAIN [0][290/922]	Time 0.247 (0.291)	Data 7.68e-05 (8.08e-05)	Tok/s 2198 (6297)	Loss/tok 6.0531 (7.8747)	LR 2.000e-03
3: TRAIN [0][290/922]	Time 0.246 (0.291)	Data 8.08e-05 (9.22e-05)	Tok/s 2224 (6296)	Loss/tok 6.1600 (7.8699)	LR 2.000e-03
2: TRAIN [0][290/922]	Time 0.247 (0.291)	Data 8.44e-05 (1.05e-04)	Tok/s 2284 (6294)	Loss/tok 5.9040 (7.8749)	LR 2.000e-03
1: TRAIN [0][300/922]	Time 0.285 (0.291)	Data 7.80e-05 (8.87e-05)	Tok/s 6381 (6271)	Loss/tok 6.7757 (7.8378)	LR 2.000e-03
0: TRAIN [0][300/922]	Time 0.285 (0.291)	Data 6.82e-05 (8.08e-05)	Tok/s 6398 (6275)	Loss/tok 6.6911 (7.8417)	LR 2.000e-03
3: TRAIN [0][300/922]	Time 0.285 (0.291)	Data 7.44e-05 (9.19e-05)	Tok/s 6224 (6273)	Loss/tok 6.5749 (7.8372)	LR 2.000e-03
2: TRAIN [0][300/922]	Time 0.285 (0.291)	Data 7.63e-05 (1.05e-04)	Tok/s 6023 (6269)	Loss/tok 6.7482 (7.8428)	LR 2.000e-03
1: TRAIN [0][310/922]	Time 0.289 (0.291)	Data 7.82e-05 (8.85e-05)	Tok/s 6252 (6282)	Loss/tok 6.6714 (7.8029)	LR 2.000e-03
0: TRAIN [0][310/922]	Time 0.289 (0.291)	Data 7.72e-05 (8.08e-05)	Tok/s 6301 (6286)	Loss/tok 6.5835 (7.8046)	LR 2.000e-03
3: TRAIN [0][310/922]	Time 0.289 (0.291)	Data 7.20e-05 (9.15e-05)	Tok/s 6217 (6284)	Loss/tok 6.7070 (7.8020)	LR 2.000e-03
2: TRAIN [0][310/922]	Time 0.289 (0.291)	Data 7.87e-05 (1.04e-04)	Tok/s 6201 (6282)	Loss/tok 6.8369 (7.8067)	LR 2.000e-03
1: TRAIN [0][320/922]	Time 0.289 (0.290)	Data 9.73e-05 (8.84e-05)	Tok/s 6260 (6227)	Loss/tok 6.7942 (7.7767)	LR 2.000e-03
0: TRAIN [0][320/922]	Time 0.290 (0.290)	Data 8.06e-05 (8.08e-05)	Tok/s 6147 (6230)	Loss/tok 6.6432 (7.7794)	LR 2.000e-03
2: TRAIN [0][320/922]	Time 0.289 (0.290)	Data 8.89e-05 (1.03e-04)	Tok/s 6178 (6229)	Loss/tok 6.6157 (7.7795)	LR 2.000e-03
3: TRAIN [0][320/922]	Time 0.290 (0.290)	Data 9.08e-05 (9.14e-05)	Tok/s 6317 (6229)	Loss/tok 6.7261 (7.7761)	LR 2.000e-03
1: TRAIN [0][330/922]	Time 0.266 (0.291)	Data 9.37e-05 (8.82e-05)	Tok/s 4251 (6262)	Loss/tok 6.2013 (7.7384)	LR 2.000e-03
0: TRAIN [0][330/922]	Time 0.266 (0.291)	Data 7.06e-05 (8.07e-05)	Tok/s 4080 (6263)	Loss/tok 6.0560 (7.7426)	LR 2.000e-03
2: TRAIN [0][330/922]	Time 0.266 (0.291)	Data 6.51e-05 (1.03e-04)	Tok/s 4023 (6262)	Loss/tok 6.5472 (7.7421)	LR 2.000e-03
3: TRAIN [0][330/922]	Time 0.266 (0.291)	Data 6.94e-05 (9.11e-05)	Tok/s 4012 (6263)	Loss/tok 6.2964 (7.7380)	LR 2.000e-03
1: TRAIN [0][340/922]	Time 0.287 (0.291)	Data 8.46e-05 (8.80e-05)	Tok/s 6317 (6273)	Loss/tok 6.5587 (7.7070)	LR 2.000e-03
0: TRAIN [0][340/922]	Time 0.287 (0.291)	Data 8.39e-05 (8.06e-05)	Tok/s 6414 (6275)	Loss/tok 6.5458 (7.7090)	LR 2.000e-03
2: TRAIN [0][340/922]	Time 0.287 (0.291)	Data 6.48e-05 (1.02e-04)	Tok/s 6302 (6272)	Loss/tok 6.5067 (7.7083)	LR 2.000e-03
3: TRAIN [0][340/922]	Time 0.287 (0.291)	Data 6.91e-05 (9.09e-05)	Tok/s 6054 (6274)	Loss/tok 6.7068 (7.7067)	LR 2.000e-03
1: TRAIN [0][350/922]	Time 0.287 (0.291)	Data 8.56e-05 (8.78e-05)	Tok/s 6205 (6304)	Loss/tok 6.5281 (7.6700)	LR 2.000e-03
0: TRAIN [0][350/922]	Time 0.287 (0.291)	Data 6.37e-05 (8.04e-05)	Tok/s 6329 (6307)	Loss/tok 6.5957 (7.6706)	LR 2.000e-03
3: TRAIN [0][350/922]	Time 0.287 (0.291)	Data 7.01e-05 (9.06e-05)	Tok/s 6147 (6304)	Loss/tok 6.4924 (7.6672)	LR 2.000e-03
2: TRAIN [0][350/922]	Time 0.287 (0.291)	Data 7.06e-05 (1.01e-04)	Tok/s 6158 (6304)	Loss/tok 6.4123 (7.6697)	LR 2.000e-03
1: TRAIN [0][360/922]	Time 0.290 (0.291)	Data 7.82e-05 (8.76e-05)	Tok/s 6138 (6319)	Loss/tok 6.2599 (7.6361)	LR 2.000e-03
0: TRAIN [0][360/922]	Time 0.290 (0.291)	Data 6.82e-05 (8.03e-05)	Tok/s 6042 (6320)	Loss/tok 6.2999 (7.6365)	LR 2.000e-03
3: TRAIN [0][360/922]	Time 0.290 (0.291)	Data 6.87e-05 (9.02e-05)	Tok/s 6200 (6317)	Loss/tok 6.7654 (7.6364)	LR 2.000e-03
2: TRAIN [0][360/922]	Time 0.290 (0.291)	Data 7.32e-05 (1.00e-04)	Tok/s 6256 (6318)	Loss/tok 6.3710 (7.6386)	LR 2.000e-03
0: TRAIN [0][370/922]	Time 0.291 (0.291)	Data 7.84e-05 (8.07e-05)	Tok/s 6166 (6313)	Loss/tok 6.3304 (7.6057)	LR 2.000e-03
3: TRAIN [0][370/922]	Time 0.291 (0.291)	Data 7.51e-05 (9.00e-05)	Tok/s 6110 (6309)	Loss/tok 6.4160 (7.6078)	LR 2.000e-03
2: TRAIN [0][370/922]	Time 0.291 (0.291)	Data 7.72e-05 (9.96e-05)	Tok/s 6072 (6310)	Loss/tok 6.3543 (7.6079)	LR 2.000e-03
1: TRAIN [0][370/922]	Time 0.291 (0.291)	Data 8.08e-05 (8.74e-05)	Tok/s 6414 (6311)	Loss/tok 6.3683 (7.6042)	LR 2.000e-03
1: TRAIN [0][380/922]	Time 0.263 (0.291)	Data 8.11e-05 (8.73e-05)	Tok/s 4172 (6295)	Loss/tok 6.0989 (7.5762)	LR 2.000e-03
0: TRAIN [0][380/922]	Time 0.263 (0.291)	Data 7.37e-05 (8.08e-05)	Tok/s 3942 (6298)	Loss/tok 5.7934 (7.5752)	LR 2.000e-03
3: TRAIN [0][380/922]	Time 0.264 (0.291)	Data 7.08e-05 (8.97e-05)	Tok/s 4138 (6295)	Loss/tok 6.2477 (7.5789)	LR 2.000e-03
2: TRAIN [0][380/922]	Time 0.264 (0.291)	Data 6.87e-05 (9.90e-05)	Tok/s 4123 (6294)	Loss/tok 5.9346 (7.5778)	LR 2.000e-03
1: TRAIN [0][390/922]	Time 0.284 (0.291)	Data 7.72e-05 (8.71e-05)	Tok/s 6347 (6330)	Loss/tok 6.1874 (7.5397)	LR 2.000e-03
0: TRAIN [0][390/922]	Time 0.284 (0.291)	Data 7.41e-05 (8.06e-05)	Tok/s 6205 (6331)	Loss/tok 6.3478 (7.5394)	LR 2.000e-03
3: TRAIN [0][390/922]	Time 0.284 (0.291)	Data 7.94e-05 (8.95e-05)	Tok/s 6387 (6330)	Loss/tok 6.3851 (7.5434)	LR 2.000e-03
2: TRAIN [0][390/922]	Time 0.284 (0.291)	Data 7.30e-05 (9.91e-05)	Tok/s 6418 (6328)	Loss/tok 6.2770 (7.5413)	LR 2.000e-03
1: TRAIN [0][400/922]	Time 0.266 (0.292)	Data 9.08e-05 (8.70e-05)	Tok/s 4129 (6344)	Loss/tok 6.1329 (7.5070)	LR 2.000e-03
0: TRAIN [0][400/922]	Time 0.266 (0.292)	Data 8.03e-05 (8.05e-05)	Tok/s 4271 (6344)	Loss/tok 5.9162 (7.5051)	LR 2.000e-03
3: TRAIN [0][400/922]	Time 0.266 (0.292)	Data 8.15e-05 (8.92e-05)	Tok/s 4032 (6343)	Loss/tok 5.9188 (7.5109)	LR 2.000e-03
2: TRAIN [0][400/922]	Time 0.266 (0.292)	Data 6.96e-05 (9.85e-05)	Tok/s 4029 (6339)	Loss/tok 5.8176 (7.5090)	LR 2.000e-03
1: TRAIN [0][410/922]	Time 0.263 (0.291)	Data 9.30e-05 (8.68e-05)	Tok/s 4110 (6326)	Loss/tok 6.0804 (7.4792)	LR 2.000e-03
0: TRAIN [0][410/922]	Time 0.263 (0.291)	Data 6.58e-05 (8.05e-05)	Tok/s 4113 (6326)	Loss/tok 5.7369 (7.4768)	LR 2.000e-03
3: TRAIN [0][410/922]	Time 0.263 (0.291)	Data 7.51e-05 (8.90e-05)	Tok/s 4111 (6324)	Loss/tok 5.9221 (7.4831)	LR 2.000e-03
2: TRAIN [0][410/922]	Time 0.263 (0.291)	Data 7.34e-05 (9.81e-05)	Tok/s 4123 (6321)	Loss/tok 5.9987 (7.4825)	LR 2.000e-03
1: TRAIN [0][420/922]	Time 0.311 (0.291)	Data 8.87e-05 (8.68e-05)	Tok/s 7964 (6327)	Loss/tok 6.3343 (7.4492)	LR 2.000e-03
0: TRAIN [0][420/922]	Time 0.311 (0.291)	Data 6.72e-05 (8.05e-05)	Tok/s 8203 (6328)	Loss/tok 6.4006 (7.4481)	LR 2.000e-03
3: TRAIN [0][420/922]	Time 0.312 (0.291)	Data 7.20e-05 (8.88e-05)	Tok/s 8022 (6325)	Loss/tok 6.3358 (7.4525)	LR 2.000e-03
2: TRAIN [0][420/922]	Time 0.312 (0.291)	Data 8.42e-05 (9.76e-05)	Tok/s 8149 (6322)	Loss/tok 6.2021 (7.4523)	LR 2.000e-03
1: TRAIN [0][430/922]	Time 0.288 (0.291)	Data 8.51e-05 (8.66e-05)	Tok/s 6432 (6336)	Loss/tok 5.9923 (7.4190)	LR 2.000e-03
0: TRAIN [0][430/922]	Time 0.288 (0.291)	Data 1.21e-04 (8.05e-05)	Tok/s 6212 (6336)	Loss/tok 5.9199 (7.4172)	LR 2.000e-03
3: TRAIN [0][430/922]	Time 0.288 (0.291)	Data 6.34e-05 (8.86e-05)	Tok/s 6230 (6334)	Loss/tok 6.2516 (7.4223)	LR 2.000e-03
2: TRAIN [0][430/922]	Time 0.288 (0.291)	Data 6.70e-05 (9.71e-05)	Tok/s 6074 (6331)	Loss/tok 5.9599 (7.4204)	LR 2.000e-03
1: TRAIN [0][440/922]	Time 0.306 (0.292)	Data 8.68e-05 (8.66e-05)	Tok/s 8282 (6357)	Loss/tok 6.4143 (7.3873)	LR 2.000e-03
0: TRAIN [0][440/922]	Time 0.306 (0.292)	Data 6.75e-05 (8.07e-05)	Tok/s 8132 (6354)	Loss/tok 6.1277 (7.3857)	LR 2.000e-03
3: TRAIN [0][440/922]	Time 0.306 (0.292)	Data 6.77e-05 (8.84e-05)	Tok/s 8149 (6353)	Loss/tok 6.0893 (7.3905)	LR 2.000e-03
2: TRAIN [0][440/922]	Time 0.306 (0.292)	Data 1.59e-04 (9.68e-05)	Tok/s 8127 (6350)	Loss/tok 6.1074 (7.3873)	LR 2.000e-03
1: TRAIN [0][450/922]	Time 0.313 (0.292)	Data 8.61e-05 (8.64e-05)	Tok/s 7987 (6378)	Loss/tok 6.0905 (7.3558)	LR 2.000e-03
0: TRAIN [0][450/922]	Time 0.313 (0.292)	Data 6.94e-05 (8.10e-05)	Tok/s 8035 (6374)	Loss/tok 6.0135 (7.3535)	LR 2.000e-03
3: TRAIN [0][450/922]	Time 0.313 (0.292)	Data 6.34e-05 (8.81e-05)	Tok/s 8015 (6374)	Loss/tok 6.0964 (7.3584)	LR 2.000e-03
2: TRAIN [0][450/922]	Time 0.313 (0.292)	Data 7.01e-05 (9.64e-05)	Tok/s 7843 (6370)	Loss/tok 6.0012 (7.3551)	LR 2.000e-03
1: TRAIN [0][460/922]	Time 0.308 (0.292)	Data 9.01e-05 (8.64e-05)	Tok/s 7932 (6403)	Loss/tok 6.0153 (7.3226)	LR 2.000e-03
0: TRAIN [0][460/922]	Time 0.308 (0.292)	Data 7.37e-05 (8.09e-05)	Tok/s 8170 (6397)	Loss/tok 6.0063 (7.3209)	LR 2.000e-03
3: TRAIN [0][460/922]	Time 0.308 (0.292)	Data 7.22e-05 (8.78e-05)	Tok/s 8295 (6400)	Loss/tok 6.2119 (7.3244)	LR 2.000e-03
2: TRAIN [0][460/922]	Time 0.308 (0.292)	Data 7.03e-05 (9.62e-05)	Tok/s 8304 (6395)	Loss/tok 6.0683 (7.3224)	LR 2.000e-03
3: TRAIN [0][470/922]	Time 0.264 (0.292)	Data 8.46e-05 (8.76e-05)	Tok/s 4023 (6390)	Loss/tok 5.8491 (7.2969)	LR 2.000e-03
2: TRAIN [0][470/922]	Time 0.264 (0.292)	Data 7.94e-05 (9.59e-05)	Tok/s 3964 (6387)	Loss/tok 5.6546 (7.2966)	LR 2.000e-03
1: TRAIN [0][470/922]	Time 0.264 (0.292)	Data 9.47e-05 (8.66e-05)	Tok/s 4009 (6394)	Loss/tok 5.4363 (7.2951)	LR 2.000e-03
0: TRAIN [0][470/922]	Time 0.265 (0.292)	Data 7.58e-05 (8.09e-05)	Tok/s 4019 (6389)	Loss/tok 5.4378 (7.2940)	LR 2.000e-03
1: TRAIN [0][480/922]	Time 0.307 (0.292)	Data 7.87e-05 (8.65e-05)	Tok/s 8127 (6390)	Loss/tok 6.0163 (7.2703)	LR 2.000e-03
3: TRAIN [0][480/922]	Time 0.307 (0.292)	Data 7.65e-05 (8.74e-05)	Tok/s 8324 (6385)	Loss/tok 6.1554 (7.2712)	LR 2.000e-03
0: TRAIN [0][480/922]	Time 0.307 (0.292)	Data 7.75e-05 (8.08e-05)	Tok/s 8243 (6384)	Loss/tok 6.0845 (7.2683)	LR 2.000e-03
2: TRAIN [0][480/922]	Time 0.307 (0.292)	Data 7.92e-05 (9.58e-05)	Tok/s 8158 (6383)	Loss/tok 5.9622 (7.2708)	LR 2.000e-03
1: TRAIN [0][490/922]	Time 0.307 (0.292)	Data 8.13e-05 (8.63e-05)	Tok/s 8246 (6418)	Loss/tok 6.1336 (7.2391)	LR 2.000e-03
0: TRAIN [0][490/922]	Time 0.307 (0.292)	Data 7.13e-05 (8.06e-05)	Tok/s 8020 (6414)	Loss/tok 6.0843 (7.2373)	LR 2.000e-03
3: TRAIN [0][490/922]	Time 0.308 (0.292)	Data 6.99e-05 (8.72e-05)	Tok/s 8246 (6416)	Loss/tok 5.9883 (7.2374)	LR 2.000e-03
2: TRAIN [0][490/922]	Time 0.308 (0.292)	Data 7.53e-05 (9.54e-05)	Tok/s 8183 (6413)	Loss/tok 6.0579 (7.2379)	LR 2.000e-03
1: TRAIN [0][500/922]	Time 0.288 (0.292)	Data 7.80e-05 (8.65e-05)	Tok/s 6336 (6433)	Loss/tok 5.5035 (7.2102)	LR 2.000e-03
0: TRAIN [0][500/922]	Time 0.288 (0.292)	Data 7.03e-05 (8.06e-05)	Tok/s 6300 (6431)	Loss/tok 5.5899 (7.2075)	LR 2.000e-03
3: TRAIN [0][500/922]	Time 0.289 (0.292)	Data 6.82e-05 (8.70e-05)	Tok/s 6225 (6433)	Loss/tok 5.6892 (7.2071)	LR 2.000e-03
2: TRAIN [0][500/922]	Time 0.289 (0.292)	Data 6.82e-05 (9.50e-05)	Tok/s 6266 (6430)	Loss/tok 5.7107 (7.2081)	LR 2.000e-03
1: TRAIN [0][510/922]	Time 0.265 (0.293)	Data 8.39e-05 (8.64e-05)	Tok/s 4214 (6449)	Loss/tok 5.3903 (7.1804)	LR 2.000e-03
0: TRAIN [0][510/922]	Time 0.265 (0.293)	Data 7.18e-05 (8.05e-05)	Tok/s 3980 (6444)	Loss/tok 5.3740 (7.1781)	LR 2.000e-03
3: TRAIN [0][510/922]	Time 0.265 (0.293)	Data 6.79e-05 (8.68e-05)	Tok/s 4430 (6450)	Loss/tok 5.3840 (7.1764)	LR 2.000e-03
2: TRAIN [0][510/922]	Time 0.265 (0.293)	Data 7.70e-05 (9.46e-05)	Tok/s 4129 (6446)	Loss/tok 5.1414 (7.1777)	LR 2.000e-03
1: TRAIN [0][520/922]	Time 0.282 (0.292)	Data 8.13e-05 (8.63e-05)	Tok/s 6291 (6441)	Loss/tok 5.6803 (7.1543)	LR 2.000e-03
0: TRAIN [0][520/922]	Time 0.282 (0.292)	Data 9.85e-05 (8.05e-05)	Tok/s 6454 (6435)	Loss/tok 5.4592 (7.1516)	LR 2.000e-03
3: TRAIN [0][520/922]	Time 0.282 (0.292)	Data 7.51e-05 (8.68e-05)	Tok/s 6407 (6439)	Loss/tok 5.6506 (7.1504)	LR 2.000e-03
2: TRAIN [0][520/922]	Time 0.282 (0.292)	Data 8.30e-05 (9.43e-05)	Tok/s 6292 (6437)	Loss/tok 5.4993 (7.1521)	LR 2.000e-03
1: TRAIN [0][530/922]	Time 0.286 (0.292)	Data 9.66e-05 (8.62e-05)	Tok/s 6194 (6437)	Loss/tok 5.7545 (7.1296)	LR 2.000e-03
0: TRAIN [0][530/922]	Time 0.286 (0.292)	Data 6.53e-05 (8.04e-05)	Tok/s 6350 (6430)	Loss/tok 5.5566 (7.1261)	LR 2.000e-03
3: TRAIN [0][530/922]	Time 0.286 (0.292)	Data 7.92e-05 (8.67e-05)	Tok/s 6316 (6435)	Loss/tok 5.5632 (7.1239)	LR 2.000e-03
2: TRAIN [0][530/922]	Time 0.286 (0.292)	Data 7.03e-05 (9.39e-05)	Tok/s 6359 (6434)	Loss/tok 5.6316 (7.1267)	LR 2.000e-03
0: TRAIN [0][540/922]	Time 0.262 (0.292)	Data 8.03e-05 (8.05e-05)	Tok/s 4071 (6412)	Loss/tok 5.3825 (7.1044)	LR 2.000e-03
1: TRAIN [0][540/922]	Time 0.263 (0.292)	Data 8.11e-05 (8.60e-05)	Tok/s 4358 (6418)	Loss/tok 5.1302 (7.1068)	LR 2.000e-03
3: TRAIN [0][540/922]	Time 0.262 (0.292)	Data 7.65e-05 (8.65e-05)	Tok/s 4303 (6419)	Loss/tok 5.2777 (7.1015)	LR 2.000e-03
2: TRAIN [0][540/922]	Time 0.262 (0.292)	Data 7.72e-05 (9.36e-05)	Tok/s 4048 (6416)	Loss/tok 5.1446 (7.1038)	LR 2.000e-03
1: TRAIN [0][550/922]	Time 0.265 (0.292)	Data 7.94e-05 (8.60e-05)	Tok/s 3942 (6420)	Loss/tok 5.5068 (7.0806)	LR 2.000e-03
0: TRAIN [0][550/922]	Time 0.265 (0.292)	Data 6.82e-05 (8.05e-05)	Tok/s 4252 (6414)	Loss/tok 5.3118 (7.0780)	LR 2.000e-03
3: TRAIN [0][550/922]	Time 0.265 (0.292)	Data 6.51e-05 (8.64e-05)	Tok/s 4042 (6421)	Loss/tok 5.1880 (7.0763)	LR 2.000e-03
2: TRAIN [0][550/922]	Time 0.265 (0.292)	Data 8.30e-05 (9.40e-05)	Tok/s 4081 (6418)	Loss/tok 5.1501 (7.0781)	LR 2.000e-03
1: TRAIN [0][560/922]	Time 0.309 (0.292)	Data 7.70e-05 (8.59e-05)	Tok/s 8123 (6412)	Loss/tok 5.6313 (7.0570)	LR 2.000e-03
0: TRAIN [0][560/922]	Time 0.309 (0.292)	Data 6.84e-05 (8.05e-05)	Tok/s 8434 (6407)	Loss/tok 5.8789 (7.0548)	LR 2.000e-03
3: TRAIN [0][560/922]	Time 0.309 (0.292)	Data 6.99e-05 (8.64e-05)	Tok/s 8001 (6412)	Loss/tok 5.9882 (7.0518)	LR 2.000e-03
2: TRAIN [0][560/922]	Time 0.309 (0.292)	Data 7.53e-05 (9.38e-05)	Tok/s 8281 (6409)	Loss/tok 5.6320 (7.0546)	LR 2.000e-03
1: TRAIN [0][570/922]	Time 0.309 (0.292)	Data 7.75e-05 (8.57e-05)	Tok/s 8260 (6419)	Loss/tok 5.6320 (7.0292)	LR 2.000e-03
0: TRAIN [0][570/922]	Time 0.309 (0.292)	Data 7.32e-05 (8.07e-05)	Tok/s 8189 (6415)	Loss/tok 5.8572 (7.0283)	LR 2.000e-03
2: TRAIN [0][570/922]	Time 0.309 (0.292)	Data 2.68e-04 (9.45e-05)	Tok/s 8273 (6416)	Loss/tok 5.7249 (7.0267)	LR 2.000e-03
3: TRAIN [0][570/922]	Time 0.309 (0.292)	Data 9.49e-05 (8.63e-05)	Tok/s 7956 (6418)	Loss/tok 5.5050 (7.0249)	LR 2.000e-03
1: TRAIN [0][580/922]	Time 0.263 (0.292)	Data 7.44e-05 (8.56e-05)	Tok/s 4452 (6438)	Loss/tok 5.0278 (7.0000)	LR 2.000e-03
0: TRAIN [0][580/922]	Time 0.263 (0.292)	Data 7.08e-05 (8.06e-05)	Tok/s 4018 (6432)	Loss/tok 4.9827 (6.9999)	LR 2.000e-03
3: TRAIN [0][580/922]	Time 0.264 (0.292)	Data 7.13e-05 (8.64e-05)	Tok/s 3991 (6435)	Loss/tok 5.1078 (6.9972)	LR 2.000e-03
2: TRAIN [0][580/922]	Time 0.264 (0.292)	Data 8.01e-05 (9.43e-05)	Tok/s 4107 (6432)	Loss/tok 5.1161 (6.9985)	LR 2.000e-03
1: TRAIN [0][590/922]	Time 0.264 (0.292)	Data 7.96e-05 (8.54e-05)	Tok/s 4094 (6449)	Loss/tok 5.2690 (6.9737)	LR 2.000e-03
0: TRAIN [0][590/922]	Time 0.264 (0.292)	Data 6.70e-05 (8.05e-05)	Tok/s 4055 (6445)	Loss/tok 5.1601 (6.9710)	LR 2.000e-03
3: TRAIN [0][590/922]	Time 0.264 (0.292)	Data 8.44e-05 (8.63e-05)	Tok/s 4113 (6446)	Loss/tok 5.0747 (6.9686)	LR 2.000e-03
2: TRAIN [0][590/922]	Time 0.264 (0.292)	Data 8.68e-05 (9.41e-05)	Tok/s 4050 (6442)	Loss/tok 5.0678 (6.9708)	LR 2.000e-03
1: TRAIN [0][600/922]	Time 0.342 (0.293)	Data 8.99e-05 (8.54e-05)	Tok/s 9355 (6460)	Loss/tok 5.7748 (6.9467)	LR 2.000e-03
0: TRAIN [0][600/922]	Time 0.342 (0.293)	Data 8.32e-05 (8.04e-05)	Tok/s 9572 (6457)	Loss/tok 5.8212 (6.9454)	LR 2.000e-03
3: TRAIN [0][600/922]	Time 0.343 (0.293)	Data 7.06e-05 (8.63e-05)	Tok/s 9615 (6458)	Loss/tok 5.7215 (6.9406)	LR 2.000e-03
2: TRAIN [0][600/922]	Time 0.342 (0.293)	Data 6.84e-05 (9.39e-05)	Tok/s 9425 (6453)	Loss/tok 5.6801 (6.9437)	LR 2.000e-03
1: TRAIN [0][610/922]	Time 0.306 (0.293)	Data 8.65e-05 (8.53e-05)	Tok/s 8319 (6451)	Loss/tok 5.3605 (6.9235)	LR 2.000e-03
0: TRAIN [0][610/922]	Time 0.306 (0.293)	Data 7.46e-05 (8.05e-05)	Tok/s 7938 (6448)	Loss/tok 5.7536 (6.9221)	LR 2.000e-03
3: TRAIN [0][610/922]	Time 0.306 (0.293)	Data 8.34e-05 (8.62e-05)	Tok/s 8285 (6450)	Loss/tok 5.5094 (6.9166)	LR 2.000e-03
2: TRAIN [0][610/922]	Time 0.306 (0.293)	Data 8.58e-05 (9.39e-05)	Tok/s 8072 (6445)	Loss/tok 5.3413 (6.9207)	LR 2.000e-03
0: TRAIN [0][620/922]	Time 0.265 (0.293)	Data 6.60e-05 (8.04e-05)	Tok/s 4226 (6477)	Loss/tok 5.0508 (6.8925)	LR 2.000e-03
3: TRAIN [0][620/922]	Time 0.265 (0.293)	Data 9.68e-05 (8.62e-05)	Tok/s 4275 (6481)	Loss/tok 5.1871 (6.8868)	LR 2.000e-03
2: TRAIN [0][620/922]	Time 0.265 (0.293)	Data 7.82e-05 (9.37e-05)	Tok/s 4266 (6475)	Loss/tok 4.9873 (6.8896)	LR 2.000e-03
1: TRAIN [0][620/922]	Time 0.265 (0.293)	Data 8.20e-05 (8.52e-05)	Tok/s 4196 (6481)	Loss/tok 5.0590 (6.8938)	LR 2.000e-03
1: TRAIN [0][630/922]	Time 0.265 (0.293)	Data 7.89e-05 (8.51e-05)	Tok/s 4203 (6475)	Loss/tok 4.8180 (6.8713)	LR 2.000e-03
0: TRAIN [0][630/922]	Time 0.265 (0.293)	Data 8.65e-05 (8.04e-05)	Tok/s 4085 (6471)	Loss/tok 4.8377 (6.8694)	LR 2.000e-03
3: TRAIN [0][630/922]	Time 0.264 (0.293)	Data 8.80e-05 (8.61e-05)	Tok/s 4047 (6475)	Loss/tok 4.7805 (6.8638)	LR 2.000e-03
2: TRAIN [0][630/922]	Time 0.264 (0.293)	Data 7.99e-05 (9.36e-05)	Tok/s 4243 (6469)	Loss/tok 4.7442 (6.8659)	LR 2.000e-03
1: TRAIN [0][640/922]	Time 0.285 (0.293)	Data 8.13e-05 (8.51e-05)	Tok/s 6482 (6458)	Loss/tok 5.1450 (6.8497)	LR 2.000e-03
0: TRAIN [0][640/922]	Time 0.285 (0.293)	Data 7.75e-05 (8.04e-05)	Tok/s 6295 (6454)	Loss/tok 5.2050 (6.8487)	LR 2.000e-03
3: TRAIN [0][640/922]	Time 0.285 (0.293)	Data 7.92e-05 (8.61e-05)	Tok/s 6383 (6457)	Loss/tok 5.0776 (6.8431)	LR 2.000e-03
2: TRAIN [0][640/922]	Time 0.285 (0.293)	Data 9.73e-05 (9.35e-05)	Tok/s 6159 (6452)	Loss/tok 5.0827 (6.8449)	LR 2.000e-03
1: TRAIN [0][650/922]	Time 0.246 (0.293)	Data 7.46e-05 (8.50e-05)	Tok/s 2065 (6451)	Loss/tok 4.1893 (6.8267)	LR 2.000e-03
0: TRAIN [0][650/922]	Time 0.246 (0.293)	Data 7.82e-05 (8.03e-05)	Tok/s 2188 (6447)	Loss/tok 4.6860 (6.8256)	LR 2.000e-03
3: TRAIN [0][650/922]	Time 0.246 (0.293)	Data 8.23e-05 (8.61e-05)	Tok/s 2307 (6451)	Loss/tok 4.7447 (6.8201)	LR 2.000e-03
2: TRAIN [0][650/922]	Time 0.246 (0.293)	Data 8.56e-05 (9.33e-05)	Tok/s 2229 (6443)	Loss/tok 4.7939 (6.8224)	LR 2.000e-03
1: TRAIN [0][660/922]	Time 0.337 (0.293)	Data 8.51e-05 (8.49e-05)	Tok/s 9665 (6456)	Loss/tok 5.5375 (6.8020)	LR 2.000e-03
0: TRAIN [0][660/922]	Time 0.337 (0.293)	Data 7.53e-05 (8.02e-05)	Tok/s 9772 (6452)	Loss/tok 5.3934 (6.8006)	LR 2.000e-03
3: TRAIN [0][660/922]	Time 0.337 (0.293)	Data 1.03e-04 (8.61e-05)	Tok/s 9726 (6455)	Loss/tok 5.4619 (6.7952)	LR 2.000e-03
2: TRAIN [0][660/922]	Time 0.336 (0.293)	Data 8.77e-05 (9.32e-05)	Tok/s 9697 (6448)	Loss/tok 5.6027 (6.7976)	LR 2.000e-03
1: TRAIN [0][670/922]	Time 0.311 (0.293)	Data 7.82e-05 (8.48e-05)	Tok/s 7997 (6472)	Loss/tok 5.1559 (6.7749)	LR 2.000e-03
0: TRAIN [0][670/922]	Time 0.311 (0.293)	Data 8.32e-05 (8.01e-05)	Tok/s 8070 (6470)	Loss/tok 5.0851 (6.7718)	LR 2.000e-03
3: TRAIN [0][670/922]	Time 0.311 (0.293)	Data 6.84e-05 (8.60e-05)	Tok/s 7999 (6474)	Loss/tok 5.2861 (6.7661)	LR 2.000e-03
2: TRAIN [0][670/922]	Time 0.311 (0.293)	Data 7.51e-05 (9.29e-05)	Tok/s 7999 (6467)	Loss/tok 5.3252 (6.7695)	LR 2.000e-03
1: TRAIN [0][680/922]	Time 0.266 (0.293)	Data 8.13e-05 (8.47e-05)	Tok/s 4066 (6475)	Loss/tok 4.8869 (6.7502)	LR 2.000e-03
0: TRAIN [0][680/922]	Time 0.266 (0.293)	Data 6.68e-05 (8.01e-05)	Tok/s 4357 (6473)	Loss/tok 4.5951 (6.7477)	LR 2.000e-03
3: TRAIN [0][680/922]	Time 0.266 (0.293)	Data 8.25e-05 (8.61e-05)	Tok/s 4033 (6477)	Loss/tok 4.4388 (6.7421)	LR 2.000e-03
2: TRAIN [0][680/922]	Time 0.266 (0.293)	Data 8.30e-05 (9.29e-05)	Tok/s 4175 (6472)	Loss/tok 4.9682 (6.7460)	LR 2.000e-03
1: TRAIN [0][690/922]	Time 0.263 (0.293)	Data 8.34e-05 (8.46e-05)	Tok/s 4276 (6469)	Loss/tok 5.0579 (6.7290)	LR 2.000e-03
0: TRAIN [0][690/922]	Time 0.263 (0.293)	Data 8.25e-05 (8.01e-05)	Tok/s 3982 (6466)	Loss/tok 4.6912 (6.7270)	LR 2.000e-03
3: TRAIN [0][690/922]	Time 0.263 (0.293)	Data 9.80e-05 (8.60e-05)	Tok/s 4102 (6472)	Loss/tok 4.8692 (6.7199)	LR 2.000e-03
2: TRAIN [0][690/922]	Time 0.263 (0.293)	Data 8.61e-05 (9.30e-05)	Tok/s 4200 (6465)	Loss/tok 4.7923 (6.7248)	LR 2.000e-03
1: TRAIN [0][700/922]	Time 0.263 (0.292)	Data 8.77e-05 (8.47e-05)	Tok/s 4145 (6450)	Loss/tok 4.4412 (6.7102)	LR 2.000e-03
0: TRAIN [0][700/922]	Time 0.263 (0.292)	Data 8.51e-05 (8.01e-05)	Tok/s 4108 (6447)	Loss/tok 4.5396 (6.7081)	LR 2.000e-03
2: TRAIN [0][700/922]	Time 0.264 (0.292)	Data 8.61e-05 (9.29e-05)	Tok/s 4186 (6447)	Loss/tok 4.6917 (6.7072)	LR 2.000e-03
3: TRAIN [0][700/922]	Time 0.264 (0.292)	Data 8.37e-05 (8.60e-05)	Tok/s 4065 (6454)	Loss/tok 4.5973 (6.7009)	LR 2.000e-03
1: TRAIN [0][710/922]	Time 0.329 (0.292)	Data 8.30e-05 (8.46e-05)	Tok/s 9755 (6448)	Loss/tok 5.6179 (6.6886)	LR 2.000e-03
0: TRAIN [0][710/922]	Time 0.329 (0.292)	Data 7.44e-05 (8.00e-05)	Tok/s 10002 (6447)	Loss/tok 5.4705 (6.6868)	LR 2.000e-03
3: TRAIN [0][710/922]	Time 0.329 (0.292)	Data 9.11e-05 (8.60e-05)	Tok/s 9870 (6453)	Loss/tok 5.5230 (6.6797)	LR 2.000e-03
2: TRAIN [0][710/922]	Time 0.329 (0.292)	Data 9.80e-05 (9.28e-05)	Tok/s 9790 (6445)	Loss/tok 5.6511 (6.6851)	LR 2.000e-03
1: TRAIN [0][720/922]	Time 0.286 (0.292)	Data 8.39e-05 (8.46e-05)	Tok/s 6419 (6436)	Loss/tok 5.1026 (6.6689)	LR 2.000e-03
0: TRAIN [0][720/922]	Time 0.286 (0.292)	Data 8.11e-05 (8.00e-05)	Tok/s 6122 (6434)	Loss/tok 4.7386 (6.6664)	LR 2.000e-03
3: TRAIN [0][720/922]	Time 0.286 (0.292)	Data 9.94e-05 (8.60e-05)	Tok/s 6465 (6442)	Loss/tok 4.9979 (6.6603)	LR 2.000e-03
2: TRAIN [0][720/922]	Time 0.286 (0.292)	Data 9.66e-05 (9.27e-05)	Tok/s 6220 (6434)	Loss/tok 4.8006 (6.6656)	LR 2.000e-03
1: TRAIN [0][730/922]	Time 0.313 (0.292)	Data 8.34e-05 (8.48e-05)	Tok/s 8003 (6436)	Loss/tok 5.2104 (6.6469)	LR 2.000e-03
0: TRAIN [0][730/922]	Time 0.313 (0.292)	Data 6.82e-05 (8.00e-05)	Tok/s 7912 (6433)	Loss/tok 5.1257 (6.6446)	LR 2.000e-03
3: TRAIN [0][730/922]	Time 0.313 (0.292)	Data 6.56e-05 (8.60e-05)	Tok/s 8040 (6441)	Loss/tok 5.0301 (6.6383)	LR 2.000e-03
2: TRAIN [0][730/922]	Time 0.313 (0.292)	Data 7.94e-05 (9.26e-05)	Tok/s 8142 (6434)	Loss/tok 5.3281 (6.6437)	LR 2.000e-03
1: TRAIN [0][740/922]	Time 0.346 (0.292)	Data 9.42e-05 (8.47e-05)	Tok/s 9430 (6439)	Loss/tok 5.2244 (6.6241)	LR 2.000e-03
0: TRAIN [0][740/922]	Time 0.346 (0.292)	Data 6.84e-05 (7.99e-05)	Tok/s 9559 (6436)	Loss/tok 5.2013 (6.6230)	LR 2.000e-03
3: TRAIN [0][740/922]	Time 0.346 (0.292)	Data 7.87e-05 (8.59e-05)	Tok/s 9587 (6443)	Loss/tok 5.4574 (6.6169)	LR 2.000e-03
2: TRAIN [0][740/922]	Time 0.346 (0.292)	Data 7.46e-05 (9.24e-05)	Tok/s 9515 (6437)	Loss/tok 5.4363 (6.6203)	LR 2.000e-03
0: TRAIN [0][750/922]	Time 0.291 (0.292)	Data 7.61e-05 (7.99e-05)	Tok/s 6315 (6438)	Loss/tok 4.8231 (6.5998)	LR 2.000e-03
1: TRAIN [0][750/922]	Time 0.291 (0.292)	Data 9.08e-05 (8.47e-05)	Tok/s 5823 (6441)	Loss/tok 4.9217 (6.6018)	LR 2.000e-03
3: TRAIN [0][750/922]	Time 0.291 (0.292)	Data 6.99e-05 (8.58e-05)	Tok/s 6187 (6444)	Loss/tok 5.0389 (6.5953)	LR 2.000e-03
2: TRAIN [0][750/922]	Time 0.291 (0.292)	Data 7.51e-05 (9.23e-05)	Tok/s 5961 (6439)	Loss/tok 4.8878 (6.5987)	LR 2.000e-03
1: TRAIN [0][760/922]	Time 0.286 (0.292)	Data 9.08e-05 (8.46e-05)	Tok/s 6143 (6446)	Loss/tok 4.7364 (6.5790)	LR 2.000e-03
0: TRAIN [0][760/922]	Time 0.286 (0.292)	Data 7.08e-05 (7.98e-05)	Tok/s 6453 (6443)	Loss/tok 5.2034 (6.5776)	LR 2.000e-03
3: TRAIN [0][760/922]	Time 0.286 (0.292)	Data 7.82e-05 (8.57e-05)	Tok/s 6269 (6449)	Loss/tok 4.5851 (6.5732)	LR 2.000e-03
2: TRAIN [0][760/922]	Time 0.286 (0.292)	Data 8.56e-05 (9.22e-05)	Tok/s 6426 (6444)	Loss/tok 4.7556 (6.5756)	LR 2.000e-03
1: TRAIN [0][770/922]	Time 0.342 (0.292)	Data 7.96e-05 (8.46e-05)	Tok/s 9510 (6445)	Loss/tok 5.1903 (6.5581)	LR 2.000e-03
0: TRAIN [0][770/922]	Time 0.342 (0.292)	Data 7.84e-05 (7.98e-05)	Tok/s 9463 (6443)	Loss/tok 5.4378 (6.5566)	LR 2.000e-03
3: TRAIN [0][770/922]	Time 0.342 (0.292)	Data 8.96e-05 (8.57e-05)	Tok/s 9650 (6448)	Loss/tok 5.4407 (6.5537)	LR 2.000e-03
2: TRAIN [0][770/922]	Time 0.342 (0.292)	Data 8.70e-05 (9.20e-05)	Tok/s 9561 (6443)	Loss/tok 5.5811 (6.5564)	LR 2.000e-03
1: TRAIN [0][780/922]	Time 0.267 (0.292)	Data 1.07e-04 (8.46e-05)	Tok/s 4266 (6451)	Loss/tok 4.7300 (6.5359)	LR 2.000e-03
0: TRAIN [0][780/922]	Time 0.267 (0.292)	Data 6.87e-05 (7.99e-05)	Tok/s 3933 (6449)	Loss/tok 4.3804 (6.5343)	LR 2.000e-03
3: TRAIN [0][780/922]	Time 0.267 (0.292)	Data 1.26e-04 (8.57e-05)	Tok/s 4225 (6454)	Loss/tok 4.6689 (6.5306)	LR 2.000e-03
2: TRAIN [0][780/922]	Time 0.267 (0.292)	Data 7.63e-05 (9.18e-05)	Tok/s 4104 (6450)	Loss/tok 4.2344 (6.5324)	LR 2.000e-03
1: TRAIN [0][790/922]	Time 0.284 (0.292)	Data 1.16e-04 (8.45e-05)	Tok/s 6458 (6452)	Loss/tok 4.5669 (6.5153)	LR 2.000e-03
0: TRAIN [0][790/922]	Time 0.284 (0.292)	Data 8.18e-05 (7.99e-05)	Tok/s 6230 (6450)	Loss/tok 4.7201 (6.5130)	LR 2.000e-03
3: TRAIN [0][790/922]	Time 0.284 (0.292)	Data 7.37e-05 (8.57e-05)	Tok/s 6303 (6455)	Loss/tok 4.7597 (6.5103)	LR 2.000e-03
2: TRAIN [0][790/922]	Time 0.284 (0.292)	Data 8.03e-05 (9.17e-05)	Tok/s 6186 (6451)	Loss/tok 4.6607 (6.5110)	LR 2.000e-03
1: TRAIN [0][800/922]	Time 0.266 (0.293)	Data 7.82e-05 (8.45e-05)	Tok/s 4107 (6464)	Loss/tok 4.6113 (6.4915)	LR 2.000e-03
0: TRAIN [0][800/922]	Time 0.266 (0.293)	Data 6.82e-05 (7.98e-05)	Tok/s 4135 (6462)	Loss/tok 4.5810 (6.4899)	LR 2.000e-03
3: TRAIN [0][800/922]	Time 0.265 (0.293)	Data 8.82e-05 (8.56e-05)	Tok/s 4080 (6468)	Loss/tok 4.2469 (6.4862)	LR 2.000e-03
2: TRAIN [0][800/922]	Time 0.265 (0.293)	Data 7.82e-05 (9.16e-05)	Tok/s 3944 (6462)	Loss/tok 4.7110 (6.4876)	LR 2.000e-03
1: TRAIN [0][810/922]	Time 0.264 (0.292)	Data 9.18e-05 (8.45e-05)	Tok/s 4105 (6452)	Loss/tok 4.4448 (6.4747)	LR 2.000e-03
0: TRAIN [0][810/922]	Time 0.264 (0.292)	Data 8.58e-05 (7.99e-05)	Tok/s 3995 (6450)	Loss/tok 4.4475 (6.4730)	LR 2.000e-03
3: TRAIN [0][810/922]	Time 0.264 (0.292)	Data 7.51e-05 (8.56e-05)	Tok/s 4116 (6455)	Loss/tok 4.7142 (6.4704)	LR 2.000e-03
2: TRAIN [0][810/922]	Time 0.264 (0.292)	Data 8.89e-05 (9.15e-05)	Tok/s 3922 (6449)	Loss/tok 4.1988 (6.4706)	LR 2.000e-03
1: TRAIN [0][820/922]	Time 0.291 (0.292)	Data 9.37e-05 (8.44e-05)	Tok/s 6348 (6450)	Loss/tok 4.8843 (6.4557)	LR 2.000e-03
0: TRAIN [0][820/922]	Time 0.291 (0.292)	Data 7.37e-05 (7.99e-05)	Tok/s 6203 (6448)	Loss/tok 4.7731 (6.4535)	LR 2.000e-03
3: TRAIN [0][820/922]	Time 0.291 (0.292)	Data 7.51e-05 (8.56e-05)	Tok/s 6192 (6452)	Loss/tok 4.6995 (6.4515)	LR 2.000e-03
2: TRAIN [0][820/922]	Time 0.291 (0.292)	Data 8.39e-05 (9.14e-05)	Tok/s 6300 (6447)	Loss/tok 4.9311 (6.4506)	LR 2.000e-03
1: TRAIN [0][830/922]	Time 0.307 (0.292)	Data 8.44e-05 (8.44e-05)	Tok/s 8243 (6448)	Loss/tok 5.2146 (6.4376)	LR 2.000e-03
0: TRAIN [0][830/922]	Time 0.307 (0.292)	Data 9.73e-05 (7.99e-05)	Tok/s 8277 (6447)	Loss/tok 4.9491 (6.4340)	LR 2.000e-03
3: TRAIN [0][830/922]	Time 0.307 (0.292)	Data 1.02e-04 (8.55e-05)	Tok/s 8167 (6449)	Loss/tok 5.0810 (6.4329)	LR 2.000e-03
2: TRAIN [0][830/922]	Time 0.307 (0.292)	Data 9.25e-05 (9.14e-05)	Tok/s 8345 (6445)	Loss/tok 5.2144 (6.4322)	LR 2.000e-03
1: TRAIN [0][840/922]	Time 0.283 (0.292)	Data 8.30e-05 (8.43e-05)	Tok/s 6293 (6455)	Loss/tok 4.5227 (6.4172)	LR 2.000e-03
0: TRAIN [0][840/922]	Time 0.283 (0.292)	Data 6.89e-05 (7.98e-05)	Tok/s 6173 (6453)	Loss/tok 4.6785 (6.4133)	LR 2.000e-03
3: TRAIN [0][840/922]	Time 0.283 (0.292)	Data 7.01e-05 (8.55e-05)	Tok/s 6351 (6457)	Loss/tok 4.6596 (6.4128)	LR 2.000e-03
2: TRAIN [0][840/922]	Time 0.283 (0.292)	Data 1.13e-04 (9.13e-05)	Tok/s 6453 (6452)	Loss/tok 4.4894 (6.4112)	LR 2.000e-03
1: TRAIN [0][850/922]	Time 0.288 (0.292)	Data 9.87e-05 (8.43e-05)	Tok/s 6295 (6456)	Loss/tok 4.6174 (6.3979)	LR 2.000e-03
0: TRAIN [0][850/922]	Time 0.289 (0.292)	Data 6.60e-05 (7.98e-05)	Tok/s 6264 (6454)	Loss/tok 4.7775 (6.3937)	LR 2.000e-03
3: TRAIN [0][850/922]	Time 0.289 (0.292)	Data 7.41e-05 (8.55e-05)	Tok/s 6356 (6457)	Loss/tok 4.5201 (6.3935)	LR 2.000e-03
2: TRAIN [0][850/922]	Time 0.289 (0.292)	Data 8.30e-05 (9.11e-05)	Tok/s 6471 (6453)	Loss/tok 4.4424 (6.3921)	LR 2.000e-03
1: TRAIN [0][860/922]	Time 0.287 (0.293)	Data 8.13e-05 (8.42e-05)	Tok/s 6322 (6463)	Loss/tok 4.5587 (6.3779)	LR 2.000e-03
0: TRAIN [0][860/922]	Time 0.287 (0.293)	Data 6.84e-05 (7.97e-05)	Tok/s 6352 (6462)	Loss/tok 4.6598 (6.3734)	LR 2.000e-03
3: TRAIN [0][860/922]	Time 0.287 (0.293)	Data 8.42e-05 (8.55e-05)	Tok/s 6230 (6464)	Loss/tok 4.6619 (6.3732)	LR 2.000e-03
2: TRAIN [0][860/922]	Time 0.287 (0.293)	Data 9.16e-05 (9.11e-05)	Tok/s 6191 (6460)	Loss/tok 4.7137 (6.3717)	LR 2.000e-03
1: TRAIN [0][870/922]	Time 0.248 (0.292)	Data 9.18e-05 (8.42e-05)	Tok/s 2325 (6458)	Loss/tok 4.0902 (6.3597)	LR 2.000e-03
0: TRAIN [0][870/922]	Time 0.248 (0.292)	Data 7.96e-05 (7.98e-05)	Tok/s 2213 (6456)	Loss/tok 4.1708 (6.3555)	LR 2.000e-03
3: TRAIN [0][870/922]	Time 0.248 (0.292)	Data 8.49e-05 (8.58e-05)	Tok/s 2297 (6459)	Loss/tok 4.1330 (6.3552)	LR 2.000e-03
2: TRAIN [0][870/922]	Time 0.248 (0.292)	Data 8.87e-05 (9.09e-05)	Tok/s 2224 (6456)	Loss/tok 4.0875 (6.3541)	LR 2.000e-03
1: TRAIN [0][880/922]	Time 0.289 (0.292)	Data 8.34e-05 (8.41e-05)	Tok/s 6298 (6456)	Loss/tok 4.5881 (6.3423)	LR 2.000e-03
0: TRAIN [0][880/922]	Time 0.289 (0.292)	Data 7.08e-05 (7.97e-05)	Tok/s 6290 (6454)	Loss/tok 4.5615 (6.3376)	LR 2.000e-03
2: TRAIN [0][880/922]	Time 0.289 (0.292)	Data 8.11e-05 (9.09e-05)	Tok/s 6335 (6456)	Loss/tok 4.6552 (6.3361)	LR 2.000e-03
3: TRAIN [0][880/922]	Time 0.289 (0.292)	Data 9.30e-05 (8.58e-05)	Tok/s 6220 (6457)	Loss/tok 4.5970 (6.3377)	LR 2.000e-03
1: TRAIN [0][890/922]	Time 0.286 (0.292)	Data 7.87e-05 (8.41e-05)	Tok/s 6249 (6457)	Loss/tok 4.3426 (6.3225)	LR 2.000e-03
0: TRAIN [0][890/922]	Time 0.286 (0.292)	Data 6.84e-05 (7.97e-05)	Tok/s 6298 (6455)	Loss/tok 4.7903 (6.3191)	LR 2.000e-03
3: TRAIN [0][890/922]	Time 0.286 (0.292)	Data 9.01e-05 (8.59e-05)	Tok/s 6378 (6459)	Loss/tok 4.4376 (6.3196)	LR 2.000e-03
2: TRAIN [0][890/922]	Time 0.286 (0.292)	Data 7.72e-05 (9.08e-05)	Tok/s 6127 (6456)	Loss/tok 4.4693 (6.3174)	LR 2.000e-03
1: TRAIN [0][900/922]	Time 0.265 (0.292)	Data 7.72e-05 (8.41e-05)	Tok/s 4124 (6451)	Loss/tok 4.3393 (6.3053)	LR 2.000e-03
0: TRAIN [0][900/922]	Time 0.265 (0.292)	Data 6.84e-05 (7.97e-05)	Tok/s 4052 (6450)	Loss/tok 4.6278 (6.3030)	LR 2.000e-03
3: TRAIN [0][900/922]	Time 0.265 (0.292)	Data 7.22e-05 (8.58e-05)	Tok/s 4096 (6453)	Loss/tok 4.2062 (6.3031)	LR 2.000e-03
2: TRAIN [0][900/922]	Time 0.265 (0.292)	Data 8.23e-05 (9.07e-05)	Tok/s 4032 (6449)	Loss/tok 4.2453 (6.3003)	LR 2.000e-03
1: TRAIN [0][910/922]	Time 0.309 (0.293)	Data 7.56e-05 (8.40e-05)	Tok/s 8057 (6464)	Loss/tok 4.7092 (6.2858)	LR 2.000e-03
0: TRAIN [0][910/922]	Time 0.309 (0.293)	Data 6.87e-05 (7.96e-05)	Tok/s 7931 (6463)	Loss/tok 4.7595 (6.2832)	LR 2.000e-03
3: TRAIN [0][910/922]	Time 0.309 (0.293)	Data 8.80e-05 (8.58e-05)	Tok/s 8402 (6467)	Loss/tok 4.7932 (6.2833)	LR 2.000e-03
2: TRAIN [0][910/922]	Time 0.309 (0.293)	Data 8.13e-05 (9.06e-05)	Tok/s 8078 (6461)	Loss/tok 4.8016 (6.2808)	LR 2.000e-03
1: TRAIN [0][920/922]	Time 0.290 (0.292)	Data 3.34e-05 (8.44e-05)	Tok/s 6332 (6462)	Loss/tok 4.5563 (6.2684)	LR 2.000e-03
0: TRAIN [0][920/922]	Time 0.290 (0.292)	Data 3.36e-05 (8.00e-05)	Tok/s 6168 (6461)	Loss/tok 4.6530 (6.2662)	LR 2.000e-03
3: TRAIN [0][920/922]	Time 0.290 (0.292)	Data 3.19e-05 (8.62e-05)	Tok/s 6223 (6466)	Loss/tok 4.5605 (6.2658)	LR 2.000e-03
2: TRAIN [0][920/922]	Time 0.290 (0.292)	Data 3.50e-05 (9.11e-05)	Tok/s 6547 (6460)	Loss/tok 4.3752 (6.2628)	LR 2.000e-03
2: Running validation on dev set
3: Running validation on dev set
1: Running validation on dev set
2: Executing preallocation
0: Running validation on dev set
3: Executing preallocation
1: Executing preallocation
0: Executing preallocation
3: VALIDATION [0][0/159]	Time 0.055 (0.000)	Data 6.92e-04 (0.00e+00)	Tok/s 23070 (0)	Loss/tok 6.1993 (6.1993)
2: VALIDATION [0][0/159]	Time 0.059 (0.000)	Data 7.83e-04 (0.00e+00)	Tok/s 22759 (0)	Loss/tok 5.9788 (5.9788)
1: VALIDATION [0][0/160]	Time 0.065 (0.000)	Data 7.31e-04 (0.00e+00)	Tok/s 22595 (0)	Loss/tok 6.6433 (6.6433)
0: VALIDATION [0][0/160]	Time 0.077 (0.000)	Data 7.06e-04 (0.00e+00)	Tok/s 21203 (0)	Loss/tok 6.3305 (6.3305)
3: VALIDATION [0][10/159]	Time 0.039 (0.043)	Data 5.06e-04 (5.16e-04)	Tok/s 21759 (22306)	Loss/tok 5.7805 (5.9839)
2: VALIDATION [0][10/159]	Time 0.038 (0.044)	Data 6.06e-04 (5.72e-04)	Tok/s 22395 (22216)	Loss/tok 5.8184 (6.0633)
1: VALIDATION [0][10/160]	Time 0.039 (0.044)	Data 5.08e-04 (5.34e-04)	Tok/s 22384 (22441)	Loss/tok 6.0357 (6.1058)
0: VALIDATION [0][10/160]	Time 0.039 (0.045)	Data 5.34e-04 (5.37e-04)	Tok/s 22216 (22018)	Loss/tok 5.7416 (6.0764)
3: VALIDATION [0][20/159]	Time 0.032 (0.039)	Data 5.06e-04 (5.13e-04)	Tok/s 22471 (22134)	Loss/tok 5.8325 (5.9658)
2: VALIDATION [0][20/159]	Time 0.033 (0.040)	Data 5.71e-04 (5.77e-04)	Tok/s 21874 (22093)	Loss/tok 6.0898 (6.0227)
1: VALIDATION [0][20/160]	Time 0.033 (0.040)	Data 5.01e-04 (5.21e-04)	Tok/s 22143 (22290)	Loss/tok 6.1182 (6.0004)
0: VALIDATION [0][20/160]	Time 0.034 (0.041)	Data 5.20e-04 (5.42e-04)	Tok/s 21898 (21899)	Loss/tok 5.5529 (6.0167)
3: VALIDATION [0][30/159]	Time 0.029 (0.037)	Data 4.98e-04 (5.14e-04)	Tok/s 22282 (22046)	Loss/tok 5.7003 (5.9035)
2: VALIDATION [0][30/159]	Time 0.031 (0.037)	Data 5.83e-04 (5.80e-04)	Tok/s 20959 (21921)	Loss/tok 5.9475 (5.9915)
1: VALIDATION [0][30/160]	Time 0.030 (0.037)	Data 4.99e-04 (5.16e-04)	Tok/s 21953 (22067)	Loss/tok 6.0293 (5.9649)
0: VALIDATION [0][30/160]	Time 0.030 (0.038)	Data 5.34e-04 (5.35e-04)	Tok/s 21591 (21866)	Loss/tok 6.1105 (5.9355)
3: VALIDATION [0][40/159]	Time 0.027 (0.035)	Data 4.96e-04 (5.11e-04)	Tok/s 21500 (21935)	Loss/tok 5.2686 (5.8862)
2: VALIDATION [0][40/159]	Time 0.027 (0.035)	Data 6.06e-04 (5.80e-04)	Tok/s 21629 (21795)	Loss/tok 5.3612 (5.9211)
1: VALIDATION [0][40/160]	Time 0.027 (0.035)	Data 4.92e-04 (5.12e-04)	Tok/s 21952 (21958)	Loss/tok 5.9541 (5.9317)
0: VALIDATION [0][40/160]	Time 0.028 (0.035)	Data 5.14e-04 (5.32e-04)	Tok/s 20928 (21777)	Loss/tok 5.2295 (5.9094)
3: VALIDATION [0][50/159]	Time 0.026 (0.033)	Data 4.99e-04 (5.09e-04)	Tok/s 20224 (21743)	Loss/tok 5.7652 (5.8491)
2: VALIDATION [0][50/159]	Time 0.026 (0.033)	Data 5.44e-04 (5.78e-04)	Tok/s 20696 (21644)	Loss/tok 5.8187 (5.8766)
1: VALIDATION [0][50/160]	Time 0.026 (0.033)	Data 4.99e-04 (5.10e-04)	Tok/s 20295 (21815)	Loss/tok 5.8586 (5.8726)
0: VALIDATION [0][50/160]	Time 0.026 (0.034)	Data 5.30e-04 (5.29e-04)	Tok/s 20511 (21575)	Loss/tok 5.4480 (5.8605)
3: VALIDATION [0][60/159]	Time 0.023 (0.031)	Data 4.97e-04 (5.07e-04)	Tok/s 20821 (21623)	Loss/tok 5.2238 (5.8218)
2: VALIDATION [0][60/159]	Time 0.024 (0.032)	Data 5.64e-04 (5.76e-04)	Tok/s 20608 (21479)	Loss/tok 5.9101 (5.8471)
1: VALIDATION [0][60/160]	Time 0.024 (0.032)	Data 4.91e-04 (5.07e-04)	Tok/s 20127 (21639)	Loss/tok 5.5573 (5.8295)
0: VALIDATION [0][60/160]	Time 0.024 (0.032)	Data 5.35e-04 (5.29e-04)	Tok/s 19919 (21370)	Loss/tok 5.5865 (5.8270)
3: VALIDATION [0][70/159]	Time 0.022 (0.030)	Data 4.92e-04 (5.06e-04)	Tok/s 19929 (21405)	Loss/tok 5.4403 (5.7968)
1: VALIDATION [0][70/160]	Time 0.022 (0.030)	Data 4.91e-04 (5.08e-04)	Tok/s 20462 (21526)	Loss/tok 5.5436 (5.8084)
2: VALIDATION [0][70/159]	Time 0.022 (0.030)	Data 5.58e-04 (5.74e-04)	Tok/s 20785 (21334)	Loss/tok 5.8373 (5.8307)
0: VALIDATION [0][70/160]	Time 0.022 (0.031)	Data 5.14e-04 (5.27e-04)	Tok/s 19968 (21306)	Loss/tok 5.2408 (5.7852)
3: VALIDATION [0][80/159]	Time 0.019 (0.029)	Data 4.91e-04 (5.05e-04)	Tok/s 20622 (21307)	Loss/tok 5.7941 (5.7715)
2: VALIDATION [0][80/159]	Time 0.020 (0.029)	Data 5.89e-04 (5.70e-04)	Tok/s 20367 (21206)	Loss/tok 5.3138 (5.7969)
1: VALIDATION [0][80/160]	Time 0.020 (0.029)	Data 4.92e-04 (5.06e-04)	Tok/s 20343 (21352)	Loss/tok 5.3547 (5.7944)
0: VALIDATION [0][80/160]	Time 0.020 (0.030)	Data 6.45e-04 (5.27e-04)	Tok/s 20099 (21188)	Loss/tok 5.4419 (5.7633)
3: VALIDATION [0][90/159]	Time 0.018 (0.028)	Data 4.94e-04 (5.03e-04)	Tok/s 19966 (21180)	Loss/tok 5.3536 (5.7561)
1: VALIDATION [0][90/160]	Time 0.018 (0.028)	Data 4.87e-04 (5.05e-04)	Tok/s 19988 (21211)	Loss/tok 5.2489 (5.7694)
2: VALIDATION [0][90/159]	Time 0.019 (0.028)	Data 5.33e-04 (5.68e-04)	Tok/s 19771 (21068)	Loss/tok 5.3233 (5.7673)
0: VALIDATION [0][90/160]	Time 0.019 (0.028)	Data 5.06e-04 (5.25e-04)	Tok/s 19675 (21031)	Loss/tok 5.4101 (5.7429)
3: VALIDATION [0][100/159]	Time 0.017 (0.027)	Data 4.92e-04 (5.02e-04)	Tok/s 19181 (20984)	Loss/tok 5.9233 (5.7379)
2: VALIDATION [0][100/159]	Time 0.017 (0.027)	Data 5.21e-04 (5.65e-04)	Tok/s 19318 (20896)	Loss/tok 5.0129 (5.7427)
1: VALIDATION [0][100/160]	Time 0.018 (0.027)	Data 4.90e-04 (5.04e-04)	Tok/s 19018 (21010)	Loss/tok 5.6680 (5.7570)
0: VALIDATION [0][100/160]	Time 0.018 (0.027)	Data 5.06e-04 (5.23e-04)	Tok/s 19166 (20880)	Loss/tok 5.9744 (5.7306)
3: VALIDATION [0][110/159]	Time 0.016 (0.026)	Data 5.06e-04 (5.01e-04)	Tok/s 18316 (20765)	Loss/tok 5.1084 (5.7174)
2: VALIDATION [0][110/159]	Time 0.017 (0.026)	Data 5.61e-04 (5.64e-04)	Tok/s 18139 (20683)	Loss/tok 5.8956 (5.7273)
1: VALIDATION [0][110/160]	Time 0.017 (0.026)	Data 4.91e-04 (5.04e-04)	Tok/s 17882 (20799)	Loss/tok 5.5926 (5.7320)
0: VALIDATION [0][110/160]	Time 0.017 (0.026)	Data 5.16e-04 (5.21e-04)	Tok/s 18398 (20659)	Loss/tok 5.4594 (5.7087)
3: VALIDATION [0][120/159]	Time 0.015 (0.025)	Data 4.88e-04 (5.00e-04)	Tok/s 17867 (20537)	Loss/tok 5.6618 (5.6960)
1: VALIDATION [0][120/160]	Time 0.016 (0.025)	Data 4.85e-04 (5.03e-04)	Tok/s 17157 (20564)	Loss/tok 5.3945 (5.7077)
2: VALIDATION [0][120/159]	Time 0.016 (0.025)	Data 5.24e-04 (5.62e-04)	Tok/s 17414 (20436)	Loss/tok 5.2876 (5.7206)
0: VALIDATION [0][120/160]	Time 0.016 (0.026)	Data 6.41e-04 (5.21e-04)	Tok/s 17432 (20439)	Loss/tok 5.6321 (5.6942)
3: VALIDATION [0][130/159]	Time 0.013 (0.024)	Data 4.89e-04 (4.99e-04)	Tok/s 17871 (20281)	Loss/tok 5.0007 (5.6758)
2: VALIDATION [0][130/159]	Time 0.014 (0.025)	Data 5.19e-04 (5.60e-04)	Tok/s 16854 (20209)	Loss/tok 5.4418 (5.6984)
1: VALIDATION [0][130/160]	Time 0.014 (0.024)	Data 4.92e-04 (5.02e-04)	Tok/s 17342 (20321)	Loss/tok 5.1010 (5.6913)
0: VALIDATION [0][130/160]	Time 0.013 (0.025)	Data 5.06e-04 (5.19e-04)	Tok/s 19073 (20203)	Loss/tok 5.2943 (5.6784)
3: VALIDATION [0][140/159]	Time 0.012 (0.023)	Data 4.89e-04 (4.99e-04)	Tok/s 16236 (20084)	Loss/tok 5.9226 (5.6668)
1: VALIDATION [0][140/160]	Time 0.012 (0.024)	Data 4.89e-04 (5.01e-04)	Tok/s 16360 (20138)	Loss/tok 5.1618 (5.6720)
2: VALIDATION [0][140/159]	Time 0.012 (0.024)	Data 5.17e-04 (5.58e-04)	Tok/s 17275 (20005)	Loss/tok 5.2775 (5.6857)
0: VALIDATION [0][140/160]	Time 0.012 (0.024)	Data 5.03e-04 (5.18e-04)	Tok/s 17190 (19996)	Loss/tok 4.7822 (5.6651)
3: VALIDATION [0][150/159]	Time 0.010 (0.023)	Data 4.82e-04 (4.99e-04)	Tok/s 15274 (19813)	Loss/tok 4.7495 (5.6485)
1: VALIDATION [0][150/160]	Time 0.010 (0.023)	Data 4.80e-04 (4.99e-04)	Tok/s 15689 (19886)	Loss/tok 4.1013 (5.6503)
2: VALIDATION [0][150/159]	Time 0.011 (0.023)	Data 5.20e-04 (5.55e-04)	Tok/s 15038 (19737)	Loss/tok 5.4898 (5.6663)
0: VALIDATION [0][150/160]	Time 0.010 (0.023)	Data 4.86e-04 (5.16e-04)	Tok/s 15773 (19736)	Loss/tok 5.3255 (5.6487)
0: Saving model to gnmt/model_best.pth
3: Running evaluation on test set
1: Running evaluation on test set
2: Running evaluation on test set
0: Running evaluation on test set
0: TEST [0][9/24]	Time 0.5493 (0.7037)	Decoder iters 149.0 (144.0)	Tok/s 3974 (4567)
3: TEST [0][9/24]	Time 0.5491 (0.7040)	Decoder iters 80.0 (142.1)	Tok/s 3591 (4037)
1: TEST [0][9/24]	Time 0.5491 (0.7040)	Decoder iters 67.0 (140.8)	Tok/s 3719 (4392)
2: TEST [0][9/24]	Time 0.5489 (0.7038)	Decoder iters 149.0 (149.0)	Tok/s 3694 (4069)
0: TEST [0][19/24]	Time 0.2015 (0.5651)	Decoder iters 42.0 (123.5)	Tok/s 5455 (4171)
3: TEST [0][19/24]	Time 0.2015 (0.5653)	Decoder iters 40.0 (111.4)	Tok/s 4974 (3842)
1: TEST [0][19/24]	Time 0.2015 (0.5652)	Decoder iters 37.0 (112.7)	Tok/s 4953 (4046)
2: TEST [0][19/24]	Time 0.2013 (0.5651)	Decoder iters 51.0 (126.9)	Tok/s 5051 (3848)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
3: Finished evaluation on test set
0: Finished evaluation on test set
2: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
3: Finished epoch 0
3: Starting epoch 1
2: Finished epoch 0
2: Starting epoch 1
1: Executing preallocation
3: Executing preallocation
2: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.2640	Validation Loss: 5.6393	Test BLEU: 3.63
0: Performance: Epoch: 0	Training: 25848 Tok/s	Validation: 77896 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
3: Sampler for epoch 1 uses seed 2606193617
1: Sampler for epoch 1 uses seed 2606193617
0: Sampler for epoch 1 uses seed 2606193617
2: Sampler for epoch 1 uses seed 2606193617
1: TRAIN [1][0/922]	Time 0.465 (0.000)	Data 1.29e-01 (0.00e+00)	Tok/s 6764 (0)	Loss/tok 4.7245 (4.7245)	LR 2.000e-03
0: TRAIN [1][0/922]	Time 0.459 (0.000)	Data 1.26e-01 (0.00e+00)	Tok/s 7055 (0)	Loss/tok 4.6249 (4.6249)	LR 2.000e-03
3: TRAIN [1][0/922]	Time 0.466 (0.000)	Data 1.20e-01 (0.00e+00)	Tok/s 6970 (0)	Loss/tok 4.7749 (4.7749)	LR 2.000e-03
2: TRAIN [1][0/922]	Time 0.463 (0.000)	Data 1.55e-01 (0.00e+00)	Tok/s 7073 (0)	Loss/tok 4.5786 (4.5786)	LR 2.000e-03
1: TRAIN [1][10/922]	Time 0.311 (0.286)	Data 8.34e-05 (8.40e-05)	Tok/s 8140 (5876)	Loss/tok 4.2702 (4.3987)	LR 2.000e-03
3: TRAIN [1][10/922]	Time 0.311 (0.286)	Data 1.12e-04 (7.80e-05)	Tok/s 8354 (5851)	Loss/tok 4.3797 (4.4818)	LR 2.000e-03
0: TRAIN [1][10/922]	Time 0.311 (0.286)	Data 1.08e-04 (7.59e-05)	Tok/s 8033 (5851)	Loss/tok 4.5017 (4.4013)	LR 2.000e-03
2: TRAIN [1][10/922]	Time 0.311 (0.286)	Data 7.46e-05 (7.24e-05)	Tok/s 8007 (5745)	Loss/tok 4.4615 (4.3792)	LR 2.000e-03
1: TRAIN [1][20/922]	Time 0.283 (0.281)	Data 9.23e-05 (8.25e-05)	Tok/s 6180 (5515)	Loss/tok 4.2344 (4.2966)	LR 2.000e-03
0: TRAIN [1][20/922]	Time 0.283 (0.281)	Data 8.01e-05 (8.79e-05)	Tok/s 6410 (5514)	Loss/tok 4.2517 (4.3269)	LR 2.000e-03
3: TRAIN [1][20/922]	Time 0.283 (0.281)	Data 7.39e-05 (7.55e-05)	Tok/s 6494 (5560)	Loss/tok 4.2653 (4.3439)	LR 2.000e-03
2: TRAIN [1][20/922]	Time 0.284 (0.281)	Data 7.80e-05 (7.23e-05)	Tok/s 6255 (5486)	Loss/tok 4.1840 (4.2803)	LR 2.000e-03
1: TRAIN [1][30/922]	Time 0.312 (0.283)	Data 9.16e-05 (8.14e-05)	Tok/s 8026 (5720)	Loss/tok 4.4095 (4.2796)	LR 2.000e-03
0: TRAIN [1][30/922]	Time 0.311 (0.283)	Data 6.58e-05 (8.39e-05)	Tok/s 7972 (5740)	Loss/tok 4.3133 (4.3268)	LR 2.000e-03
3: TRAIN [1][30/922]	Time 0.312 (0.283)	Data 7.06e-05 (7.44e-05)	Tok/s 8048 (5756)	Loss/tok 4.2601 (4.3718)	LR 2.000e-03
2: TRAIN [1][30/922]	Time 0.311 (0.283)	Data 6.96e-05 (7.23e-05)	Tok/s 8014 (5721)	Loss/tok 4.6140 (4.3071)	LR 2.000e-03
1: TRAIN [1][40/922]	Time 0.312 (0.286)	Data 8.65e-05 (8.16e-05)	Tok/s 8131 (5936)	Loss/tok 4.6120 (4.3121)	LR 2.000e-03
0: TRAIN [1][40/922]	Time 0.312 (0.286)	Data 8.15e-05 (8.12e-05)	Tok/s 8049 (5940)	Loss/tok 4.4185 (4.3106)	LR 2.000e-03
3: TRAIN [1][40/922]	Time 0.312 (0.286)	Data 7.20e-05 (7.54e-05)	Tok/s 7993 (5952)	Loss/tok 4.3463 (4.3566)	LR 2.000e-03
2: TRAIN [1][40/922]	Time 0.312 (0.286)	Data 7.80e-05 (7.26e-05)	Tok/s 8234 (5926)	Loss/tok 4.6698 (4.3071)	LR 2.000e-03
1: TRAIN [1][50/922]	Time 0.346 (0.289)	Data 7.68e-05 (8.40e-05)	Tok/s 9492 (6203)	Loss/tok 4.7982 (4.3707)	LR 2.000e-03
0: TRAIN [1][50/922]	Time 0.346 (0.289)	Data 7.34e-05 (8.01e-05)	Tok/s 9628 (6200)	Loss/tok 4.8154 (4.3500)	LR 2.000e-03
3: TRAIN [1][50/922]	Time 0.346 (0.289)	Data 6.77e-05 (7.53e-05)	Tok/s 9433 (6222)	Loss/tok 4.6681 (4.3774)	LR 2.000e-03
2: TRAIN [1][50/922]	Time 0.346 (0.289)	Data 6.72e-05 (7.18e-05)	Tok/s 9470 (6194)	Loss/tok 4.6980 (4.3372)	LR 2.000e-03
0: TRAIN [1][60/922]	Time 0.266 (0.288)	Data 6.63e-05 (7.91e-05)	Tok/s 4012 (6140)	Loss/tok 3.9214 (4.3423)	LR 2.000e-03
1: TRAIN [1][60/922]	Time 0.266 (0.288)	Data 8.11e-05 (8.26e-05)	Tok/s 4067 (6138)	Loss/tok 3.9549 (4.3435)	LR 2.000e-03
3: TRAIN [1][60/922]	Time 0.266 (0.288)	Data 7.63e-05 (7.53e-05)	Tok/s 4109 (6151)	Loss/tok 3.8103 (4.3485)	LR 2.000e-03
2: TRAIN [1][60/922]	Time 0.266 (0.288)	Data 6.84e-05 (7.49e-05)	Tok/s 4275 (6124)	Loss/tok 3.9443 (4.3179)	LR 2.000e-03
1: TRAIN [1][70/922]	Time 0.287 (0.288)	Data 8.49e-05 (8.22e-05)	Tok/s 6170 (6152)	Loss/tok 4.2590 (4.3450)	LR 2.000e-03
0: TRAIN [1][70/922]	Time 0.287 (0.288)	Data 7.70e-05 (8.00e-05)	Tok/s 6365 (6158)	Loss/tok 4.1330 (4.3480)	LR 2.000e-03
3: TRAIN [1][70/922]	Time 0.287 (0.288)	Data 7.20e-05 (7.50e-05)	Tok/s 6178 (6163)	Loss/tok 4.3555 (4.3620)	LR 2.000e-03
2: TRAIN [1][70/922]	Time 0.287 (0.288)	Data 7.94e-05 (7.49e-05)	Tok/s 6336 (6148)	Loss/tok 4.3613 (4.3337)	LR 2.000e-03
1: TRAIN [1][80/922]	Time 0.307 (0.289)	Data 7.63e-05 (8.13e-05)	Tok/s 8104 (6255)	Loss/tok 4.5109 (4.3448)	LR 2.000e-03
3: TRAIN [1][80/922]	Time 0.307 (0.289)	Data 6.32e-05 (7.43e-05)	Tok/s 8103 (6256)	Loss/tok 4.4243 (4.3631)	LR 2.000e-03
0: TRAIN [1][80/922]	Time 0.307 (0.289)	Data 6.91e-05 (7.92e-05)	Tok/s 8382 (6264)	Loss/tok 4.6243 (4.3576)	LR 2.000e-03
2: TRAIN [1][80/922]	Time 0.307 (0.289)	Data 6.99e-05 (7.41e-05)	Tok/s 8347 (6261)	Loss/tok 4.6087 (4.3411)	LR 2.000e-03
1: TRAIN [1][90/922]	Time 0.309 (0.289)	Data 8.77e-05 (8.31e-05)	Tok/s 8098 (6272)	Loss/tok 4.5300 (4.3450)	LR 2.000e-03
0: TRAIN [1][90/922]	Time 0.309 (0.289)	Data 7.96e-05 (7.96e-05)	Tok/s 7975 (6278)	Loss/tok 4.3676 (4.3545)	LR 2.000e-03
3: TRAIN [1][90/922]	Time 0.309 (0.289)	Data 6.63e-05 (7.43e-05)	Tok/s 8352 (6267)	Loss/tok 4.2980 (4.3515)	LR 2.000e-03
2: TRAIN [1][90/922]	Time 0.309 (0.289)	Data 6.82e-05 (7.33e-05)	Tok/s 8170 (6269)	Loss/tok 4.4467 (4.3384)	LR 2.000e-03
1: TRAIN [1][100/922]	Time 0.307 (0.291)	Data 7.87e-05 (8.22e-05)	Tok/s 8290 (6381)	Loss/tok 4.5313 (4.3549)	LR 2.000e-03
0: TRAIN [1][100/922]	Time 0.307 (0.291)	Data 6.68e-05 (7.88e-05)	Tok/s 8173 (6370)	Loss/tok 4.3031 (4.3648)	LR 2.000e-03
3: TRAIN [1][100/922]	Time 0.307 (0.291)	Data 7.34e-05 (7.42e-05)	Tok/s 8212 (6369)	Loss/tok 4.2599 (4.3550)	LR 2.000e-03
2: TRAIN [1][100/922]	Time 0.307 (0.291)	Data 6.29e-05 (7.43e-05)	Tok/s 8216 (6374)	Loss/tok 4.4570 (4.3418)	LR 2.000e-03
1: TRAIN [1][110/922]	Time 0.309 (0.292)	Data 7.87e-05 (8.16e-05)	Tok/s 8339 (6490)	Loss/tok 4.2277 (4.3576)	LR 2.000e-03
0: TRAIN [1][110/922]	Time 0.309 (0.292)	Data 7.44e-05 (7.87e-05)	Tok/s 8116 (6485)	Loss/tok 4.3509 (4.3761)	LR 2.000e-03
3: TRAIN [1][110/922]	Time 0.309 (0.292)	Data 6.99e-05 (7.39e-05)	Tok/s 8460 (6478)	Loss/tok 4.5899 (4.3675)	LR 2.000e-03
2: TRAIN [1][110/922]	Time 0.309 (0.292)	Data 6.32e-05 (7.36e-05)	Tok/s 8181 (6485)	Loss/tok 4.5275 (4.3517)	LR 2.000e-03
1: TRAIN [1][120/922]	Time 0.289 (0.293)	Data 7.94e-05 (8.11e-05)	Tok/s 6496 (6563)	Loss/tok 4.3426 (4.3673)	LR 2.000e-03
0: TRAIN [1][120/922]	Time 0.289 (0.293)	Data 7.22e-05 (7.85e-05)	Tok/s 6254 (6557)	Loss/tok 4.3594 (4.3837)	LR 2.000e-03
3: TRAIN [1][120/922]	Time 0.289 (0.293)	Data 7.08e-05 (7.40e-05)	Tok/s 6257 (6553)	Loss/tok 4.1905 (4.3797)	LR 2.000e-03
2: TRAIN [1][120/922]	Time 0.289 (0.293)	Data 6.41e-05 (7.28e-05)	Tok/s 6151 (6555)	Loss/tok 3.9566 (4.3666)	LR 2.000e-03
1: TRAIN [1][130/922]	Time 0.263 (0.293)	Data 8.15e-05 (8.06e-05)	Tok/s 4184 (6558)	Loss/tok 3.9986 (4.3683)	LR 2.000e-03
0: TRAIN [1][130/922]	Time 0.263 (0.293)	Data 1.13e-04 (7.90e-05)	Tok/s 4083 (6553)	Loss/tok 3.8054 (4.3744)	LR 2.000e-03
3: TRAIN [1][130/922]	Time 0.263 (0.293)	Data 7.49e-05 (7.38e-05)	Tok/s 4063 (6550)	Loss/tok 3.7196 (4.3734)	LR 2.000e-03
2: TRAIN [1][130/922]	Time 0.263 (0.293)	Data 7.30e-05 (7.35e-05)	Tok/s 3877 (6549)	Loss/tok 4.1590 (4.3606)	LR 2.000e-03
0: TRAIN [1][140/922]	Time 0.244 (0.293)	Data 7.94e-05 (7.89e-05)	Tok/s 2161 (6523)	Loss/tok 3.5575 (4.3693)	LR 2.000e-03
1: TRAIN [1][140/922]	Time 0.244 (0.293)	Data 8.34e-05 (8.02e-05)	Tok/s 2328 (6526)	Loss/tok 3.6452 (4.3652)	LR 2.000e-03
3: TRAIN [1][140/922]	Time 0.244 (0.293)	Data 7.03e-05 (7.37e-05)	Tok/s 2173 (6520)	Loss/tok 3.3497 (4.3740)	LR 2.000e-03
2: TRAIN [1][140/922]	Time 0.244 (0.293)	Data 7.06e-05 (7.33e-05)	Tok/s 2095 (6511)	Loss/tok 3.6770 (4.3608)	LR 2.000e-03
1: TRAIN [1][150/922]	Time 0.267 (0.293)	Data 8.01e-05 (8.00e-05)	Tok/s 4012 (6549)	Loss/tok 3.9552 (4.3597)	LR 2.000e-03
0: TRAIN [1][150/922]	Time 0.267 (0.293)	Data 6.70e-05 (7.86e-05)	Tok/s 3922 (6544)	Loss/tok 3.7158 (4.3670)	LR 2.000e-03
3: TRAIN [1][150/922]	Time 0.267 (0.293)	Data 7.39e-05 (7.44e-05)	Tok/s 4092 (6539)	Loss/tok 3.8873 (4.3723)	LR 2.000e-03
2: TRAIN [1][150/922]	Time 0.267 (0.293)	Data 6.13e-05 (7.29e-05)	Tok/s 4109 (6534)	Loss/tok 3.7862 (4.3557)	LR 2.000e-03
1: TRAIN [1][160/922]	Time 0.265 (0.294)	Data 7.94e-05 (7.97e-05)	Tok/s 4143 (6555)	Loss/tok 4.1933 (4.3570)	LR 2.000e-03
0: TRAIN [1][160/922]	Time 0.265 (0.294)	Data 8.03e-05 (7.88e-05)	Tok/s 4055 (6545)	Loss/tok 3.7556 (4.3631)	LR 2.000e-03
3: TRAIN [1][160/922]	Time 0.265 (0.294)	Data 6.79e-05 (7.43e-05)	Tok/s 4252 (6547)	Loss/tok 3.5098 (4.3714)	LR 2.000e-03
2: TRAIN [1][160/922]	Time 0.265 (0.294)	Data 6.29e-05 (7.24e-05)	Tok/s 4113 (6543)	Loss/tok 3.9207 (4.3558)	LR 2.000e-03
0: TRAIN [1][170/922]	Time 0.290 (0.293)	Data 8.20e-05 (7.87e-05)	Tok/s 6236 (6490)	Loss/tok 4.3629 (4.3563)	LR 2.000e-03
1: TRAIN [1][170/922]	Time 0.290 (0.293)	Data 7.68e-05 (7.98e-05)	Tok/s 6208 (6496)	Loss/tok 4.1041 (4.3484)	LR 2.000e-03
3: TRAIN [1][170/922]	Time 0.290 (0.293)	Data 7.03e-05 (7.45e-05)	Tok/s 6166 (6496)	Loss/tok 4.2512 (4.3597)	LR 2.000e-03
2: TRAIN [1][170/922]	Time 0.290 (0.293)	Data 6.18e-05 (7.21e-05)	Tok/s 6290 (6485)	Loss/tok 4.1652 (4.3467)	LR 2.000e-03
1: TRAIN [1][180/922]	Time 0.345 (0.292)	Data 8.51e-05 (7.96e-05)	Tok/s 9361 (6453)	Loss/tok 4.5706 (4.3379)	LR 2.000e-03
0: TRAIN [1][180/922]	Time 0.345 (0.292)	Data 8.70e-05 (7.87e-05)	Tok/s 9421 (6447)	Loss/tok 4.7422 (4.3447)	LR 2.000e-03
3: TRAIN [1][180/922]	Time 0.345 (0.292)	Data 7.34e-05 (7.45e-05)	Tok/s 9297 (6452)	Loss/tok 4.8484 (4.3533)	LR 2.000e-03
2: TRAIN [1][180/922]	Time 0.345 (0.292)	Data 7.06e-05 (7.19e-05)	Tok/s 9443 (6447)	Loss/tok 4.7043 (4.3375)	LR 2.000e-03
1: TRAIN [1][190/922]	Time 0.310 (0.293)	Data 8.85e-05 (7.95e-05)	Tok/s 8083 (6499)	Loss/tok 4.2708 (4.3383)	LR 2.000e-03
0: TRAIN [1][190/922]	Time 0.310 (0.293)	Data 7.87e-05 (7.86e-05)	Tok/s 7952 (6494)	Loss/tok 4.2452 (4.3384)	LR 2.000e-03
3: TRAIN [1][190/922]	Time 0.310 (0.293)	Data 7.10e-05 (7.45e-05)	Tok/s 8078 (6500)	Loss/tok 4.2504 (4.3496)	LR 2.000e-03
2: TRAIN [1][190/922]	Time 0.310 (0.293)	Data 7.44e-05 (7.18e-05)	Tok/s 8191 (6493)	Loss/tok 4.4505 (4.3381)	LR 2.000e-03
1: TRAIN [1][200/922]	Time 0.285 (0.293)	Data 8.51e-05 (7.94e-05)	Tok/s 6480 (6483)	Loss/tok 4.0346 (4.3275)	LR 2.000e-03
0: TRAIN [1][200/922]	Time 0.286 (0.293)	Data 8.89e-05 (7.85e-05)	Tok/s 6235 (6477)	Loss/tok 4.0831 (4.3287)	LR 2.000e-03
3: TRAIN [1][200/922]	Time 0.285 (0.293)	Data 7.53e-05 (7.48e-05)	Tok/s 6074 (6487)	Loss/tok 4.0504 (4.3411)	LR 2.000e-03
2: TRAIN [1][200/922]	Time 0.285 (0.293)	Data 7.32e-05 (7.25e-05)	Tok/s 6244 (6478)	Loss/tok 4.0947 (4.3287)	LR 2.000e-03
0: TRAIN [1][210/922]	Time 0.342 (0.293)	Data 7.72e-05 (7.84e-05)	Tok/s 9744 (6487)	Loss/tok 4.5329 (4.3301)	LR 2.000e-03
1: TRAIN [1][210/922]	Time 0.342 (0.293)	Data 8.18e-05 (7.94e-05)	Tok/s 9242 (6489)	Loss/tok 4.8797 (4.3330)	LR 2.000e-03
3: TRAIN [1][210/922]	Time 0.343 (0.293)	Data 7.44e-05 (7.47e-05)	Tok/s 9592 (6492)	Loss/tok 4.6039 (4.3414)	LR 2.000e-03
2: TRAIN [1][210/922]	Time 0.342 (0.293)	Data 7.37e-05 (7.23e-05)	Tok/s 9717 (6487)	Loss/tok 4.4251 (4.3299)	LR 2.000e-03
0: TRAIN [1][220/922]	Time 0.313 (0.294)	Data 7.63e-05 (7.82e-05)	Tok/s 8182 (6552)	Loss/tok 4.3162 (4.3302)	LR 2.000e-03
1: TRAIN [1][220/922]	Time 0.314 (0.294)	Data 8.06e-05 (7.93e-05)	Tok/s 7948 (6557)	Loss/tok 4.1489 (4.3308)	LR 2.000e-03
3: TRAIN [1][220/922]	Time 0.313 (0.294)	Data 6.87e-05 (7.48e-05)	Tok/s 8247 (6558)	Loss/tok 4.4353 (4.3430)	LR 2.000e-03
2: TRAIN [1][220/922]	Time 0.314 (0.294)	Data 7.08e-05 (7.22e-05)	Tok/s 8015 (6556)	Loss/tok 4.3690 (4.3337)	LR 2.000e-03
1: TRAIN [1][230/922]	Time 0.287 (0.294)	Data 7.46e-05 (7.93e-05)	Tok/s 6243 (6551)	Loss/tok 4.0100 (4.3245)	LR 2.000e-03
0: TRAIN [1][230/922]	Time 0.287 (0.294)	Data 7.87e-05 (7.83e-05)	Tok/s 6244 (6542)	Loss/tok 4.1101 (4.3261)	LR 2.000e-03
3: TRAIN [1][230/922]	Time 0.287 (0.294)	Data 7.39e-05 (7.50e-05)	Tok/s 6274 (6548)	Loss/tok 4.1332 (4.3362)	LR 2.000e-03
2: TRAIN [1][230/922]	Time 0.287 (0.294)	Data 6.27e-05 (7.21e-05)	Tok/s 6516 (6545)	Loss/tok 4.4063 (4.3276)	LR 2.000e-03
1: TRAIN [1][240/922]	Time 0.338 (0.294)	Data 8.70e-05 (7.93e-05)	Tok/s 9678 (6543)	Loss/tok 4.5077 (4.3221)	LR 2.000e-03
0: TRAIN [1][240/922]	Time 0.338 (0.294)	Data 8.39e-05 (7.82e-05)	Tok/s 9406 (6533)	Loss/tok 4.4807 (4.3238)	LR 2.000e-03
3: TRAIN [1][240/922]	Time 0.338 (0.294)	Data 7.25e-05 (7.50e-05)	Tok/s 9767 (6541)	Loss/tok 4.4357 (4.3368)	LR 2.000e-03
2: TRAIN [1][240/922]	Time 0.338 (0.294)	Data 6.99e-05 (7.22e-05)	Tok/s 9538 (6538)	Loss/tok 4.6341 (4.3270)	LR 2.000e-03
1: TRAIN [1][250/922]	Time 0.268 (0.294)	Data 8.23e-05 (7.93e-05)	Tok/s 3953 (6539)	Loss/tok 3.6841 (4.3154)	LR 2.000e-03
0: TRAIN [1][250/922]	Time 0.268 (0.294)	Data 7.32e-05 (7.82e-05)	Tok/s 4006 (6528)	Loss/tok 3.8867 (4.3229)	LR 2.000e-03
3: TRAIN [1][250/922]	Time 0.268 (0.294)	Data 7.27e-05 (7.51e-05)	Tok/s 4150 (6537)	Loss/tok 3.8271 (4.3312)	LR 2.000e-03
2: TRAIN [1][250/922]	Time 0.268 (0.294)	Data 7.87e-05 (7.27e-05)	Tok/s 3975 (6538)	Loss/tok 3.9018 (4.3226)	LR 2.000e-03
1: TRAIN [1][260/922]	Time 0.312 (0.294)	Data 7.99e-05 (7.94e-05)	Tok/s 8002 (6545)	Loss/tok 4.2521 (4.3091)	LR 2.000e-03
0: TRAIN [1][260/922]	Time 0.312 (0.294)	Data 7.49e-05 (7.81e-05)	Tok/s 8125 (6537)	Loss/tok 4.3794 (4.3206)	LR 2.000e-03
3: TRAIN [1][260/922]	Time 0.311 (0.294)	Data 7.49e-05 (7.51e-05)	Tok/s 8147 (6543)	Loss/tok 4.3549 (4.3278)	LR 2.000e-03
2: TRAIN [1][260/922]	Time 0.311 (0.294)	Data 7.72e-05 (7.28e-05)	Tok/s 8075 (6545)	Loss/tok 4.2015 (4.3185)	LR 2.000e-03
1: TRAIN [1][270/922]	Time 0.303 (0.294)	Data 8.18e-05 (7.94e-05)	Tok/s 8199 (6537)	Loss/tok 4.3111 (4.3035)	LR 2.000e-03
0: TRAIN [1][270/922]	Time 0.304 (0.294)	Data 6.84e-05 (7.80e-05)	Tok/s 8097 (6530)	Loss/tok 4.2047 (4.3149)	LR 2.000e-03
3: TRAIN [1][270/922]	Time 0.304 (0.294)	Data 6.53e-05 (7.58e-05)	Tok/s 8106 (6532)	Loss/tok 4.3470 (4.3231)	LR 2.000e-03
2: TRAIN [1][270/922]	Time 0.304 (0.294)	Data 7.27e-05 (7.36e-05)	Tok/s 8386 (6540)	Loss/tok 4.3296 (4.3138)	LR 2.000e-03
1: TRAIN [1][280/922]	Time 0.335 (0.293)	Data 7.99e-05 (7.94e-05)	Tok/s 9742 (6524)	Loss/tok 4.5039 (4.3003)	LR 2.000e-03
0: TRAIN [1][280/922]	Time 0.335 (0.293)	Data 7.49e-05 (7.82e-05)	Tok/s 9737 (6518)	Loss/tok 4.7876 (4.3120)	LR 2.000e-03
3: TRAIN [1][280/922]	Time 0.335 (0.293)	Data 6.91e-05 (7.60e-05)	Tok/s 9954 (6523)	Loss/tok 4.5738 (4.3183)	LR 2.000e-03
2: TRAIN [1][280/922]	Time 0.335 (0.293)	Data 7.63e-05 (7.37e-05)	Tok/s 9585 (6529)	Loss/tok 4.3352 (4.3072)	LR 2.000e-03
1: TRAIN [1][290/922]	Time 0.265 (0.293)	Data 8.46e-05 (8.01e-05)	Tok/s 4197 (6488)	Loss/tok 3.9068 (4.2943)	LR 2.000e-03
0: TRAIN [1][290/922]	Time 0.264 (0.293)	Data 8.92e-05 (7.81e-05)	Tok/s 4265 (6487)	Loss/tok 3.8933 (4.3069)	LR 2.000e-03
3: TRAIN [1][290/922]	Time 0.265 (0.293)	Data 6.70e-05 (7.59e-05)	Tok/s 4013 (6487)	Loss/tok 3.6938 (4.3108)	LR 2.000e-03
2: TRAIN [1][290/922]	Time 0.265 (0.293)	Data 7.10e-05 (7.38e-05)	Tok/s 4164 (6496)	Loss/tok 3.9365 (4.3029)	LR 2.000e-03
1: TRAIN [1][300/922]	Time 0.337 (0.293)	Data 8.44e-05 (8.02e-05)	Tok/s 9507 (6513)	Loss/tok 4.4127 (4.2929)	LR 2.000e-03
0: TRAIN [1][300/922]	Time 0.337 (0.293)	Data 7.49e-05 (7.81e-05)	Tok/s 9842 (6515)	Loss/tok 4.3709 (4.3027)	LR 2.000e-03
2: TRAIN [1][300/922]	Time 0.337 (0.293)	Data 6.99e-05 (7.39e-05)	Tok/s 9934 (6522)	Loss/tok 4.2602 (4.3019)	LR 2.000e-03
3: TRAIN [1][300/922]	Time 0.337 (0.293)	Data 6.77e-05 (7.59e-05)	Tok/s 9514 (6514)	Loss/tok 4.5584 (4.3084)	LR 2.000e-03
1: TRAIN [1][310/922]	Time 0.262 (0.293)	Data 8.56e-05 (8.01e-05)	Tok/s 4283 (6501)	Loss/tok 3.8429 (4.2893)	LR 1.000e-03
0: TRAIN [1][310/922]	Time 0.262 (0.293)	Data 7.44e-05 (7.80e-05)	Tok/s 4113 (6501)	Loss/tok 3.6917 (4.2984)	LR 1.000e-03
3: TRAIN [1][310/922]	Time 0.262 (0.293)	Data 7.53e-05 (7.65e-05)	Tok/s 4038 (6504)	Loss/tok 3.7988 (4.3052)	LR 1.000e-03
2: TRAIN [1][310/922]	Time 0.262 (0.293)	Data 7.30e-05 (7.38e-05)	Tok/s 4003 (6509)	Loss/tok 3.9731 (4.2998)	LR 1.000e-03
1: TRAIN [1][320/922]	Time 0.334 (0.294)	Data 7.82e-05 (8.01e-05)	Tok/s 9645 (6549)	Loss/tok 4.2094 (4.2893)	LR 1.000e-03
0: TRAIN [1][320/922]	Time 0.334 (0.294)	Data 7.46e-05 (7.80e-05)	Tok/s 9985 (6550)	Loss/tok 4.4179 (4.2995)	LR 1.000e-03
3: TRAIN [1][320/922]	Time 0.334 (0.294)	Data 7.70e-05 (7.67e-05)	Tok/s 9873 (6554)	Loss/tok 4.5003 (4.3075)	LR 1.000e-03
2: TRAIN [1][320/922]	Time 0.334 (0.294)	Data 7.10e-05 (7.38e-05)	Tok/s 9947 (6559)	Loss/tok 4.5052 (4.3037)	LR 1.000e-03
1: TRAIN [1][330/922]	Time 0.309 (0.294)	Data 9.27e-05 (8.02e-05)	Tok/s 8212 (6541)	Loss/tok 4.2899 (4.2839)	LR 1.000e-03
0: TRAIN [1][330/922]	Time 0.309 (0.294)	Data 8.15e-05 (7.80e-05)	Tok/s 7907 (6544)	Loss/tok 4.2768 (4.2957)	LR 1.000e-03
3: TRAIN [1][330/922]	Time 0.309 (0.294)	Data 7.41e-05 (7.67e-05)	Tok/s 8206 (6544)	Loss/tok 4.1796 (4.3042)	LR 1.000e-03
2: TRAIN [1][330/922]	Time 0.309 (0.294)	Data 7.20e-05 (7.40e-05)	Tok/s 7873 (6552)	Loss/tok 4.2471 (4.2986)	LR 1.000e-03
1: TRAIN [1][340/922]	Time 0.288 (0.294)	Data 7.80e-05 (8.01e-05)	Tok/s 6057 (6523)	Loss/tok 3.9077 (4.2774)	LR 1.000e-03
0: TRAIN [1][340/922]	Time 0.288 (0.294)	Data 7.68e-05 (7.81e-05)	Tok/s 6109 (6525)	Loss/tok 3.8278 (4.2871)	LR 1.000e-03
3: TRAIN [1][340/922]	Time 0.288 (0.294)	Data 6.46e-05 (7.67e-05)	Tok/s 6123 (6527)	Loss/tok 3.7464 (4.2977)	LR 1.000e-03
2: TRAIN [1][340/922]	Time 0.288 (0.294)	Data 7.46e-05 (7.40e-05)	Tok/s 6338 (6533)	Loss/tok 3.8470 (4.2893)	LR 1.000e-03
1: TRAIN [1][350/922]	Time 0.339 (0.293)	Data 8.20e-05 (8.02e-05)	Tok/s 9635 (6512)	Loss/tok 4.3725 (4.2718)	LR 1.000e-03
0: TRAIN [1][350/922]	Time 0.339 (0.293)	Data 7.80e-05 (7.80e-05)	Tok/s 9735 (6515)	Loss/tok 4.3401 (4.2801)	LR 1.000e-03
2: TRAIN [1][350/922]	Time 0.339 (0.293)	Data 7.56e-05 (7.46e-05)	Tok/s 9704 (6522)	Loss/tok 4.4301 (4.2808)	LR 1.000e-03
3: TRAIN [1][350/922]	Time 0.340 (0.293)	Data 6.60e-05 (7.72e-05)	Tok/s 9656 (6516)	Loss/tok 4.3418 (4.2914)	LR 1.000e-03
1: TRAIN [1][360/922]	Time 0.306 (0.294)	Data 8.82e-05 (8.02e-05)	Tok/s 8267 (6527)	Loss/tok 4.0175 (4.2697)	LR 1.000e-03
0: TRAIN [1][360/922]	Time 0.306 (0.294)	Data 7.82e-05 (7.80e-05)	Tok/s 8052 (6531)	Loss/tok 4.2654 (4.2756)	LR 1.000e-03
3: TRAIN [1][360/922]	Time 0.306 (0.294)	Data 7.25e-05 (7.72e-05)	Tok/s 8177 (6529)	Loss/tok 4.1486 (4.2880)	LR 1.000e-03
2: TRAIN [1][360/922]	Time 0.306 (0.294)	Data 7.30e-05 (7.46e-05)	Tok/s 8209 (6537)	Loss/tok 4.3981 (4.2769)	LR 1.000e-03
1: TRAIN [1][370/922]	Time 0.286 (0.293)	Data 9.42e-05 (8.01e-05)	Tok/s 6046 (6519)	Loss/tok 4.1383 (4.2637)	LR 1.000e-03
0: TRAIN [1][370/922]	Time 0.287 (0.293)	Data 7.87e-05 (7.79e-05)	Tok/s 6247 (6520)	Loss/tok 3.7026 (4.2692)	LR 1.000e-03
3: TRAIN [1][370/922]	Time 0.286 (0.293)	Data 7.22e-05 (7.72e-05)	Tok/s 6264 (6519)	Loss/tok 3.8382 (4.2802)	LR 1.000e-03
2: TRAIN [1][370/922]	Time 0.287 (0.293)	Data 7.89e-05 (7.48e-05)	Tok/s 6218 (6528)	Loss/tok 4.0581 (4.2695)	LR 1.000e-03
1: TRAIN [1][380/922]	Time 0.288 (0.294)	Data 8.30e-05 (8.01e-05)	Tok/s 6434 (6550)	Loss/tok 4.0063 (4.2589)	LR 1.000e-03
0: TRAIN [1][380/922]	Time 0.288 (0.294)	Data 7.58e-05 (7.79e-05)	Tok/s 6236 (6551)	Loss/tok 3.7593 (4.2657)	LR 1.000e-03
3: TRAIN [1][380/922]	Time 0.288 (0.294)	Data 7.94e-05 (7.72e-05)	Tok/s 6228 (6551)	Loss/tok 3.8624 (4.2754)	LR 1.000e-03
2: TRAIN [1][380/922]	Time 0.288 (0.294)	Data 9.11e-05 (7.49e-05)	Tok/s 6481 (6560)	Loss/tok 3.9140 (4.2648)	LR 1.000e-03
0: TRAIN [1][390/922]	Time 0.265 (0.294)	Data 7.58e-05 (7.79e-05)	Tok/s 4111 (6544)	Loss/tok 3.4050 (4.2613)	LR 1.000e-03
1: TRAIN [1][390/922]	Time 0.265 (0.294)	Data 8.68e-05 (8.00e-05)	Tok/s 4114 (6543)	Loss/tok 3.8408 (4.2534)	LR 1.000e-03
3: TRAIN [1][390/922]	Time 0.265 (0.294)	Data 6.60e-05 (7.71e-05)	Tok/s 4343 (6543)	Loss/tok 3.7978 (4.2709)	LR 1.000e-03
2: TRAIN [1][390/922]	Time 0.265 (0.294)	Data 8.51e-05 (7.49e-05)	Tok/s 3976 (6551)	Loss/tok 3.8868 (4.2583)	LR 1.000e-03
1: TRAIN [1][400/922]	Time 0.340 (0.294)	Data 8.70e-05 (8.00e-05)	Tok/s 9725 (6538)	Loss/tok 4.4140 (4.2467)	LR 1.000e-03
0: TRAIN [1][400/922]	Time 0.340 (0.294)	Data 8.44e-05 (7.78e-05)	Tok/s 9619 (6538)	Loss/tok 4.2866 (4.2540)	LR 1.000e-03
2: TRAIN [1][400/922]	Time 0.340 (0.294)	Data 7.37e-05 (7.50e-05)	Tok/s 9541 (6546)	Loss/tok 4.2952 (4.2507)	LR 1.000e-03
3: TRAIN [1][400/922]	Time 0.340 (0.294)	Data 7.39e-05 (7.70e-05)	Tok/s 9378 (6537)	Loss/tok 4.4130 (4.2636)	LR 1.000e-03
0: TRAIN [1][410/922]	Time 0.285 (0.293)	Data 7.70e-05 (7.78e-05)	Tok/s 6491 (6527)	Loss/tok 4.1197 (4.2484)	LR 1.000e-03
1: TRAIN [1][410/922]	Time 0.285 (0.293)	Data 8.23e-05 (8.01e-05)	Tok/s 6445 (6528)	Loss/tok 3.9190 (4.2401)	LR 1.000e-03
3: TRAIN [1][410/922]	Time 0.286 (0.293)	Data 6.94e-05 (7.70e-05)	Tok/s 6243 (6528)	Loss/tok 4.0765 (4.2583)	LR 1.000e-03
2: TRAIN [1][410/922]	Time 0.286 (0.293)	Data 6.82e-05 (7.56e-05)	Tok/s 6263 (6537)	Loss/tok 4.0518 (4.2453)	LR 1.000e-03
1: TRAIN [1][420/922]	Time 0.265 (0.293)	Data 1.12e-04 (8.01e-05)	Tok/s 4191 (6528)	Loss/tok 3.6682 (4.2356)	LR 1.000e-03
0: TRAIN [1][420/922]	Time 0.265 (0.293)	Data 1.06e-04 (7.78e-05)	Tok/s 4085 (6527)	Loss/tok 3.5067 (4.2415)	LR 1.000e-03
3: TRAIN [1][420/922]	Time 0.265 (0.293)	Data 8.08e-05 (7.70e-05)	Tok/s 4005 (6527)	Loss/tok 3.9342 (4.2519)	LR 1.000e-03
2: TRAIN [1][420/922]	Time 0.266 (0.293)	Data 7.34e-05 (7.56e-05)	Tok/s 4168 (6537)	Loss/tok 3.6666 (4.2396)	LR 1.000e-03
0: TRAIN [1][430/922]	Time 0.341 (0.294)	Data 7.70e-05 (7.78e-05)	Tok/s 9571 (6542)	Loss/tok 4.3966 (4.2395)	LR 1.000e-03
3: TRAIN [1][430/922]	Time 0.341 (0.294)	Data 7.34e-05 (7.70e-05)	Tok/s 9479 (6542)	Loss/tok 4.2941 (4.2465)	LR 1.000e-03
1: TRAIN [1][430/922]	Time 0.341 (0.294)	Data 8.49e-05 (8.02e-05)	Tok/s 9868 (6544)	Loss/tok 4.2893 (4.2319)	LR 1.000e-03
2: TRAIN [1][430/922]	Time 0.341 (0.294)	Data 8.06e-05 (7.56e-05)	Tok/s 9383 (6553)	Loss/tok 4.5023 (4.2360)	LR 1.000e-03
1: TRAIN [1][440/922]	Time 0.266 (0.293)	Data 7.96e-05 (8.03e-05)	Tok/s 3874 (6517)	Loss/tok 3.5140 (4.2246)	LR 1.000e-03
0: TRAIN [1][440/922]	Time 0.266 (0.293)	Data 7.30e-05 (7.78e-05)	Tok/s 4159 (6516)	Loss/tok 3.8607 (4.2319)	LR 1.000e-03
3: TRAIN [1][440/922]	Time 0.266 (0.293)	Data 6.56e-05 (7.71e-05)	Tok/s 4061 (6517)	Loss/tok 3.7178 (4.2412)	LR 1.000e-03
2: TRAIN [1][440/922]	Time 0.266 (0.293)	Data 7.77e-05 (7.57e-05)	Tok/s 3917 (6526)	Loss/tok 3.3847 (4.2296)	LR 1.000e-03
1: TRAIN [1][450/922]	Time 0.286 (0.293)	Data 9.06e-05 (8.03e-05)	Tok/s 6321 (6521)	Loss/tok 3.7784 (4.2204)	LR 1.000e-03
0: TRAIN [1][450/922]	Time 0.286 (0.293)	Data 7.30e-05 (7.78e-05)	Tok/s 6075 (6520)	Loss/tok 4.0849 (4.2286)	LR 1.000e-03
3: TRAIN [1][450/922]	Time 0.286 (0.293)	Data 6.72e-05 (7.71e-05)	Tok/s 6299 (6521)	Loss/tok 3.8758 (4.2361)	LR 1.000e-03
2: TRAIN [1][450/922]	Time 0.286 (0.293)	Data 7.27e-05 (7.57e-05)	Tok/s 6436 (6530)	Loss/tok 3.7621 (4.2249)	LR 1.000e-03
0: TRAIN [1][460/922]	Time 0.285 (0.293)	Data 8.11e-05 (7.78e-05)	Tok/s 6344 (6513)	Loss/tok 3.9309 (4.2251)	LR 5.000e-04
1: TRAIN [1][460/922]	Time 0.286 (0.293)	Data 8.68e-05 (8.03e-05)	Tok/s 6195 (6516)	Loss/tok 4.0848 (4.2162)	LR 5.000e-04
3: TRAIN [1][460/922]	Time 0.285 (0.293)	Data 7.03e-05 (7.71e-05)	Tok/s 6356 (6514)	Loss/tok 4.0337 (4.2318)	LR 5.000e-04
2: TRAIN [1][460/922]	Time 0.285 (0.293)	Data 7.84e-05 (7.58e-05)	Tok/s 6397 (6523)	Loss/tok 4.0074 (4.2189)	LR 5.000e-04
0: TRAIN [1][470/922]	Time 0.307 (0.293)	Data 8.15e-05 (7.78e-05)	Tok/s 8141 (6513)	Loss/tok 4.0093 (4.2223)	LR 5.000e-04
1: TRAIN [1][470/922]	Time 0.307 (0.293)	Data 8.11e-05 (8.03e-05)	Tok/s 8034 (6516)	Loss/tok 4.1664 (4.2123)	LR 5.000e-04
3: TRAIN [1][470/922]	Time 0.307 (0.293)	Data 6.56e-05 (7.71e-05)	Tok/s 8308 (6514)	Loss/tok 4.0479 (4.2278)	LR 5.000e-04
2: TRAIN [1][470/922]	Time 0.307 (0.293)	Data 7.72e-05 (7.59e-05)	Tok/s 8364 (6522)	Loss/tok 4.1507 (4.2141)	LR 5.000e-04
1: TRAIN [1][480/922]	Time 0.309 (0.293)	Data 8.80e-05 (8.03e-05)	Tok/s 8113 (6516)	Loss/tok 4.1848 (4.2076)	LR 5.000e-04
0: TRAIN [1][480/922]	Time 0.309 (0.293)	Data 8.15e-05 (7.78e-05)	Tok/s 8057 (6513)	Loss/tok 4.1073 (4.2175)	LR 5.000e-04
3: TRAIN [1][480/922]	Time 0.309 (0.293)	Data 7.18e-05 (7.71e-05)	Tok/s 8121 (6515)	Loss/tok 4.2020 (4.2233)	LR 5.000e-04
2: TRAIN [1][480/922]	Time 0.309 (0.293)	Data 8.63e-05 (7.60e-05)	Tok/s 8149 (6524)	Loss/tok 4.0491 (4.2081)	LR 5.000e-04
1: TRAIN [1][490/922]	Time 0.311 (0.293)	Data 9.06e-05 (8.03e-05)	Tok/s 8140 (6539)	Loss/tok 3.9916 (4.2039)	LR 5.000e-04
0: TRAIN [1][490/922]	Time 0.311 (0.293)	Data 7.65e-05 (7.77e-05)	Tok/s 8142 (6535)	Loss/tok 4.2074 (4.2142)	LR 5.000e-04
2: TRAIN [1][490/922]	Time 0.311 (0.293)	Data 7.99e-05 (7.60e-05)	Tok/s 7971 (6544)	Loss/tok 4.0596 (4.2062)	LR 5.000e-04
3: TRAIN [1][490/922]	Time 0.311 (0.293)	Data 7.25e-05 (7.70e-05)	Tok/s 8187 (6537)	Loss/tok 3.9393 (4.2187)	LR 5.000e-04
1: TRAIN [1][500/922]	Time 0.312 (0.293)	Data 7.89e-05 (8.03e-05)	Tok/s 8072 (6539)	Loss/tok 4.1296 (4.1992)	LR 5.000e-04
0: TRAIN [1][500/922]	Time 0.312 (0.293)	Data 7.41e-05 (7.77e-05)	Tok/s 8057 (6536)	Loss/tok 4.1927 (4.2086)	LR 5.000e-04
3: TRAIN [1][500/922]	Time 0.312 (0.293)	Data 7.34e-05 (7.70e-05)	Tok/s 8122 (6537)	Loss/tok 3.9597 (4.2125)	LR 5.000e-04
2: TRAIN [1][500/922]	Time 0.312 (0.293)	Data 7.70e-05 (7.61e-05)	Tok/s 8028 (6545)	Loss/tok 4.0396 (4.2018)	LR 5.000e-04
0: TRAIN [1][510/922]	Time 0.285 (0.293)	Data 7.65e-05 (7.77e-05)	Tok/s 6404 (6519)	Loss/tok 3.7478 (4.2031)	LR 5.000e-04
1: TRAIN [1][510/922]	Time 0.284 (0.293)	Data 1.19e-04 (8.04e-05)	Tok/s 6167 (6522)	Loss/tok 3.9117 (4.1926)	LR 5.000e-04
3: TRAIN [1][510/922]	Time 0.285 (0.293)	Data 6.91e-05 (7.70e-05)	Tok/s 6264 (6519)	Loss/tok 3.6301 (4.2062)	LR 5.000e-04
2: TRAIN [1][510/922]	Time 0.285 (0.293)	Data 7.61e-05 (7.62e-05)	Tok/s 6279 (6527)	Loss/tok 3.8010 (4.1949)	LR 5.000e-04
1: TRAIN [1][520/922]	Time 0.285 (0.293)	Data 8.94e-05 (8.03e-05)	Tok/s 6428 (6533)	Loss/tok 3.5817 (4.1874)	LR 5.000e-04
0: TRAIN [1][520/922]	Time 0.285 (0.293)	Data 7.99e-05 (7.76e-05)	Tok/s 6156 (6529)	Loss/tok 3.7050 (4.1963)	LR 5.000e-04
3: TRAIN [1][520/922]	Time 0.285 (0.293)	Data 6.72e-05 (7.70e-05)	Tok/s 6345 (6530)	Loss/tok 3.7079 (4.1992)	LR 5.000e-04
2: TRAIN [1][520/922]	Time 0.285 (0.293)	Data 7.39e-05 (7.63e-05)	Tok/s 6434 (6537)	Loss/tok 3.7269 (4.1887)	LR 5.000e-04
0: TRAIN [1][530/922]	Time 0.285 (0.293)	Data 7.92e-05 (7.76e-05)	Tok/s 6435 (6510)	Loss/tok 4.1314 (4.1918)	LR 5.000e-04
1: TRAIN [1][530/922]	Time 0.285 (0.293)	Data 8.87e-05 (8.03e-05)	Tok/s 6324 (6513)	Loss/tok 3.7588 (4.1817)	LR 5.000e-04
3: TRAIN [1][530/922]	Time 0.285 (0.293)	Data 6.75e-05 (7.69e-05)	Tok/s 6049 (6508)	Loss/tok 3.7777 (4.1927)	LR 5.000e-04
2: TRAIN [1][530/922]	Time 0.285 (0.293)	Data 7.77e-05 (7.63e-05)	Tok/s 6249 (6516)	Loss/tok 4.0200 (4.1832)	LR 5.000e-04
1: TRAIN [1][540/922]	Time 0.338 (0.293)	Data 9.30e-05 (8.04e-05)	Tok/s 9800 (6526)	Loss/tok 4.3787 (4.1779)	LR 5.000e-04
0: TRAIN [1][540/922]	Time 0.338 (0.293)	Data 7.49e-05 (7.76e-05)	Tok/s 9408 (6521)	Loss/tok 4.3171 (4.1898)	LR 5.000e-04
3: TRAIN [1][540/922]	Time 0.338 (0.293)	Data 9.89e-05 (7.69e-05)	Tok/s 9559 (6519)	Loss/tok 4.4362 (4.1884)	LR 5.000e-04
2: TRAIN [1][540/922]	Time 0.338 (0.293)	Data 6.39e-05 (7.63e-05)	Tok/s 9667 (6528)	Loss/tok 4.2946 (4.1801)	LR 5.000e-04
1: TRAIN [1][550/922]	Time 0.284 (0.293)	Data 8.73e-05 (8.04e-05)	Tok/s 6416 (6532)	Loss/tok 3.7907 (4.1731)	LR 5.000e-04
0: TRAIN [1][550/922]	Time 0.284 (0.293)	Data 8.20e-05 (7.76e-05)	Tok/s 6479 (6527)	Loss/tok 3.6475 (4.1843)	LR 5.000e-04
3: TRAIN [1][550/922]	Time 0.284 (0.293)	Data 8.87e-05 (7.69e-05)	Tok/s 6280 (6526)	Loss/tok 3.9073 (4.1838)	LR 5.000e-04
2: TRAIN [1][550/922]	Time 0.284 (0.293)	Data 7.68e-05 (7.64e-05)	Tok/s 6106 (6533)	Loss/tok 3.8692 (4.1760)	LR 5.000e-04
0: TRAIN [1][560/922]	Time 0.267 (0.293)	Data 7.61e-05 (7.75e-05)	Tok/s 4161 (6524)	Loss/tok 3.6419 (4.1797)	LR 5.000e-04
1: TRAIN [1][560/922]	Time 0.267 (0.293)	Data 7.89e-05 (8.04e-05)	Tok/s 3939 (6529)	Loss/tok 3.7421 (4.1680)	LR 5.000e-04
3: TRAIN [1][560/922]	Time 0.267 (0.293)	Data 6.48e-05 (7.68e-05)	Tok/s 4126 (6522)	Loss/tok 3.4535 (4.1776)	LR 5.000e-04
2: TRAIN [1][560/922]	Time 0.267 (0.293)	Data 7.25e-05 (7.64e-05)	Tok/s 4095 (6528)	Loss/tok 3.5543 (4.1716)	LR 5.000e-04
1: TRAIN [1][570/922]	Time 0.343 (0.293)	Data 7.99e-05 (8.04e-05)	Tok/s 9476 (6549)	Loss/tok 4.1825 (4.1634)	LR 5.000e-04
0: TRAIN [1][570/922]	Time 0.343 (0.293)	Data 7.46e-05 (7.76e-05)	Tok/s 9593 (6545)	Loss/tok 4.3508 (4.1761)	LR 5.000e-04
3: TRAIN [1][570/922]	Time 0.343 (0.293)	Data 6.53e-05 (7.68e-05)	Tok/s 9711 (6542)	Loss/tok 4.2499 (4.1735)	LR 5.000e-04
2: TRAIN [1][570/922]	Time 0.343 (0.293)	Data 1.53e-04 (7.66e-05)	Tok/s 9485 (6547)	Loss/tok 4.2952 (4.1676)	LR 5.000e-04
1: TRAIN [1][580/922]	Time 0.263 (0.293)	Data 9.18e-05 (8.04e-05)	Tok/s 3876 (6541)	Loss/tok 3.5030 (4.1582)	LR 5.000e-04
0: TRAIN [1][580/922]	Time 0.264 (0.293)	Data 7.37e-05 (7.75e-05)	Tok/s 3920 (6538)	Loss/tok 3.4647 (4.1715)	LR 5.000e-04
3: TRAIN [1][580/922]	Time 0.263 (0.293)	Data 6.79e-05 (7.68e-05)	Tok/s 3819 (6534)	Loss/tok 3.6401 (4.1705)	LR 5.000e-04
2: TRAIN [1][580/922]	Time 0.264 (0.293)	Data 6.63e-05 (7.65e-05)	Tok/s 4171 (6540)	Loss/tok 3.7023 (4.1633)	LR 5.000e-04
1: TRAIN [1][590/922]	Time 0.266 (0.293)	Data 7.75e-05 (8.04e-05)	Tok/s 3940 (6554)	Loss/tok 3.2703 (4.1537)	LR 5.000e-04
0: TRAIN [1][590/922]	Time 0.266 (0.293)	Data 7.37e-05 (7.75e-05)	Tok/s 3964 (6551)	Loss/tok 3.5525 (4.1676)	LR 5.000e-04
3: TRAIN [1][590/922]	Time 0.266 (0.293)	Data 7.03e-05 (7.67e-05)	Tok/s 3896 (6547)	Loss/tok 3.5053 (4.1659)	LR 5.000e-04
2: TRAIN [1][590/922]	Time 0.266 (0.293)	Data 7.68e-05 (7.65e-05)	Tok/s 4110 (6554)	Loss/tok 3.4791 (4.1589)	LR 5.000e-04
1: TRAIN [1][600/922]	Time 0.287 (0.293)	Data 9.01e-05 (8.04e-05)	Tok/s 6088 (6541)	Loss/tok 3.6799 (4.1484)	LR 5.000e-04
0: TRAIN [1][600/922]	Time 0.287 (0.293)	Data 7.65e-05 (7.75e-05)	Tok/s 6489 (6539)	Loss/tok 3.9073 (4.1626)	LR 5.000e-04
3: TRAIN [1][600/922]	Time 0.287 (0.293)	Data 7.65e-05 (7.68e-05)	Tok/s 6489 (6535)	Loss/tok 3.8087 (4.1617)	LR 5.000e-04
2: TRAIN [1][600/922]	Time 0.286 (0.293)	Data 9.25e-05 (7.68e-05)	Tok/s 6362 (6541)	Loss/tok 3.7167 (4.1532)	LR 5.000e-04
1: TRAIN [1][610/922]	Time 0.266 (0.293)	Data 8.89e-05 (8.04e-05)	Tok/s 3764 (6528)	Loss/tok 3.4398 (4.1430)	LR 5.000e-04
0: TRAIN [1][610/922]	Time 0.266 (0.293)	Data 8.32e-05 (7.75e-05)	Tok/s 3855 (6527)	Loss/tok 3.4057 (4.1571)	LR 5.000e-04
3: TRAIN [1][610/922]	Time 0.266 (0.293)	Data 7.70e-05 (7.68e-05)	Tok/s 4268 (6522)	Loss/tok 3.5872 (4.1567)	LR 5.000e-04
2: TRAIN [1][610/922]	Time 0.267 (0.293)	Data 7.53e-05 (7.67e-05)	Tok/s 4034 (6528)	Loss/tok 3.7160 (4.1484)	LR 5.000e-04
1: TRAIN [1][620/922]	Time 0.245 (0.293)	Data 8.06e-05 (8.04e-05)	Tok/s 2218 (6524)	Loss/tok 2.9406 (4.1401)	LR 2.500e-04
0: TRAIN [1][620/922]	Time 0.245 (0.293)	Data 1.16e-04 (7.76e-05)	Tok/s 2301 (6524)	Loss/tok 3.6876 (4.1532)	LR 2.500e-04
3: TRAIN [1][620/922]	Time 0.245 (0.293)	Data 9.68e-05 (7.68e-05)	Tok/s 2235 (6519)	Loss/tok 3.5128 (4.1534)	LR 2.500e-04
2: TRAIN [1][620/922]	Time 0.245 (0.293)	Data 6.99e-05 (7.66e-05)	Tok/s 2075 (6524)	Loss/tok 3.3307 (4.1454)	LR 2.500e-04
1: TRAIN [1][630/922]	Time 0.340 (0.293)	Data 8.42e-05 (8.04e-05)	Tok/s 9610 (6523)	Loss/tok 3.9730 (4.1351)	LR 2.500e-04
0: TRAIN [1][630/922]	Time 0.340 (0.293)	Data 7.27e-05 (7.76e-05)	Tok/s 9520 (6525)	Loss/tok 4.2357 (4.1500)	LR 2.500e-04
3: TRAIN [1][630/922]	Time 0.340 (0.293)	Data 6.91e-05 (7.68e-05)	Tok/s 9494 (6519)	Loss/tok 4.2982 (4.1512)	LR 2.500e-04
2: TRAIN [1][630/922]	Time 0.340 (0.293)	Data 6.44e-05 (7.66e-05)	Tok/s 9711 (6523)	Loss/tok 4.2101 (4.1419)	LR 2.500e-04
1: TRAIN [1][640/922]	Time 0.335 (0.293)	Data 8.23e-05 (8.07e-05)	Tok/s 9648 (6530)	Loss/tok 3.9581 (4.1305)	LR 2.500e-04
0: TRAIN [1][640/922]	Time 0.335 (0.293)	Data 7.22e-05 (7.75e-05)	Tok/s 9867 (6533)	Loss/tok 4.2206 (4.1462)	LR 2.500e-04
3: TRAIN [1][640/922]	Time 0.335 (0.293)	Data 7.27e-05 (7.68e-05)	Tok/s 9654 (6526)	Loss/tok 4.2992 (4.1479)	LR 2.500e-04
2: TRAIN [1][640/922]	Time 0.335 (0.293)	Data 6.72e-05 (7.71e-05)	Tok/s 9977 (6531)	Loss/tok 3.9230 (4.1372)	LR 2.500e-04
1: TRAIN [1][650/922]	Time 0.305 (0.293)	Data 1.10e-04 (8.07e-05)	Tok/s 8219 (6527)	Loss/tok 3.8710 (4.1267)	LR 2.500e-04
0: TRAIN [1][650/922]	Time 0.305 (0.293)	Data 7.63e-05 (7.75e-05)	Tok/s 8252 (6529)	Loss/tok 3.8351 (4.1428)	LR 2.500e-04
3: TRAIN [1][650/922]	Time 0.305 (0.293)	Data 7.75e-05 (7.68e-05)	Tok/s 8272 (6522)	Loss/tok 3.9249 (4.1438)	LR 2.500e-04
2: TRAIN [1][650/922]	Time 0.305 (0.293)	Data 6.96e-05 (7.71e-05)	Tok/s 8318 (6527)	Loss/tok 3.8740 (4.1340)	LR 2.500e-04
1: TRAIN [1][660/922]	Time 0.338 (0.293)	Data 8.94e-05 (8.07e-05)	Tok/s 9471 (6521)	Loss/tok 4.1526 (4.1216)	LR 2.500e-04
0: TRAIN [1][660/922]	Time 0.338 (0.293)	Data 7.58e-05 (7.74e-05)	Tok/s 9618 (6523)	Loss/tok 4.2733 (4.1388)	LR 2.500e-04
3: TRAIN [1][660/922]	Time 0.338 (0.293)	Data 8.34e-05 (7.68e-05)	Tok/s 9551 (6518)	Loss/tok 4.2961 (4.1384)	LR 2.500e-04
2: TRAIN [1][660/922]	Time 0.338 (0.293)	Data 7.58e-05 (7.71e-05)	Tok/s 9741 (6521)	Loss/tok 4.0817 (4.1289)	LR 2.500e-04
1: TRAIN [1][670/922]	Time 0.261 (0.293)	Data 8.58e-05 (8.06e-05)	Tok/s 3909 (6513)	Loss/tok 3.6642 (4.1169)	LR 2.500e-04
0: TRAIN [1][670/922]	Time 0.261 (0.293)	Data 8.11e-05 (7.76e-05)	Tok/s 4204 (6515)	Loss/tok 3.4525 (4.1342)	LR 2.500e-04
3: TRAIN [1][670/922]	Time 0.261 (0.293)	Data 7.15e-05 (7.67e-05)	Tok/s 4185 (6511)	Loss/tok 3.5737 (4.1335)	LR 2.500e-04
2: TRAIN [1][670/922]	Time 0.261 (0.293)	Data 7.58e-05 (7.70e-05)	Tok/s 4204 (6514)	Loss/tok 3.4684 (4.1243)	LR 2.500e-04
1: TRAIN [1][680/922]	Time 0.285 (0.293)	Data 8.01e-05 (8.10e-05)	Tok/s 6314 (6498)	Loss/tok 3.7820 (4.1124)	LR 2.500e-04
0: TRAIN [1][680/922]	Time 0.285 (0.293)	Data 7.49e-05 (7.79e-05)	Tok/s 6179 (6500)	Loss/tok 3.7442 (4.1294)	LR 2.500e-04
3: TRAIN [1][680/922]	Time 0.285 (0.293)	Data 7.49e-05 (7.68e-05)	Tok/s 6493 (6496)	Loss/tok 3.8113 (4.1285)	LR 2.500e-04
2: TRAIN [1][680/922]	Time 0.285 (0.293)	Data 1.02e-04 (7.73e-05)	Tok/s 6366 (6499)	Loss/tok 3.7031 (4.1195)	LR 2.500e-04
0: TRAIN [1][690/922]	Time 0.341 (0.293)	Data 8.01e-05 (7.78e-05)	Tok/s 9717 (6510)	Loss/tok 4.1484 (4.1249)	LR 2.500e-04
1: TRAIN [1][690/922]	Time 0.341 (0.293)	Data 1.06e-04 (8.10e-05)	Tok/s 9429 (6507)	Loss/tok 4.0911 (4.1092)	LR 2.500e-04
3: TRAIN [1][690/922]	Time 0.341 (0.293)	Data 8.61e-05 (7.67e-05)	Tok/s 9583 (6506)	Loss/tok 3.9899 (4.1249)	LR 2.500e-04
2: TRAIN [1][690/922]	Time 0.341 (0.293)	Data 7.41e-05 (7.72e-05)	Tok/s 9566 (6510)	Loss/tok 4.2009 (4.1164)	LR 2.500e-04
0: TRAIN [1][700/922]	Time 0.314 (0.293)	Data 7.08e-05 (7.78e-05)	Tok/s 7962 (6518)	Loss/tok 3.8034 (4.1210)	LR 2.500e-04
3: TRAIN [1][700/922]	Time 0.314 (0.293)	Data 6.82e-05 (7.67e-05)	Tok/s 8171 (6514)	Loss/tok 3.9651 (4.1224)	LR 2.500e-04
2: TRAIN [1][700/922]	Time 0.314 (0.293)	Data 7.44e-05 (7.72e-05)	Tok/s 8067 (6519)	Loss/tok 4.0457 (4.1128)	LR 2.500e-04
1: TRAIN [1][700/922]	Time 0.314 (0.293)	Data 7.75e-05 (8.10e-05)	Tok/s 7995 (6516)	Loss/tok 3.9661 (4.1057)	LR 2.500e-04
0: TRAIN [1][710/922]	Time 0.305 (0.293)	Data 7.06e-05 (7.78e-05)	Tok/s 8084 (6517)	Loss/tok 3.9205 (4.1162)	LR 2.500e-04
1: TRAIN [1][710/922]	Time 0.305 (0.293)	Data 8.61e-05 (8.10e-05)	Tok/s 8415 (6516)	Loss/tok 3.9419 (4.1023)	LR 2.500e-04
3: TRAIN [1][710/922]	Time 0.305 (0.293)	Data 6.53e-05 (7.68e-05)	Tok/s 8227 (6515)	Loss/tok 3.8656 (4.1185)	LR 2.500e-04
2: TRAIN [1][710/922]	Time 0.305 (0.293)	Data 1.01e-04 (7.72e-05)	Tok/s 8033 (6519)	Loss/tok 4.0460 (4.1092)	LR 2.500e-04
1: TRAIN [1][720/922]	Time 0.307 (0.293)	Data 9.13e-05 (8.10e-05)	Tok/s 8336 (6507)	Loss/tok 3.9606 (4.0983)	LR 2.500e-04
0: TRAIN [1][720/922]	Time 0.307 (0.293)	Data 7.89e-05 (7.77e-05)	Tok/s 8258 (6508)	Loss/tok 4.0830 (4.1125)	LR 2.500e-04
3: TRAIN [1][720/922]	Time 0.307 (0.293)	Data 7.30e-05 (7.67e-05)	Tok/s 7996 (6505)	Loss/tok 3.9943 (4.1147)	LR 2.500e-04
2: TRAIN [1][720/922]	Time 0.307 (0.293)	Data 7.70e-05 (7.71e-05)	Tok/s 8214 (6508)	Loss/tok 3.8006 (4.1047)	LR 2.500e-04
1: TRAIN [1][730/922]	Time 0.309 (0.292)	Data 9.04e-05 (8.10e-05)	Tok/s 8077 (6489)	Loss/tok 4.1596 (4.0943)	LR 2.500e-04
0: TRAIN [1][730/922]	Time 0.309 (0.292)	Data 8.49e-05 (7.77e-05)	Tok/s 8109 (6491)	Loss/tok 4.1076 (4.1079)	LR 2.500e-04
2: TRAIN [1][730/922]	Time 0.309 (0.292)	Data 8.54e-05 (7.71e-05)	Tok/s 8236 (6490)	Loss/tok 3.8526 (4.1003)	LR 2.500e-04
3: TRAIN [1][730/922]	Time 0.309 (0.292)	Data 8.32e-05 (7.67e-05)	Tok/s 8041 (6487)	Loss/tok 3.8424 (4.1104)	LR 2.500e-04
1: TRAIN [1][740/922]	Time 0.263 (0.292)	Data 8.89e-05 (8.10e-05)	Tok/s 4064 (6485)	Loss/tok 3.4496 (4.0895)	LR 2.500e-04
0: TRAIN [1][740/922]	Time 0.263 (0.292)	Data 8.44e-05 (7.77e-05)	Tok/s 4344 (6486)	Loss/tok 3.3505 (4.1029)	LR 2.500e-04
3: TRAIN [1][740/922]	Time 0.263 (0.292)	Data 6.96e-05 (7.67e-05)	Tok/s 4098 (6483)	Loss/tok 3.3634 (4.1056)	LR 2.500e-04
2: TRAIN [1][740/922]	Time 0.263 (0.292)	Data 6.99e-05 (7.72e-05)	Tok/s 4289 (6486)	Loss/tok 3.2800 (4.0970)	LR 2.500e-04
1: TRAIN [1][750/922]	Time 0.290 (0.293)	Data 8.30e-05 (8.09e-05)	Tok/s 6268 (6501)	Loss/tok 3.6566 (4.0860)	LR 2.500e-04
0: TRAIN [1][750/922]	Time 0.290 (0.293)	Data 7.20e-05 (7.76e-05)	Tok/s 6216 (6503)	Loss/tok 3.7272 (4.0999)	LR 2.500e-04
3: TRAIN [1][750/922]	Time 0.290 (0.293)	Data 7.13e-05 (7.67e-05)	Tok/s 6181 (6500)	Loss/tok 3.7759 (4.1033)	LR 2.500e-04
2: TRAIN [1][750/922]	Time 0.290 (0.293)	Data 6.82e-05 (7.72e-05)	Tok/s 6457 (6504)	Loss/tok 3.9180 (4.0940)	LR 2.500e-04
1: TRAIN [1][760/922]	Time 0.308 (0.293)	Data 8.49e-05 (8.09e-05)	Tok/s 8002 (6507)	Loss/tok 3.8503 (4.0827)	LR 2.500e-04
0: TRAIN [1][760/922]	Time 0.308 (0.293)	Data 7.92e-05 (7.78e-05)	Tok/s 8189 (6510)	Loss/tok 3.8923 (4.0950)	LR 2.500e-04
2: TRAIN [1][760/922]	Time 0.308 (0.293)	Data 7.82e-05 (7.71e-05)	Tok/s 8019 (6509)	Loss/tok 4.2589 (4.0913)	LR 2.500e-04
3: TRAIN [1][760/922]	Time 0.308 (0.293)	Data 7.25e-05 (7.67e-05)	Tok/s 8119 (6505)	Loss/tok 4.0151 (4.1001)	LR 2.500e-04
1: TRAIN [1][770/922]	Time 0.287 (0.293)	Data 7.99e-05 (8.09e-05)	Tok/s 6236 (6508)	Loss/tok 3.7598 (4.0807)	LR 1.250e-04
0: TRAIN [1][770/922]	Time 0.287 (0.293)	Data 8.61e-05 (7.78e-05)	Tok/s 6227 (6512)	Loss/tok 3.6812 (4.0927)	LR 1.250e-04
3: TRAIN [1][770/922]	Time 0.287 (0.293)	Data 7.56e-05 (7.67e-05)	Tok/s 6226 (6507)	Loss/tok 3.5091 (4.0971)	LR 1.250e-04
2: TRAIN [1][770/922]	Time 0.287 (0.293)	Data 7.53e-05 (7.71e-05)	Tok/s 6158 (6511)	Loss/tok 3.8133 (4.0885)	LR 1.250e-04
1: TRAIN [1][780/922]	Time 0.290 (0.293)	Data 8.34e-05 (8.08e-05)	Tok/s 6191 (6513)	Loss/tok 3.8038 (4.0777)	LR 1.250e-04
0: TRAIN [1][780/922]	Time 0.290 (0.293)	Data 7.49e-05 (7.77e-05)	Tok/s 6387 (6517)	Loss/tok 3.7519 (4.0894)	LR 1.250e-04
3: TRAIN [1][780/922]	Time 0.290 (0.293)	Data 7.46e-05 (7.67e-05)	Tok/s 6045 (6511)	Loss/tok 3.7532 (4.0940)	LR 1.250e-04
2: TRAIN [1][780/922]	Time 0.290 (0.293)	Data 7.34e-05 (7.71e-05)	Tok/s 6424 (6517)	Loss/tok 3.6963 (4.0850)	LR 1.250e-04
0: TRAIN [1][790/922]	Time 0.309 (0.293)	Data 7.44e-05 (7.77e-05)	Tok/s 8040 (6510)	Loss/tok 4.1056 (4.0876)	LR 1.250e-04
1: TRAIN [1][790/922]	Time 0.309 (0.293)	Data 7.77e-05 (8.08e-05)	Tok/s 8081 (6509)	Loss/tok 3.8420 (4.0744)	LR 1.250e-04
3: TRAIN [1][790/922]	Time 0.309 (0.293)	Data 7.06e-05 (7.67e-05)	Tok/s 8283 (6505)	Loss/tok 3.9998 (4.0916)	LR 1.250e-04
2: TRAIN [1][790/922]	Time 0.309 (0.293)	Data 2.93e-04 (7.73e-05)	Tok/s 8023 (6512)	Loss/tok 3.9241 (4.0826)	LR 1.250e-04
0: TRAIN [1][800/922]	Time 0.311 (0.293)	Data 7.10e-05 (7.77e-05)	Tok/s 7934 (6516)	Loss/tok 4.0388 (4.0848)	LR 1.250e-04
1: TRAIN [1][800/922]	Time 0.311 (0.293)	Data 7.87e-05 (8.08e-05)	Tok/s 7967 (6514)	Loss/tok 3.9124 (4.0726)	LR 1.250e-04
3: TRAIN [1][800/922]	Time 0.311 (0.293)	Data 6.75e-05 (7.67e-05)	Tok/s 8201 (6511)	Loss/tok 4.0120 (4.0894)	LR 1.250e-04
2: TRAIN [1][800/922]	Time 0.311 (0.293)	Data 7.99e-05 (7.73e-05)	Tok/s 8096 (6519)	Loss/tok 3.8836 (4.0812)	LR 1.250e-04
1: TRAIN [1][810/922]	Time 0.265 (0.293)	Data 8.18e-05 (8.08e-05)	Tok/s 4204 (6512)	Loss/tok 3.6997 (4.0693)	LR 1.250e-04
0: TRAIN [1][810/922]	Time 0.265 (0.293)	Data 8.03e-05 (7.76e-05)	Tok/s 4049 (6513)	Loss/tok 3.4659 (4.0814)	LR 1.250e-04
3: TRAIN [1][810/922]	Time 0.265 (0.293)	Data 1.12e-04 (7.67e-05)	Tok/s 4380 (6510)	Loss/tok 3.5820 (4.0855)	LR 1.250e-04
2: TRAIN [1][810/922]	Time 0.265 (0.293)	Data 6.65e-05 (7.72e-05)	Tok/s 3851 (6516)	Loss/tok 3.3744 (4.0783)	LR 1.250e-04
0: TRAIN [1][820/922]	Time 0.315 (0.293)	Data 8.06e-05 (7.76e-05)	Tok/s 7912 (6520)	Loss/tok 4.1908 (4.0785)	LR 1.250e-04
1: TRAIN [1][820/922]	Time 0.315 (0.293)	Data 8.23e-05 (8.07e-05)	Tok/s 7720 (6520)	Loss/tok 3.7996 (4.0666)	LR 1.250e-04
3: TRAIN [1][820/922]	Time 0.315 (0.293)	Data 7.37e-05 (7.67e-05)	Tok/s 7918 (6516)	Loss/tok 3.8153 (4.0817)	LR 1.250e-04
2: TRAIN [1][820/922]	Time 0.315 (0.293)	Data 7.49e-05 (7.71e-05)	Tok/s 8064 (6523)	Loss/tok 4.0436 (4.0759)	LR 1.250e-04
1: TRAIN [1][830/922]	Time 0.287 (0.293)	Data 7.94e-05 (8.07e-05)	Tok/s 6348 (6512)	Loss/tok 3.7826 (4.0632)	LR 1.250e-04
0: TRAIN [1][830/922]	Time 0.287 (0.293)	Data 7.61e-05 (7.75e-05)	Tok/s 6411 (6512)	Loss/tok 3.6918 (4.0751)	LR 1.250e-04
3: TRAIN [1][830/922]	Time 0.288 (0.293)	Data 6.60e-05 (7.67e-05)	Tok/s 6276 (6509)	Loss/tok 3.6833 (4.0787)	LR 1.250e-04
2: TRAIN [1][830/922]	Time 0.288 (0.293)	Data 7.65e-05 (7.71e-05)	Tok/s 6318 (6515)	Loss/tok 3.5523 (4.0730)	LR 1.250e-04
1: TRAIN [1][840/922]	Time 0.308 (0.293)	Data 8.89e-05 (8.07e-05)	Tok/s 8226 (6504)	Loss/tok 3.7558 (4.0593)	LR 1.250e-04
0: TRAIN [1][840/922]	Time 0.308 (0.293)	Data 8.03e-05 (7.75e-05)	Tok/s 8155 (6504)	Loss/tok 3.8652 (4.0717)	LR 1.250e-04
3: TRAIN [1][840/922]	Time 0.308 (0.293)	Data 8.44e-05 (7.67e-05)	Tok/s 8057 (6500)	Loss/tok 3.7611 (4.0747)	LR 1.250e-04
2: TRAIN [1][840/922]	Time 0.308 (0.293)	Data 7.80e-05 (7.71e-05)	Tok/s 7978 (6506)	Loss/tok 3.8369 (4.0692)	LR 1.250e-04
0: TRAIN [1][850/922]	Time 0.261 (0.293)	Data 7.63e-05 (7.75e-05)	Tok/s 4390 (6511)	Loss/tok 3.4700 (4.0690)	LR 1.250e-04
1: TRAIN [1][850/922]	Time 0.261 (0.293)	Data 8.96e-05 (8.07e-05)	Tok/s 3904 (6510)	Loss/tok 3.4979 (4.0564)	LR 1.250e-04
3: TRAIN [1][850/922]	Time 0.261 (0.293)	Data 7.87e-05 (7.66e-05)	Tok/s 4053 (6506)	Loss/tok 3.4955 (4.0720)	LR 1.250e-04
2: TRAIN [1][850/922]	Time 0.261 (0.293)	Data 7.01e-05 (7.71e-05)	Tok/s 4060 (6513)	Loss/tok 3.1914 (4.0670)	LR 1.250e-04
1: TRAIN [1][860/922]	Time 0.335 (0.293)	Data 8.23e-05 (8.06e-05)	Tok/s 9751 (6514)	Loss/tok 3.9616 (4.0535)	LR 1.250e-04
0: TRAIN [1][860/922]	Time 0.335 (0.293)	Data 7.49e-05 (7.75e-05)	Tok/s 9955 (6517)	Loss/tok 4.1286 (4.0665)	LR 1.250e-04
3: TRAIN [1][860/922]	Time 0.335 (0.293)	Data 6.79e-05 (7.66e-05)	Tok/s 10050 (6511)	Loss/tok 4.0424 (4.0692)	LR 1.250e-04
2: TRAIN [1][860/922]	Time 0.335 (0.293)	Data 6.41e-05 (7.70e-05)	Tok/s 9954 (6517)	Loss/tok 4.0035 (4.0641)	LR 1.250e-04
1: TRAIN [1][870/922]	Time 0.344 (0.293)	Data 8.11e-05 (8.07e-05)	Tok/s 9517 (6500)	Loss/tok 4.0045 (4.0508)	LR 1.250e-04
0: TRAIN [1][870/922]	Time 0.344 (0.293)	Data 7.56e-05 (7.74e-05)	Tok/s 9311 (6503)	Loss/tok 3.9688 (4.0639)	LR 1.250e-04
2: TRAIN [1][870/922]	Time 0.344 (0.293)	Data 7.22e-05 (7.70e-05)	Tok/s 9606 (6503)	Loss/tok 4.1325 (4.0620)	LR 1.250e-04
3: TRAIN [1][870/922]	Time 0.344 (0.293)	Data 6.82e-05 (7.67e-05)	Tok/s 9520 (6497)	Loss/tok 4.0709 (4.0669)	LR 1.250e-04
1: TRAIN [1][880/922]	Time 0.285 (0.292)	Data 7.89e-05 (8.06e-05)	Tok/s 6158 (6497)	Loss/tok 3.4960 (4.0473)	LR 1.250e-04
0: TRAIN [1][880/922]	Time 0.285 (0.292)	Data 7.30e-05 (7.74e-05)	Tok/s 6270 (6500)	Loss/tok 3.9134 (4.0609)	LR 1.250e-04
3: TRAIN [1][880/922]	Time 0.285 (0.292)	Data 7.01e-05 (7.66e-05)	Tok/s 6400 (6494)	Loss/tok 3.6892 (4.0627)	LR 1.250e-04
2: TRAIN [1][880/922]	Time 0.285 (0.292)	Data 6.89e-05 (7.70e-05)	Tok/s 6496 (6499)	Loss/tok 3.5576 (4.0578)	LR 1.250e-04
1: TRAIN [1][890/922]	Time 0.266 (0.292)	Data 8.37e-05 (8.06e-05)	Tok/s 4267 (6487)	Loss/tok 3.4119 (4.0441)	LR 1.250e-04
0: TRAIN [1][890/922]	Time 0.266 (0.292)	Data 8.25e-05 (7.74e-05)	Tok/s 4155 (6489)	Loss/tok 3.3830 (4.0571)	LR 1.250e-04
3: TRAIN [1][890/922]	Time 0.266 (0.292)	Data 7.03e-05 (7.66e-05)	Tok/s 4201 (6484)	Loss/tok 3.3554 (4.0595)	LR 1.250e-04
2: TRAIN [1][890/922]	Time 0.266 (0.292)	Data 8.46e-05 (7.69e-05)	Tok/s 4160 (6489)	Loss/tok 3.3854 (4.0540)	LR 1.250e-04
1: TRAIN [1][900/922]	Time 0.339 (0.292)	Data 8.80e-05 (8.06e-05)	Tok/s 9392 (6473)	Loss/tok 4.1355 (4.0415)	LR 1.250e-04
0: TRAIN [1][900/922]	Time 0.339 (0.292)	Data 8.20e-05 (7.74e-05)	Tok/s 9565 (6476)	Loss/tok 4.2036 (4.0545)	LR 1.250e-04
3: TRAIN [1][900/922]	Time 0.339 (0.292)	Data 8.03e-05 (7.66e-05)	Tok/s 9772 (6470)	Loss/tok 4.0020 (4.0567)	LR 1.250e-04
2: TRAIN [1][900/922]	Time 0.339 (0.292)	Data 7.77e-05 (7.70e-05)	Tok/s 9598 (6475)	Loss/tok 4.1778 (4.0524)	LR 1.250e-04
0: TRAIN [1][910/922]	Time 0.264 (0.292)	Data 7.51e-05 (7.74e-05)	Tok/s 4021 (6477)	Loss/tok 3.4249 (4.0528)	LR 1.250e-04
3: TRAIN [1][910/922]	Time 0.264 (0.292)	Data 7.80e-05 (7.66e-05)	Tok/s 4089 (6471)	Loss/tok 3.5998 (4.0539)	LR 1.250e-04
1: TRAIN [1][910/922]	Time 0.264 (0.292)	Data 8.42e-05 (8.06e-05)	Tok/s 4107 (6474)	Loss/tok 3.5648 (4.0387)	LR 1.250e-04
2: TRAIN [1][910/922]	Time 0.264 (0.292)	Data 7.75e-05 (7.69e-05)	Tok/s 4064 (6476)	Loss/tok 3.2052 (4.0496)	LR 1.250e-04
1: TRAIN [1][920/922]	Time 0.308 (0.292)	Data 3.31e-05 (8.09e-05)	Tok/s 8240 (6466)	Loss/tok 4.0367 (4.0358)	LR 1.250e-04
0: TRAIN [1][920/922]	Time 0.308 (0.292)	Data 3.17e-05 (7.79e-05)	Tok/s 8086 (6469)	Loss/tok 3.9887 (4.0507)	LR 1.250e-04
3: TRAIN [1][920/922]	Time 0.308 (0.292)	Data 3.31e-05 (7.70e-05)	Tok/s 8010 (6463)	Loss/tok 3.8911 (4.0512)	LR 1.250e-04
2: TRAIN [1][920/922]	Time 0.308 (0.292)	Data 3.39e-05 (7.77e-05)	Tok/s 8237 (6468)	Loss/tok 3.9172 (4.0469)	LR 1.250e-04
0: Running validation on dev set
3: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
2: Running validation on dev set
3: Executing preallocation
1: Executing preallocation
2: Executing preallocation
3: VALIDATION [1][0/159]	Time 0.055 (0.000)	Data 6.57e-04 (0.00e+00)	Tok/s 23009 (0)	Loss/tok 5.4309 (5.4309)
2: VALIDATION [1][0/159]	Time 0.059 (0.000)	Data 8.46e-04 (0.00e+00)	Tok/s 22648 (0)	Loss/tok 5.2321 (5.2321)
1: VALIDATION [1][0/160]	Time 0.065 (0.000)	Data 6.72e-04 (0.00e+00)	Tok/s 22584 (0)	Loss/tok 5.8929 (5.8929)
0: VALIDATION [1][0/160]	Time 0.077 (0.000)	Data 6.47e-04 (0.00e+00)	Tok/s 21157 (0)	Loss/tok 5.6549 (5.6549)
3: VALIDATION [1][10/159]	Time 0.040 (0.043)	Data 5.08e-04 (5.24e-04)	Tok/s 21605 (22311)	Loss/tok 5.0855 (5.2312)
2: VALIDATION [1][10/159]	Time 0.039 (0.044)	Data 6.55e-04 (6.47e-04)	Tok/s 22180 (22060)	Loss/tok 4.8802 (5.2916)
1: VALIDATION [1][10/160]	Time 0.039 (0.044)	Data 5.16e-04 (5.21e-04)	Tok/s 22273 (22445)	Loss/tok 5.3285 (5.2594)
0: VALIDATION [1][10/160]	Time 0.039 (0.045)	Data 5.17e-04 (5.28e-04)	Tok/s 22417 (22062)	Loss/tok 4.9758 (5.2467)
3: VALIDATION [1][20/159]	Time 0.032 (0.039)	Data 5.04e-04 (5.19e-04)	Tok/s 22459 (22135)	Loss/tok 5.0645 (5.1891)
2: VALIDATION [1][20/159]	Time 0.034 (0.040)	Data 6.00e-04 (6.26e-04)	Tok/s 21741 (21970)	Loss/tok 5.2984 (5.2571)
1: VALIDATION [1][20/160]	Time 0.033 (0.040)	Data 5.04e-04 (5.17e-04)	Tok/s 22096 (22286)	Loss/tok 5.0698 (5.1494)
0: VALIDATION [1][20/160]	Time 0.034 (0.041)	Data 5.15e-04 (5.24e-04)	Tok/s 21555 (21901)	Loss/tok 4.7467 (5.2005)
3: VALIDATION [1][30/159]	Time 0.029 (0.037)	Data 5.01e-04 (5.16e-04)	Tok/s 22300 (22044)	Loss/tok 4.9599 (5.1106)
2: VALIDATION [1][30/159]	Time 0.031 (0.037)	Data 5.81e-04 (6.15e-04)	Tok/s 20781 (21815)	Loss/tok 5.2883 (5.2263)
1: VALIDATION [1][30/160]	Time 0.030 (0.037)	Data 5.19e-04 (5.14e-04)	Tok/s 21664 (22044)	Loss/tok 5.2086 (5.1251)
0: VALIDATION [1][30/160]	Time 0.030 (0.038)	Data 5.11e-04 (5.20e-04)	Tok/s 21578 (21873)	Loss/tok 5.2464 (5.1367)
3: VALIDATION [1][40/159]	Time 0.027 (0.035)	Data 5.01e-04 (5.13e-04)	Tok/s 21790 (21932)	Loss/tok 4.4813 (5.0980)
1: VALIDATION [1][40/160]	Time 0.027 (0.035)	Data 4.94e-04 (5.13e-04)	Tok/s 21979 (21942)	Loss/tok 5.3208 (5.1049)
2: VALIDATION [1][40/159]	Time 0.027 (0.035)	Data 5.44e-04 (6.06e-04)	Tok/s 21623 (21717)	Loss/tok 4.5139 (5.1566)
0: VALIDATION [1][40/160]	Time 0.028 (0.035)	Data 5.03e-04 (5.17e-04)	Tok/s 20681 (21791)	Loss/tok 4.4113 (5.1147)
3: VALIDATION [1][50/159]	Time 0.026 (0.033)	Data 5.03e-04 (5.12e-04)	Tok/s 20206 (21731)	Loss/tok 5.3102 (5.0697)
1: VALIDATION [1][50/160]	Time 0.026 (0.033)	Data 4.99e-04 (5.10e-04)	Tok/s 20570 (21806)	Loss/tok 5.0039 (5.0517)
2: VALIDATION [1][50/159]	Time 0.026 (0.033)	Data 5.63e-04 (5.96e-04)	Tok/s 20757 (21578)	Loss/tok 4.9819 (5.1118)
0: VALIDATION [1][50/160]	Time 0.026 (0.034)	Data 5.05e-04 (5.15e-04)	Tok/s 20511 (21590)	Loss/tok 4.9706 (5.0741)
3: VALIDATION [1][60/159]	Time 0.023 (0.031)	Data 4.97e-04 (5.10e-04)	Tok/s 21012 (21620)	Loss/tok 4.5017 (5.0390)
1: VALIDATION [1][60/160]	Time 0.024 (0.032)	Data 4.92e-04 (5.09e-04)	Tok/s 20103 (21641)	Loss/tok 4.7335 (5.0191)
2: VALIDATION [1][60/159]	Time 0.024 (0.032)	Data 5.31e-04 (5.89e-04)	Tok/s 20401 (21404)	Loss/tok 5.0980 (5.0793)
0: VALIDATION [1][60/160]	Time 0.024 (0.032)	Data 5.05e-04 (5.14e-04)	Tok/s 19997 (21387)	Loss/tok 4.8416 (5.0373)
3: VALIDATION [1][70/159]	Time 0.022 (0.030)	Data 4.95e-04 (5.09e-04)	Tok/s 19929 (21398)	Loss/tok 4.6500 (5.0164)
1: VALIDATION [1][70/160]	Time 0.022 (0.030)	Data 4.92e-04 (5.07e-04)	Tok/s 20160 (21522)	Loss/tok 4.8893 (5.0074)
2: VALIDATION [1][70/159]	Time 0.022 (0.031)	Data 5.49e-04 (5.85e-04)	Tok/s 20736 (21263)	Loss/tok 5.0641 (5.0632)
0: VALIDATION [1][70/160]	Time 0.022 (0.031)	Data 5.06e-04 (5.13e-04)	Tok/s 20289 (21328)	Loss/tok 4.2689 (4.9941)
3: VALIDATION [1][80/159]	Time 0.020 (0.029)	Data 4.96e-04 (5.09e-04)	Tok/s 20323 (21302)	Loss/tok 5.0079 (4.9899)
1: VALIDATION [1][80/160]	Time 0.020 (0.029)	Data 4.96e-04 (5.08e-04)	Tok/s 20367 (21343)	Loss/tok 4.5541 (4.9985)
2: VALIDATION [1][80/159]	Time 0.020 (0.029)	Data 5.50e-04 (5.81e-04)	Tok/s 20270 (21136)	Loss/tok 4.5955 (5.0291)
0: VALIDATION [1][80/160]	Time 0.020 (0.030)	Data 4.98e-04 (5.13e-04)	Tok/s 20406 (21211)	Loss/tok 4.5039 (4.9711)
3: VALIDATION [1][90/159]	Time 0.018 (0.028)	Data 4.96e-04 (5.09e-04)	Tok/s 20007 (21180)	Loss/tok 4.4589 (4.9776)
1: VALIDATION [1][90/160]	Time 0.019 (0.028)	Data 4.86e-04 (5.08e-04)	Tok/s 19620 (21204)	Loss/tok 4.4843 (4.9787)
2: VALIDATION [1][90/159]	Time 0.019 (0.028)	Data 5.36e-04 (5.76e-04)	Tok/s 19685 (21002)	Loss/tok 4.6045 (4.9965)
0: VALIDATION [1][90/160]	Time 0.018 (0.028)	Data 4.97e-04 (5.12e-04)	Tok/s 20110 (21056)	Loss/tok 4.6731 (4.9544)
3: VALIDATION [1][100/159]	Time 0.017 (0.027)	Data 5.05e-04 (5.08e-04)	Tok/s 19165 (20985)	Loss/tok 5.1078 (4.9616)
1: VALIDATION [1][100/160]	Time 0.018 (0.027)	Data 5.05e-04 (5.07e-04)	Tok/s 19013 (21008)	Loss/tok 5.0700 (4.9697)
2: VALIDATION [1][100/159]	Time 0.017 (0.027)	Data 5.14e-04 (5.72e-04)	Tok/s 18988 (20830)	Loss/tok 4.2747 (4.9735)
0: VALIDATION [1][100/160]	Time 0.017 (0.027)	Data 5.00e-04 (5.10e-04)	Tok/s 19268 (20900)	Loss/tok 5.2062 (4.9444)
3: VALIDATION [1][110/159]	Time 0.016 (0.026)	Data 4.94e-04 (5.07e-04)	Tok/s 18314 (20764)	Loss/tok 4.4319 (4.9450)
1: VALIDATION [1][110/160]	Time 0.017 (0.026)	Data 4.89e-04 (5.06e-04)	Tok/s 18266 (20801)	Loss/tok 5.0963 (4.9483)
2: VALIDATION [1][110/159]	Time 0.016 (0.026)	Data 5.42e-04 (5.68e-04)	Tok/s 18565 (20622)	Loss/tok 5.2676 (4.9601)
0: VALIDATION [1][110/160]	Time 0.017 (0.026)	Data 5.00e-04 (5.10e-04)	Tok/s 18036 (20677)	Loss/tok 4.5187 (4.9208)
3: VALIDATION [1][120/159]	Time 0.015 (0.025)	Data 4.89e-04 (5.07e-04)	Tok/s 17819 (20540)	Loss/tok 4.7552 (4.9260)
1: VALIDATION [1][120/160]	Time 0.016 (0.025)	Data 4.88e-04 (5.04e-04)	Tok/s 17164 (20564)	Loss/tok 4.6916 (4.9274)
2: VALIDATION [1][120/159]	Time 0.016 (0.025)	Data 5.18e-04 (5.65e-04)	Tok/s 17344 (20378)	Loss/tok 4.3887 (4.9562)
0: VALIDATION [1][120/160]	Time 0.015 (0.026)	Data 4.91e-04 (5.10e-04)	Tok/s 17605 (20457)	Loss/tok 4.8772 (4.9078)
3: VALIDATION [1][130/159]	Time 0.013 (0.024)	Data 4.92e-04 (5.06e-04)	Tok/s 17891 (20280)	Loss/tok 4.5156 (4.9082)
1: VALIDATION [1][130/160]	Time 0.014 (0.024)	Data 5.11e-04 (5.03e-04)	Tok/s 17694 (20324)	Loss/tok 4.3208 (4.9138)
2: VALIDATION [1][130/159]	Time 0.014 (0.025)	Data 5.23e-04 (5.61e-04)	Tok/s 17279 (20157)	Loss/tok 4.7139 (4.9381)
0: VALIDATION [1][130/160]	Time 0.013 (0.025)	Data 4.91e-04 (5.09e-04)	Tok/s 18718 (20220)	Loss/tok 4.5645 (4.8971)
3: VALIDATION [1][140/159]	Time 0.012 (0.023)	Data 4.88e-04 (5.05e-04)	Tok/s 16641 (20087)	Loss/tok 5.0727 (4.9000)
1: VALIDATION [1][140/160]	Time 0.012 (0.024)	Data 4.89e-04 (5.02e-04)	Tok/s 16330 (20138)	Loss/tok 4.6851 (4.8952)
2: VALIDATION [1][140/159]	Time 0.012 (0.024)	Data 5.08e-04 (5.58e-04)	Tok/s 17297 (19955)	Loss/tok 4.4700 (4.9274)
0: VALIDATION [1][140/160]	Time 0.012 (0.024)	Data 4.91e-04 (5.08e-04)	Tok/s 17217 (20011)	Loss/tok 4.2425 (4.8856)
3: VALIDATION [1][150/159]	Time 0.010 (0.023)	Data 4.83e-04 (5.04e-04)	Tok/s 15252 (19813)	Loss/tok 4.2804 (4.8862)
1: VALIDATION [1][150/160]	Time 0.010 (0.023)	Data 4.82e-04 (5.01e-04)	Tok/s 15702 (19887)	Loss/tok 3.8564 (4.8752)
2: VALIDATION [1][150/159]	Time 0.011 (0.023)	Data 5.26e-04 (5.56e-04)	Tok/s 14575 (19685)	Loss/tok 4.9274 (4.9101)
0: VALIDATION [1][150/160]	Time 0.010 (0.023)	Data 5.01e-04 (5.07e-04)	Tok/s 15800 (19752)	Loss/tok 4.4428 (4.8709)
0: Saving model to gnmt/model_best.pth
2: Running evaluation on test set
3: Running evaluation on test set
1: Running evaluation on test set
0: Running evaluation on test set
3: TEST [1][9/24]	Time 0.5204 (0.6019)	Decoder iters 62.0 (129.0)	Tok/s 3768 (4375)
0: TEST [1][9/24]	Time 0.5208 (0.6017)	Decoder iters 149.0 (128.2)	Tok/s 3871 (4754)
1: TEST [1][9/24]	Time 0.5205 (0.6019)	Decoder iters 47.0 (111.1)	Tok/s 3773 (4492)
2: TEST [1][9/24]	Time 0.5208 (0.6019)	Decoder iters 46.0 (102.1)	Tok/s 3702 (4395)
0: TEST [1][19/24]	Time 0.1514 (0.4733)	Decoder iters 29.0 (94.0)	Tok/s 7309 (4815)
1: TEST [1][19/24]	Time 0.1517 (0.4734)	Decoder iters 31.0 (85.2)	Tok/s 6756 (4622)
3: TEST [1][19/24]	Time 0.1519 (0.4734)	Decoder iters 24.0 (96.7)	Tok/s 6624 (4507)
2: TEST [1][19/24]	Time 0.1518 (0.4734)	Decoder iters 32.0 (77.8)	Tok/s 6838 (4570)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
3: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
2: Finished evaluation on test set
3: Finished epoch 1
1: Finished epoch 1
2: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.0458	Validation Loss: 4.8740	Test BLEU: 8.24
0: Performance: Epoch: 1	Training: 25855 Tok/s	Validation: 77862 Tok/s
0: Finished epoch 1
1: Total training time 606 s
2: Total training time 606 s
3: Total training time 606 s
0: Total training time 606 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       4|                  40|                      8.24|                      25851.3|                         10.10|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
