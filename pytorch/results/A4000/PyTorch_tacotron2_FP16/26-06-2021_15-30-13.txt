DLL 2021-06-26 15:30:14.754601 - PARAMETER output : ./ 
DLL 2021-06-26 15:30:14.754659 - PARAMETER dataset_path : /data/tacotron2/LJSpeech-1.1 
DLL 2021-06-26 15:30:14.754683 - PARAMETER model_name : Tacotron2 
DLL 2021-06-26 15:30:14.754701 - PARAMETER log_file : nvlog.json 
DLL 2021-06-26 15:30:14.754717 - PARAMETER anneal_steps : None 
DLL 2021-06-26 15:30:14.754734 - PARAMETER anneal_factor : 0.1 
DLL 2021-06-26 15:30:14.754751 - PARAMETER epochs : 2 
DLL 2021-06-26 15:30:14.754767 - PARAMETER epochs_per_checkpoint : 50 
DLL 2021-06-26 15:30:14.754783 - PARAMETER checkpoint_path :  
DLL 2021-06-26 15:30:14.754799 - PARAMETER resume_from_last : False 
DLL 2021-06-26 15:30:14.754816 - PARAMETER dynamic_loss_scaling : True 
DLL 2021-06-26 15:30:14.754832 - PARAMETER amp : False 
DLL 2021-06-26 15:30:14.754848 - PARAMETER cudnn_enabled : True 
DLL 2021-06-26 15:30:14.754864 - PARAMETER cudnn_benchmark : False 
DLL 2021-06-26 15:30:14.754879 - PARAMETER disable_uniform_initialize_bn_weight : False 
DLL 2021-06-26 15:30:14.754899 - PARAMETER use_saved_learning_rate : False 
DLL 2021-06-26 15:30:14.754915 - PARAMETER learning_rate : 0.0 
DLL 2021-06-26 15:30:14.754931 - PARAMETER weight_decay : 1e-06 
DLL 2021-06-26 15:30:14.754947 - PARAMETER grad_clip_thresh : 1.0 
DLL 2021-06-26 15:30:14.754963 - PARAMETER batch_size : 64 
DLL 2021-06-26 15:30:14.754979 - PARAMETER grad_clip : 5.0 
DLL 2021-06-26 15:30:14.754994 - PARAMETER load_mel_from_disk : False 
DLL 2021-06-26 15:30:14.755010 - PARAMETER training_files : filelists/ljs_audio_text_train_subset_625_filelist.txt 
DLL 2021-06-26 15:30:14.755024 - PARAMETER validation_files : filelists/ljs_audio_text_val_filelist.txt 
DLL 2021-06-26 15:30:14.755039 - PARAMETER text_cleaners : ['english_cleaners'] 
DLL 2021-06-26 15:30:14.755055 - PARAMETER max_wav_value : 32768.0 
DLL 2021-06-26 15:30:14.755070 - PARAMETER sampling_rate : 22050 
DLL 2021-06-26 15:30:14.755085 - PARAMETER filter_length : 1024 
DLL 2021-06-26 15:30:14.755099 - PARAMETER hop_length : 256 
DLL 2021-06-26 15:30:14.755113 - PARAMETER win_length : 1024 
DLL 2021-06-26 15:30:14.755127 - PARAMETER mel_fmin : 0.0 
DLL 2021-06-26 15:30:14.755142 - PARAMETER mel_fmax : 8000.0 
DLL 2021-06-26 15:30:14.755157 - PARAMETER rank : 0 
DLL 2021-06-26 15:30:14.755171 - PARAMETER world_size : 1 
DLL 2021-06-26 15:30:14.755185 - PARAMETER dist_url : tcp://localhost:23456 
DLL 2021-06-26 15:30:14.755199 - PARAMETER group_name : group_name 
DLL 2021-06-26 15:30:14.755213 - PARAMETER dist_backend : nccl 
DLL 2021-06-26 15:30:14.755227 - PARAMETER bench_class :  
DLL 2021-06-26 15:30:14.755241 - PARAMETER model_name : Tacotron2_PyT 
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/workspace/examples/tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return s in _symbol_to_id and s is not '_' and s is not '~'
/workspace/examples/tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return s in _symbol_to_id and s is not '_' and s is not '~'
DLL 2021-06-26 15:30:30.627516 - (0, 0) glob_iter/iters_per_epoch : 0/9 
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 670, in forward
    mel_outputs_postnet = self.postnet(mel_outputs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 177, in forward
    x = F.dropout(torch.tanh(conv(x)), 0.5, training=self.training)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py", line 118, in forward
    input = module(input)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py", line 136, in forward
    return F.batch_norm(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 2146, in batch_norm
    return torch.batch_norm(
RuntimeError: CUDA out of memory. Tried to allocate 108.00 MiB (GPU 0; 15.74 GiB total capacity; 13.71 GiB already allocated; 26.69 MiB free; 14.12 GiB reserved in total by PyTorch)
DONE!
