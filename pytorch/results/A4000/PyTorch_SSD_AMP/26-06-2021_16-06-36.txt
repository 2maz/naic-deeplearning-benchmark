Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 1.04M/97.8M [00:00<00:09, 10.9MB/s]  7%|▋         | 7.02M/97.8M [00:00<00:06, 14.4MB/s] 12%|█▏        | 11.8M/97.8M [00:00<00:04, 18.4MB/s] 15%|█▌        | 15.0M/97.8M [00:00<00:04, 21.2MB/s] 18%|█▊        | 18.0M/97.8M [00:00<00:03, 23.2MB/s] 21%|██▏       | 20.9M/97.8M [00:00<00:03, 23.7MB/s] 25%|██▌       | 24.5M/97.8M [00:00<00:02, 26.7MB/s] 28%|██▊       | 27.5M/97.8M [00:00<00:02, 26.7MB/s] 31%|███       | 30.4M/97.8M [00:00<00:02, 26.5MB/s] 36%|███▌      | 35.4M/97.8M [00:01<00:02, 31.1MB/s] 41%|████      | 40.1M/97.8M [00:01<00:01, 35.0MB/s] 45%|████▍     | 43.9M/97.8M [00:01<00:01, 35.8MB/s] 49%|████▉     | 47.7M/97.8M [00:01<00:01, 36.7MB/s] 53%|█████▎    | 51.4M/97.8M [00:01<00:01, 31.5MB/s] 57%|█████▋    | 55.2M/97.8M [00:01<00:01, 33.7MB/s] 61%|██████    | 59.3M/97.8M [00:01<00:01, 35.8MB/s] 66%|██████▌   | 64.7M/97.8M [00:01<00:00, 40.3MB/s] 71%|███████   | 69.4M/97.8M [00:01<00:00, 42.7MB/s] 76%|███████▋  | 74.7M/97.8M [00:02<00:00, 45.3MB/s] 81%|████████  | 79.2M/97.8M [00:02<00:00, 42.7MB/s] 86%|████████▌ | 84.1M/97.8M [00:02<00:00, 45.0MB/s] 91%|█████████ | 88.6M/97.8M [00:02<00:00, 38.9MB/s] 95%|█████████▌| 93.1M/97.8M [00:02<00:00, 41.1MB/s]100%|█████████▉| 97.3M/97.8M [00:02<00:00, 37.8MB/s]100%|██████████| 97.8M/97.8M [00:02<00:00, 38.2MB/s]
DLL 2021-06-26 16:06:41.849431 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 88  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 477
loading annotations into memory...
Done (t=0.38s)
creating index...
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
DLL 2021-06-26 16:12:53.292546 - () avg_img/sec : 125.71475532278981  med_img/sec : 125.6767369564986  min_img/sec : 124.93568785943249  max_img/sec : 126.5310799229361 
Done benchmarking. Total images: 35200	total time: 279.999	Average images/sec: 125.715	Median images/sec: 125.677
Training performance = 125.6767349243164 FPS
DLL 2021-06-26 16:12:53.293052 - (0,) time : 352.58343148231506 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-26 16:12:53.293599 - () total time : 352.58343148231506 
DLL 2021-06-26 16:12:53.293618 - () 
DONE!
