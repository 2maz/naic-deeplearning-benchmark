Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|▏         | 1.25M/97.8M [00:00<00:08, 12.1MB/s]  3%|▎         | 2.94M/97.8M [00:00<00:07, 12.5MB/s]  4%|▍         | 4.30M/97.8M [00:00<00:07, 13.0MB/s]  6%|▋         | 6.24M/97.8M [00:00<00:06, 14.0MB/s]  8%|▊         | 8.00M/97.8M [00:00<00:06, 14.9MB/s] 10%|▉         | 9.74M/97.8M [00:00<00:05, 15.8MB/s] 12%|█▏        | 11.7M/97.8M [00:00<00:05, 16.9MB/s] 14%|█▎        | 13.2M/97.8M [00:00<00:05, 16.1MB/s] 15%|█▌        | 14.8M/97.8M [00:00<00:05, 15.0MB/s] 17%|█▋        | 16.6M/97.8M [00:01<00:05, 15.7MB/s] 19%|█▉        | 18.6M/97.8M [00:01<00:04, 17.0MB/s] 21%|██        | 20.3M/97.8M [00:01<00:04, 16.9MB/s] 23%|██▎       | 22.3M/97.8M [00:01<00:04, 17.9MB/s] 25%|██▍       | 24.1M/97.8M [00:01<00:04, 16.4MB/s] 27%|██▋       | 26.1M/97.8M [00:01<00:04, 17.6MB/s] 28%|██▊       | 27.8M/97.8M [00:01<00:05, 13.7MB/s] 30%|██▉       | 29.3M/97.8M [00:02<00:06, 10.3MB/s] 32%|███▏      | 30.8M/97.8M [00:02<00:06, 11.2MB/s] 33%|███▎      | 32.5M/97.8M [00:02<00:05, 12.5MB/s] 35%|███▌      | 34.4M/97.8M [00:02<00:04, 14.0MB/s] 37%|███▋      | 36.0M/97.8M [00:02<00:04, 14.6MB/s] 38%|███▊      | 37.6M/97.8M [00:02<00:04, 13.8MB/s] 40%|███▉      | 39.0M/97.8M [00:02<00:04, 13.5MB/s] 41%|████▏     | 40.4M/97.8M [00:02<00:04, 12.4MB/s] 43%|████▎     | 42.0M/97.8M [00:03<00:04, 13.4MB/s] 44%|████▍     | 43.3M/97.8M [00:03<00:05, 9.71MB/s] 45%|████▌     | 44.4M/97.8M [00:03<00:05, 9.59MB/s] 47%|████▋     | 45.5M/97.8M [00:03<00:05, 9.43MB/s] 48%|████▊     | 46.5M/97.8M [00:03<00:05, 9.80MB/s] 49%|████▉     | 48.0M/97.8M [00:03<00:04, 10.9MB/s] 51%|█████     | 49.4M/97.8M [00:03<00:04, 11.9MB/s] 52%|█████▏    | 50.9M/97.8M [00:03<00:03, 12.7MB/s] 53%|█████▎    | 52.2M/97.8M [00:04<00:04, 11.3MB/s] 55%|█████▍    | 53.4M/97.8M [00:04<00:04, 11.6MB/s] 56%|█████▌    | 54.5M/97.8M [00:04<00:04, 10.9MB/s] 58%|█████▊    | 56.4M/97.8M [00:04<00:03, 12.5MB/s] 59%|█████▉    | 57.7M/97.8M [00:04<00:03, 12.8MB/s] 60%|██████    | 59.1M/97.8M [00:04<00:03, 13.2MB/s] 62%|██████▏   | 60.4M/97.8M [00:04<00:03, 12.6MB/s] 63%|██████▎   | 61.7M/97.8M [00:04<00:04, 9.19MB/s] 64%|██████▍   | 62.7M/97.8M [00:05<00:04, 8.47MB/s] 65%|██████▌   | 63.9M/97.8M [00:05<00:03, 9.34MB/s] 67%|██████▋   | 65.3M/97.8M [00:05<00:03, 10.5MB/s] 69%|██████▊   | 67.1M/97.8M [00:05<00:02, 12.0MB/s] 71%|███████   | 69.1M/97.8M [00:05<00:02, 13.7MB/s] 72%|███████▏  | 70.6M/97.8M [00:05<00:02, 14.0MB/s] 74%|███████▎  | 72.1M/97.8M [00:05<00:01, 14.0MB/s] 76%|███████▌  | 74.0M/97.8M [00:05<00:01, 15.4MB/s] 77%|███████▋  | 75.6M/97.8M [00:05<00:01, 14.7MB/s] 79%|███████▉  | 77.1M/97.8M [00:06<00:01, 14.3MB/s] 81%|████████  | 78.8M/97.8M [00:06<00:01, 15.1MB/s] 82%|████████▏ | 80.6M/97.8M [00:06<00:01, 15.9MB/s] 84%|████████▍ | 82.1M/97.8M [00:06<00:01, 15.8MB/s] 86%|████████▌ | 83.8M/97.8M [00:06<00:00, 16.2MB/s] 87%|████████▋ | 85.5M/97.8M [00:06<00:00, 16.7MB/s] 89%|████████▉ | 87.3M/97.8M [00:06<00:00, 17.1MB/s] 91%|█████████▏| 89.2M/97.8M [00:06<00:00, 17.5MB/s] 93%|█████████▎| 91.1M/97.8M [00:06<00:00, 18.0MB/s] 95%|█████████▍| 92.8M/97.8M [00:07<00:00, 16.4MB/s] 97%|█████████▋| 94.4M/97.8M [00:07<00:00, 15.8MB/s] 98%|█████████▊| 96.3M/97.8M [00:07<00:00, 16.7MB/s]100%|██████████| 97.8M/97.8M [00:07<00:00, 14.0MB/s]
DLL 2021-06-03 06:15:40.284719 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 7043
loading annotations into memory...
Done (t=0.51s)
creating index...
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
DLL 2021-06-03 06:21:39.290864 - () avg_img/sec : 200.035506147546  med_img/sec : 199.96736241749275  min_img/sec : 199.04233664922228  max_img/sec : 201.81438313015082 
Done benchmarking. Total images: 56000	total time: 279.950	Average images/sec: 200.036	Median images/sec: 199.967
Training performance = 199.9673614501953 FPS
DLL 2021-06-03 06:21:39.291070 - (0,) time : 345.8994493484497 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-03 06:21:39.291238 - () total time : 345.8994493484497 
DLL 2021-06-03 06:21:39.291251 - () 
DONE!
