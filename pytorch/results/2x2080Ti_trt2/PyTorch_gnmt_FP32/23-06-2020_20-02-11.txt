1: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 909
1: Scheduler decay interval: 114
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 909
0: Scheduler decay interval: 114
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/683]	Time 0.606 (0.606)	Data 1.49e-01 (1.49e-01)	Tok/s 11220 (11220)	Loss/tok 10.7036 (10.7036)	LR 2.047e-05
1: TRAIN [0][0/683]	Time 0.608 (0.608)	Data 1.33e-01 (1.33e-01)	Tok/s 11130 (11130)	Loss/tok 10.6986 (10.6986)	LR 2.047e-05
0: TRAIN [0][10/683]	Time 0.388 (0.391)	Data 1.34e-04 (1.36e-02)	Tok/s 12706 (11281)	Loss/tok 9.7140 (10.1935)	LR 2.576e-05
1: TRAIN [0][10/683]	Time 0.388 (0.391)	Data 1.03e-04 (1.22e-02)	Tok/s 12342 (11246)	Loss/tok 9.6614 (10.1808)	LR 2.576e-05
0: TRAIN [0][20/683]	Time 0.313 (0.392)	Data 9.58e-05 (7.18e-03)	Tok/s 9796 (11296)	Loss/tok 9.1079 (9.8237)	LR 3.244e-05
1: TRAIN [0][20/683]	Time 0.312 (0.392)	Data 1.18e-04 (6.47e-03)	Tok/s 9523 (11280)	Loss/tok 9.1135 (9.8124)	LR 3.244e-05
0: TRAIN [0][30/683]	Time 0.315 (0.412)	Data 1.50e-04 (4.90e-03)	Tok/s 9683 (11838)	Loss/tok 8.8299 (9.5483)	LR 4.083e-05
1: TRAIN [0][30/683]	Time 0.315 (0.411)	Data 1.57e-04 (4.43e-03)	Tok/s 9671 (11833)	Loss/tok 8.7528 (9.5369)	LR 4.083e-05
0: TRAIN [0][40/683]	Time 0.593 (0.414)	Data 9.47e-05 (3.73e-03)	Tok/s 14897 (11842)	Loss/tok 8.7691 (9.3681)	LR 5.141e-05
1: TRAIN [0][40/683]	Time 0.603 (0.414)	Data 1.30e-04 (3.38e-03)	Tok/s 14521 (11843)	Loss/tok 8.8338 (9.3544)	LR 5.141e-05
0: TRAIN [0][50/683]	Time 0.594 (0.415)	Data 9.08e-05 (3.01e-03)	Tok/s 14888 (11866)	Loss/tok 8.6714 (9.2263)	LR 6.472e-05
1: TRAIN [0][50/683]	Time 0.595 (0.415)	Data 1.27e-04 (2.74e-03)	Tok/s 15031 (11886)	Loss/tok 8.6443 (9.2155)	LR 6.472e-05
0: TRAIN [0][60/683]	Time 0.483 (0.416)	Data 1.09e-04 (2.54e-03)	Tok/s 14147 (11905)	Loss/tok 8.3371 (9.0865)	LR 8.148e-05
1: TRAIN [0][60/683]	Time 0.483 (0.416)	Data 1.18e-04 (2.31e-03)	Tok/s 14077 (11911)	Loss/tok 8.3308 (9.0796)	LR 8.148e-05
0: TRAIN [0][70/683]	Time 0.390 (0.415)	Data 1.01e-04 (2.20e-03)	Tok/s 12376 (11921)	Loss/tok 8.1363 (8.9897)	LR 1.026e-04
1: TRAIN [0][70/683]	Time 0.390 (0.415)	Data 1.23e-04 (2.00e-03)	Tok/s 12323 (11924)	Loss/tok 8.0898 (8.9825)	LR 1.026e-04
0: TRAIN [0][80/683]	Time 0.389 (0.419)	Data 8.61e-05 (1.94e-03)	Tok/s 12469 (12067)	Loss/tok 7.9632 (8.8651)	LR 1.291e-04
1: TRAIN [0][80/683]	Time 0.388 (0.419)	Data 1.08e-04 (1.77e-03)	Tok/s 12751 (12071)	Loss/tok 7.9535 (8.8594)	LR 1.291e-04
0: TRAIN [0][90/683]	Time 0.312 (0.422)	Data 9.25e-05 (1.73e-03)	Tok/s 9285 (12117)	Loss/tok 7.5812 (8.7541)	LR 1.626e-04
1: TRAIN [0][90/683]	Time 0.313 (0.422)	Data 1.29e-04 (1.59e-03)	Tok/s 9052 (12117)	Loss/tok 7.6269 (8.7506)	LR 1.626e-04
0: TRAIN [0][100/683]	Time 0.489 (0.418)	Data 1.10e-04 (1.57e-03)	Tok/s 13974 (12055)	Loss/tok 7.9487 (8.6730)	LR 2.047e-04
1: TRAIN [0][100/683]	Time 0.489 (0.418)	Data 1.26e-04 (1.44e-03)	Tok/s 13766 (12052)	Loss/tok 7.9330 (8.6726)	LR 2.047e-04
0: TRAIN [0][110/683]	Time 0.488 (0.416)	Data 9.44e-05 (1.44e-03)	Tok/s 13753 (11981)	Loss/tok 7.8416 (8.6038)	LR 2.576e-04
1: TRAIN [0][110/683]	Time 0.487 (0.416)	Data 1.09e-04 (1.32e-03)	Tok/s 13965 (11982)	Loss/tok 7.8757 (8.6022)	LR 2.576e-04
0: TRAIN [0][120/683]	Time 0.316 (0.416)	Data 8.42e-05 (1.33e-03)	Tok/s 9329 (12004)	Loss/tok 7.5888 (8.5328)	LR 3.244e-04
1: TRAIN [0][120/683]	Time 0.316 (0.416)	Data 1.05e-04 (1.22e-03)	Tok/s 9213 (11993)	Loss/tok 7.5120 (8.5332)	LR 3.244e-04
0: TRAIN [0][130/683]	Time 0.391 (0.416)	Data 9.30e-05 (1.23e-03)	Tok/s 12274 (12007)	Loss/tok 7.6551 (8.4735)	LR 4.083e-04
1: TRAIN [0][130/683]	Time 0.391 (0.416)	Data 1.07e-04 (1.14e-03)	Tok/s 12622 (11996)	Loss/tok 7.8411 (8.4756)	LR 4.083e-04
0: TRAIN [0][140/683]	Time 0.314 (0.421)	Data 9.35e-05 (1.15e-03)	Tok/s 9005 (12105)	Loss/tok 7.4503 (8.4119)	LR 5.141e-04
1: TRAIN [0][140/683]	Time 0.314 (0.420)	Data 1.03e-04 (1.07e-03)	Tok/s 9232 (12089)	Loss/tok 7.3878 (8.4149)	LR 5.141e-04
1: TRAIN [0][150/683]	Time 0.489 (0.417)	Data 1.20e-04 (1.00e-03)	Tok/s 13803 (11990)	Loss/tok 7.7500 (8.3742)	LR 6.472e-04
0: TRAIN [0][150/683]	Time 0.489 (0.417)	Data 8.11e-05 (1.08e-03)	Tok/s 13581 (12006)	Loss/tok 7.8080 (8.3718)	LR 6.472e-04
0: TRAIN [0][160/683]	Time 0.316 (0.416)	Data 9.70e-05 (1.02e-03)	Tok/s 9406 (11996)	Loss/tok 7.2540 (8.3270)	LR 8.148e-04
1: TRAIN [0][160/683]	Time 0.316 (0.416)	Data 1.02e-04 (9.46e-04)	Tok/s 9169 (11981)	Loss/tok 7.2619 (8.3295)	LR 8.148e-04
0: TRAIN [0][170/683]	Time 0.316 (0.416)	Data 9.61e-05 (9.66e-04)	Tok/s 9431 (11987)	Loss/tok 7.2389 (8.2893)	LR 1.026e-03
1: TRAIN [0][170/683]	Time 0.316 (0.416)	Data 1.11e-04 (8.98e-04)	Tok/s 9363 (11971)	Loss/tok 7.3130 (8.2929)	LR 1.026e-03
0: TRAIN [0][180/683]	Time 0.592 (0.417)	Data 9.04e-05 (9.18e-04)	Tok/s 14830 (12017)	Loss/tok 7.7220 (8.2472)	LR 1.291e-03
1: TRAIN [0][180/683]	Time 0.591 (0.417)	Data 1.03e-04 (8.54e-04)	Tok/s 14725 (11998)	Loss/tok 7.7656 (8.2537)	LR 1.291e-03
0: TRAIN [0][190/683]	Time 0.489 (0.423)	Data 9.27e-05 (8.75e-04)	Tok/s 13875 (12130)	Loss/tok 7.5919 (8.2073)	LR 1.626e-03
1: TRAIN [0][190/683]	Time 0.489 (0.423)	Data 1.13e-04 (8.15e-04)	Tok/s 13848 (12109)	Loss/tok 7.5037 (8.2123)	LR 1.626e-03
0: TRAIN [0][200/683]	Time 0.638 (0.424)	Data 8.25e-05 (8.36e-04)	Tok/s 13842 (12145)	Loss/tok 7.6365 (8.1695)	LR 2.000e-03
1: TRAIN [0][200/683]	Time 0.565 (0.424)	Data 1.10e-04 (7.80e-04)	Tok/s 15653 (12127)	Loss/tok 7.6480 (8.1747)	LR 2.000e-03
0: TRAIN [0][210/683]	Time 0.315 (0.426)	Data 9.11e-05 (8.01e-04)	Tok/s 9198 (12192)	Loss/tok 7.0430 (8.1326)	LR 2.000e-03
1: TRAIN [0][210/683]	Time 0.315 (0.426)	Data 1.22e-04 (7.48e-04)	Tok/s 9447 (12181)	Loss/tok 6.9877 (8.1349)	LR 2.000e-03
0: TRAIN [0][220/683]	Time 0.316 (0.425)	Data 7.25e-05 (7.69e-04)	Tok/s 9076 (12145)	Loss/tok 6.8149 (8.1009)	LR 2.000e-03
1: TRAIN [0][220/683]	Time 0.315 (0.425)	Data 1.14e-04 (7.19e-04)	Tok/s 9514 (12133)	Loss/tok 7.1496 (8.1025)	LR 2.000e-03
0: TRAIN [0][230/683]	Time 0.494 (0.426)	Data 8.80e-05 (7.40e-04)	Tok/s 13751 (12181)	Loss/tok 7.3008 (8.0593)	LR 2.000e-03
1: TRAIN [0][230/683]	Time 0.493 (0.426)	Data 1.51e-04 (6.93e-04)	Tok/s 14063 (12170)	Loss/tok 7.2349 (8.0608)	LR 2.000e-03
0: TRAIN [0][240/683]	Time 0.316 (0.425)	Data 9.18e-05 (7.13e-04)	Tok/s 9391 (12155)	Loss/tok 6.6536 (8.0242)	LR 2.000e-03
1: TRAIN [0][240/683]	Time 0.316 (0.425)	Data 1.02e-04 (6.71e-04)	Tok/s 9126 (12144)	Loss/tok 6.7558 (8.0246)	LR 2.000e-03
0: TRAIN [0][250/683]	Time 0.600 (0.426)	Data 9.18e-05 (6.88e-04)	Tok/s 14623 (12165)	Loss/tok 7.1560 (7.9827)	LR 2.000e-03
1: TRAIN [0][250/683]	Time 0.600 (0.426)	Data 1.07e-04 (6.48e-04)	Tok/s 14736 (12157)	Loss/tok 7.1709 (7.9827)	LR 2.000e-03
0: TRAIN [0][260/683]	Time 0.392 (0.426)	Data 8.99e-05 (6.65e-04)	Tok/s 12529 (12161)	Loss/tok 6.7672 (7.9431)	LR 2.000e-03
1: TRAIN [0][260/683]	Time 0.392 (0.426)	Data 1.07e-04 (6.27e-04)	Tok/s 12502 (12157)	Loss/tok 6.8870 (7.9425)	LR 2.000e-03
0: TRAIN [0][270/683]	Time 0.598 (0.426)	Data 8.44e-05 (6.44e-04)	Tok/s 14773 (12149)	Loss/tok 7.1152 (7.9065)	LR 2.000e-03
1: TRAIN [0][270/683]	Time 0.598 (0.426)	Data 1.05e-04 (6.08e-04)	Tok/s 14845 (12147)	Loss/tok 7.1596 (7.9052)	LR 2.000e-03
0: TRAIN [0][280/683]	Time 0.315 (0.426)	Data 8.99e-05 (6.24e-04)	Tok/s 9291 (12162)	Loss/tok 6.4325 (7.8660)	LR 2.000e-03
1: TRAIN [0][280/683]	Time 0.315 (0.426)	Data 1.03e-04 (5.90e-04)	Tok/s 9435 (12158)	Loss/tok 6.4590 (7.8640)	LR 2.000e-03
0: TRAIN [0][290/683]	Time 0.316 (0.426)	Data 1.07e-04 (6.06e-04)	Tok/s 9537 (12168)	Loss/tok 6.3289 (7.8257)	LR 2.000e-03
1: TRAIN [0][290/683]	Time 0.316 (0.426)	Data 1.04e-04 (5.74e-04)	Tok/s 9530 (12164)	Loss/tok 6.1563 (7.8231)	LR 2.000e-03
0: TRAIN [0][300/683]	Time 0.311 (0.426)	Data 9.13e-05 (5.89e-04)	Tok/s 9338 (12180)	Loss/tok 6.1421 (7.7828)	LR 2.000e-03
1: TRAIN [0][300/683]	Time 0.311 (0.426)	Data 1.27e-04 (5.58e-04)	Tok/s 9422 (12176)	Loss/tok 6.2510 (7.7814)	LR 2.000e-03
0: TRAIN [0][310/683]	Time 0.314 (0.427)	Data 9.75e-05 (5.73e-04)	Tok/s 9328 (12194)	Loss/tok 6.1453 (7.7411)	LR 2.000e-03
1: TRAIN [0][310/683]	Time 0.314 (0.427)	Data 1.08e-04 (5.44e-04)	Tok/s 9242 (12192)	Loss/tok 6.2446 (7.7389)	LR 2.000e-03
0: TRAIN [0][320/683]	Time 0.395 (0.426)	Data 1.00e-04 (5.58e-04)	Tok/s 12470 (12184)	Loss/tok 6.4144 (7.7043)	LR 2.000e-03
1: TRAIN [0][320/683]	Time 0.395 (0.426)	Data 1.05e-04 (5.30e-04)	Tok/s 12481 (12182)	Loss/tok 6.3232 (7.7019)	LR 2.000e-03
0: TRAIN [0][330/683]	Time 0.491 (0.426)	Data 9.85e-05 (5.44e-04)	Tok/s 14071 (12178)	Loss/tok 6.4252 (7.6660)	LR 2.000e-03
1: TRAIN [0][330/683]	Time 0.490 (0.426)	Data 1.05e-04 (5.18e-04)	Tok/s 13985 (12174)	Loss/tok 6.5624 (7.6634)	LR 2.000e-03
0: TRAIN [0][340/683]	Time 0.491 (0.425)	Data 9.89e-05 (5.31e-04)	Tok/s 13720 (12171)	Loss/tok 6.5178 (7.6300)	LR 2.000e-03
1: TRAIN [0][340/683]	Time 0.491 (0.425)	Data 1.06e-04 (5.06e-04)	Tok/s 13815 (12170)	Loss/tok 6.5000 (7.6263)	LR 2.000e-03
0: TRAIN [0][350/683]	Time 0.390 (0.424)	Data 9.42e-05 (5.18e-04)	Tok/s 12354 (12130)	Loss/tok 6.1459 (7.5987)	LR 2.000e-03
1: TRAIN [0][350/683]	Time 0.390 (0.424)	Data 1.03e-04 (4.94e-04)	Tok/s 12570 (12129)	Loss/tok 6.0956 (7.5936)	LR 2.000e-03
0: TRAIN [0][360/683]	Time 0.492 (0.423)	Data 8.49e-05 (5.07e-04)	Tok/s 13726 (12101)	Loss/tok 6.3558 (7.5644)	LR 2.000e-03
1: TRAIN [0][360/683]	Time 0.492 (0.423)	Data 1.14e-04 (4.84e-04)	Tok/s 13690 (12105)	Loss/tok 6.2774 (7.5598)	LR 2.000e-03
0: TRAIN [0][370/683]	Time 0.394 (0.423)	Data 9.32e-05 (4.95e-04)	Tok/s 12488 (12121)	Loss/tok 5.9992 (7.5253)	LR 2.000e-03
1: TRAIN [0][370/683]	Time 0.394 (0.423)	Data 1.07e-04 (4.74e-04)	Tok/s 12411 (12125)	Loss/tok 5.9601 (7.5185)	LR 2.000e-03
0: TRAIN [0][380/683]	Time 0.488 (0.423)	Data 9.44e-05 (4.85e-04)	Tok/s 13913 (12105)	Loss/tok 6.4090 (7.4919)	LR 2.000e-03
1: TRAIN [0][380/683]	Time 0.488 (0.423)	Data 1.04e-04 (4.64e-04)	Tok/s 13973 (12110)	Loss/tok 6.4117 (7.4850)	LR 2.000e-03
0: TRAIN [0][390/683]	Time 0.394 (0.423)	Data 8.82e-05 (4.75e-04)	Tok/s 12441 (12126)	Loss/tok 5.8829 (7.4534)	LR 2.000e-03
1: TRAIN [0][390/683]	Time 0.394 (0.423)	Data 1.10e-04 (4.55e-04)	Tok/s 12433 (12130)	Loss/tok 5.9874 (7.4458)	LR 2.000e-03
0: TRAIN [0][400/683]	Time 0.493 (0.424)	Data 1.02e-04 (4.65e-04)	Tok/s 13859 (12142)	Loss/tok 6.1240 (7.4144)	LR 2.000e-03
1: TRAIN [0][400/683]	Time 0.504 (0.424)	Data 1.08e-04 (4.46e-04)	Tok/s 13448 (12144)	Loss/tok 6.0763 (7.4083)	LR 2.000e-03
0: TRAIN [0][410/683]	Time 0.489 (0.425)	Data 1.01e-04 (4.56e-04)	Tok/s 13921 (12154)	Loss/tok 6.1180 (7.3769)	LR 2.000e-03
1: TRAIN [0][410/683]	Time 0.489 (0.425)	Data 1.18e-04 (4.38e-04)	Tok/s 13788 (12155)	Loss/tok 6.0743 (7.3705)	LR 2.000e-03
1: TRAIN [0][420/683]	Time 0.316 (0.424)	Data 1.03e-04 (4.30e-04)	Tok/s 9669 (12145)	Loss/tok 5.4368 (7.3389)	LR 2.000e-03
0: TRAIN [0][420/683]	Time 0.309 (0.424)	Data 1.52e-04 (4.48e-04)	Tok/s 9506 (12144)	Loss/tok 5.4446 (7.3435)	LR 2.000e-03
1: TRAIN [0][430/683]	Time 0.389 (0.424)	Data 9.97e-05 (4.23e-04)	Tok/s 12216 (12143)	Loss/tok 5.6800 (7.3035)	LR 2.000e-03
0: TRAIN [0][430/683]	Time 0.389 (0.424)	Data 1.02e-04 (4.40e-04)	Tok/s 12513 (12143)	Loss/tok 5.6132 (7.3089)	LR 2.000e-03
1: TRAIN [0][440/683]	Time 0.389 (0.423)	Data 1.00e-04 (4.16e-04)	Tok/s 12386 (12113)	Loss/tok 5.6688 (7.2748)	LR 2.000e-03
0: TRAIN [0][440/683]	Time 0.391 (0.423)	Data 9.63e-05 (4.32e-04)	Tok/s 12605 (12113)	Loss/tok 5.6911 (7.2798)	LR 2.000e-03
0: TRAIN [0][450/683]	Time 0.487 (0.423)	Data 8.46e-05 (4.25e-04)	Tok/s 13756 (12129)	Loss/tok 5.8419 (7.2442)	LR 2.000e-03
1: TRAIN [0][450/683]	Time 0.487 (0.423)	Data 1.03e-04 (4.09e-04)	Tok/s 14148 (12131)	Loss/tok 5.8228 (7.2381)	LR 2.000e-03
0: TRAIN [0][460/683]	Time 0.394 (0.424)	Data 8.73e-05 (4.18e-04)	Tok/s 12440 (12143)	Loss/tok 5.5662 (7.2076)	LR 2.000e-03
1: TRAIN [0][460/683]	Time 0.394 (0.424)	Data 1.08e-04 (4.02e-04)	Tok/s 12364 (12145)	Loss/tok 5.6452 (7.2021)	LR 2.000e-03
0: TRAIN [0][470/683]	Time 0.595 (0.424)	Data 9.30e-05 (4.11e-04)	Tok/s 14989 (12137)	Loss/tok 5.8780 (7.1760)	LR 2.000e-03
1: TRAIN [0][470/683]	Time 0.595 (0.424)	Data 1.13e-04 (3.96e-04)	Tok/s 14880 (12140)	Loss/tok 5.9562 (7.1701)	LR 2.000e-03
0: TRAIN [0][480/683]	Time 0.389 (0.423)	Data 8.68e-05 (4.04e-04)	Tok/s 12651 (12129)	Loss/tok 5.5923 (7.1464)	LR 2.000e-03
1: TRAIN [0][480/683]	Time 0.389 (0.423)	Data 1.07e-04 (3.90e-04)	Tok/s 12606 (12132)	Loss/tok 5.4928 (7.1400)	LR 2.000e-03
0: TRAIN [0][490/683]	Time 0.312 (0.422)	Data 8.63e-05 (3.98e-04)	Tok/s 9601 (12113)	Loss/tok 5.1063 (7.1180)	LR 2.000e-03
1: TRAIN [0][490/683]	Time 0.312 (0.422)	Data 1.03e-04 (3.84e-04)	Tok/s 9163 (12115)	Loss/tok 5.0624 (7.1117)	LR 2.000e-03
0: TRAIN [0][500/683]	Time 0.390 (0.422)	Data 9.68e-05 (3.92e-04)	Tok/s 12275 (12113)	Loss/tok 5.3570 (7.0867)	LR 2.000e-03
1: TRAIN [0][500/683]	Time 0.390 (0.422)	Data 1.03e-04 (3.79e-04)	Tok/s 12365 (12116)	Loss/tok 5.3777 (7.0794)	LR 2.000e-03
1: TRAIN [0][510/683]	Time 0.315 (0.422)	Data 1.08e-04 (3.74e-04)	Tok/s 9520 (12118)	Loss/tok 5.0633 (7.0484)	LR 2.000e-03
0: TRAIN [0][510/683]	Time 0.315 (0.422)	Data 9.49e-05 (3.86e-04)	Tok/s 9681 (12114)	Loss/tok 4.9918 (7.0552)	LR 2.000e-03
0: TRAIN [0][520/683]	Time 0.390 (0.421)	Data 7.94e-05 (3.81e-04)	Tok/s 12601 (12092)	Loss/tok 5.3268 (7.0283)	LR 2.000e-03
1: TRAIN [0][520/683]	Time 0.390 (0.421)	Data 1.02e-04 (3.68e-04)	Tok/s 12743 (12096)	Loss/tok 5.2719 (7.0211)	LR 2.000e-03
0: TRAIN [0][530/683]	Time 0.313 (0.421)	Data 9.30e-05 (3.75e-04)	Tok/s 9406 (12100)	Loss/tok 4.7283 (6.9953)	LR 2.000e-03
1: TRAIN [0][530/683]	Time 0.312 (0.421)	Data 1.08e-04 (3.64e-04)	Tok/s 9236 (12106)	Loss/tok 4.9238 (6.9892)	LR 2.000e-03
0: TRAIN [0][540/683]	Time 0.390 (0.420)	Data 8.44e-05 (3.70e-04)	Tok/s 12482 (12082)	Loss/tok 5.0601 (6.9676)	LR 2.000e-03
1: TRAIN [0][540/683]	Time 0.390 (0.420)	Data 1.06e-04 (3.59e-04)	Tok/s 12336 (12087)	Loss/tok 5.0200 (6.9623)	LR 2.000e-03
0: TRAIN [0][550/683]	Time 0.313 (0.419)	Data 8.99e-05 (3.65e-04)	Tok/s 9366 (12056)	Loss/tok 4.6941 (6.9411)	LR 2.000e-03
1: TRAIN [0][550/683]	Time 0.313 (0.419)	Data 1.40e-04 (3.54e-04)	Tok/s 9554 (12063)	Loss/tok 4.8886 (6.9359)	LR 2.000e-03
0: TRAIN [0][560/683]	Time 0.391 (0.419)	Data 9.94e-05 (3.60e-04)	Tok/s 12532 (12060)	Loss/tok 4.9828 (6.9113)	LR 2.000e-03
1: TRAIN [0][560/683]	Time 0.391 (0.419)	Data 1.02e-04 (3.50e-04)	Tok/s 12581 (12068)	Loss/tok 5.1036 (6.9056)	LR 2.000e-03
0: TRAIN [0][570/683]	Time 0.314 (0.419)	Data 9.18e-05 (3.55e-04)	Tok/s 8817 (12058)	Loss/tok 4.5420 (6.8823)	LR 2.000e-03
1: TRAIN [0][570/683]	Time 0.314 (0.419)	Data 1.03e-04 (3.46e-04)	Tok/s 9438 (12067)	Loss/tok 4.7587 (6.8761)	LR 2.000e-03
0: TRAIN [0][580/683]	Time 0.311 (0.419)	Data 1.00e-04 (3.51e-04)	Tok/s 9504 (12055)	Loss/tok 4.7219 (6.8536)	LR 2.000e-03
1: TRAIN [0][580/683]	Time 0.311 (0.419)	Data 1.26e-04 (3.42e-04)	Tok/s 9392 (12063)	Loss/tok 4.8329 (6.8485)	LR 2.000e-03
0: TRAIN [0][590/683]	Time 0.388 (0.418)	Data 1.17e-04 (3.47e-04)	Tok/s 12634 (12057)	Loss/tok 5.0150 (6.8245)	LR 2.000e-03
1: TRAIN [0][590/683]	Time 0.388 (0.418)	Data 1.08e-04 (3.38e-04)	Tok/s 12614 (12064)	Loss/tok 5.0233 (6.8183)	LR 2.000e-03
0: TRAIN [0][600/683]	Time 0.310 (0.419)	Data 9.30e-05 (3.43e-04)	Tok/s 9384 (12061)	Loss/tok 4.6430 (6.7951)	LR 2.000e-03
1: TRAIN [0][600/683]	Time 0.310 (0.419)	Data 1.07e-04 (3.34e-04)	Tok/s 9260 (12066)	Loss/tok 4.7587 (6.7894)	LR 2.000e-03
0: TRAIN [0][610/683]	Time 0.388 (0.418)	Data 9.39e-05 (3.39e-04)	Tok/s 12626 (12053)	Loss/tok 4.8819 (6.7678)	LR 2.000e-03
1: TRAIN [0][610/683]	Time 0.388 (0.418)	Data 9.39e-05 (3.31e-04)	Tok/s 12743 (12057)	Loss/tok 4.9249 (6.7624)	LR 2.000e-03
0: TRAIN [0][620/683]	Time 0.252 (0.418)	Data 9.66e-05 (3.35e-04)	Tok/s 5685 (12043)	Loss/tok 4.1116 (6.7411)	LR 2.000e-03
1: TRAIN [0][620/683]	Time 0.252 (0.418)	Data 1.29e-04 (3.27e-04)	Tok/s 5796 (12049)	Loss/tok 4.3549 (6.7353)	LR 2.000e-03
0: TRAIN [0][630/683]	Time 0.390 (0.418)	Data 9.56e-05 (3.31e-04)	Tok/s 12413 (12047)	Loss/tok 4.8525 (6.7137)	LR 2.000e-03
1: TRAIN [0][630/683]	Time 0.390 (0.418)	Data 1.09e-04 (3.24e-04)	Tok/s 12429 (12051)	Loss/tok 4.9713 (6.7078)	LR 2.000e-03
0: TRAIN [0][640/683]	Time 0.390 (0.417)	Data 8.56e-05 (3.27e-04)	Tok/s 12501 (12041)	Loss/tok 4.8032 (6.6876)	LR 2.000e-03
1: TRAIN [0][640/683]	Time 0.390 (0.417)	Data 1.49e-04 (3.20e-04)	Tok/s 12345 (12046)	Loss/tok 4.8835 (6.6818)	LR 2.000e-03
0: TRAIN [0][650/683]	Time 0.313 (0.417)	Data 9.63e-05 (3.24e-04)	Tok/s 9804 (12042)	Loss/tok 4.4350 (6.6607)	LR 2.000e-03
1: TRAIN [0][650/683]	Time 0.312 (0.417)	Data 1.12e-04 (3.17e-04)	Tok/s 9443 (12047)	Loss/tok 4.4237 (6.6551)	LR 2.000e-03
0: TRAIN [0][660/683]	Time 0.390 (0.417)	Data 1.18e-04 (3.20e-04)	Tok/s 12894 (12046)	Loss/tok 4.6863 (6.6333)	LR 2.000e-03
1: TRAIN [0][660/683]	Time 0.390 (0.417)	Data 1.06e-04 (3.14e-04)	Tok/s 12587 (12050)	Loss/tok 4.6104 (6.6286)	LR 2.000e-03
0: TRAIN [0][670/683]	Time 0.249 (0.417)	Data 9.58e-05 (3.17e-04)	Tok/s 5827 (12038)	Loss/tok 4.0984 (6.6089)	LR 2.000e-03
1: TRAIN [0][670/683]	Time 0.248 (0.417)	Data 1.08e-04 (3.11e-04)	Tok/s 6079 (12042)	Loss/tok 4.1326 (6.6044)	LR 2.000e-03
0: TRAIN [0][680/683]	Time 0.594 (0.417)	Data 4.65e-05 (3.15e-04)	Tok/s 15009 (12038)	Loss/tok 5.1471 (6.5829)	LR 2.000e-03
1: TRAIN [0][680/683]	Time 0.595 (0.417)	Data 4.53e-05 (3.09e-04)	Tok/s 14691 (12040)	Loss/tok 5.1330 (6.5784)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.112 (0.112)	Data 3.55e-03 (3.55e-03)	Tok/s 42461 (42461)	Loss/tok 6.3268 (6.3268)
0: VALIDATION [0][0/80]	Time 0.147 (0.147)	Data 1.79e-03 (1.79e-03)	Tok/s 38823 (38823)	Loss/tok 6.3937 (6.3937)
1: VALIDATION [0][10/80]	Time 0.061 (0.076)	Data 1.37e-03 (1.79e-03)	Tok/s 47198 (46388)	Loss/tok 5.9613 (6.1619)
0: VALIDATION [0][10/80]	Time 0.061 (0.081)	Data 1.35e-03 (1.47e-03)	Tok/s 48340 (46181)	Loss/tok 6.0127 (6.1429)
1: VALIDATION [0][20/80]	Time 0.050 (0.066)	Data 1.33e-03 (1.60e-03)	Tok/s 46524 (46557)	Loss/tok 5.6847 (6.0661)
0: VALIDATION [0][20/80]	Time 0.049 (0.069)	Data 1.40e-03 (1.42e-03)	Tok/s 47754 (46417)	Loss/tok 5.6955 (6.0775)
1: VALIDATION [0][30/80]	Time 0.042 (0.059)	Data 1.35e-03 (1.52e-03)	Tok/s 46195 (46772)	Loss/tok 5.7983 (5.9885)
0: VALIDATION [0][30/80]	Time 0.042 (0.061)	Data 1.28e-03 (1.39e-03)	Tok/s 46662 (46574)	Loss/tok 5.6825 (6.0026)
1: VALIDATION [0][40/80]	Time 0.035 (0.054)	Data 1.31e-03 (1.48e-03)	Tok/s 45167 (46361)	Loss/tok 5.7069 (5.9313)
0: VALIDATION [0][40/80]	Time 0.036 (0.056)	Data 1.44e-03 (1.38e-03)	Tok/s 45730 (46503)	Loss/tok 5.6281 (5.9619)
1: VALIDATION [0][50/80]	Time 0.029 (0.050)	Data 1.26e-03 (1.45e-03)	Tok/s 44612 (46095)	Loss/tok 5.6237 (5.8980)
0: VALIDATION [0][50/80]	Time 0.029 (0.051)	Data 1.29e-03 (1.37e-03)	Tok/s 45578 (46189)	Loss/tok 5.8013 (5.9157)
1: VALIDATION [0][60/80]	Time 0.024 (0.046)	Data 1.29e-03 (1.43e-03)	Tok/s 43283 (45698)	Loss/tok 5.4517 (5.8619)
0: VALIDATION [0][60/80]	Time 0.025 (0.047)	Data 1.31e-03 (1.36e-03)	Tok/s 42686 (45819)	Loss/tok 5.5812 (5.8777)
1: VALIDATION [0][70/80]	Time 0.020 (0.043)	Data 1.31e-03 (1.41e-03)	Tok/s 39631 (45139)	Loss/tok 5.3748 (5.8290)
0: VALIDATION [0][70/80]	Time 0.020 (0.044)	Data 1.32e-03 (1.35e-03)	Tok/s 39646 (45254)	Loss/tok 5.3553 (5.8465)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
0: TEST [0][9/47]	Time 0.6350 (0.8719)	Decoder iters 149.0 (149.0)	Tok/s 4181 (4329)
1: TEST [0][9/47]	Time 0.6353 (0.8719)	Decoder iters 149.0 (149.0)	Tok/s 4433 (4258)
0: TEST [0][19/47]	Time 0.5365 (0.7312)	Decoder iters 149.0 (149.0)	Tok/s 3457 (4158)
1: TEST [0][19/47]	Time 0.5366 (0.7311)	Decoder iters 149.0 (149.0)	Tok/s 3889 (4131)
1: TEST [0][29/47]	Time 0.4915 (0.6565)	Decoder iters 149.0 (146.2)	Tok/s 3288 (3895)
0: TEST [0][29/47]	Time 0.4913 (0.6566)	Decoder iters 149.0 (149.0)	Tok/s 3149 (3927)
0: TEST [0][39/47]	Time 0.2970 (0.5886)	Decoder iters 40.0 (137.1)	Tok/s 3350 (3757)
1: TEST [0][39/47]	Time 0.2972 (0.5886)	Decoder iters 90.0 (130.9)	Tok/s 3372 (3724)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.5745	Validation Loss: 5.8058	Test BLEU: 3.21
0: Performance: Epoch: 0	Training: 24086 Tok/s	Validation: 88520 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/683]	Time 0.632 (0.632)	Data 1.33e-01 (1.33e-01)	Tok/s 10635 (10635)	Loss/tok 4.7673 (4.7673)	LR 2.000e-03
0: TRAIN [1][0/683]	Time 0.638 (0.638)	Data 1.67e-01 (1.67e-01)	Tok/s 10563 (10563)	Loss/tok 4.8184 (4.8184)	LR 2.000e-03
0: TRAIN [1][10/683]	Time 0.312 (0.480)	Data 9.42e-05 (1.53e-02)	Tok/s 9393 (12827)	Loss/tok 4.0389 (4.7088)	LR 2.000e-03
1: TRAIN [1][10/683]	Time 0.312 (0.480)	Data 1.07e-04 (1.22e-02)	Tok/s 9652 (12894)	Loss/tok 4.1552 (4.7275)	LR 2.000e-03
0: TRAIN [1][20/683]	Time 0.389 (0.471)	Data 8.61e-05 (8.05e-03)	Tok/s 12416 (13080)	Loss/tok 4.5161 (4.7244)	LR 2.000e-03
1: TRAIN [1][20/683]	Time 0.389 (0.471)	Data 1.14e-04 (6.46e-03)	Tok/s 12539 (13137)	Loss/tok 4.5043 (4.7256)	LR 2.000e-03
0: TRAIN [1][30/683]	Time 0.488 (0.468)	Data 1.13e-04 (5.49e-03)	Tok/s 13990 (12903)	Loss/tok 4.5287 (4.7179)	LR 2.000e-03
1: TRAIN [1][30/683]	Time 0.488 (0.468)	Data 1.30e-04 (4.41e-03)	Tok/s 14111 (12939)	Loss/tok 4.7983 (4.7279)	LR 2.000e-03
0: TRAIN [1][40/683]	Time 0.309 (0.451)	Data 9.70e-05 (4.17e-03)	Tok/s 9875 (12591)	Loss/tok 3.9495 (4.6786)	LR 2.000e-03
1: TRAIN [1][40/683]	Time 0.310 (0.451)	Data 1.26e-04 (3.36e-03)	Tok/s 9311 (12606)	Loss/tok 4.0743 (4.6716)	LR 2.000e-03
0: TRAIN [1][50/683]	Time 0.482 (0.449)	Data 8.75e-05 (3.37e-03)	Tok/s 14150 (12576)	Loss/tok 4.6081 (4.6669)	LR 2.000e-03
1: TRAIN [1][50/683]	Time 0.482 (0.449)	Data 1.21e-04 (2.73e-03)	Tok/s 13947 (12573)	Loss/tok 4.4523 (4.6490)	LR 2.000e-03
0: TRAIN [1][60/683]	Time 0.252 (0.444)	Data 8.89e-05 (2.84e-03)	Tok/s 5916 (12482)	Loss/tok 3.6131 (4.6495)	LR 2.000e-03
1: TRAIN [1][60/683]	Time 0.251 (0.444)	Data 1.11e-04 (2.30e-03)	Tok/s 5737 (12484)	Loss/tok 3.7968 (4.6341)	LR 2.000e-03
0: TRAIN [1][70/683]	Time 0.490 (0.444)	Data 9.30e-05 (2.45e-03)	Tok/s 13801 (12546)	Loss/tok 4.7407 (4.6341)	LR 2.000e-03
1: TRAIN [1][70/683]	Time 0.490 (0.444)	Data 1.30e-04 (1.99e-03)	Tok/s 13783 (12551)	Loss/tok 4.5367 (4.6112)	LR 2.000e-03
0: TRAIN [1][80/683]	Time 0.389 (0.442)	Data 1.04e-04 (2.16e-03)	Tok/s 12454 (12509)	Loss/tok 4.4191 (4.6219)	LR 2.000e-03
1: TRAIN [1][80/683]	Time 0.389 (0.442)	Data 1.12e-04 (1.76e-03)	Tok/s 12846 (12526)	Loss/tok 4.3496 (4.6034)	LR 2.000e-03
0: TRAIN [1][90/683]	Time 0.389 (0.437)	Data 8.99e-05 (1.93e-03)	Tok/s 12567 (12429)	Loss/tok 4.0834 (4.5979)	LR 2.000e-03
1: TRAIN [1][90/683]	Time 0.389 (0.437)	Data 9.87e-05 (1.58e-03)	Tok/s 12373 (12450)	Loss/tok 4.3310 (4.5853)	LR 2.000e-03
0: TRAIN [1][100/683]	Time 0.392 (0.433)	Data 9.32e-05 (1.75e-03)	Tok/s 12346 (12404)	Loss/tok 4.2685 (4.5734)	LR 2.000e-03
1: TRAIN [1][100/683]	Time 0.391 (0.433)	Data 1.00e-04 (1.43e-03)	Tok/s 12272 (12421)	Loss/tok 4.4402 (4.5654)	LR 2.000e-03
0: TRAIN [1][110/683]	Time 0.310 (0.426)	Data 8.82e-05 (1.60e-03)	Tok/s 9632 (12255)	Loss/tok 3.9479 (4.5462)	LR 2.000e-03
1: TRAIN [1][110/683]	Time 0.310 (0.426)	Data 1.08e-04 (1.31e-03)	Tok/s 9358 (12269)	Loss/tok 3.9820 (4.5423)	LR 2.000e-03
0: TRAIN [1][120/683]	Time 0.489 (0.426)	Data 9.04e-05 (1.48e-03)	Tok/s 14041 (12279)	Loss/tok 4.6258 (4.5399)	LR 2.000e-03
1: TRAIN [1][120/683]	Time 0.489 (0.426)	Data 1.21e-04 (1.21e-03)	Tok/s 13911 (12282)	Loss/tok 4.5654 (4.5362)	LR 2.000e-03
0: TRAIN [1][130/683]	Time 0.489 (0.424)	Data 1.02e-04 (1.37e-03)	Tok/s 13724 (12266)	Loss/tok 4.6251 (4.5278)	LR 2.000e-03
1: TRAIN [1][130/683]	Time 0.489 (0.424)	Data 9.99e-05 (1.13e-03)	Tok/s 13827 (12268)	Loss/tok 4.5415 (4.5240)	LR 2.000e-03
0: TRAIN [1][140/683]	Time 0.317 (0.425)	Data 9.23e-05 (1.28e-03)	Tok/s 9548 (12289)	Loss/tok 4.0002 (4.5198)	LR 2.000e-03
1: TRAIN [1][140/683]	Time 0.317 (0.425)	Data 1.07e-04 (1.06e-03)	Tok/s 9180 (12285)	Loss/tok 3.6205 (4.5167)	LR 2.000e-03
0: TRAIN [1][150/683]	Time 0.390 (0.426)	Data 8.75e-05 (1.20e-03)	Tok/s 12466 (12299)	Loss/tok 4.2787 (4.5156)	LR 2.000e-03
1: TRAIN [1][150/683]	Time 0.390 (0.426)	Data 1.03e-04 (9.94e-04)	Tok/s 12416 (12295)	Loss/tok 4.1826 (4.5113)	LR 2.000e-03
0: TRAIN [1][160/683]	Time 0.490 (0.426)	Data 9.04e-05 (1.13e-03)	Tok/s 13815 (12308)	Loss/tok 4.5075 (4.5112)	LR 2.000e-03
1: TRAIN [1][160/683]	Time 0.490 (0.426)	Data 1.14e-04 (9.40e-04)	Tok/s 13675 (12303)	Loss/tok 4.3595 (4.5009)	LR 2.000e-03
0: TRAIN [1][170/683]	Time 0.490 (0.425)	Data 9.23e-05 (1.07e-03)	Tok/s 13927 (12290)	Loss/tok 4.6186 (4.5025)	LR 2.000e-03
1: TRAIN [1][170/683]	Time 0.490 (0.425)	Data 1.45e-04 (8.92e-04)	Tok/s 13822 (12281)	Loss/tok 4.5561 (4.4908)	LR 2.000e-03
0: TRAIN [1][180/683]	Time 0.486 (0.428)	Data 1.04e-04 (1.02e-03)	Tok/s 14084 (12335)	Loss/tok 4.4694 (4.5036)	LR 2.000e-03
1: TRAIN [1][180/683]	Time 0.486 (0.428)	Data 1.15e-04 (8.49e-04)	Tok/s 13952 (12326)	Loss/tok 4.4937 (4.4930)	LR 2.000e-03
0: TRAIN [1][190/683]	Time 0.316 (0.429)	Data 9.68e-05 (9.70e-04)	Tok/s 9147 (12361)	Loss/tok 3.9184 (4.4974)	LR 2.000e-03
1: TRAIN [1][190/683]	Time 0.315 (0.429)	Data 1.16e-04 (8.10e-04)	Tok/s 9270 (12354)	Loss/tok 3.8336 (4.4845)	LR 2.000e-03
0: TRAIN [1][200/683]	Time 0.488 (0.429)	Data 9.68e-05 (9.26e-04)	Tok/s 13840 (12362)	Loss/tok 4.4557 (4.4891)	LR 2.000e-03
1: TRAIN [1][200/683]	Time 0.488 (0.429)	Data 1.10e-04 (7.75e-04)	Tok/s 13750 (12356)	Loss/tok 4.3236 (4.4774)	LR 2.000e-03
0: TRAIN [1][210/683]	Time 0.252 (0.425)	Data 9.39e-05 (8.87e-04)	Tok/s 5769 (12261)	Loss/tok 3.8102 (4.4755)	LR 2.000e-03
1: TRAIN [1][210/683]	Time 0.252 (0.425)	Data 1.17e-04 (7.44e-04)	Tok/s 5927 (12266)	Loss/tok 3.7206 (4.4644)	LR 2.000e-03
0: TRAIN [1][220/683]	Time 0.394 (0.423)	Data 9.27e-05 (8.51e-04)	Tok/s 12254 (12217)	Loss/tok 4.1478 (4.4653)	LR 2.000e-03
1: TRAIN [1][220/683]	Time 0.394 (0.423)	Data 1.13e-04 (7.16e-04)	Tok/s 12314 (12222)	Loss/tok 4.1636 (4.4552)	LR 2.000e-03
0: TRAIN [1][230/683]	Time 0.318 (0.422)	Data 9.39e-05 (8.18e-04)	Tok/s 8927 (12205)	Loss/tok 3.8876 (4.4566)	LR 1.000e-03
1: TRAIN [1][230/683]	Time 0.317 (0.422)	Data 1.44e-04 (6.90e-04)	Tok/s 9159 (12212)	Loss/tok 3.8316 (4.4480)	LR 1.000e-03
1: TRAIN [1][240/683]	Time 0.599 (0.424)	Data 1.22e-04 (6.66e-04)	Tok/s 14691 (12212)	Loss/tok 4.5462 (4.4451)	LR 1.000e-03
0: TRAIN [1][240/683]	Time 0.603 (0.424)	Data 9.68e-05 (7.88e-04)	Tok/s 14558 (12203)	Loss/tok 4.6907 (4.4535)	LR 1.000e-03
0: TRAIN [1][250/683]	Time 0.390 (0.425)	Data 8.65e-05 (7.61e-04)	Tok/s 12770 (12239)	Loss/tok 4.1412 (4.4471)	LR 1.000e-03
1: TRAIN [1][250/683]	Time 0.390 (0.425)	Data 1.11e-04 (6.44e-04)	Tok/s 12611 (12246)	Loss/tok 3.9233 (4.4350)	LR 1.000e-03
0: TRAIN [1][260/683]	Time 0.491 (0.425)	Data 9.49e-05 (7.35e-04)	Tok/s 13832 (12239)	Loss/tok 4.3655 (4.4373)	LR 1.000e-03
1: TRAIN [1][260/683]	Time 0.491 (0.425)	Data 1.04e-04 (6.23e-04)	Tok/s 13768 (12246)	Loss/tok 4.2358 (4.4231)	LR 1.000e-03
0: TRAIN [1][270/683]	Time 0.392 (0.424)	Data 9.11e-05 (7.12e-04)	Tok/s 12336 (12232)	Loss/tok 4.0610 (4.4266)	LR 1.000e-03
1: TRAIN [1][270/683]	Time 0.391 (0.424)	Data 1.05e-04 (6.04e-04)	Tok/s 12401 (12241)	Loss/tok 3.8903 (4.4139)	LR 1.000e-03
0: TRAIN [1][280/683]	Time 0.312 (0.424)	Data 9.73e-05 (6.90e-04)	Tok/s 9089 (12210)	Loss/tok 3.7133 (4.4166)	LR 1.000e-03
1: TRAIN [1][280/683]	Time 0.312 (0.423)	Data 9.92e-05 (5.87e-04)	Tok/s 9569 (12220)	Loss/tok 3.7946 (4.4060)	LR 1.000e-03
0: TRAIN [1][290/683]	Time 0.488 (0.423)	Data 8.73e-05 (6.69e-04)	Tok/s 13801 (12200)	Loss/tok 4.2553 (4.4061)	LR 1.000e-03
1: TRAIN [1][290/683]	Time 0.488 (0.423)	Data 9.49e-05 (5.70e-04)	Tok/s 13822 (12213)	Loss/tok 4.2411 (4.3951)	LR 1.000e-03
0: TRAIN [1][300/683]	Time 0.603 (0.422)	Data 9.49e-05 (6.50e-04)	Tok/s 14525 (12160)	Loss/tok 4.3690 (4.3998)	LR 1.000e-03
1: TRAIN [1][300/683]	Time 0.603 (0.422)	Data 1.01e-04 (5.55e-04)	Tok/s 14653 (12174)	Loss/tok 4.5387 (4.3883)	LR 1.000e-03
0: TRAIN [1][310/683]	Time 0.312 (0.421)	Data 9.63e-05 (6.32e-04)	Tok/s 9665 (12133)	Loss/tok 3.5188 (4.3908)	LR 1.000e-03
1: TRAIN [1][310/683]	Time 0.312 (0.421)	Data 1.05e-04 (5.41e-04)	Tok/s 9462 (12144)	Loss/tok 3.7129 (4.3794)	LR 1.000e-03
0: TRAIN [1][320/683]	Time 0.389 (0.420)	Data 9.35e-05 (6.15e-04)	Tok/s 12601 (12113)	Loss/tok 3.8346 (4.3800)	LR 1.000e-03
1: TRAIN [1][320/683]	Time 0.389 (0.420)	Data 9.99e-05 (5.27e-04)	Tok/s 12655 (12130)	Loss/tok 4.0100 (4.3686)	LR 1.000e-03
0: TRAIN [1][330/683]	Time 0.389 (0.418)	Data 8.92e-05 (6.00e-04)	Tok/s 12492 (12060)	Loss/tok 3.9126 (4.3691)	LR 1.000e-03
1: TRAIN [1][330/683]	Time 0.389 (0.418)	Data 1.01e-04 (5.15e-04)	Tok/s 12622 (12076)	Loss/tok 4.0298 (4.3568)	LR 1.000e-03
0: TRAIN [1][340/683]	Time 0.391 (0.417)	Data 9.32e-05 (5.85e-04)	Tok/s 12427 (12041)	Loss/tok 3.9844 (4.3610)	LR 5.000e-04
1: TRAIN [1][340/683]	Time 0.392 (0.417)	Data 1.08e-04 (5.03e-04)	Tok/s 12483 (12060)	Loss/tok 3.8743 (4.3481)	LR 5.000e-04
0: TRAIN [1][350/683]	Time 0.595 (0.418)	Data 9.66e-05 (5.71e-04)	Tok/s 14956 (12068)	Loss/tok 4.2999 (4.3533)	LR 5.000e-04
1: TRAIN [1][350/683]	Time 0.595 (0.418)	Data 1.34e-04 (4.91e-04)	Tok/s 14909 (12086)	Loss/tok 4.4274 (4.3443)	LR 5.000e-04
0: TRAIN [1][360/683]	Time 0.313 (0.418)	Data 9.11e-05 (5.57e-04)	Tok/s 9718 (12060)	Loss/tok 3.4907 (4.3443)	LR 5.000e-04
1: TRAIN [1][360/683]	Time 0.313 (0.418)	Data 1.03e-04 (4.81e-04)	Tok/s 9107 (12078)	Loss/tok 3.6855 (4.3342)	LR 5.000e-04
0: TRAIN [1][370/683]	Time 0.391 (0.417)	Data 8.87e-05 (5.45e-04)	Tok/s 12348 (12053)	Loss/tok 3.8424 (4.3349)	LR 5.000e-04
1: TRAIN [1][370/683]	Time 0.391 (0.417)	Data 1.01e-04 (4.72e-04)	Tok/s 12487 (12070)	Loss/tok 3.8510 (4.3241)	LR 5.000e-04
0: TRAIN [1][380/683]	Time 0.493 (0.418)	Data 9.68e-05 (5.33e-04)	Tok/s 13666 (12079)	Loss/tok 4.0785 (4.3271)	LR 5.000e-04
1: TRAIN [1][380/683]	Time 0.493 (0.418)	Data 9.97e-05 (4.62e-04)	Tok/s 13785 (12093)	Loss/tok 4.1043 (4.3184)	LR 5.000e-04
0: TRAIN [1][390/683]	Time 0.488 (0.418)	Data 9.80e-05 (5.22e-04)	Tok/s 14257 (12080)	Loss/tok 4.0120 (4.3181)	LR 5.000e-04
1: TRAIN [1][390/683]	Time 0.488 (0.418)	Data 1.14e-04 (4.53e-04)	Tok/s 13958 (12098)	Loss/tok 4.0534 (4.3100)	LR 5.000e-04
0: TRAIN [1][400/683]	Time 0.595 (0.418)	Data 9.20e-05 (5.11e-04)	Tok/s 14636 (12071)	Loss/tok 4.2520 (4.3092)	LR 5.000e-04
1: TRAIN [1][400/683]	Time 0.594 (0.418)	Data 1.03e-04 (4.45e-04)	Tok/s 14681 (12088)	Loss/tok 4.2899 (4.3024)	LR 5.000e-04
0: TRAIN [1][410/683]	Time 0.487 (0.418)	Data 9.39e-05 (5.01e-04)	Tok/s 13997 (12071)	Loss/tok 4.0681 (4.3017)	LR 5.000e-04
1: TRAIN [1][410/683]	Time 0.486 (0.418)	Data 1.06e-04 (4.37e-04)	Tok/s 14027 (12089)	Loss/tok 3.9495 (4.2946)	LR 5.000e-04
0: TRAIN [1][420/683]	Time 0.316 (0.417)	Data 9.89e-05 (4.92e-04)	Tok/s 9198 (12052)	Loss/tok 3.6170 (4.2939)	LR 5.000e-04
1: TRAIN [1][420/683]	Time 0.316 (0.417)	Data 9.92e-05 (4.29e-04)	Tok/s 9360 (12070)	Loss/tok 3.5588 (4.2883)	LR 5.000e-04
0: TRAIN [1][430/683]	Time 0.390 (0.417)	Data 8.89e-05 (4.82e-04)	Tok/s 12468 (12042)	Loss/tok 3.7338 (4.2857)	LR 5.000e-04
1: TRAIN [1][430/683]	Time 0.390 (0.417)	Data 1.07e-04 (4.22e-04)	Tok/s 12672 (12057)	Loss/tok 3.7951 (4.2801)	LR 5.000e-04
0: TRAIN [1][440/683]	Time 0.313 (0.417)	Data 9.18e-05 (4.74e-04)	Tok/s 9654 (12027)	Loss/tok 3.8254 (4.2807)	LR 5.000e-04
1: TRAIN [1][440/683]	Time 0.313 (0.417)	Data 1.06e-04 (4.15e-04)	Tok/s 9293 (12042)	Loss/tok 3.6017 (4.2740)	LR 5.000e-04
0: TRAIN [1][450/683]	Time 0.389 (0.416)	Data 8.80e-05 (4.65e-04)	Tok/s 12563 (12017)	Loss/tok 3.9322 (4.2737)	LR 5.000e-04
1: TRAIN [1][450/683]	Time 0.389 (0.416)	Data 1.12e-04 (4.08e-04)	Tok/s 12557 (12029)	Loss/tok 3.7527 (4.2677)	LR 5.000e-04
0: TRAIN [1][460/683]	Time 0.248 (0.417)	Data 8.96e-05 (4.57e-04)	Tok/s 5788 (12021)	Loss/tok 3.2367 (4.2684)	LR 2.500e-04
1: TRAIN [1][460/683]	Time 0.248 (0.417)	Data 1.02e-04 (4.02e-04)	Tok/s 5737 (12034)	Loss/tok 3.3951 (4.2611)	LR 2.500e-04
0: TRAIN [1][470/683]	Time 0.311 (0.417)	Data 8.80e-05 (4.49e-04)	Tok/s 9478 (12034)	Loss/tok 3.4504 (4.2610)	LR 2.500e-04
1: TRAIN [1][470/683]	Time 0.311 (0.417)	Data 1.03e-04 (3.95e-04)	Tok/s 9720 (12046)	Loss/tok 3.5626 (4.2539)	LR 2.500e-04
0: TRAIN [1][480/683]	Time 0.389 (0.416)	Data 8.61e-05 (4.42e-04)	Tok/s 12443 (12032)	Loss/tok 3.7745 (4.2534)	LR 2.500e-04
1: TRAIN [1][480/683]	Time 0.389 (0.416)	Data 1.03e-04 (3.89e-04)	Tok/s 12728 (12042)	Loss/tok 3.7424 (4.2465)	LR 2.500e-04
1: TRAIN [1][490/683]	Time 0.311 (0.416)	Data 9.99e-05 (3.84e-04)	Tok/s 9538 (12033)	Loss/tok 3.4888 (4.2408)	LR 2.500e-04
0: TRAIN [1][490/683]	Time 0.322 (0.416)	Data 9.82e-05 (4.35e-04)	Tok/s 9121 (12024)	Loss/tok 3.6245 (4.2477)	LR 2.500e-04
0: TRAIN [1][500/683]	Time 0.316 (0.416)	Data 9.80e-05 (4.28e-04)	Tok/s 9568 (12025)	Loss/tok 3.6089 (4.2428)	LR 2.500e-04
1: TRAIN [1][500/683]	Time 0.316 (0.416)	Data 1.49e-04 (3.78e-04)	Tok/s 9514 (12034)	Loss/tok 3.5913 (4.2350)	LR 2.500e-04
1: TRAIN [1][510/683]	Time 0.389 (0.416)	Data 1.05e-04 (3.73e-04)	Tok/s 12694 (12026)	Loss/tok 3.9163 (4.2293)	LR 2.500e-04
0: TRAIN [1][510/683]	Time 0.389 (0.416)	Data 8.96e-05 (4.22e-04)	Tok/s 12825 (12018)	Loss/tok 3.8251 (4.2370)	LR 2.500e-04
0: TRAIN [1][520/683]	Time 0.489 (0.417)	Data 9.08e-05 (4.16e-04)	Tok/s 13990 (12030)	Loss/tok 4.0248 (4.2314)	LR 2.500e-04
1: TRAIN [1][520/683]	Time 0.489 (0.417)	Data 1.01e-04 (3.68e-04)	Tok/s 13716 (12038)	Loss/tok 3.9852 (4.2238)	LR 2.500e-04
1: TRAIN [1][530/683]	Time 0.596 (0.416)	Data 1.05e-04 (3.63e-04)	Tok/s 14930 (12039)	Loss/tok 4.1990 (4.2184)	LR 2.500e-04
0: TRAIN [1][530/683]	Time 0.598 (0.416)	Data 9.01e-05 (4.10e-04)	Tok/s 14642 (12031)	Loss/tok 4.2286 (4.2245)	LR 2.500e-04
0: TRAIN [1][540/683]	Time 0.392 (0.416)	Data 8.85e-05 (4.04e-04)	Tok/s 12681 (12014)	Loss/tok 3.7754 (4.2184)	LR 2.500e-04
1: TRAIN [1][540/683]	Time 0.396 (0.416)	Data 1.05e-04 (3.58e-04)	Tok/s 12333 (12022)	Loss/tok 3.8519 (4.2120)	LR 2.500e-04
1: TRAIN [1][550/683]	Time 0.488 (0.417)	Data 1.19e-04 (3.54e-04)	Tok/s 13921 (12053)	Loss/tok 4.1334 (4.2082)	LR 2.500e-04
0: TRAIN [1][550/683]	Time 0.488 (0.417)	Data 1.04e-04 (3.98e-04)	Tok/s 13882 (12044)	Loss/tok 3.9981 (4.2146)	LR 2.500e-04
0: TRAIN [1][560/683]	Time 0.391 (0.417)	Data 1.04e-04 (3.93e-04)	Tok/s 12254 (12056)	Loss/tok 3.7815 (4.2094)	LR 2.500e-04
1: TRAIN [1][560/683]	Time 0.392 (0.417)	Data 1.08e-04 (3.50e-04)	Tok/s 12326 (12067)	Loss/tok 3.6992 (4.2022)	LR 2.500e-04
0: TRAIN [1][570/683]	Time 0.312 (0.417)	Data 8.49e-05 (3.88e-04)	Tok/s 9636 (12062)	Loss/tok 3.7327 (4.2034)	LR 1.250e-04
1: TRAIN [1][570/683]	Time 0.309 (0.417)	Data 1.56e-04 (3.46e-04)	Tok/s 9307 (12070)	Loss/tok 3.5538 (4.1958)	LR 1.250e-04
0: TRAIN [1][580/683]	Time 0.249 (0.417)	Data 1.14e-04 (3.83e-04)	Tok/s 5921 (12056)	Loss/tok 3.1522 (4.1986)	LR 1.250e-04
1: TRAIN [1][580/683]	Time 0.249 (0.417)	Data 9.85e-05 (3.42e-04)	Tok/s 5835 (12062)	Loss/tok 3.2344 (4.1908)	LR 1.250e-04
0: TRAIN [1][590/683]	Time 0.485 (0.416)	Data 1.53e-04 (3.78e-04)	Tok/s 13843 (12033)	Loss/tok 4.1033 (4.1926)	LR 1.250e-04
1: TRAIN [1][590/683]	Time 0.490 (0.416)	Data 9.92e-05 (3.38e-04)	Tok/s 14069 (12040)	Loss/tok 3.9365 (4.1851)	LR 1.250e-04
0: TRAIN [1][600/683]	Time 0.593 (0.417)	Data 9.16e-05 (3.74e-04)	Tok/s 14870 (12044)	Loss/tok 4.2108 (4.1895)	LR 1.250e-04
1: TRAIN [1][600/683]	Time 0.593 (0.417)	Data 1.03e-04 (3.34e-04)	Tok/s 14950 (12052)	Loss/tok 4.0530 (4.1812)	LR 1.250e-04
0: TRAIN [1][610/683]	Time 0.595 (0.416)	Data 9.18e-05 (3.69e-04)	Tok/s 14739 (12037)	Loss/tok 4.1848 (4.1843)	LR 1.250e-04
1: TRAIN [1][610/683]	Time 0.595 (0.416)	Data 1.08e-04 (3.30e-04)	Tok/s 14895 (12045)	Loss/tok 4.1538 (4.1762)	LR 1.250e-04
0: TRAIN [1][620/683]	Time 0.252 (0.417)	Data 1.01e-04 (3.65e-04)	Tok/s 5561 (12039)	Loss/tok 2.9818 (4.1815)	LR 1.250e-04
1: TRAIN [1][620/683]	Time 0.252 (0.417)	Data 1.02e-04 (3.27e-04)	Tok/s 5866 (12047)	Loss/tok 3.1554 (4.1727)	LR 1.250e-04
0: TRAIN [1][630/683]	Time 0.391 (0.417)	Data 9.01e-05 (3.60e-04)	Tok/s 12633 (12050)	Loss/tok 3.7390 (4.1768)	LR 1.250e-04
1: TRAIN [1][630/683]	Time 0.395 (0.417)	Data 1.03e-04 (3.23e-04)	Tok/s 12306 (12056)	Loss/tok 3.8279 (4.1685)	LR 1.250e-04
0: TRAIN [1][640/683]	Time 0.387 (0.417)	Data 9.92e-05 (3.56e-04)	Tok/s 12565 (12060)	Loss/tok 3.7914 (4.1715)	LR 1.250e-04
1: TRAIN [1][640/683]	Time 0.386 (0.417)	Data 1.03e-04 (3.20e-04)	Tok/s 12569 (12067)	Loss/tok 3.7940 (4.1635)	LR 1.250e-04
1: TRAIN [1][650/683]	Time 0.390 (0.418)	Data 1.02e-04 (3.17e-04)	Tok/s 12196 (12085)	Loss/tok 3.6839 (4.1593)	LR 1.250e-04
0: TRAIN [1][650/683]	Time 0.395 (0.418)	Data 9.78e-05 (3.52e-04)	Tok/s 12707 (12080)	Loss/tok 3.9252 (4.1672)	LR 1.250e-04
0: TRAIN [1][660/683]	Time 0.388 (0.417)	Data 8.80e-05 (3.49e-04)	Tok/s 12487 (12062)	Loss/tok 3.7531 (4.1634)	LR 1.250e-04
1: TRAIN [1][660/683]	Time 0.388 (0.417)	Data 1.11e-04 (3.14e-04)	Tok/s 12226 (12067)	Loss/tok 3.8070 (4.1561)	LR 1.250e-04
0: TRAIN [1][670/683]	Time 0.595 (0.417)	Data 9.23e-05 (3.45e-04)	Tok/s 15165 (12059)	Loss/tok 4.1836 (4.1595)	LR 1.250e-04
1: TRAIN [1][670/683]	Time 0.598 (0.417)	Data 1.17e-04 (3.11e-04)	Tok/s 14619 (12064)	Loss/tok 4.2247 (4.1522)	LR 1.250e-04
0: TRAIN [1][680/683]	Time 0.386 (0.417)	Data 3.65e-05 (3.42e-04)	Tok/s 12584 (12051)	Loss/tok 3.6943 (4.1560)	LR 1.250e-04
1: TRAIN [1][680/683]	Time 0.386 (0.417)	Data 4.58e-05 (3.10e-04)	Tok/s 12594 (12058)	Loss/tok 3.6638 (4.1485)	LR 1.250e-04
1: Running validation on dev set
1: Executing preallocation
0: Running validation on dev set
0: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.106 (0.106)	Data 1.66e-03 (1.66e-03)	Tok/s 44880 (44880)	Loss/tok 5.4504 (5.4504)
0: VALIDATION [1][0/80]	Time 0.148 (0.148)	Data 1.64e-03 (1.64e-03)	Tok/s 38572 (38572)	Loss/tok 5.6004 (5.6004)
1: VALIDATION [1][10/80]	Time 0.061 (0.075)	Data 1.33e-03 (1.42e-03)	Tok/s 47257 (46575)	Loss/tok 4.9392 (5.2203)
0: VALIDATION [1][10/80]	Time 0.062 (0.081)	Data 1.40e-03 (1.44e-03)	Tok/s 47564 (46169)	Loss/tok 5.0559 (5.2165)
1: VALIDATION [1][20/80]	Time 0.049 (0.065)	Data 1.31e-03 (1.39e-03)	Tok/s 46921 (46841)	Loss/tok 4.6740 (5.1294)
0: VALIDATION [1][20/80]	Time 0.049 (0.069)	Data 1.31e-03 (1.40e-03)	Tok/s 47455 (46405)	Loss/tok 4.7561 (5.1570)
1: VALIDATION [1][30/80]	Time 0.042 (0.059)	Data 1.28e-03 (1.37e-03)	Tok/s 46312 (46943)	Loss/tok 4.9290 (5.0580)
0: VALIDATION [1][30/80]	Time 0.042 (0.061)	Data 1.37e-03 (1.39e-03)	Tok/s 46167 (46623)	Loss/tok 4.7876 (5.0904)
1: VALIDATION [1][40/80]	Time 0.036 (0.054)	Data 1.32e-03 (1.36e-03)	Tok/s 44490 (46474)	Loss/tok 4.7958 (5.0043)
0: VALIDATION [1][40/80]	Time 0.036 (0.056)	Data 1.34e-03 (1.37e-03)	Tok/s 45483 (46413)	Loss/tok 4.6494 (5.0553)
1: VALIDATION [1][50/80]	Time 0.029 (0.049)	Data 1.31e-03 (1.35e-03)	Tok/s 45294 (46265)	Loss/tok 4.8957 (4.9801)
0: VALIDATION [1][50/80]	Time 0.029 (0.051)	Data 1.27e-03 (1.35e-03)	Tok/s 46025 (46168)	Loss/tok 4.9230 (5.0128)
1: VALIDATION [1][60/80]	Time 0.025 (0.046)	Data 1.25e-03 (1.34e-03)	Tok/s 42679 (45818)	Loss/tok 4.6307 (4.9524)
0: VALIDATION [1][60/80]	Time 0.025 (0.047)	Data 1.25e-03 (1.34e-03)	Tok/s 42821 (45786)	Loss/tok 4.6578 (4.9782)
1: VALIDATION [1][70/80]	Time 0.020 (0.042)	Data 1.33e-03 (1.33e-03)	Tok/s 39179 (45255)	Loss/tok 4.5687 (4.9236)
0: VALIDATION [1][70/80]	Time 0.020 (0.044)	Data 1.25e-03 (1.33e-03)	Tok/s 39574 (45227)	Loss/tok 4.5418 (4.9540)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
0: TEST [1][9/47]	Time 0.5907 (0.7322)	Decoder iters 149.0 (140.3)	Tok/s 4376 (4801)
1: TEST [1][9/47]	Time 0.5908 (0.7322)	Decoder iters 149.0 (149.0)	Tok/s 4475 (4616)
0: TEST [1][19/47]	Time 0.3041 (0.6256)	Decoder iters 49.0 (125.8)	Tok/s 6185 (4650)
1: TEST [1][19/47]	Time 0.3042 (0.6256)	Decoder iters 57.0 (126.3)	Tok/s 6556 (4532)
1: TEST [1][29/47]	Time 0.4668 (0.5447)	Decoder iters 149.0 (108.0)	Tok/s 3203 (4581)
0: TEST [1][29/47]	Time 0.4670 (0.5447)	Decoder iters 149.0 (115.4)	Tok/s 3234 (4685)
0: TEST [1][39/47]	Time 0.4276 (0.4790)	Decoder iters 149.0 (105.0)	Tok/s 2423 (4800)
1: TEST [1][39/47]	Time 0.4278 (0.4790)	Decoder iters 31.0 (91.2)	Tok/s 2373 (4686)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.1510	Validation Loss: 4.9119	Test BLEU: 7.61
0: Performance: Epoch: 1	Training: 24104 Tok/s	Validation: 88619 Tok/s
0: Finished epoch 1
1: Total training time 652 s
0: Total training time 652 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 108|                      7.61|                      24094.9|                         10.86|
DONE!
