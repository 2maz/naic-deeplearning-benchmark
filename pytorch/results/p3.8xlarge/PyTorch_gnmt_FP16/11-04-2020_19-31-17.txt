3: Collecting environment information...
2: Collecting environment information...
1: Collecting environment information...
0: Collecting environment information...
3: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
3: Saving results to: results/gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
3: Using master seed from command line: 2
2: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
2: Saving results to: results/gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
2: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-16GB
GPU 1: Tesla V100-SXM2-16GB
GPU 2: Tesla V100-SXM2-16GB
GPU 3: Tesla V100-SXM2-16GB

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
3: Worker 3 is using worker seed: 1323436024
2: Worker 2 is using worker seed: 3588440356
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
1: Size of vocabulary: 31800
3: Size of vocabulary: 31800
2: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 125
2: Filtering data, min len: 0, max len: 125
0: Filtering data, min len: 0, max len: 125
3: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
2: Pairs before: 5100, after: 5100
0: Pairs before: 5100, after: 5100
3: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
2: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
2: Pairs before: 3003, after: 3003
3: Filtering data, min len: 0, max len: 150
0: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
0: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 185
1: Scheduler decay interval: 23
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
1: Initializing amp optimizer
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
1: Starting epoch 0
1: Executing preallocation
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 185
0: Scheduler decay interval: 23
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159605817
2: Saving state of the tokenizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 185
2: Scheduler decay interval: 23
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
2: Initializing amp optimizer
2: Starting epoch 0
2: Executing preallocation
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159605817
3: Saving state of the tokenizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 185
3: Scheduler decay interval: 23
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
3: Initializing amp optimizer
3: Starting epoch 0
3: Executing preallocation
0: Sampler for epoch 0 uses seed 2602510382
1: Sampler for epoch 0 uses seed 2602510382
2: Sampler for epoch 0 uses seed 2602510382
3: Sampler for epoch 0 uses seed 2602510382
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
3: TRAIN [0][0/139]	Time 0.410 (0.410)	Data 1.42e-01 (1.42e-01)	Tok/s 39936 (39936)	Loss/tok 10.6475 (10.6475)	LR 2.050e-05
0: TRAIN [0][0/139]	Time 0.410 (0.410)	Data 1.45e-01 (1.45e-01)	Tok/s 39736 (39736)	Loss/tok 10.6533 (10.6533)	LR 2.050e-05
2: TRAIN [0][0/139]	Time 0.410 (0.410)	Data 1.37e-01 (1.37e-01)	Tok/s 40307 (40307)	Loss/tok 10.6610 (10.6610)	LR 2.050e-05
1: TRAIN [0][0/139]	Time 0.410 (0.410)	Data 1.36e-01 (1.36e-01)	Tok/s 40014 (40014)	Loss/tok 10.6451 (10.6451)	LR 2.050e-05
3: TRAIN [0][10/139]	Time 0.278 (0.230)	Data 1.58e-04 (1.30e-02)	Tok/s 58997 (50735)	Loss/tok 9.6724 (10.1379)	LR 2.630e-05
1: TRAIN [0][10/139]	Time 0.278 (0.230)	Data 1.68e-04 (1.25e-02)	Tok/s 59080 (50550)	Loss/tok 9.6492 (10.1335)	LR 2.630e-05
0: TRAIN [0][10/139]	Time 0.278 (0.230)	Data 1.71e-04 (1.33e-02)	Tok/s 59010 (50782)	Loss/tok 9.6684 (10.1339)	LR 2.630e-05
2: TRAIN [0][10/139]	Time 0.278 (0.230)	Data 1.60e-04 (1.27e-02)	Tok/s 58575 (51076)	Loss/tok 9.6748 (10.1382)	LR 2.630e-05
3: TRAIN [0][20/139]	Time 0.424 (0.264)	Data 1.55e-04 (6.90e-03)	Tok/s 50016 (51256)	Loss/tok 9.2796 (9.7268)	LR 3.373e-05
1: TRAIN [0][20/139]	Time 0.424 (0.264)	Data 1.59e-04 (6.63e-03)	Tok/s 49948 (51153)	Loss/tok 9.2802 (9.7338)	LR 3.373e-05
0: TRAIN [0][20/139]	Time 0.425 (0.264)	Data 1.63e-04 (7.06e-03)	Tok/s 49986 (51208)	Loss/tok 9.3116 (9.7302)	LR 3.373e-05
2: TRAIN [0][20/139]	Time 0.426 (0.264)	Data 1.59e-04 (6.74e-03)	Tok/s 49841 (51486)	Loss/tok 9.2725 (9.7393)	LR 3.373e-05
2: TRAIN [0][30/139]	Time 0.281 (0.249)	Data 1.60e-04 (4.63e-03)	Tok/s 58231 (50466)	Loss/tok 8.9347 (9.5379)	LR 4.327e-05
1: TRAIN [0][30/139]	Time 0.281 (0.249)	Data 1.64e-04 (4.54e-03)	Tok/s 57824 (50180)	Loss/tok 8.9640 (9.5324)	LR 4.327e-05
0: TRAIN [0][30/139]	Time 0.281 (0.249)	Data 1.68e-04 (4.83e-03)	Tok/s 58606 (50294)	Loss/tok 8.9610 (9.5345)	LR 4.327e-05
3: TRAIN [0][30/139]	Time 0.282 (0.249)	Data 1.55e-04 (4.72e-03)	Tok/s 57960 (50209)	Loss/tok 8.9743 (9.5345)	LR 4.327e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
2: TRAIN [0][40/139]	Time 0.208 (0.255)	Data 1.65e-04 (3.54e-03)	Tok/s 56853 (51197)	Loss/tok 8.5763 (9.3337)	LR 5.550e-05
1: TRAIN [0][40/139]	Time 0.208 (0.255)	Data 1.70e-04 (3.47e-03)	Tok/s 56362 (50957)	Loss/tok 8.5574 (9.3254)	LR 5.550e-05
0: TRAIN [0][40/139]	Time 0.208 (0.255)	Data 1.74e-04 (3.70e-03)	Tok/s 56746 (51060)	Loss/tok 8.5894 (9.3329)	LR 5.550e-05
3: TRAIN [0][40/139]	Time 0.208 (0.255)	Data 1.70e-04 (3.61e-03)	Tok/s 55812 (50971)	Loss/tok 8.6036 (9.3297)	LR 5.550e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
2: TRAIN [0][50/139]	Time 0.219 (0.255)	Data 1.45e-04 (2.88e-03)	Tok/s 53019 (50818)	Loss/tok 8.3856 (9.1997)	LR 7.118e-05
1: TRAIN [0][50/139]	Time 0.219 (0.255)	Data 1.53e-04 (2.83e-03)	Tok/s 53780 (50698)	Loss/tok 8.4045 (9.1915)	LR 7.118e-05
0: TRAIN [0][50/139]	Time 0.219 (0.255)	Data 1.60e-04 (3.00e-03)	Tok/s 53321 (50729)	Loss/tok 8.3618 (9.1957)	LR 7.118e-05
3: TRAIN [0][50/139]	Time 0.219 (0.255)	Data 1.54e-04 (2.94e-03)	Tok/s 53775 (50722)	Loss/tok 8.4166 (9.1954)	LR 7.118e-05
2: TRAIN [0][60/139]	Time 0.284 (0.249)	Data 1.54e-04 (2.43e-03)	Tok/s 57818 (51124)	Loss/tok 8.2272 (9.0599)	LR 9.130e-05
1: TRAIN [0][60/139]	Time 0.284 (0.249)	Data 1.61e-04 (2.39e-03)	Tok/s 58001 (50964)	Loss/tok 8.2240 (9.0537)	LR 9.130e-05
0: TRAIN [0][60/139]	Time 0.284 (0.249)	Data 1.76e-04 (2.54e-03)	Tok/s 57878 (51020)	Loss/tok 8.1657 (9.0539)	LR 9.130e-05
3: TRAIN [0][60/139]	Time 0.284 (0.249)	Data 1.73e-04 (2.48e-03)	Tok/s 57382 (51030)	Loss/tok 8.2360 (9.0567)	LR 9.130e-05
2: TRAIN [0][70/139]	Time 0.215 (0.246)	Data 1.51e-04 (2.11e-03)	Tok/s 55098 (51681)	Loss/tok 7.8763 (8.9164)	LR 1.171e-04
0: TRAIN [0][70/139]	Time 0.215 (0.246)	Data 1.63e-04 (2.20e-03)	Tok/s 54544 (51561)	Loss/tok 7.9325 (8.9125)	LR 1.171e-04
1: TRAIN [0][70/139]	Time 0.215 (0.246)	Data 1.58e-04 (2.07e-03)	Tok/s 55233 (51525)	Loss/tok 7.9543 (8.9112)	LR 1.171e-04
3: TRAIN [0][70/139]	Time 0.215 (0.246)	Data 1.65e-04 (2.16e-03)	Tok/s 54986 (51581)	Loss/tok 7.9082 (8.9137)	LR 1.171e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
2: TRAIN [0][80/139]	Time 0.443 (0.249)	Data 2.02e-04 (1.87e-03)	Tok/s 47443 (52006)	Loss/tok 8.1628 (8.7967)	LR 1.502e-04
1: TRAIN [0][80/139]	Time 0.443 (0.249)	Data 1.60e-04 (1.84e-03)	Tok/s 47874 (51844)	Loss/tok 8.1268 (8.7906)	LR 1.502e-04
0: TRAIN [0][80/139]	Time 0.443 (0.249)	Data 1.64e-04 (1.95e-03)	Tok/s 47754 (51878)	Loss/tok 8.1765 (8.7960)	LR 1.502e-04
3: TRAIN [0][80/139]	Time 0.443 (0.249)	Data 1.62e-04 (1.91e-03)	Tok/s 48182 (51909)	Loss/tok 8.1540 (8.7927)	LR 1.502e-04
2: TRAIN [0][90/139]	Time 0.280 (0.243)	Data 1.46e-04 (1.68e-03)	Tok/s 58233 (51903)	Loss/tok 7.9128 (8.7057)	LR 1.927e-04
1: TRAIN [0][90/139]	Time 0.280 (0.243)	Data 1.51e-04 (1.65e-03)	Tok/s 58933 (51765)	Loss/tok 7.8606 (8.7007)	LR 1.927e-04
0: TRAIN [0][90/139]	Time 0.280 (0.243)	Data 1.58e-04 (1.75e-03)	Tok/s 58567 (51805)	Loss/tok 7.8654 (8.7067)	LR 1.927e-04
3: TRAIN [0][90/139]	Time 0.280 (0.243)	Data 1.57e-04 (1.72e-03)	Tok/s 58357 (51838)	Loss/tok 7.8235 (8.7008)	LR 1.927e-04
2: TRAIN [0][100/139]	Time 0.281 (0.247)	Data 1.45e-04 (1.53e-03)	Tok/s 58557 (51696)	Loss/tok 7.8420 (8.6107)	LR 2.471e-04
1: TRAIN [0][100/139]	Time 0.281 (0.247)	Data 1.55e-04 (1.51e-03)	Tok/s 57561 (51594)	Loss/tok 7.8963 (8.6060)	LR 2.471e-04
0: TRAIN [0][100/139]	Time 0.281 (0.247)	Data 1.58e-04 (1.60e-03)	Tok/s 57984 (51609)	Loss/tok 7.9069 (8.6120)	LR 2.471e-04
3: TRAIN [0][100/139]	Time 0.282 (0.247)	Data 1.54e-04 (1.56e-03)	Tok/s 58144 (51664)	Loss/tok 7.8375 (8.6063)	LR 2.471e-04
2: TRAIN [0][110/139]	Time 0.282 (0.244)	Data 1.56e-04 (1.41e-03)	Tok/s 58036 (51932)	Loss/tok 7.7879 (8.5327)	LR 3.170e-04
1: TRAIN [0][110/139]	Time 0.282 (0.244)	Data 1.62e-04 (1.39e-03)	Tok/s 57393 (51824)	Loss/tok 7.7980 (8.5279)	LR 3.170e-04
0: TRAIN [0][110/139]	Time 0.282 (0.244)	Data 1.69e-04 (1.47e-03)	Tok/s 57145 (51839)	Loss/tok 7.8632 (8.5356)	LR 3.170e-04
3: TRAIN [0][110/139]	Time 0.282 (0.244)	Data 1.63e-04 (1.44e-03)	Tok/s 57679 (51906)	Loss/tok 7.7838 (8.5298)	LR 3.170e-04
2: TRAIN [0][120/139]	Time 0.214 (0.242)	Data 1.97e-04 (1.31e-03)	Tok/s 54755 (51767)	Loss/tok 7.6295 (8.4694)	LR 4.066e-04
1: TRAIN [0][120/139]	Time 0.214 (0.242)	Data 1.79e-04 (1.28e-03)	Tok/s 54848 (51667)	Loss/tok 7.6398 (8.4647)	LR 4.066e-04
0: TRAIN [0][120/139]	Time 0.214 (0.242)	Data 1.94e-04 (1.36e-03)	Tok/s 53838 (51659)	Loss/tok 7.6132 (8.4728)	LR 4.066e-04
3: TRAIN [0][120/139]	Time 0.214 (0.242)	Data 1.79e-04 (1.33e-03)	Tok/s 54859 (51740)	Loss/tok 7.6404 (8.4660)	LR 4.066e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
2: TRAIN [0][130/139]	Time 0.423 (0.244)	Data 1.52e-04 (1.22e-03)	Tok/s 50430 (51778)	Loss/tok 7.8917 (8.4072)	LR 5.215e-04
0: TRAIN [0][130/139]	Time 0.423 (0.244)	Data 1.67e-04 (1.27e-03)	Tok/s 50094 (51680)	Loss/tok 7.9477 (8.4116)	LR 5.215e-04
1: TRAIN [0][130/139]	Time 0.423 (0.244)	Data 1.58e-04 (1.20e-03)	Tok/s 50265 (51710)	Loss/tok 7.9590 (8.4027)	LR 5.215e-04
3: TRAIN [0][130/139]	Time 0.423 (0.244)	Data 1.62e-04 (1.24e-03)	Tok/s 50170 (51757)	Loss/tok 7.8455 (8.4023)	LR 5.215e-04
0: Running validation on dev set
2: Running validation on dev set
1: Running validation on dev set
3: Running validation on dev set
2: Executing preallocation
1: Executing preallocation
0: Executing preallocation
3: Executing preallocation
3: VALIDATION [0][0/20]	Time 0.039 (0.039)	Data 3.70e-03 (3.70e-03)	Tok/s 192681 (192681)	Loss/tok 8.3999 (8.3999)
2: VALIDATION [0][0/20]	Time 0.041 (0.041)	Data 3.81e-03 (3.81e-03)	Tok/s 191255 (191255)	Loss/tok 8.4250 (8.4250)
1: VALIDATION [0][0/20]	Time 0.045 (0.045)	Data 3.76e-03 (3.76e-03)	Tok/s 189053 (189053)	Loss/tok 8.4804 (8.4804)
0: VALIDATION [0][0/20]	Time 0.069 (0.069)	Data 3.85e-03 (3.85e-03)	Tok/s 151400 (151400)	Loss/tok 8.4696 (8.4696)
3: VALIDATION [0][10/20]	Time 0.019 (0.025)	Data 3.42e-03 (3.51e-03)	Tok/s 164368 (182176)	Loss/tok 8.0586 (8.2641)
2: VALIDATION [0][10/20]	Time 0.019 (0.026)	Data 3.56e-03 (3.60e-03)	Tok/s 167050 (184420)	Loss/tok 8.0498 (8.2513)
1: VALIDATION [0][10/20]	Time 0.019 (0.027)	Data 3.47e-03 (3.55e-03)	Tok/s 167529 (181001)	Loss/tok 8.0012 (8.2728)
0: VALIDATION [0][10/20]	Time 0.019 (0.030)	Data 3.57e-03 (3.64e-03)	Tok/s 170305 (179254)	Loss/tok 8.1069 (8.2612)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
2: Running evaluation on test set
3: Running evaluation on test set
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
2: Finished evaluation on test set
3: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
2: Finished epoch 0
1: Finished epoch 0
3: Finished epoch 0
1: Starting epoch 1
3: Starting epoch 1
2: Starting epoch 1
1: Executing preallocation
3: Executing preallocation
2: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 8.3660	Validation Loss: 8.1608	Test BLEU: 0.08
0: Performance: Epoch: 0	Training: 206869 Tok/s	Validation: 636829 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2606193617
1: Sampler for epoch 1 uses seed 2606193617
2: Sampler for epoch 1 uses seed 2606193617
3: Sampler for epoch 1 uses seed 2606193617
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
1: TRAIN [1][0/139]	Time 0.269 (0.269)	Data 1.29e-01 (1.29e-01)	Tok/s 26302 (26302)	Loss/tok 7.3126 (7.3126)	LR 6.524e-04
3: TRAIN [1][0/139]	Time 0.269 (0.269)	Data 1.34e-01 (1.34e-01)	Tok/s 26416 (26416)	Loss/tok 7.2216 (7.2216)	LR 6.524e-04
2: TRAIN [1][0/139]	Time 0.269 (0.269)	Data 1.26e-01 (1.26e-01)	Tok/s 26094 (26094)	Loss/tok 7.2180 (7.2180)	LR 6.524e-04
0: TRAIN [1][0/139]	Time 0.269 (0.269)	Data 1.21e-01 (1.21e-01)	Tok/s 26781 (26781)	Loss/tok 7.2324 (7.2324)	LR 6.524e-04
3: TRAIN [1][10/139]	Time 0.152 (0.237)	Data 1.66e-04 (1.24e-02)	Tok/s 44925 (52738)	Loss/tok 7.1662 (7.5480)	LR 8.369e-04
1: TRAIN [1][10/139]	Time 0.152 (0.237)	Data 1.64e-04 (1.19e-02)	Tok/s 46095 (52781)	Loss/tok 7.2303 (7.5542)	LR 8.369e-04
2: TRAIN [1][10/139]	Time 0.152 (0.237)	Data 1.66e-04 (1.16e-02)	Tok/s 46251 (52541)	Loss/tok 7.1681 (7.5594)	LR 8.369e-04
0: TRAIN [1][10/139]	Time 0.152 (0.237)	Data 1.70e-04 (1.12e-02)	Tok/s 45555 (52596)	Loss/tok 7.1706 (7.5560)	LR 8.369e-04
3: TRAIN [1][20/139]	Time 0.423 (0.266)	Data 1.58e-04 (6.55e-03)	Tok/s 50233 (53170)	Loss/tok 7.7018 (7.6041)	LR 1.073e-03
1: TRAIN [1][20/139]	Time 0.423 (0.266)	Data 1.64e-04 (6.32e-03)	Tok/s 50091 (53195)	Loss/tok 7.7522 (7.6079)	LR 1.073e-03
2: TRAIN [1][20/139]	Time 0.423 (0.266)	Data 1.59e-04 (6.15e-03)	Tok/s 50406 (53066)	Loss/tok 7.7274 (7.6118)	LR 1.073e-03
0: TRAIN [1][20/139]	Time 0.423 (0.266)	Data 1.65e-04 (5.94e-03)	Tok/s 50032 (53177)	Loss/tok 7.7829 (7.6173)	LR 1.073e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
3: TRAIN [1][30/139]	Time 0.215 (0.253)	Data 1.68e-04 (4.49e-03)	Tok/s 54645 (53027)	Loss/tok 7.4649 (7.6259)	LR 1.377e-03
1: TRAIN [1][30/139]	Time 0.215 (0.253)	Data 1.69e-04 (4.33e-03)	Tok/s 53818 (53027)	Loss/tok 7.4300 (7.6249)	LR 1.377e-03
2: TRAIN [1][30/139]	Time 0.215 (0.254)	Data 1.69e-04 (4.22e-03)	Tok/s 54941 (52980)	Loss/tok 7.4308 (7.6285)	LR 1.377e-03
0: TRAIN [1][30/139]	Time 0.215 (0.254)	Data 1.70e-04 (4.07e-03)	Tok/s 54634 (53069)	Loss/tok 7.4521 (7.6381)	LR 1.377e-03
2: TRAIN [1][40/139]	Time 0.214 (0.249)	Data 1.57e-04 (3.23e-03)	Tok/s 54550 (53077)	Loss/tok 7.2317 (7.5894)	LR 1.766e-03
1: TRAIN [1][40/139]	Time 0.214 (0.249)	Data 1.64e-04 (3.32e-03)	Tok/s 55304 (53157)	Loss/tok 7.3478 (7.5935)	LR 1.766e-03
3: TRAIN [1][40/139]	Time 0.214 (0.249)	Data 1.55e-04 (3.43e-03)	Tok/s 55096 (53114)	Loss/tok 7.2189 (7.5870)	LR 1.766e-03
0: TRAIN [1][40/139]	Time 0.214 (0.249)	Data 1.61e-04 (3.12e-03)	Tok/s 55271 (53182)	Loss/tok 7.2749 (7.5975)	LR 1.766e-03
3: TRAIN [1][50/139]	Time 0.094 (0.245)	Data 1.55e-04 (2.79e-03)	Tok/s 37099 (52906)	Loss/tok 6.6385 (7.5292)	LR 1.000e-03
1: TRAIN [1][50/139]	Time 0.094 (0.245)	Data 1.61e-04 (2.70e-03)	Tok/s 37147 (53002)	Loss/tok 6.6342 (7.5350)	LR 1.000e-03
2: TRAIN [1][50/139]	Time 0.095 (0.245)	Data 1.54e-04 (2.63e-03)	Tok/s 37859 (52932)	Loss/tok 6.4327 (7.5284)	LR 1.000e-03
0: TRAIN [1][50/139]	Time 0.095 (0.245)	Data 1.60e-04 (2.54e-03)	Tok/s 36729 (52971)	Loss/tok 6.6032 (7.5365)	LR 1.000e-03
1: TRAIN [1][60/139]	Time 0.213 (0.242)	Data 1.66e-04 (2.28e-03)	Tok/s 54732 (52876)	Loss/tok 7.0352 (7.4702)	LR 1.000e-03
2: TRAIN [1][60/139]	Time 0.213 (0.242)	Data 1.62e-04 (2.23e-03)	Tok/s 54903 (52790)	Loss/tok 7.0247 (7.4632)	LR 1.000e-03
0: TRAIN [1][60/139]	Time 0.213 (0.242)	Data 1.65e-04 (2.15e-03)	Tok/s 54863 (52852)	Loss/tok 6.9734 (7.4759)	LR 1.000e-03
3: TRAIN [1][60/139]	Time 0.214 (0.242)	Data 1.63e-04 (2.36e-03)	Tok/s 55388 (52795)	Loss/tok 7.0104 (7.4653)	LR 1.000e-03
3: TRAIN [1][70/139]	Time 0.212 (0.244)	Data 1.70e-04 (2.05e-03)	Tok/s 55556 (52915)	Loss/tok 6.7704 (7.3938)	LR 5.000e-04
1: TRAIN [1][70/139]	Time 0.212 (0.244)	Data 1.75e-04 (1.99e-03)	Tok/s 54711 (52991)	Loss/tok 6.7918 (7.4036)	LR 5.000e-04
2: TRAIN [1][70/139]	Time 0.211 (0.244)	Data 1.69e-04 (1.94e-03)	Tok/s 55719 (52904)	Loss/tok 6.8482 (7.3956)	LR 5.000e-04
0: TRAIN [1][70/139]	Time 0.212 (0.244)	Data 1.73e-04 (1.87e-03)	Tok/s 55036 (52942)	Loss/tok 6.8335 (7.4042)	LR 5.000e-04
3: TRAIN [1][80/139]	Time 0.280 (0.247)	Data 1.73e-04 (1.82e-03)	Tok/s 58316 (52947)	Loss/tok 6.8620 (7.3290)	LR 5.000e-04
1: TRAIN [1][80/139]	Time 0.280 (0.247)	Data 1.76e-04 (1.76e-03)	Tok/s 58345 (52958)	Loss/tok 6.9092 (7.3384)	LR 5.000e-04
2: TRAIN [1][80/139]	Time 0.280 (0.247)	Data 1.65e-04 (1.72e-03)	Tok/s 59065 (52904)	Loss/tok 6.9296 (7.3291)	LR 5.000e-04
0: TRAIN [1][80/139]	Time 0.280 (0.247)	Data 1.70e-04 (1.66e-03)	Tok/s 58030 (52947)	Loss/tok 6.8830 (7.3405)	LR 5.000e-04
1: TRAIN [1][90/139]	Time 0.212 (0.247)	Data 1.50e-04 (1.59e-03)	Tok/s 55500 (52826)	Loss/tok 6.6860 (7.2810)	LR 5.000e-04
2: TRAIN [1][90/139]	Time 0.212 (0.247)	Data 1.52e-04 (1.55e-03)	Tok/s 55503 (52774)	Loss/tok 6.7025 (7.2717)	LR 5.000e-04
0: TRAIN [1][90/139]	Time 0.212 (0.247)	Data 1.56e-04 (1.50e-03)	Tok/s 55778 (52831)	Loss/tok 6.6682 (7.2840)	LR 5.000e-04
3: TRAIN [1][90/139]	Time 0.213 (0.247)	Data 1.51e-04 (1.64e-03)	Tok/s 54171 (52803)	Loss/tok 6.6528 (7.2711)	LR 5.000e-04
1: TRAIN [1][100/139]	Time 0.213 (0.242)	Data 1.56e-04 (1.45e-03)	Tok/s 55078 (52509)	Loss/tok 6.6480 (7.2298)	LR 2.500e-04
2: TRAIN [1][100/139]	Time 0.213 (0.242)	Data 1.56e-04 (1.42e-03)	Tok/s 55784 (52476)	Loss/tok 6.6485 (7.2197)	LR 2.500e-04
0: TRAIN [1][100/139]	Time 0.213 (0.242)	Data 1.57e-04 (1.37e-03)	Tok/s 55678 (52539)	Loss/tok 6.6699 (7.2305)	LR 2.500e-04
3: TRAIN [1][100/139]	Time 0.213 (0.242)	Data 1.51e-04 (1.49e-03)	Tok/s 54630 (52502)	Loss/tok 6.6694 (7.2195)	LR 2.500e-04
1: TRAIN [1][110/139]	Time 0.280 (0.243)	Data 1.60e-04 (1.33e-03)	Tok/s 58182 (52645)	Loss/tok 6.7441 (7.1777)	LR 2.500e-04
3: TRAIN [1][110/139]	Time 0.280 (0.243)	Data 1.56e-04 (1.37e-03)	Tok/s 57701 (52623)	Loss/tok 6.7063 (7.1702)	LR 2.500e-04
2: TRAIN [1][110/139]	Time 0.280 (0.243)	Data 1.55e-04 (1.30e-03)	Tok/s 58325 (52583)	Loss/tok 6.7442 (7.1699)	LR 2.500e-04
0: TRAIN [1][110/139]	Time 0.280 (0.243)	Data 1.62e-04 (1.26e-03)	Tok/s 58609 (52668)	Loss/tok 6.7976 (7.1805)	LR 2.500e-04
1: TRAIN [1][120/139]	Time 0.422 (0.243)	Data 1.56e-04 (1.23e-03)	Tok/s 50148 (52540)	Loss/tok 6.8728 (7.1318)	LR 1.250e-04
3: TRAIN [1][120/139]	Time 0.422 (0.243)	Data 1.55e-04 (1.27e-03)	Tok/s 50588 (52508)	Loss/tok 6.8909 (7.1256)	LR 1.250e-04
2: TRAIN [1][120/139]	Time 0.422 (0.243)	Data 1.53e-04 (1.21e-03)	Tok/s 50195 (52487)	Loss/tok 6.8656 (7.1264)	LR 1.250e-04
0: TRAIN [1][120/139]	Time 0.422 (0.243)	Data 1.57e-04 (1.17e-03)	Tok/s 50048 (52558)	Loss/tok 6.8764 (7.1341)	LR 1.250e-04
1: TRAIN [1][130/139]	Time 0.149 (0.240)	Data 1.54e-04 (1.15e-03)	Tok/s 46639 (52256)	Loss/tok 6.0643 (7.0917)	LR 1.250e-04
3: TRAIN [1][130/139]	Time 0.149 (0.240)	Data 1.53e-04 (1.18e-03)	Tok/s 47462 (52231)	Loss/tok 6.1359 (7.0856)	LR 1.250e-04
2: TRAIN [1][130/139]	Time 0.149 (0.240)	Data 1.53e-04 (1.13e-03)	Tok/s 47641 (52187)	Loss/tok 6.1709 (7.0859)	LR 1.250e-04
0: TRAIN [1][130/139]	Time 0.149 (0.240)	Data 1.58e-04 (1.09e-03)	Tok/s 46966 (52277)	Loss/tok 6.1498 (7.0935)	LR 1.250e-04
0: Running validation on dev set
2: Running validation on dev set
1: Running validation on dev set
3: Running validation on dev set
2: Executing preallocation
1: Executing preallocation
3: Executing preallocation
0: Executing preallocation
1: VALIDATION [1][0/20]	Time 0.046 (0.046)	Data 4.15e-03 (4.15e-03)	Tok/s 187672 (187672)	Loss/tok 7.5267 (7.5267)
2: VALIDATION [1][0/20]	Time 0.041 (0.041)	Data 3.80e-03 (3.80e-03)	Tok/s 189602 (189602)	Loss/tok 7.4878 (7.4878)
0: VALIDATION [1][0/20]	Time 0.069 (0.069)	Data 3.95e-03 (3.95e-03)	Tok/s 151213 (151213)	Loss/tok 7.5703 (7.5703)
3: VALIDATION [1][0/20]	Time 0.044 (0.044)	Data 7.25e-03 (7.25e-03)	Tok/s 168248 (168248)	Loss/tok 7.4382 (7.4382)
2: VALIDATION [1][10/20]	Time 0.019 (0.026)	Data 3.53e-03 (3.58e-03)	Tok/s 165754 (183453)	Loss/tok 7.0685 (7.2917)
1: VALIDATION [1][10/20]	Time 0.019 (0.028)	Data 3.42e-03 (4.03e-03)	Tok/s 168477 (177599)	Loss/tok 7.0380 (7.3206)
0: VALIDATION [1][10/20]	Time 0.019 (0.030)	Data 3.58e-03 (3.70e-03)	Tok/s 167762 (178805)	Loss/tok 7.0894 (7.3039)
3: VALIDATION [1][10/20]	Time 0.018 (0.026)	Data 3.39e-03 (3.80e-03)	Tok/s 166159 (180938)	Loss/tok 7.1094 (7.2920)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
3: Running evaluation on test set
2: Running evaluation on test set
1: Running evaluation on test set
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
3: Finished evaluation on test set
2: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
2: Finished epoch 1
1: Finished epoch 1
3: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 7.0588	Validation Loss: 7.1973	Test BLEU: 0.25
0: Performance: Epoch: 1	Training: 208732 Tok/s	Validation: 635800 Tok/s
0: Finished epoch 1
1: Total training time 121 s
2: Total training time 121 s
3: Total training time 121 s
0: Total training time 121 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       4|                 260|                      0.25|                     207800.8|                         2.018|
DONE!
