1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
1: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 614
1: Scheduler decay interval: 77
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 614
0: Scheduler decay interval: 77
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
1: Initializing amp optimizer
0: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/461]	Time 0.250 (0.250)	Data 1.31e-01 (1.31e-01)	Tok/s 17319 (17319)	Loss/tok 10.5304 (10.5304)	LR 2.047e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
1: TRAIN [0][0/461]	Time 0.253 (0.253)	Data 1.41e-01 (1.41e-01)	Tok/s 17212 (17212)	Loss/tok 10.5276 (10.5276)	LR 2.047e-05
1: TRAIN [0][10/461]	Time 0.227 (0.190)	Data 1.17e-04 (1.30e-02)	Tok/s 44337 (37906)	Loss/tok 9.6755 (10.1164)	LR 2.576e-05
0: TRAIN [0][10/461]	Time 0.222 (0.190)	Data 1.68e-04 (1.20e-02)	Tok/s 45492 (37821)	Loss/tok 9.6749 (10.1140)	LR 2.576e-05
1: TRAIN [0][20/461]	Time 0.176 (0.198)	Data 9.73e-05 (6.84e-03)	Tok/s 41109 (39458)	Loss/tok 9.1712 (9.7515)	LR 3.244e-05
0: TRAIN [0][20/461]	Time 0.176 (0.198)	Data 9.47e-05 (6.36e-03)	Tok/s 40742 (39357)	Loss/tok 9.1431 (9.7447)	LR 3.244e-05
0: TRAIN [0][30/461]	Time 0.230 (0.200)	Data 1.05e-04 (4.35e-03)	Tok/s 44101 (40028)	Loss/tok 8.9737 (9.5039)	LR 4.083e-05
1: TRAIN [0][30/461]	Time 0.230 (0.200)	Data 1.46e-04 (4.67e-03)	Tok/s 44478 (39995)	Loss/tok 9.0141 (9.5167)	LR 4.083e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
1: TRAIN [0][40/461]	Time 0.133 (0.199)	Data 1.07e-04 (3.56e-03)	Tok/s 32251 (39947)	Loss/tok 8.5335 (9.3562)	LR 5.141e-05
0: TRAIN [0][40/461]	Time 0.133 (0.199)	Data 1.09e-04 (3.32e-03)	Tok/s 32450 (39988)	Loss/tok 8.5479 (9.3446)	LR 5.141e-05
0: TRAIN [0][50/461]	Time 0.132 (0.192)	Data 1.08e-04 (2.69e-03)	Tok/s 33416 (39527)	Loss/tok 8.3001 (9.2156)	LR 6.472e-05
1: TRAIN [0][50/461]	Time 0.135 (0.192)	Data 9.89e-05 (2.88e-03)	Tok/s 32580 (39459)	Loss/tok 8.3469 (9.2237)	LR 6.472e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][60/461]	Time 0.157 (0.192)	Data 1.22e-04 (2.27e-03)	Tok/s 45864 (39879)	Loss/tok 8.2765 (9.0823)	LR 8.148e-05
1: TRAIN [0][60/461]	Time 0.159 (0.192)	Data 1.04e-04 (2.43e-03)	Tok/s 45181 (39815)	Loss/tok 8.3745 (9.0908)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][70/461]	Time 0.134 (0.187)	Data 9.73e-05 (1.96e-03)	Tok/s 32208 (39332)	Loss/tok 7.9267 (9.0056)	LR 1.026e-04
1: TRAIN [0][70/461]	Time 0.133 (0.187)	Data 9.68e-05 (2.10e-03)	Tok/s 33304 (39267)	Loss/tok 7.8586 (9.0143)	LR 1.026e-04
1: TRAIN [0][80/461]	Time 0.182 (0.190)	Data 1.00e-04 (1.85e-03)	Tok/s 39687 (39461)	Loss/tok 8.0758 (8.8962)	LR 1.291e-04
0: TRAIN [0][80/461]	Time 0.182 (0.190)	Data 1.53e-04 (1.74e-03)	Tok/s 39871 (39462)	Loss/tok 8.0391 (8.8913)	LR 1.291e-04
1: TRAIN [0][90/461]	Time 0.185 (0.193)	Data 9.87e-05 (1.66e-03)	Tok/s 38587 (39619)	Loss/tok 7.8262 (8.7818)	LR 1.626e-04
0: TRAIN [0][90/461]	Time 0.182 (0.193)	Data 1.54e-04 (1.56e-03)	Tok/s 39316 (39649)	Loss/tok 7.7786 (8.7746)	LR 1.626e-04
1: TRAIN [0][100/461]	Time 0.290 (0.198)	Data 9.63e-05 (1.51e-03)	Tok/s 45121 (39828)	Loss/tok 7.9019 (8.6706)	LR 2.047e-04
0: TRAIN [0][100/461]	Time 0.291 (0.198)	Data 1.79e-04 (1.42e-03)	Tok/s 44799 (39876)	Loss/tok 7.9050 (8.6624)	LR 2.047e-04
1: TRAIN [0][110/461]	Time 0.141 (0.195)	Data 1.03e-04 (1.38e-03)	Tok/s 30105 (39475)	Loss/tok 7.4319 (8.6061)	LR 2.576e-04
0: TRAIN [0][110/461]	Time 0.137 (0.195)	Data 1.58e-04 (1.31e-03)	Tok/s 31170 (39532)	Loss/tok 7.4383 (8.5973)	LR 2.576e-04
1: TRAIN [0][120/461]	Time 0.134 (0.193)	Data 1.02e-04 (1.28e-03)	Tok/s 33231 (39358)	Loss/tok 7.2521 (8.5389)	LR 3.244e-04
0: TRAIN [0][120/461]	Time 0.134 (0.193)	Data 1.53e-04 (1.22e-03)	Tok/s 32486 (39389)	Loss/tok 7.3430 (8.5315)	LR 3.244e-04
1: TRAIN [0][130/461]	Time 0.293 (0.195)	Data 1.01e-04 (1.19e-03)	Tok/s 44614 (39066)	Loss/tok 8.0600 (8.4787)	LR 4.083e-04
0: TRAIN [0][130/461]	Time 0.295 (0.195)	Data 1.49e-04 (1.14e-03)	Tok/s 44830 (39139)	Loss/tok 8.0817 (8.4740)	LR 4.083e-04
1: TRAIN [0][140/461]	Time 0.233 (0.193)	Data 9.68e-05 (1.11e-03)	Tok/s 43153 (38801)	Loss/tok 7.7906 (8.4303)	LR 5.141e-04
0: TRAIN [0][140/461]	Time 0.231 (0.193)	Data 1.46e-04 (1.07e-03)	Tok/s 42801 (38858)	Loss/tok 7.7703 (8.4253)	LR 5.141e-04
1: TRAIN [0][150/461]	Time 0.181 (0.192)	Data 9.13e-05 (1.04e-03)	Tok/s 38936 (38867)	Loss/tok 7.6302 (8.3797)	LR 6.472e-04
0: TRAIN [0][150/461]	Time 0.183 (0.192)	Data 9.73e-05 (1.00e-03)	Tok/s 38725 (38924)	Loss/tok 7.5758 (8.3741)	LR 6.472e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
1: TRAIN [0][160/461]	Time 0.231 (0.191)	Data 9.56e-05 (9.83e-04)	Tok/s 43547 (38815)	Loss/tok 7.7852 (8.3439)	LR 8.148e-04
0: TRAIN [0][160/461]	Time 0.232 (0.191)	Data 9.27e-05 (9.47e-04)	Tok/s 43538 (38880)	Loss/tok 7.7836 (8.3383)	LR 8.148e-04
1: TRAIN [0][170/461]	Time 0.233 (0.190)	Data 1.01e-04 (9.32e-04)	Tok/s 43259 (38699)	Loss/tok 7.7534 (8.3058)	LR 1.026e-03
0: TRAIN [0][170/461]	Time 0.233 (0.190)	Data 9.68e-05 (8.97e-04)	Tok/s 43407 (38751)	Loss/tok 7.7165 (8.3000)	LR 1.026e-03
1: TRAIN [0][180/461]	Time 0.229 (0.190)	Data 1.01e-04 (8.86e-04)	Tok/s 43932 (38775)	Loss/tok 7.6282 (8.2646)	LR 1.291e-03
0: TRAIN [0][180/461]	Time 0.229 (0.190)	Data 1.18e-04 (8.54e-04)	Tok/s 43939 (38821)	Loss/tok 7.6046 (8.2570)	LR 1.291e-03
1: TRAIN [0][190/461]	Time 0.235 (0.191)	Data 1.00e-04 (8.45e-04)	Tok/s 42542 (38882)	Loss/tok 7.5165 (8.2228)	LR 1.626e-03
0: TRAIN [0][190/461]	Time 0.234 (0.191)	Data 1.59e-04 (8.16e-04)	Tok/s 43127 (38926)	Loss/tok 7.5104 (8.2162)	LR 1.626e-03
1: TRAIN [0][200/461]	Time 0.235 (0.192)	Data 1.38e-04 (8.08e-04)	Tok/s 42443 (38925)	Loss/tok 7.3621 (8.1792)	LR 2.000e-03
0: TRAIN [0][200/461]	Time 0.235 (0.192)	Data 1.32e-04 (7.83e-04)	Tok/s 42560 (38975)	Loss/tok 7.3645 (8.1724)	LR 2.000e-03
1: TRAIN [0][210/461]	Time 0.230 (0.193)	Data 1.01e-04 (7.75e-04)	Tok/s 43699 (38970)	Loss/tok 7.2944 (8.1365)	LR 2.000e-03
0: TRAIN [0][210/461]	Time 0.229 (0.193)	Data 1.02e-04 (7.51e-04)	Tok/s 44013 (39031)	Loss/tok 7.3077 (8.1310)	LR 2.000e-03
0: TRAIN [0][220/461]	Time 0.134 (0.193)	Data 1.13e-04 (7.22e-04)	Tok/s 33084 (39041)	Loss/tok 6.6980 (8.0863)	LR 2.000e-03
1: TRAIN [0][220/461]	Time 0.135 (0.193)	Data 1.13e-04 (7.45e-04)	Tok/s 31301 (38965)	Loss/tok 6.7669 (8.0927)	LR 2.000e-03
1: TRAIN [0][230/461]	Time 0.229 (0.193)	Data 1.07e-04 (7.17e-04)	Tok/s 43902 (39007)	Loss/tok 7.0588 (8.0483)	LR 2.000e-03
0: TRAIN [0][230/461]	Time 0.230 (0.193)	Data 1.77e-04 (6.96e-04)	Tok/s 43678 (39079)	Loss/tok 7.1647 (8.0429)	LR 2.000e-03
1: TRAIN [0][240/461]	Time 0.230 (0.193)	Data 1.01e-04 (6.91e-04)	Tok/s 43510 (39064)	Loss/tok 6.9313 (7.9996)	LR 2.000e-03
0: TRAIN [0][240/461]	Time 0.230 (0.193)	Data 1.05e-04 (6.72e-04)	Tok/s 44098 (39137)	Loss/tok 6.8802 (7.9946)	LR 2.000e-03
0: TRAIN [0][250/461]	Time 0.183 (0.193)	Data 9.97e-05 (6.50e-04)	Tok/s 39404 (39065)	Loss/tok 6.7273 (7.9530)	LR 2.000e-03
1: TRAIN [0][250/461]	Time 0.182 (0.193)	Data 1.04e-04 (6.68e-04)	Tok/s 39668 (38986)	Loss/tok 6.7239 (7.9583)	LR 2.000e-03
1: TRAIN [0][260/461]	Time 0.295 (0.194)	Data 1.07e-04 (6.46e-04)	Tok/s 44276 (39102)	Loss/tok 6.7790 (7.9020)	LR 2.000e-03
0: TRAIN [0][260/461]	Time 0.293 (0.194)	Data 1.11e-04 (6.29e-04)	Tok/s 44616 (39178)	Loss/tok 6.9175 (7.8980)	LR 2.000e-03
0: TRAIN [0][270/461]	Time 0.183 (0.194)	Data 1.04e-04 (6.10e-04)	Tok/s 39088 (39191)	Loss/tok 6.4186 (7.8503)	LR 2.000e-03
1: TRAIN [0][270/461]	Time 0.184 (0.194)	Data 1.55e-04 (6.28e-04)	Tok/s 39198 (39120)	Loss/tok 6.4424 (7.8537)	LR 2.000e-03
0: TRAIN [0][280/461]	Time 0.134 (0.194)	Data 9.82e-05 (5.93e-04)	Tok/s 32152 (39139)	Loss/tok 5.9728 (7.8062)	LR 2.000e-03
1: TRAIN [0][280/461]	Time 0.136 (0.194)	Data 1.52e-04 (6.11e-04)	Tok/s 32141 (39072)	Loss/tok 6.0491 (7.8091)	LR 2.000e-03
0: TRAIN [0][290/461]	Time 0.135 (0.193)	Data 1.12e-04 (5.77e-04)	Tok/s 32068 (39092)	Loss/tok 5.9552 (7.7620)	LR 2.000e-03
1: TRAIN [0][290/461]	Time 0.134 (0.193)	Data 1.58e-04 (5.95e-04)	Tok/s 32927 (39031)	Loss/tok 6.0398 (7.7655)	LR 2.000e-03
0: TRAIN [0][300/461]	Time 0.133 (0.194)	Data 1.12e-04 (5.62e-04)	Tok/s 32625 (39162)	Loss/tok 5.9188 (7.7092)	LR 2.000e-03
1: TRAIN [0][300/461]	Time 0.134 (0.194)	Data 1.08e-04 (5.80e-04)	Tok/s 32215 (39110)	Loss/tok 5.8783 (7.7128)	LR 2.000e-03
0: TRAIN [0][310/461]	Time 0.231 (0.193)	Data 1.13e-04 (5.48e-04)	Tok/s 43599 (39111)	Loss/tok 6.3724 (7.6676)	LR 2.000e-03
1: TRAIN [0][310/461]	Time 0.233 (0.193)	Data 1.04e-04 (5.64e-04)	Tok/s 43035 (39060)	Loss/tok 6.4286 (7.6721)	LR 2.000e-03
1: TRAIN [0][320/461]	Time 0.293 (0.195)	Data 1.26e-04 (5.50e-04)	Tok/s 44530 (39174)	Loss/tok 6.4553 (7.6183)	LR 2.000e-03
0: TRAIN [0][320/461]	Time 0.293 (0.195)	Data 1.03e-04 (5.34e-04)	Tok/s 44530 (39220)	Loss/tok 6.4248 (7.6131)	LR 2.000e-03
0: TRAIN [0][330/461]	Time 0.134 (0.196)	Data 1.06e-04 (5.21e-04)	Tok/s 32269 (39313)	Loss/tok 5.7416 (7.5585)	LR 2.000e-03
1: TRAIN [0][330/461]	Time 0.135 (0.196)	Data 1.57e-04 (5.37e-04)	Tok/s 31388 (39260)	Loss/tok 5.7550 (7.5637)	LR 2.000e-03
0: TRAIN [0][340/461]	Time 0.188 (0.196)	Data 1.38e-04 (5.10e-04)	Tok/s 38260 (39281)	Loss/tok 5.9291 (7.5164)	LR 2.000e-03
1: TRAIN [0][340/461]	Time 0.187 (0.196)	Data 1.47e-04 (5.26e-04)	Tok/s 38631 (39234)	Loss/tok 5.9233 (7.5210)	LR 2.000e-03
1: TRAIN [0][350/461]	Time 0.180 (0.196)	Data 1.05e-04 (5.14e-04)	Tok/s 40133 (39183)	Loss/tok 5.9077 (7.4827)	LR 2.000e-03
0: TRAIN [0][350/461]	Time 0.181 (0.196)	Data 1.11e-04 (4.98e-04)	Tok/s 39943 (39227)	Loss/tok 5.8002 (7.4785)	LR 2.000e-03
0: TRAIN [0][360/461]	Time 0.133 (0.196)	Data 9.47e-05 (4.88e-04)	Tok/s 33307 (39227)	Loss/tok 5.4934 (7.4345)	LR 2.000e-03
1: TRAIN [0][360/461]	Time 0.132 (0.196)	Data 1.03e-04 (5.03e-04)	Tok/s 32800 (39178)	Loss/tok 5.5459 (7.4401)	LR 2.000e-03
1: TRAIN [0][370/461]	Time 0.184 (0.196)	Data 1.01e-04 (4.92e-04)	Tok/s 39127 (39180)	Loss/tok 5.7324 (7.3990)	LR 2.000e-03
0: TRAIN [0][370/461]	Time 0.188 (0.196)	Data 9.92e-05 (4.78e-04)	Tok/s 38321 (39225)	Loss/tok 5.7336 (7.3936)	LR 2.000e-03
0: TRAIN [0][380/461]	Time 0.296 (0.196)	Data 9.92e-05 (4.69e-04)	Tok/s 43937 (39144)	Loss/tok 6.0615 (7.3584)	LR 2.000e-03
1: TRAIN [0][380/461]	Time 0.300 (0.196)	Data 9.37e-05 (4.82e-04)	Tok/s 44290 (39108)	Loss/tok 6.1640 (7.3639)	LR 2.000e-03
0: TRAIN [0][390/461]	Time 0.132 (0.194)	Data 1.21e-04 (4.60e-04)	Tok/s 32513 (39015)	Loss/tok 5.4208 (7.3284)	LR 2.000e-03
1: TRAIN [0][390/461]	Time 0.135 (0.194)	Data 1.05e-04 (4.72e-04)	Tok/s 31995 (38990)	Loss/tok 5.2951 (7.3337)	LR 2.000e-03
0: TRAIN [0][400/461]	Time 0.177 (0.194)	Data 1.09e-04 (4.51e-04)	Tok/s 40856 (38998)	Loss/tok 5.6195 (7.2894)	LR 2.000e-03
1: TRAIN [0][400/461]	Time 0.175 (0.194)	Data 1.08e-04 (4.63e-04)	Tok/s 41888 (38979)	Loss/tok 5.6937 (7.2953)	LR 2.000e-03
1: TRAIN [0][410/461]	Time 0.231 (0.194)	Data 1.09e-04 (4.54e-04)	Tok/s 43385 (38988)	Loss/tok 5.7810 (7.2555)	LR 2.000e-03
0: TRAIN [0][410/461]	Time 0.231 (0.194)	Data 1.26e-04 (4.43e-04)	Tok/s 43423 (39008)	Loss/tok 5.7140 (7.2489)	LR 2.000e-03
0: TRAIN [0][420/461]	Time 0.087 (0.193)	Data 1.16e-04 (4.36e-04)	Tok/s 24902 (38914)	Loss/tok 4.8760 (7.2177)	LR 2.000e-03
1: TRAIN [0][420/461]	Time 0.089 (0.193)	Data 1.06e-04 (4.46e-04)	Tok/s 24178 (38891)	Loss/tok 4.4811 (7.2236)	LR 2.000e-03
1: TRAIN [0][430/461]	Time 0.231 (0.193)	Data 1.02e-04 (4.39e-04)	Tok/s 43879 (38895)	Loss/tok 5.7583 (7.1870)	LR 2.000e-03
0: TRAIN [0][430/461]	Time 0.231 (0.193)	Data 1.14e-04 (4.29e-04)	Tok/s 43206 (38916)	Loss/tok 5.7488 (7.1808)	LR 2.000e-03
0: TRAIN [0][440/461]	Time 0.135 (0.193)	Data 1.05e-04 (4.22e-04)	Tok/s 31520 (38933)	Loss/tok 5.1189 (7.1407)	LR 2.000e-03
1: TRAIN [0][440/461]	Time 0.136 (0.193)	Data 9.73e-05 (4.31e-04)	Tok/s 32097 (38913)	Loss/tok 4.8935 (7.1472)	LR 2.000e-03
0: TRAIN [0][450/461]	Time 0.233 (0.193)	Data 1.42e-04 (4.15e-04)	Tok/s 43246 (38941)	Loss/tok 5.4468 (7.1012)	LR 2.000e-03
1: TRAIN [0][450/461]	Time 0.233 (0.193)	Data 1.10e-04 (4.24e-04)	Tok/s 42834 (38921)	Loss/tok 5.5249 (7.1085)	LR 2.000e-03
1: TRAIN [0][460/461]	Time 0.129 (0.193)	Data 3.43e-05 (4.19e-04)	Tok/s 34190 (38866)	Loss/tok 4.8684 (7.0756)	LR 2.000e-03
0: TRAIN [0][460/461]	Time 0.133 (0.193)	Data 3.74e-05 (4.10e-04)	Tok/s 32757 (38887)	Loss/tok 4.8204 (7.0685)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.059 (0.059)	Data 1.74e-03 (1.74e-03)	Tok/s 81078 (81078)	Loss/tok 6.7183 (6.7183)
0: VALIDATION [0][0/80]	Time 0.080 (0.080)	Data 1.68e-03 (1.68e-03)	Tok/s 71120 (71120)	Loss/tok 6.7713 (6.7713)
1: VALIDATION [0][10/80]	Time 0.033 (0.041)	Data 1.35e-03 (1.44e-03)	Tok/s 87555 (85947)	Loss/tok 6.5083 (6.6082)
0: VALIDATION [0][10/80]	Time 0.034 (0.044)	Data 1.50e-03 (1.52e-03)	Tok/s 87466 (84299)	Loss/tok 6.4634 (6.5734)
1: VALIDATION [0][20/80]	Time 0.027 (0.035)	Data 1.42e-03 (1.41e-03)	Tok/s 85099 (86603)	Loss/tok 6.1557 (6.5092)
0: VALIDATION [0][20/80]	Time 0.026 (0.037)	Data 1.31e-03 (1.45e-03)	Tok/s 88477 (85374)	Loss/tok 6.1087 (6.4939)
1: VALIDATION [0][30/80]	Time 0.023 (0.032)	Data 1.34e-03 (1.40e-03)	Tok/s 84294 (86791)	Loss/tok 6.2302 (6.4363)
0: VALIDATION [0][30/80]	Time 0.023 (0.033)	Data 1.28e-03 (1.41e-03)	Tok/s 84785 (85744)	Loss/tok 6.0957 (6.4228)
1: VALIDATION [0][40/80]	Time 0.019 (0.029)	Data 1.29e-03 (1.39e-03)	Tok/s 83031 (85966)	Loss/tok 6.1125 (6.3775)
0: VALIDATION [0][40/80]	Time 0.019 (0.030)	Data 1.35e-03 (1.40e-03)	Tok/s 84979 (85514)	Loss/tok 6.0571 (6.3834)
1: VALIDATION [0][50/80]	Time 0.016 (0.027)	Data 1.35e-03 (1.39e-03)	Tok/s 83181 (85465)	Loss/tok 5.9529 (6.3396)
0: VALIDATION [0][50/80]	Time 0.016 (0.028)	Data 1.32e-03 (1.39e-03)	Tok/s 83766 (85082)	Loss/tok 6.2129 (6.3395)
1: VALIDATION [0][60/80]	Time 0.013 (0.025)	Data 1.36e-03 (1.38e-03)	Tok/s 79035 (84443)	Loss/tok 5.8968 (6.3041)
0: VALIDATION [0][60/80]	Time 0.014 (0.026)	Data 1.31e-03 (1.38e-03)	Tok/s 78370 (84270)	Loss/tok 6.0900 (6.3029)
1: VALIDATION [0][70/80]	Time 0.012 (0.023)	Data 1.32e-03 (1.38e-03)	Tok/s 69411 (83034)	Loss/tok 5.9282 (6.2695)
0: VALIDATION [0][70/80]	Time 0.011 (0.024)	Data 1.37e-03 (1.38e-03)	Tok/s 72022 (82741)	Loss/tok 5.9240 (6.2747)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [0][9/47]	Time 0.3871 (0.4278)	Decoder iters 149.0 (149.0)	Tok/s 6161 (7140)
0: TEST [0][9/47]	Time 0.3875 (0.4279)	Decoder iters 149.0 (149.0)	Tok/s 6485 (7709)
1: TEST [0][19/47]	Time 0.3791 (0.4099)	Decoder iters 105.0 (146.8)	Tok/s 4872 (6396)
0: TEST [0][19/47]	Time 0.3790 (0.4099)	Decoder iters 149.0 (149.0)	Tok/s 5103 (6886)
1: TEST [0][29/47]	Time 0.3649 (0.3969)	Decoder iters 149.0 (147.5)	Tok/s 4447 (5783)
0: TEST [0][29/47]	Time 0.3649 (0.3969)	Decoder iters 149.0 (149.0)	Tok/s 3930 (6153)
1: TEST [0][39/47]	Time 0.1440 (0.3871)	Decoder iters 33.0 (137.8)	Tok/s 7165 (5323)
0: TEST [0][39/47]	Time 0.1440 (0.3871)	Decoder iters 52.0 (142.1)	Tok/s 6970 (5627)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.0721	Validation Loss: 6.2412	Test BLEU: 1.78
0: Performance: Epoch: 0	Training: 77753 Tok/s	Validation: 161498 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/461]	Time 0.299 (0.299)	Data 1.19e-01 (1.19e-01)	Tok/s 24129 (24129)	Loss/tok 5.1183 (5.1183)	LR 2.000e-03
0: TRAIN [1][0/461]	Time 0.302 (0.302)	Data 1.37e-01 (1.37e-01)	Tok/s 23852 (23852)	Loss/tok 5.0469 (5.0469)	LR 2.000e-03
1: TRAIN [1][10/461]	Time 0.135 (0.185)	Data 9.51e-05 (1.09e-02)	Tok/s 31069 (35722)	Loss/tok 4.6164 (5.0690)	LR 2.000e-03
0: TRAIN [1][10/461]	Time 0.135 (0.186)	Data 2.11e-04 (1.26e-02)	Tok/s 31856 (36002)	Loss/tok 4.6511 (5.0197)	LR 2.000e-03
0: TRAIN [1][20/461]	Time 0.187 (0.204)	Data 1.03e-04 (6.68e-03)	Tok/s 38737 (38640)	Loss/tok 4.9585 (5.1777)	LR 2.000e-03
1: TRAIN [1][20/461]	Time 0.190 (0.204)	Data 1.00e-04 (5.77e-03)	Tok/s 38086 (38297)	Loss/tok 4.9093 (5.1716)	LR 2.000e-03
1: TRAIN [1][30/461]	Time 0.293 (0.205)	Data 9.32e-05 (3.95e-03)	Tok/s 44983 (39105)	Loss/tok 5.4420 (5.1686)	LR 2.000e-03
0: TRAIN [1][30/461]	Time 0.298 (0.206)	Data 1.11e-04 (4.56e-03)	Tok/s 44181 (39151)	Loss/tok 5.4235 (5.1610)	LR 2.000e-03
1: TRAIN [1][40/461]	Time 0.233 (0.199)	Data 9.49e-05 (3.01e-03)	Tok/s 43254 (38958)	Loss/tok 5.1701 (5.1097)	LR 2.000e-03
0: TRAIN [1][40/461]	Time 0.232 (0.199)	Data 1.62e-04 (3.48e-03)	Tok/s 43485 (38930)	Loss/tok 5.1807 (5.0994)	LR 2.000e-03
0: TRAIN [1][50/461]	Time 0.182 (0.199)	Data 1.10e-04 (2.82e-03)	Tok/s 39962 (39046)	Loss/tok 4.7843 (5.0786)	LR 2.000e-03
1: TRAIN [1][50/461]	Time 0.188 (0.199)	Data 9.39e-05 (2.44e-03)	Tok/s 38104 (39031)	Loss/tok 4.8780 (5.0907)	LR 2.000e-03
1: TRAIN [1][60/461]	Time 0.179 (0.196)	Data 8.96e-05 (2.05e-03)	Tok/s 40391 (39021)	Loss/tok 4.7421 (5.0535)	LR 2.000e-03
0: TRAIN [1][60/461]	Time 0.180 (0.196)	Data 1.00e-04 (2.38e-03)	Tok/s 40229 (38982)	Loss/tok 4.5802 (5.0376)	LR 2.000e-03
0: TRAIN [1][70/461]	Time 0.131 (0.194)	Data 1.03e-04 (2.06e-03)	Tok/s 33772 (38810)	Loss/tok 4.3728 (5.0102)	LR 2.000e-03
1: TRAIN [1][70/461]	Time 0.131 (0.194)	Data 9.42e-05 (1.78e-03)	Tok/s 33357 (38841)	Loss/tok 4.3413 (5.0238)	LR 2.000e-03
1: TRAIN [1][80/461]	Time 0.180 (0.193)	Data 9.11e-05 (1.57e-03)	Tok/s 39401 (38898)	Loss/tok 4.6750 (5.0058)	LR 2.000e-03
0: TRAIN [1][80/461]	Time 0.181 (0.193)	Data 1.12e-04 (1.81e-03)	Tok/s 40043 (38860)	Loss/tok 4.6725 (4.9948)	LR 2.000e-03
0: TRAIN [1][90/461]	Time 0.178 (0.191)	Data 1.13e-04 (1.63e-03)	Tok/s 40451 (38707)	Loss/tok 4.6620 (4.9850)	LR 2.000e-03
1: TRAIN [1][90/461]	Time 0.185 (0.191)	Data 8.85e-05 (1.41e-03)	Tok/s 39172 (38716)	Loss/tok 4.6991 (4.9922)	LR 2.000e-03
0: TRAIN [1][100/461]	Time 0.130 (0.190)	Data 1.06e-04 (1.48e-03)	Tok/s 33704 (38525)	Loss/tok 4.2725 (4.9664)	LR 2.000e-03
1: TRAIN [1][100/461]	Time 0.130 (0.190)	Data 8.82e-05 (1.28e-03)	Tok/s 33221 (38549)	Loss/tok 4.3590 (4.9771)	LR 2.000e-03
1: TRAIN [1][110/461]	Time 0.178 (0.189)	Data 9.04e-05 (1.17e-03)	Tok/s 40589 (38484)	Loss/tok 4.5951 (4.9503)	LR 2.000e-03
0: TRAIN [1][110/461]	Time 0.178 (0.189)	Data 1.10e-04 (1.36e-03)	Tok/s 40069 (38467)	Loss/tok 4.6700 (4.9424)	LR 2.000e-03
0: TRAIN [1][120/461]	Time 0.088 (0.189)	Data 1.08e-04 (1.25e-03)	Tok/s 24367 (38503)	Loss/tok 3.7402 (4.9227)	LR 2.000e-03
1: TRAIN [1][120/461]	Time 0.090 (0.189)	Data 1.06e-04 (1.08e-03)	Tok/s 24309 (38483)	Loss/tok 4.0537 (4.9332)	LR 2.000e-03
1: TRAIN [1][130/461]	Time 0.231 (0.189)	Data 9.68e-05 (1.01e-03)	Tok/s 43707 (38496)	Loss/tok 4.8464 (4.9154)	LR 2.000e-03
0: TRAIN [1][130/461]	Time 0.234 (0.189)	Data 1.38e-04 (1.16e-03)	Tok/s 42849 (38526)	Loss/tok 4.7267 (4.9053)	LR 2.000e-03
0: TRAIN [1][140/461]	Time 0.241 (0.191)	Data 1.08e-04 (1.09e-03)	Tok/s 42021 (38813)	Loss/tok 4.7028 (4.8920)	LR 2.000e-03
1: TRAIN [1][140/461]	Time 0.244 (0.191)	Data 1.54e-04 (9.43e-04)	Tok/s 41339 (38742)	Loss/tok 4.6826 (4.8979)	LR 2.000e-03
1: TRAIN [1][150/461]	Time 0.233 (0.193)	Data 9.20e-05 (8.89e-04)	Tok/s 43880 (38843)	Loss/tok 4.7794 (4.8838)	LR 2.000e-03
0: TRAIN [1][150/461]	Time 0.232 (0.193)	Data 1.58e-04 (1.03e-03)	Tok/s 43026 (38882)	Loss/tok 4.7445 (4.8803)	LR 2.000e-03
1: TRAIN [1][160/461]	Time 0.180 (0.194)	Data 9.80e-05 (8.40e-04)	Tok/s 39726 (38898)	Loss/tok 4.4150 (4.8720)	LR 1.000e-03
0: TRAIN [1][160/461]	Time 0.181 (0.194)	Data 1.08e-04 (9.70e-04)	Tok/s 40385 (38943)	Loss/tok 4.3144 (4.8670)	LR 1.000e-03
0: TRAIN [1][170/461]	Time 0.231 (0.195)	Data 1.06e-04 (9.20e-04)	Tok/s 44063 (38998)	Loss/tok 4.4894 (4.8458)	LR 1.000e-03
1: TRAIN [1][170/461]	Time 0.235 (0.195)	Data 9.89e-05 (7.96e-04)	Tok/s 43570 (38959)	Loss/tok 4.6345 (4.8534)	LR 1.000e-03
0: TRAIN [1][180/461]	Time 0.129 (0.195)	Data 1.05e-04 (8.75e-04)	Tok/s 32969 (39013)	Loss/tok 4.0577 (4.8246)	LR 1.000e-03
1: TRAIN [1][180/461]	Time 0.135 (0.195)	Data 1.05e-04 (7.57e-04)	Tok/s 31654 (38971)	Loss/tok 4.1065 (4.8334)	LR 1.000e-03
0: TRAIN [1][190/461]	Time 0.292 (0.196)	Data 1.05e-04 (8.35e-04)	Tok/s 44831 (39035)	Loss/tok 4.7112 (4.8086)	LR 1.000e-03
1: TRAIN [1][190/461]	Time 0.294 (0.196)	Data 9.97e-05 (7.23e-04)	Tok/s 44245 (38995)	Loss/tok 4.7447 (4.8165)	LR 1.000e-03
1: TRAIN [1][200/461]	Time 0.289 (0.196)	Data 9.61e-05 (6.93e-04)	Tok/s 45056 (39063)	Loss/tok 4.7435 (4.7976)	LR 1.000e-03
0: TRAIN [1][200/461]	Time 0.292 (0.196)	Data 1.18e-04 (7.99e-04)	Tok/s 44871 (39094)	Loss/tok 4.6253 (4.7885)	LR 1.000e-03
1: TRAIN [1][210/461]	Time 0.133 (0.195)	Data 1.09e-04 (6.64e-04)	Tok/s 32016 (39085)	Loss/tok 3.9221 (4.7753)	LR 1.000e-03
0: TRAIN [1][210/461]	Time 0.134 (0.195)	Data 1.03e-04 (7.66e-04)	Tok/s 32295 (39122)	Loss/tok 3.9242 (4.7668)	LR 1.000e-03
1: TRAIN [1][220/461]	Time 0.183 (0.196)	Data 1.02e-04 (6.39e-04)	Tok/s 40206 (39110)	Loss/tok 4.1802 (4.7584)	LR 1.000e-03
0: TRAIN [1][220/461]	Time 0.183 (0.196)	Data 1.07e-04 (7.36e-04)	Tok/s 39060 (39152)	Loss/tok 4.1628 (4.7501)	LR 1.000e-03
1: TRAIN [1][230/461]	Time 0.231 (0.196)	Data 9.61e-05 (6.15e-04)	Tok/s 43982 (39186)	Loss/tok 4.4961 (4.7395)	LR 5.000e-04
0: TRAIN [1][230/461]	Time 0.230 (0.196)	Data 1.13e-04 (7.09e-04)	Tok/s 43711 (39245)	Loss/tok 4.2612 (4.7309)	LR 5.000e-04
0: TRAIN [1][240/461]	Time 0.130 (0.195)	Data 1.14e-04 (6.84e-04)	Tok/s 33780 (39127)	Loss/tok 3.8896 (4.7130)	LR 5.000e-04
1: TRAIN [1][240/461]	Time 0.132 (0.195)	Data 1.05e-04 (5.94e-04)	Tok/s 32990 (39063)	Loss/tok 3.9471 (4.7219)	LR 5.000e-04
1: TRAIN [1][250/461]	Time 0.134 (0.195)	Data 9.92e-05 (5.75e-04)	Tok/s 32333 (39040)	Loss/tok 3.6451 (4.7043)	LR 5.000e-04
0: TRAIN [1][250/461]	Time 0.134 (0.195)	Data 1.08e-04 (6.62e-04)	Tok/s 32102 (39091)	Loss/tok 3.9814 (4.6968)	LR 5.000e-04
0: TRAIN [1][260/461]	Time 0.132 (0.195)	Data 1.25e-04 (6.41e-04)	Tok/s 33409 (39125)	Loss/tok 4.0231 (4.6837)	LR 5.000e-04
1: TRAIN [1][260/461]	Time 0.137 (0.195)	Data 9.56e-05 (5.57e-04)	Tok/s 32596 (39078)	Loss/tok 4.0455 (4.6892)	LR 5.000e-04
0: TRAIN [1][270/461]	Time 0.179 (0.195)	Data 1.08e-04 (6.21e-04)	Tok/s 40190 (39181)	Loss/tok 4.1529 (4.6666)	LR 5.000e-04
1: TRAIN [1][270/461]	Time 0.183 (0.195)	Data 9.32e-05 (5.40e-04)	Tok/s 38684 (39123)	Loss/tok 4.2684 (4.6731)	LR 5.000e-04
0: TRAIN [1][280/461]	Time 0.185 (0.195)	Data 1.07e-04 (6.03e-04)	Tok/s 39191 (39183)	Loss/tok 4.0587 (4.6490)	LR 5.000e-04
1: TRAIN [1][280/461]	Time 0.186 (0.195)	Data 9.68e-05 (5.24e-04)	Tok/s 37896 (39114)	Loss/tok 4.0016 (4.6555)	LR 5.000e-04
1: TRAIN [1][290/461]	Time 0.229 (0.195)	Data 9.13e-05 (5.09e-04)	Tok/s 43967 (39125)	Loss/tok 4.4749 (4.6420)	LR 5.000e-04
0: TRAIN [1][290/461]	Time 0.229 (0.195)	Data 1.05e-04 (5.86e-04)	Tok/s 43374 (39185)	Loss/tok 4.3580 (4.6340)	LR 5.000e-04
1: TRAIN [1][300/461]	Time 0.181 (0.196)	Data 9.37e-05 (4.96e-04)	Tok/s 39491 (39118)	Loss/tok 4.0544 (4.6309)	LR 5.000e-04
0: TRAIN [1][300/461]	Time 0.181 (0.196)	Data 1.07e-04 (5.70e-04)	Tok/s 40651 (39194)	Loss/tok 4.1953 (4.6239)	LR 5.000e-04
0: TRAIN [1][310/461]	Time 0.185 (0.196)	Data 1.58e-04 (5.56e-04)	Tok/s 39575 (39130)	Loss/tok 3.9242 (4.6109)	LR 2.500e-04
1: TRAIN [1][310/461]	Time 0.185 (0.196)	Data 1.49e-04 (4.83e-04)	Tok/s 39295 (39060)	Loss/tok 4.0419 (4.6171)	LR 2.500e-04
0: TRAIN [1][320/461]	Time 0.241 (0.197)	Data 1.13e-04 (5.42e-04)	Tok/s 41462 (39185)	Loss/tok 4.2377 (4.5968)	LR 2.500e-04
1: TRAIN [1][320/461]	Time 0.237 (0.197)	Data 1.62e-04 (4.73e-04)	Tok/s 42316 (39124)	Loss/tok 4.2774 (4.6054)	LR 2.500e-04
0: TRAIN [1][330/461]	Time 0.294 (0.197)	Data 1.19e-04 (5.29e-04)	Tok/s 44128 (39171)	Loss/tok 4.4039 (4.5838)	LR 2.500e-04
1: TRAIN [1][330/461]	Time 0.295 (0.197)	Data 1.08e-04 (4.63e-04)	Tok/s 44341 (39111)	Loss/tok 4.5468 (4.5928)	LR 2.500e-04
1: TRAIN [1][340/461]	Time 0.223 (0.198)	Data 9.99e-05 (4.52e-04)	Tok/s 44977 (39179)	Loss/tok 4.3916 (4.5832)	LR 2.500e-04
0: TRAIN [1][340/461]	Time 0.226 (0.198)	Data 1.00e-04 (5.17e-04)	Tok/s 43953 (39234)	Loss/tok 4.2110 (4.5739)	LR 2.500e-04
1: TRAIN [1][350/461]	Time 0.232 (0.198)	Data 9.54e-05 (4.42e-04)	Tok/s 43520 (39167)	Loss/tok 4.3262 (4.5743)	LR 2.500e-04
0: TRAIN [1][350/461]	Time 0.230 (0.198)	Data 1.33e-04 (5.05e-04)	Tok/s 43902 (39227)	Loss/tok 4.1901 (4.5637)	LR 2.500e-04
1: TRAIN [1][360/461]	Time 0.182 (0.197)	Data 1.05e-04 (4.33e-04)	Tok/s 39877 (39082)	Loss/tok 3.9820 (4.5633)	LR 2.500e-04
0: TRAIN [1][360/461]	Time 0.182 (0.197)	Data 1.16e-04 (4.94e-04)	Tok/s 39432 (39139)	Loss/tok 3.9914 (4.5527)	LR 2.500e-04
1: TRAIN [1][370/461]	Time 0.232 (0.197)	Data 1.00e-04 (4.24e-04)	Tok/s 43235 (39054)	Loss/tok 4.2345 (4.5516)	LR 2.500e-04
0: TRAIN [1][370/461]	Time 0.232 (0.197)	Data 1.14e-04 (4.84e-04)	Tok/s 43029 (39107)	Loss/tok 4.2694 (4.5411)	LR 2.500e-04
1: TRAIN [1][380/461]	Time 0.178 (0.196)	Data 9.70e-05 (4.15e-04)	Tok/s 39523 (39017)	Loss/tok 3.9859 (4.5403)	LR 2.500e-04
0: TRAIN [1][380/461]	Time 0.182 (0.196)	Data 9.82e-05 (4.74e-04)	Tok/s 39612 (39076)	Loss/tok 4.0265 (4.5304)	LR 2.500e-04
1: TRAIN [1][390/461]	Time 0.133 (0.196)	Data 8.96e-05 (4.07e-04)	Tok/s 32216 (39011)	Loss/tok 3.8005 (4.5286)	LR 1.250e-04
0: TRAIN [1][390/461]	Time 0.134 (0.196)	Data 1.01e-04 (4.65e-04)	Tok/s 31638 (39072)	Loss/tok 3.6178 (4.5190)	LR 1.250e-04
1: TRAIN [1][400/461]	Time 0.176 (0.195)	Data 1.04e-04 (3.99e-04)	Tok/s 41354 (38921)	Loss/tok 3.9000 (4.5181)	LR 1.250e-04
0: TRAIN [1][400/461]	Time 0.182 (0.195)	Data 9.97e-05 (4.56e-04)	Tok/s 39797 (38985)	Loss/tok 4.0081 (4.5084)	LR 1.250e-04
0: TRAIN [1][410/461]	Time 0.136 (0.194)	Data 1.06e-04 (4.47e-04)	Tok/s 31335 (38826)	Loss/tok 3.6738 (4.4995)	LR 1.250e-04
1: TRAIN [1][410/461]	Time 0.140 (0.194)	Data 9.85e-05 (3.92e-04)	Tok/s 31821 (38756)	Loss/tok 3.6824 (4.5095)	LR 1.250e-04
0: TRAIN [1][420/461]	Time 0.183 (0.193)	Data 1.02e-04 (4.39e-04)	Tok/s 39098 (38819)	Loss/tok 3.8865 (4.4887)	LR 1.250e-04
1: TRAIN [1][420/461]	Time 0.183 (0.193)	Data 1.49e-04 (3.85e-04)	Tok/s 38710 (38746)	Loss/tok 3.9246 (4.4990)	LR 1.250e-04
0: TRAIN [1][430/461]	Time 0.234 (0.194)	Data 1.08e-04 (4.31e-04)	Tok/s 43435 (38878)	Loss/tok 4.1861 (4.4792)	LR 1.250e-04
1: TRAIN [1][430/461]	Time 0.235 (0.194)	Data 1.75e-04 (3.80e-04)	Tok/s 42776 (38807)	Loss/tok 4.3210 (4.4897)	LR 1.250e-04
0: TRAIN [1][440/461]	Time 0.133 (0.194)	Data 9.87e-05 (4.24e-04)	Tok/s 33093 (38859)	Loss/tok 3.8069 (4.4710)	LR 1.250e-04
1: TRAIN [1][440/461]	Time 0.132 (0.194)	Data 1.46e-04 (3.75e-04)	Tok/s 33182 (38795)	Loss/tok 3.6699 (4.4803)	LR 1.250e-04
0: TRAIN [1][450/461]	Time 0.293 (0.194)	Data 1.05e-04 (4.17e-04)	Tok/s 44561 (38815)	Loss/tok 4.3821 (4.4643)	LR 1.250e-04
1: TRAIN [1][450/461]	Time 0.296 (0.194)	Data 1.44e-04 (3.70e-04)	Tok/s 43862 (38750)	Loss/tok 4.4904 (4.4737)	LR 1.250e-04
0: TRAIN [1][460/461]	Time 0.131 (0.193)	Data 3.62e-05 (4.12e-04)	Tok/s 32748 (38774)	Loss/tok 3.8250 (4.4564)	LR 1.250e-04
1: TRAIN [1][460/461]	Time 0.133 (0.193)	Data 4.91e-05 (3.68e-04)	Tok/s 33010 (38713)	Loss/tok 3.7591 (4.4661)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.060 (0.060)	Data 2.77e-03 (2.77e-03)	Tok/s 79635 (79635)	Loss/tok 5.6560 (5.6560)
0: VALIDATION [1][0/80]	Time 0.080 (0.080)	Data 1.73e-03 (1.73e-03)	Tok/s 71794 (71794)	Loss/tok 5.8017 (5.8017)
1: VALIDATION [1][10/80]	Time 0.032 (0.042)	Data 1.33e-03 (2.37e-03)	Tok/s 90183 (84142)	Loss/tok 5.1937 (5.4568)
0: VALIDATION [1][10/80]	Time 0.035 (0.044)	Data 3.06e-03 (1.60e-03)	Tok/s 83403 (83987)	Loss/tok 5.3058 (5.4378)
1: VALIDATION [1][20/80]	Time 0.027 (0.036)	Data 1.33e-03 (1.90e-03)	Tok/s 86097 (85140)	Loss/tok 4.9586 (5.3611)
0: VALIDATION [1][20/80]	Time 0.027 (0.038)	Data 2.08e-03 (1.91e-03)	Tok/s 84972 (84043)	Loss/tok 4.9655 (5.3763)
1: VALIDATION [1][30/80]	Time 0.023 (0.032)	Data 1.29e-03 (1.73e-03)	Tok/s 84316 (85701)	Loss/tok 5.1010 (5.2825)
0: VALIDATION [1][30/80]	Time 0.024 (0.034)	Data 2.02e-03 (1.96e-03)	Tok/s 80710 (83858)	Loss/tok 4.9882 (5.3086)
1: VALIDATION [1][40/80]	Time 0.019 (0.029)	Data 1.32e-03 (1.64e-03)	Tok/s 83621 (85056)	Loss/tok 5.0261 (5.2255)
0: VALIDATION [1][40/80]	Time 0.020 (0.031)	Data 2.07e-03 (1.99e-03)	Tok/s 79881 (83196)	Loss/tok 4.9442 (5.2754)
1: VALIDATION [1][50/80]	Time 0.016 (0.027)	Data 1.41e-03 (1.58e-03)	Tok/s 82109 (84570)	Loss/tok 5.0518 (5.1975)
0: VALIDATION [1][50/80]	Time 0.017 (0.028)	Data 2.00e-03 (2.00e-03)	Tok/s 79184 (82344)	Loss/tok 5.1435 (5.2314)
1: VALIDATION [1][60/80]	Time 0.014 (0.025)	Data 1.40e-03 (1.55e-03)	Tok/s 77627 (83626)	Loss/tok 4.7860 (5.1683)
0: VALIDATION [1][60/80]	Time 0.015 (0.026)	Data 1.98e-03 (2.01e-03)	Tok/s 73254 (81186)	Loss/tok 4.8259 (5.1929)
1: VALIDATION [1][70/80]	Time 0.011 (0.023)	Data 1.25e-03 (1.52e-03)	Tok/s 69737 (82276)	Loss/tok 4.7404 (5.1382)
0: VALIDATION [1][70/80]	Time 0.013 (0.025)	Data 2.25e-03 (2.10e-03)	Tok/s 62004 (78609)	Loss/tok 4.7526 (5.1669)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
1: TEST [1][9/47]	Time 0.3889 (0.4325)	Decoder iters 149.0 (149.0)	Tok/s 6825 (7710)
0: TEST [1][9/47]	Time 0.3890 (0.4326)	Decoder iters 149.0 (149.0)	Tok/s 6635 (7906)
1: TEST [1][19/47]	Time 0.4170 (0.4146)	Decoder iters 149.0 (149.0)	Tok/s 5140 (6841)
0: TEST [1][19/47]	Time 0.3998 (0.4146)	Decoder iters 128.0 (143.2)	Tok/s 4757 (6925)
1: TEST [1][29/47]	Time 0.4323 (0.3958)	Decoder iters 149.0 (145.9)	Tok/s 3821 (6261)
0: TEST [1][29/47]	Time 0.4350 (0.3958)	Decoder iters 149.0 (131.8)	Tok/s 3657 (6318)
1: TEST [1][39/47]	Time 0.1153 (0.3667)	Decoder iters 29.0 (126.8)	Tok/s 8899 (6026)
0: TEST [1][39/47]	Time 0.1276 (0.3669)	Decoder iters 40.0 (122.0)	Tok/s 7913 (6072)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.4612	Validation Loss: 5.1239	Test BLEU: 6.22
0: Performance: Epoch: 1	Training: 77487 Tok/s	Validation: 155651 Tok/s
0: Finished epoch 1
1: Total training time 244 s
0: Total training time 244 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 160|                      6.22|                      77619.9|                         4.068|
DONE!
