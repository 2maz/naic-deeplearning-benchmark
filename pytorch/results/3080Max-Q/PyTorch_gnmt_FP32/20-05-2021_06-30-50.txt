0: thread affinity: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15}
0: Collecting environment information...
0: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: GPU 0: GeForce RTX 3080 Laptop GPU
Nvidia driver version: 460.73.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=128, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1537
0: Scheduler decay interval: 192
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/1154]	Time 0.636 (0.000)	Data 1.38e-01 (0.00e+00)	Tok/s 12637 (0)	Loss/tok 10.7104 (10.7104)	LR 2.047e-05
0: TRAIN [0][10/1154]	Time 0.448 (0.464)	Data 9.56e-05 (9.46e-05)	Tok/s 12597 (13979)	Loss/tok 9.7176 (10.1331)	LR 2.576e-05
0: TRAIN [0][20/1154]	Time 0.429 (0.427)	Data 9.87e-05 (9.15e-05)	Tok/s 13241 (13704)	Loss/tok 9.2475 (9.8285)	LR 3.244e-05
0: TRAIN [0][30/1154]	Time 0.449 (0.421)	Data 7.94e-05 (9.05e-05)	Tok/s 12507 (13618)	Loss/tok 8.9842 (9.6012)	LR 4.083e-05
0: TRAIN [0][40/1154]	Time 0.436 (0.428)	Data 1.01e-04 (9.07e-05)	Tok/s 13332 (13625)	Loss/tok 8.6598 (9.4144)	LR 5.141e-05
0: TRAIN [0][50/1154]	Time 0.447 (0.442)	Data 9.42e-05 (9.06e-05)	Tok/s 12861 (13711)	Loss/tok 8.5237 (9.2580)	LR 6.472e-05
0: TRAIN [0][60/1154]	Time 0.564 (0.438)	Data 9.11e-05 (9.03e-05)	Tok/s 14351 (13691)	Loss/tok 8.4452 (9.1298)	LR 8.148e-05
0: TRAIN [0][70/1154]	Time 0.441 (0.446)	Data 9.61e-05 (9.07e-05)	Tok/s 13059 (13715)	Loss/tok 8.0711 (8.9949)	LR 1.026e-04
0: TRAIN [0][80/1154]	Time 0.447 (0.452)	Data 9.35e-05 (9.09e-05)	Tok/s 12959 (13719)	Loss/tok 7.9158 (8.8723)	LR 1.291e-04
0: TRAIN [0][90/1154]	Time 0.437 (0.449)	Data 9.73e-05 (9.12e-05)	Tok/s 13203 (13702)	Loss/tok 8.0563 (8.7897)	LR 1.626e-04
0: TRAIN [0][100/1154]	Time 0.700 (0.454)	Data 8.23e-05 (9.11e-05)	Tok/s 14771 (13709)	Loss/tok 8.0880 (8.7048)	LR 2.047e-04
0: TRAIN [0][110/1154]	Time 0.150 (0.451)	Data 1.03e-04 (9.12e-05)	Tok/s 11598 (13663)	Loss/tok 7.1149 (8.6289)	LR 2.576e-04
0: TRAIN [0][120/1154]	Time 0.685 (0.459)	Data 9.01e-05 (9.14e-05)	Tok/s 15461 (13675)	Loss/tok 7.8949 (8.5500)	LR 3.244e-04
0: TRAIN [0][130/1154]	Time 0.428 (0.456)	Data 9.32e-05 (9.15e-05)	Tok/s 13657 (13648)	Loss/tok 7.7471 (8.4969)	LR 4.083e-04
0: TRAIN [0][140/1154]	Time 0.279 (0.459)	Data 1.03e-04 (9.19e-05)	Tok/s 12906 (13633)	Loss/tok 7.4506 (8.4417)	LR 5.141e-04
0: TRAIN [0][150/1154]	Time 0.689 (0.464)	Data 9.51e-05 (9.21e-05)	Tok/s 15125 (13651)	Loss/tok 7.9151 (8.3957)	LR 6.472e-04
0: TRAIN [0][160/1154]	Time 0.287 (0.466)	Data 9.66e-05 (9.24e-05)	Tok/s 12109 (13639)	Loss/tok 7.4444 (8.3520)	LR 8.148e-04
0: TRAIN [0][170/1154]	Time 0.678 (0.466)	Data 9.16e-05 (9.25e-05)	Tok/s 15401 (13625)	Loss/tok 7.8238 (8.3130)	LR 1.026e-03
0: TRAIN [0][180/1154]	Time 0.441 (0.467)	Data 1.07e-04 (9.27e-05)	Tok/s 12841 (13605)	Loss/tok 7.5076 (8.2772)	LR 1.291e-03
0: TRAIN [0][190/1154]	Time 0.456 (0.465)	Data 1.07e-04 (9.28e-05)	Tok/s 12436 (13575)	Loss/tok 7.4177 (8.2429)	LR 1.626e-03
0: TRAIN [0][200/1154]	Time 0.451 (0.471)	Data 1.09e-04 (9.31e-05)	Tok/s 12889 (13597)	Loss/tok 7.5148 (8.2058)	LR 2.000e-03
0: TRAIN [0][210/1154]	Time 0.452 (0.472)	Data 9.13e-05 (9.32e-05)	Tok/s 13023 (13593)	Loss/tok 7.3742 (8.1732)	LR 2.000e-03
0: TRAIN [0][220/1154]	Time 0.282 (0.471)	Data 9.51e-05 (9.33e-05)	Tok/s 12756 (13570)	Loss/tok 7.0265 (8.1376)	LR 2.000e-03
0: TRAIN [0][230/1154]	Time 0.456 (0.465)	Data 9.20e-05 (9.33e-05)	Tok/s 12622 (13526)	Loss/tok 7.2499 (8.1105)	LR 2.000e-03
0: TRAIN [0][240/1154]	Time 0.461 (0.463)	Data 8.99e-05 (9.34e-05)	Tok/s 12420 (13497)	Loss/tok 7.1240 (8.0766)	LR 2.000e-03
0: TRAIN [0][250/1154]	Time 0.281 (0.460)	Data 9.68e-05 (9.35e-05)	Tok/s 12111 (13462)	Loss/tok 6.6337 (8.0444)	LR 2.000e-03
0: TRAIN [0][260/1154]	Time 0.286 (0.460)	Data 9.11e-05 (9.36e-05)	Tok/s 12518 (13451)	Loss/tok 6.7045 (8.0082)	LR 2.000e-03
0: TRAIN [0][270/1154]	Time 0.288 (0.466)	Data 1.11e-04 (9.40e-05)	Tok/s 12196 (13475)	Loss/tok 6.6274 (7.9653)	LR 2.000e-03
0: TRAIN [0][280/1154]	Time 0.588 (0.468)	Data 1.13e-04 (9.42e-05)	Tok/s 13754 (13472)	Loss/tok 6.9388 (7.9281)	LR 2.000e-03
0: TRAIN [0][290/1154]	Time 0.579 (0.471)	Data 1.12e-04 (9.45e-05)	Tok/s 13820 (13470)	Loss/tok 6.8656 (7.8864)	LR 2.000e-03
0: TRAIN [0][300/1154]	Time 0.573 (0.471)	Data 8.92e-05 (9.46e-05)	Tok/s 13862 (13457)	Loss/tok 6.9233 (7.8509)	LR 2.000e-03
0: TRAIN [0][310/1154]	Time 0.453 (0.471)	Data 8.99e-05 (9.46e-05)	Tok/s 12959 (13458)	Loss/tok 6.6305 (7.8155)	LR 2.000e-03
0: TRAIN [0][320/1154]	Time 0.454 (0.468)	Data 9.27e-05 (9.45e-05)	Tok/s 12694 (13432)	Loss/tok 6.5941 (7.7858)	LR 2.000e-03
0: TRAIN [0][330/1154]	Time 0.585 (0.468)	Data 1.03e-04 (9.45e-05)	Tok/s 13854 (13419)	Loss/tok 6.7127 (7.7512)	LR 2.000e-03
0: TRAIN [0][340/1154]	Time 0.568 (0.467)	Data 9.73e-05 (9.45e-05)	Tok/s 14287 (13411)	Loss/tok 6.6088 (7.7181)	LR 2.000e-03
0: TRAIN [0][350/1154]	Time 0.577 (0.467)	Data 9.63e-05 (9.45e-05)	Tok/s 13901 (13402)	Loss/tok 6.5784 (7.6841)	LR 2.000e-03
0: TRAIN [0][360/1154]	Time 0.448 (0.466)	Data 8.61e-05 (9.45e-05)	Tok/s 12511 (13389)	Loss/tok 6.4050 (7.6534)	LR 2.000e-03
0: TRAIN [0][370/1154]	Time 0.278 (0.465)	Data 8.68e-05 (9.44e-05)	Tok/s 12545 (13374)	Loss/tok 5.9367 (7.6217)	LR 2.000e-03
0: TRAIN [0][380/1154]	Time 0.451 (0.466)	Data 8.94e-05 (9.45e-05)	Tok/s 12780 (13374)	Loss/tok 6.2136 (7.5873)	LR 2.000e-03
0: TRAIN [0][390/1154]	Time 0.563 (0.465)	Data 9.97e-05 (9.47e-05)	Tok/s 14232 (13363)	Loss/tok 6.4035 (7.5570)	LR 2.000e-03
0: TRAIN [0][400/1154]	Time 0.284 (0.465)	Data 9.25e-05 (9.47e-05)	Tok/s 11877 (13365)	Loss/tok 5.9389 (7.5272)	LR 2.000e-03
0: TRAIN [0][410/1154]	Time 0.695 (0.467)	Data 9.18e-05 (9.48e-05)	Tok/s 15104 (13374)	Loss/tok 6.4539 (7.4917)	LR 2.000e-03
0: TRAIN [0][420/1154]	Time 0.446 (0.467)	Data 8.96e-05 (9.48e-05)	Tok/s 13190 (13368)	Loss/tok 6.0598 (7.4607)	LR 2.000e-03
0: TRAIN [0][430/1154]	Time 0.575 (0.467)	Data 1.05e-04 (9.48e-05)	Tok/s 14075 (13360)	Loss/tok 6.2408 (7.4291)	LR 2.000e-03
0: TRAIN [0][440/1154]	Time 0.582 (0.467)	Data 9.68e-05 (9.47e-05)	Tok/s 14066 (13362)	Loss/tok 6.0769 (7.3987)	LR 2.000e-03
0: TRAIN [0][450/1154]	Time 0.276 (0.466)	Data 9.01e-05 (9.47e-05)	Tok/s 12261 (13353)	Loss/tok 5.5727 (7.3705)	LR 2.000e-03
0: TRAIN [0][460/1154]	Time 0.281 (0.465)	Data 8.75e-05 (9.47e-05)	Tok/s 11777 (13346)	Loss/tok 5.6094 (7.3431)	LR 2.000e-03
0: TRAIN [0][470/1154]	Time 0.702 (0.468)	Data 9.85e-05 (9.48e-05)	Tok/s 15120 (13365)	Loss/tok 6.1658 (7.3080)	LR 2.000e-03
0: TRAIN [0][480/1154]	Time 0.693 (0.469)	Data 1.05e-04 (9.48e-05)	Tok/s 15138 (13366)	Loss/tok 6.0884 (7.2765)	LR 2.000e-03
0: TRAIN [0][490/1154]	Time 0.280 (0.468)	Data 9.39e-05 (9.48e-05)	Tok/s 12322 (13360)	Loss/tok 5.2896 (7.2510)	LR 2.000e-03
0: TRAIN [0][500/1154]	Time 0.708 (0.468)	Data 9.82e-05 (9.48e-05)	Tok/s 14712 (13361)	Loss/tok 6.0383 (7.2228)	LR 2.000e-03
0: TRAIN [0][510/1154]	Time 0.692 (0.468)	Data 9.66e-05 (9.48e-05)	Tok/s 15019 (13356)	Loss/tok 6.1877 (7.1962)	LR 2.000e-03
0: TRAIN [0][520/1154]	Time 0.279 (0.467)	Data 1.02e-04 (9.48e-05)	Tok/s 12485 (13350)	Loss/tok 5.3764 (7.1706)	LR 2.000e-03
0: TRAIN [0][530/1154]	Time 0.565 (0.468)	Data 9.18e-05 (9.48e-05)	Tok/s 14419 (13349)	Loss/tok 5.6365 (7.1409)	LR 2.000e-03
0: TRAIN [0][540/1154]	Time 0.568 (0.468)	Data 9.87e-05 (9.48e-05)	Tok/s 14179 (13347)	Loss/tok 5.8171 (7.1137)	LR 2.000e-03
0: TRAIN [0][550/1154]	Time 0.455 (0.468)	Data 8.34e-05 (9.47e-05)	Tok/s 12980 (13345)	Loss/tok 5.5007 (7.0858)	LR 2.000e-03
0: TRAIN [0][560/1154]	Time 0.454 (0.468)	Data 8.73e-05 (9.47e-05)	Tok/s 12863 (13341)	Loss/tok 5.6166 (7.0619)	LR 2.000e-03
0: TRAIN [0][570/1154]	Time 0.150 (0.467)	Data 9.82e-05 (9.47e-05)	Tok/s 11787 (13339)	Loss/tok 4.6379 (7.0386)	LR 2.000e-03
0: TRAIN [0][580/1154]	Time 0.455 (0.466)	Data 8.77e-05 (9.47e-05)	Tok/s 12663 (13336)	Loss/tok 5.4273 (7.0144)	LR 2.000e-03
0: TRAIN [0][590/1154]	Time 0.277 (0.466)	Data 1.03e-04 (9.47e-05)	Tok/s 12486 (13337)	Loss/tok 5.0115 (6.9886)	LR 2.000e-03
0: TRAIN [0][600/1154]	Time 0.459 (0.465)	Data 1.05e-04 (9.46e-05)	Tok/s 12569 (13331)	Loss/tok 5.3923 (6.9666)	LR 2.000e-03
0: TRAIN [0][610/1154]	Time 0.576 (0.466)	Data 9.20e-05 (9.46e-05)	Tok/s 13873 (13329)	Loss/tok 5.5264 (6.9406)	LR 2.000e-03
0: TRAIN [0][620/1154]	Time 0.272 (0.464)	Data 8.65e-05 (9.45e-05)	Tok/s 12682 (13322)	Loss/tok 4.9313 (6.9207)	LR 2.000e-03
0: TRAIN [0][630/1154]	Time 0.281 (0.464)	Data 9.11e-05 (9.45e-05)	Tok/s 12489 (13321)	Loss/tok 5.0363 (6.8971)	LR 2.000e-03
0: TRAIN [0][640/1154]	Time 0.562 (0.464)	Data 8.94e-05 (9.45e-05)	Tok/s 14187 (13319)	Loss/tok 5.2874 (6.8726)	LR 2.000e-03
0: TRAIN [0][650/1154]	Time 0.460 (0.464)	Data 9.49e-05 (9.45e-05)	Tok/s 12547 (13311)	Loss/tok 5.1832 (6.8486)	LR 2.000e-03
0: TRAIN [0][660/1154]	Time 0.447 (0.463)	Data 8.37e-05 (9.45e-05)	Tok/s 12809 (13306)	Loss/tok 5.0583 (6.8263)	LR 2.000e-03
0: TRAIN [0][670/1154]	Time 0.693 (0.463)	Data 9.58e-05 (9.45e-05)	Tok/s 15164 (13304)	Loss/tok 5.5502 (6.8042)	LR 2.000e-03
0: TRAIN [0][680/1154]	Time 0.700 (0.463)	Data 8.99e-05 (9.45e-05)	Tok/s 14697 (13305)	Loss/tok 5.5113 (6.7805)	LR 2.000e-03
0: TRAIN [0][690/1154]	Time 0.589 (0.463)	Data 1.02e-04 (9.46e-05)	Tok/s 13881 (13304)	Loss/tok 5.3522 (6.7569)	LR 2.000e-03
0: TRAIN [0][700/1154]	Time 0.287 (0.462)	Data 9.82e-05 (9.46e-05)	Tok/s 12087 (13298)	Loss/tok 4.7242 (6.7368)	LR 2.000e-03
0: TRAIN [0][710/1154]	Time 0.578 (0.463)	Data 1.03e-04 (9.46e-05)	Tok/s 14088 (13303)	Loss/tok 5.1662 (6.7134)	LR 2.000e-03
0: TRAIN [0][720/1154]	Time 0.143 (0.462)	Data 8.46e-05 (9.46e-05)	Tok/s 12159 (13297)	Loss/tok 4.4994 (6.6947)	LR 2.000e-03
0: TRAIN [0][730/1154]	Time 0.458 (0.462)	Data 1.09e-04 (9.46e-05)	Tok/s 12454 (13294)	Loss/tok 4.9429 (6.6733)	LR 2.000e-03
0: TRAIN [0][740/1154]	Time 0.466 (0.462)	Data 9.63e-05 (9.46e-05)	Tok/s 12277 (13295)	Loss/tok 4.9863 (6.6530)	LR 2.000e-03
0: TRAIN [0][750/1154]	Time 0.571 (0.461)	Data 9.20e-05 (9.46e-05)	Tok/s 14081 (13287)	Loss/tok 5.2787 (6.6343)	LR 2.000e-03
0: TRAIN [0][760/1154]	Time 0.277 (0.461)	Data 9.18e-05 (9.46e-05)	Tok/s 12940 (13284)	Loss/tok 4.5973 (6.6135)	LR 2.000e-03
0: TRAIN [0][770/1154]	Time 0.696 (0.462)	Data 1.02e-04 (9.47e-05)	Tok/s 15107 (13288)	Loss/tok 5.4328 (6.5924)	LR 2.000e-03
0: TRAIN [0][780/1154]	Time 0.281 (0.462)	Data 9.58e-05 (9.47e-05)	Tok/s 12618 (13283)	Loss/tok 4.5758 (6.5724)	LR 2.000e-03
0: TRAIN [0][790/1154]	Time 0.466 (0.461)	Data 9.54e-05 (9.47e-05)	Tok/s 12505 (13280)	Loss/tok 4.8626 (6.5540)	LR 2.000e-03
0: TRAIN [0][800/1154]	Time 0.458 (0.461)	Data 9.42e-05 (9.47e-05)	Tok/s 12767 (13278)	Loss/tok 4.7424 (6.5347)	LR 2.000e-03
0: TRAIN [0][810/1154]	Time 0.445 (0.460)	Data 8.77e-05 (9.47e-05)	Tok/s 12936 (13273)	Loss/tok 4.8937 (6.5171)	LR 2.000e-03
0: TRAIN [0][820/1154]	Time 0.686 (0.461)	Data 1.04e-04 (9.48e-05)	Tok/s 15346 (13274)	Loss/tok 5.2504 (6.4971)	LR 2.000e-03
0: TRAIN [0][830/1154]	Time 0.453 (0.461)	Data 9.82e-05 (9.48e-05)	Tok/s 12804 (13273)	Loss/tok 4.7029 (6.4773)	LR 2.000e-03
0: TRAIN [0][840/1154]	Time 0.565 (0.462)	Data 1.03e-04 (9.48e-05)	Tok/s 14239 (13272)	Loss/tok 4.9034 (6.4577)	LR 2.000e-03
0: TRAIN [0][850/1154]	Time 0.283 (0.461)	Data 1.12e-04 (9.48e-05)	Tok/s 12078 (13271)	Loss/tok 4.4460 (6.4406)	LR 2.000e-03
0: TRAIN [0][860/1154]	Time 0.454 (0.461)	Data 9.56e-05 (9.48e-05)	Tok/s 13002 (13266)	Loss/tok 4.7600 (6.4230)	LR 2.000e-03
0: TRAIN [0][870/1154]	Time 0.581 (0.460)	Data 9.23e-05 (9.48e-05)	Tok/s 13897 (13261)	Loss/tok 5.0161 (6.4075)	LR 2.000e-03
0: TRAIN [0][880/1154]	Time 0.280 (0.460)	Data 1.03e-04 (9.48e-05)	Tok/s 12670 (13261)	Loss/tok 4.3423 (6.3904)	LR 2.000e-03
0: TRAIN [0][890/1154]	Time 0.279 (0.459)	Data 8.75e-05 (9.48e-05)	Tok/s 12397 (13258)	Loss/tok 4.3618 (6.3735)	LR 2.000e-03
0: TRAIN [0][900/1154]	Time 0.146 (0.459)	Data 9.06e-05 (9.49e-05)	Tok/s 11757 (13254)	Loss/tok 4.0223 (6.3568)	LR 2.000e-03
0: TRAIN [0][910/1154]	Time 0.591 (0.460)	Data 1.04e-04 (9.49e-05)	Tok/s 13683 (13255)	Loss/tok 4.8379 (6.3382)	LR 2.000e-03
0: TRAIN [0][920/1154]	Time 0.693 (0.459)	Data 9.82e-05 (9.49e-05)	Tok/s 15164 (13252)	Loss/tok 5.0573 (6.3225)	LR 2.000e-03
0: TRAIN [0][930/1154]	Time 0.284 (0.459)	Data 9.75e-05 (9.50e-05)	Tok/s 12279 (13252)	Loss/tok 4.4641 (6.3045)	LR 2.000e-03
0: TRAIN [0][940/1154]	Time 0.699 (0.459)	Data 9.04e-05 (9.50e-05)	Tok/s 14889 (13250)	Loss/tok 5.0596 (6.2889)	LR 2.000e-03
0: TRAIN [0][950/1154]	Time 0.570 (0.459)	Data 1.00e-04 (9.50e-05)	Tok/s 14020 (13251)	Loss/tok 4.8001 (6.2713)	LR 2.000e-03
0: TRAIN [0][960/1154]	Time 0.571 (0.460)	Data 9.51e-05 (9.51e-05)	Tok/s 14178 (13253)	Loss/tok 4.7657 (6.2525)	LR 2.000e-03
0: TRAIN [0][970/1154]	Time 0.709 (0.459)	Data 9.18e-05 (9.51e-05)	Tok/s 14589 (13248)	Loss/tok 4.9458 (6.2389)	LR 2.000e-03
0: TRAIN [0][980/1154]	Time 0.691 (0.461)	Data 9.51e-05 (9.51e-05)	Tok/s 15356 (13255)	Loss/tok 4.9409 (6.2195)	LR 2.000e-03
0: TRAIN [0][990/1154]	Time 0.689 (0.461)	Data 9.61e-05 (9.52e-05)	Tok/s 15128 (13259)	Loss/tok 4.9701 (6.2015)	LR 2.000e-03
0: TRAIN [0][1000/1154]	Time 0.284 (0.461)	Data 1.09e-04 (9.52e-05)	Tok/s 12440 (13256)	Loss/tok 4.2063 (6.1878)	LR 2.000e-03
0: TRAIN [0][1010/1154]	Time 0.274 (0.460)	Data 8.92e-05 (9.52e-05)	Tok/s 13073 (13255)	Loss/tok 4.0854 (6.1746)	LR 2.000e-03
0: TRAIN [0][1020/1154]	Time 0.679 (0.460)	Data 1.03e-04 (9.52e-05)	Tok/s 15168 (13256)	Loss/tok 4.9013 (6.1595)	LR 2.000e-03
0: TRAIN [0][1030/1154]	Time 0.584 (0.460)	Data 9.25e-05 (9.52e-05)	Tok/s 13687 (13255)	Loss/tok 4.5317 (6.1443)	LR 2.000e-03
0: TRAIN [0][1040/1154]	Time 0.271 (0.460)	Data 9.56e-05 (9.52e-05)	Tok/s 13126 (13254)	Loss/tok 4.3155 (6.1308)	LR 2.000e-03
0: TRAIN [0][1050/1154]	Time 0.572 (0.460)	Data 8.87e-05 (9.53e-05)	Tok/s 14060 (13254)	Loss/tok 4.5483 (6.1138)	LR 2.000e-03
0: TRAIN [0][1060/1154]	Time 0.574 (0.460)	Data 8.70e-05 (9.53e-05)	Tok/s 14229 (13251)	Loss/tok 4.6403 (6.1002)	LR 2.000e-03
0: TRAIN [0][1070/1154]	Time 0.711 (0.460)	Data 1.03e-04 (9.54e-05)	Tok/s 14751 (13252)	Loss/tok 4.8368 (6.0844)	LR 2.000e-03
0: TRAIN [0][1080/1154]	Time 0.450 (0.460)	Data 9.11e-05 (9.54e-05)	Tok/s 13002 (13251)	Loss/tok 4.5350 (6.0719)	LR 2.000e-03
0: TRAIN [0][1090/1154]	Time 0.468 (0.460)	Data 9.44e-05 (9.54e-05)	Tok/s 12405 (13250)	Loss/tok 4.4228 (6.0570)	LR 2.000e-03
0: TRAIN [0][1100/1154]	Time 0.147 (0.459)	Data 9.66e-05 (9.54e-05)	Tok/s 11513 (13244)	Loss/tok 3.8409 (6.0457)	LR 2.000e-03
0: TRAIN [0][1110/1154]	Time 0.455 (0.459)	Data 9.01e-05 (9.54e-05)	Tok/s 12629 (13242)	Loss/tok 4.4623 (6.0314)	LR 2.000e-03
0: TRAIN [0][1120/1154]	Time 0.456 (0.459)	Data 1.02e-04 (9.54e-05)	Tok/s 12568 (13239)	Loss/tok 4.3566 (6.0174)	LR 2.000e-03
0: TRAIN [0][1130/1154]	Time 0.711 (0.459)	Data 1.09e-04 (9.54e-05)	Tok/s 14731 (13237)	Loss/tok 4.8154 (6.0048)	LR 2.000e-03
0: TRAIN [0][1140/1154]	Time 0.577 (0.459)	Data 9.06e-05 (9.54e-05)	Tok/s 14104 (13236)	Loss/tok 4.6645 (5.9918)	LR 2.000e-03
0: TRAIN [0][1150/1154]	Time 0.292 (0.459)	Data 1.22e-03 (9.64e-05)	Tok/s 11804 (13236)	Loss/tok 4.0742 (5.9784)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/160]	Time 0.191 (0.000)	Data 1.01e-03 (0.00e+00)	Tok/s 29899 (0)	Loss/tok 6.0919 (6.0919)
0: VALIDATION [0][10/160]	Time 0.094 (0.107)	Data 7.87e-04 (8.15e-04)	Tok/s 36681 (36708)	Loss/tok 5.6451 (5.8392)
0: VALIDATION [0][20/160]	Time 0.075 (0.095)	Data 8.69e-04 (7.99e-04)	Tok/s 38910 (37192)	Loss/tok 5.6006 (5.7895)
0: VALIDATION [0][30/160]	Time 0.063 (0.086)	Data 7.57e-04 (7.89e-04)	Tok/s 41281 (38395)	Loss/tok 5.9470 (5.7441)
0: VALIDATION [0][40/160]	Time 0.057 (0.079)	Data 7.56e-04 (7.81e-04)	Tok/s 40837 (39215)	Loss/tok 5.3081 (5.7083)
0: VALIDATION [0][50/160]	Time 0.051 (0.074)	Data 7.43e-04 (7.74e-04)	Tok/s 41949 (39774)	Loss/tok 5.4646 (5.6618)
0: VALIDATION [0][60/160]	Time 0.047 (0.070)	Data 7.36e-04 (7.70e-04)	Tok/s 41791 (40103)	Loss/tok 5.1912 (5.6268)
0: VALIDATION [0][70/160]	Time 0.043 (0.066)	Data 7.31e-04 (7.66e-04)	Tok/s 41602 (40203)	Loss/tok 5.0586 (5.6029)
0: VALIDATION [0][80/160]	Time 0.044 (0.064)	Data 7.23e-04 (7.62e-04)	Tok/s 37077 (39786)	Loss/tok 5.4800 (5.5786)
0: VALIDATION [0][90/160]	Time 0.042 (0.061)	Data 7.24e-04 (7.59e-04)	Tok/s 35214 (39382)	Loss/tok 5.3445 (5.5593)
0: VALIDATION [0][100/160]	Time 0.034 (0.059)	Data 7.55e-04 (7.57e-04)	Tok/s 39668 (39146)	Loss/tok 5.3542 (5.5425)
0: VALIDATION [0][110/160]	Time 0.029 (0.057)	Data 7.23e-04 (7.54e-04)	Tok/s 41775 (39180)	Loss/tok 5.2246 (5.5215)
0: VALIDATION [0][120/160]	Time 0.028 (0.054)	Data 7.37e-04 (7.52e-04)	Tok/s 38925 (39267)	Loss/tok 5.0955 (5.5071)
0: VALIDATION [0][130/160]	Time 0.024 (0.052)	Data 7.44e-04 (7.50e-04)	Tok/s 39901 (39193)	Loss/tok 4.9563 (5.4881)
0: VALIDATION [0][140/160]	Time 0.023 (0.050)	Data 7.24e-04 (7.49e-04)	Tok/s 35832 (39084)	Loss/tok 4.9135 (5.4745)
0: VALIDATION [0][150/160]	Time 0.017 (0.048)	Data 7.21e-04 (7.47e-04)	Tok/s 36496 (38874)	Loss/tok 4.7677 (5.4574)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/94]	Time 0.9534 (1.0940)	Decoder iters 149.0 (149.0)	Tok/s 4266 (4149)
0: TEST [0][19/94]	Time 0.7084 (0.9397)	Decoder iters 149.0 (149.0)	Tok/s 3917 (4123)
0: TEST [0][29/94]	Time 0.6434 (0.8502)	Decoder iters 149.0 (149.0)	Tok/s 3987 (4099)
0: TEST [0][39/94]	Time 0.5801 (0.7751)	Decoder iters 149.0 (144.3)	Tok/s 3625 (4099)
0: TEST [0][49/94]	Time 0.3000 (0.7141)	Decoder iters 53.0 (139.7)	Tok/s 5634 (4122)
0: TEST [0][59/94]	Time 0.2344 (0.6679)	Decoder iters 40.0 (136.6)	Tok/s 6229 (4102)
0: TEST [0][69/94]	Time 0.4515 (0.6171)	Decoder iters 149.0 (128.3)	Tok/s 3141 (4215)
0: TEST [0][79/94]	Time 0.2346 (0.5728)	Decoder iters 60.0 (121.0)	Tok/s 4458 (4280)
0: TEST [0][89/94]	Time 0.1046 (0.5285)	Decoder iters 19.0 (112.3)	Tok/s 6615 (4396)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 5.9746	Validation Loss: 5.4439	Test BLEU: 4.46
0: Performance: Epoch: 0	Training: 13235 Tok/s	Validation: 38321 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3588440356
0: TRAIN [1][0/1154]	Time 0.629 (0.000)	Data 1.07e-01 (0.00e+00)	Tok/s 12912 (0)	Loss/tok 4.4257 (4.4257)	LR 2.000e-03
0: TRAIN [1][10/1154]	Time 0.452 (0.496)	Data 8.96e-05 (9.43e-05)	Tok/s 12638 (13346)	Loss/tok 3.9705 (4.2645)	LR 2.000e-03
0: TRAIN [1][20/1154]	Time 0.578 (0.465)	Data 9.51e-05 (9.43e-05)	Tok/s 14077 (13212)	Loss/tok 4.4233 (4.2074)	LR 2.000e-03
0: TRAIN [1][30/1154]	Time 0.584 (0.457)	Data 1.01e-04 (9.54e-05)	Tok/s 13789 (13147)	Loss/tok 4.3502 (4.2052)	LR 2.000e-03
0: TRAIN [1][40/1154]	Time 0.592 (0.469)	Data 1.03e-04 (9.59e-05)	Tok/s 13622 (13224)	Loss/tok 4.2245 (4.2178)	LR 2.000e-03
0: TRAIN [1][50/1154]	Time 0.452 (0.457)	Data 8.92e-05 (9.57e-05)	Tok/s 12862 (13169)	Loss/tok 4.0783 (4.2137)	LR 2.000e-03
0: TRAIN [1][60/1154]	Time 0.685 (0.454)	Data 9.08e-05 (9.51e-05)	Tok/s 15180 (13196)	Loss/tok 4.4839 (4.2043)	LR 2.000e-03
0: TRAIN [1][70/1154]	Time 0.690 (0.460)	Data 1.01e-04 (9.53e-05)	Tok/s 15080 (13196)	Loss/tok 4.5268 (4.2137)	LR 2.000e-03
0: TRAIN [1][80/1154]	Time 0.465 (0.459)	Data 9.75e-05 (9.52e-05)	Tok/s 12407 (13170)	Loss/tok 3.9438 (4.1986)	LR 2.000e-03
0: TRAIN [1][90/1154]	Time 0.570 (0.459)	Data 1.03e-04 (9.53e-05)	Tok/s 14023 (13151)	Loss/tok 4.3295 (4.1938)	LR 2.000e-03
0: TRAIN [1][100/1154]	Time 0.465 (0.453)	Data 9.13e-05 (9.52e-05)	Tok/s 12550 (13103)	Loss/tok 4.0724 (4.1760)	LR 2.000e-03
0: TRAIN [1][110/1154]	Time 0.283 (0.450)	Data 1.01e-04 (9.53e-05)	Tok/s 12387 (13078)	Loss/tok 3.7766 (4.1703)	LR 2.000e-03
0: TRAIN [1][120/1154]	Time 0.713 (0.456)	Data 9.61e-05 (9.53e-05)	Tok/s 14711 (13119)	Loss/tok 4.3695 (4.1856)	LR 2.000e-03
0: TRAIN [1][130/1154]	Time 0.280 (0.453)	Data 1.06e-04 (9.54e-05)	Tok/s 12366 (13086)	Loss/tok 3.7057 (4.1756)	LR 2.000e-03
0: TRAIN [1][140/1154]	Time 0.703 (0.457)	Data 8.51e-05 (9.54e-05)	Tok/s 14846 (13106)	Loss/tok 4.4460 (4.1769)	LR 2.000e-03
0: TRAIN [1][150/1154]	Time 0.445 (0.455)	Data 9.13e-05 (9.52e-05)	Tok/s 12914 (13102)	Loss/tok 4.2104 (4.1847)	LR 2.000e-03
0: TRAIN [1][160/1154]	Time 0.571 (0.451)	Data 9.39e-05 (9.51e-05)	Tok/s 14263 (13091)	Loss/tok 4.4002 (4.1811)	LR 2.000e-03
0: TRAIN [1][170/1154]	Time 0.568 (0.453)	Data 1.10e-04 (9.52e-05)	Tok/s 14187 (13107)	Loss/tok 4.2070 (4.1844)	LR 2.000e-03
0: TRAIN [1][180/1154]	Time 0.580 (0.454)	Data 8.94e-05 (9.53e-05)	Tok/s 13767 (13121)	Loss/tok 4.1898 (4.1814)	LR 2.000e-03
0: TRAIN [1][190/1154]	Time 0.588 (0.453)	Data 9.66e-05 (9.52e-05)	Tok/s 13912 (13107)	Loss/tok 4.2061 (4.1759)	LR 2.000e-03
0: TRAIN [1][200/1154]	Time 0.280 (0.453)	Data 9.23e-05 (9.51e-05)	Tok/s 12490 (13104)	Loss/tok 3.6693 (4.1726)	LR 2.000e-03
0: TRAIN [1][210/1154]	Time 0.282 (0.455)	Data 8.73e-05 (9.52e-05)	Tok/s 12089 (13121)	Loss/tok 3.6752 (4.1749)	LR 2.000e-03
0: TRAIN [1][220/1154]	Time 0.598 (0.453)	Data 9.87e-05 (9.53e-05)	Tok/s 13513 (13118)	Loss/tok 4.1650 (4.1718)	LR 2.000e-03
0: TRAIN [1][230/1154]	Time 0.699 (0.451)	Data 9.92e-05 (9.52e-05)	Tok/s 15151 (13112)	Loss/tok 4.4691 (4.1660)	LR 2.000e-03
0: TRAIN [1][240/1154]	Time 0.564 (0.453)	Data 8.34e-05 (9.52e-05)	Tok/s 14346 (13108)	Loss/tok 4.3216 (4.1629)	LR 2.000e-03
0: TRAIN [1][250/1154]	Time 0.275 (0.450)	Data 9.04e-05 (9.50e-05)	Tok/s 12178 (13095)	Loss/tok 3.7145 (4.1583)	LR 2.000e-03
0: TRAIN [1][260/1154]	Time 0.470 (0.452)	Data 1.02e-04 (9.50e-05)	Tok/s 12289 (13100)	Loss/tok 3.8241 (4.1583)	LR 2.000e-03
0: TRAIN [1][270/1154]	Time 0.681 (0.454)	Data 9.42e-05 (9.51e-05)	Tok/s 15339 (13110)	Loss/tok 4.4675 (4.1595)	LR 2.000e-03
0: TRAIN [1][280/1154]	Time 0.579 (0.456)	Data 9.92e-05 (9.52e-05)	Tok/s 13813 (13114)	Loss/tok 4.0124 (4.1562)	LR 2.000e-03
0: TRAIN [1][290/1154]	Time 0.284 (0.454)	Data 9.56e-05 (9.51e-05)	Tok/s 12186 (13107)	Loss/tok 3.7311 (4.1513)	LR 2.000e-03
0: TRAIN [1][300/1154]	Time 0.709 (0.457)	Data 1.00e-04 (9.53e-05)	Tok/s 14675 (13125)	Loss/tok 4.5050 (4.1548)	LR 2.000e-03
0: TRAIN [1][310/1154]	Time 0.149 (0.458)	Data 9.25e-05 (9.53e-05)	Tok/s 11447 (13126)	Loss/tok 3.3379 (4.1543)	LR 2.000e-03
0: TRAIN [1][320/1154]	Time 0.690 (0.459)	Data 9.37e-05 (9.52e-05)	Tok/s 15220 (13129)	Loss/tok 4.3932 (4.1535)	LR 2.000e-03
0: TRAIN [1][330/1154]	Time 0.727 (0.461)	Data 9.78e-05 (9.52e-05)	Tok/s 14569 (13139)	Loss/tok 4.4465 (4.1552)	LR 2.000e-03
0: TRAIN [1][340/1154]	Time 0.473 (0.461)	Data 9.01e-05 (9.52e-05)	Tok/s 12397 (13140)	Loss/tok 3.9859 (4.1535)	LR 2.000e-03
0: TRAIN [1][350/1154]	Time 0.571 (0.461)	Data 1.05e-04 (9.52e-05)	Tok/s 13984 (13133)	Loss/tok 4.0893 (4.1485)	LR 2.000e-03
0: TRAIN [1][360/1154]	Time 0.276 (0.459)	Data 9.42e-05 (9.51e-05)	Tok/s 13021 (13126)	Loss/tok 3.6523 (4.1432)	LR 2.000e-03
0: TRAIN [1][370/1154]	Time 0.586 (0.460)	Data 9.54e-05 (9.51e-05)	Tok/s 13738 (13129)	Loss/tok 4.1932 (4.1412)	LR 2.000e-03
0: TRAIN [1][380/1154]	Time 0.445 (0.458)	Data 8.68e-05 (9.50e-05)	Tok/s 12949 (13119)	Loss/tok 3.8548 (4.1375)	LR 2.000e-03
0: TRAIN [1][390/1154]	Time 0.285 (0.458)	Data 9.89e-05 (9.50e-05)	Tok/s 11875 (13118)	Loss/tok 3.7403 (4.1361)	LR 1.000e-03
0: TRAIN [1][400/1154]	Time 0.721 (0.459)	Data 1.05e-04 (9.50e-05)	Tok/s 14753 (13123)	Loss/tok 4.2250 (4.1350)	LR 1.000e-03
0: TRAIN [1][410/1154]	Time 0.274 (0.458)	Data 8.75e-05 (9.50e-05)	Tok/s 12568 (13121)	Loss/tok 3.6235 (4.1322)	LR 1.000e-03
0: TRAIN [1][420/1154]	Time 0.476 (0.458)	Data 9.37e-05 (9.50e-05)	Tok/s 12260 (13118)	Loss/tok 3.8842 (4.1277)	LR 1.000e-03
0: TRAIN [1][430/1154]	Time 0.272 (0.456)	Data 8.70e-05 (9.49e-05)	Tok/s 12699 (13105)	Loss/tok 3.5506 (4.1209)	LR 1.000e-03
0: TRAIN [1][440/1154]	Time 0.281 (0.455)	Data 9.25e-05 (9.49e-05)	Tok/s 12224 (13101)	Loss/tok 3.4347 (4.1165)	LR 1.000e-03
0: TRAIN [1][450/1154]	Time 0.280 (0.455)	Data 9.06e-05 (9.49e-05)	Tok/s 12591 (13101)	Loss/tok 3.6250 (4.1133)	LR 1.000e-03
0: TRAIN [1][460/1154]	Time 0.588 (0.456)	Data 1.11e-04 (9.49e-05)	Tok/s 13623 (13109)	Loss/tok 4.0870 (4.1101)	LR 1.000e-03
0: TRAIN [1][470/1154]	Time 0.574 (0.457)	Data 9.32e-05 (9.49e-05)	Tok/s 14107 (13111)	Loss/tok 4.1050 (4.1069)	LR 1.000e-03
0: TRAIN [1][480/1154]	Time 0.474 (0.458)	Data 1.07e-04 (9.51e-05)	Tok/s 12210 (13111)	Loss/tok 3.8122 (4.1038)	LR 1.000e-03
0: TRAIN [1][490/1154]	Time 0.597 (0.458)	Data 9.54e-05 (9.50e-05)	Tok/s 13423 (13115)	Loss/tok 4.0454 (4.1006)	LR 1.000e-03
0: TRAIN [1][500/1154]	Time 0.571 (0.458)	Data 8.96e-05 (9.50e-05)	Tok/s 14228 (13117)	Loss/tok 3.9635 (4.0978)	LR 1.000e-03
0: TRAIN [1][510/1154]	Time 0.462 (0.458)	Data 1.05e-04 (9.50e-05)	Tok/s 12495 (13114)	Loss/tok 3.8400 (4.0939)	LR 1.000e-03
0: TRAIN [1][520/1154]	Time 0.452 (0.457)	Data 9.94e-05 (9.50e-05)	Tok/s 12788 (13109)	Loss/tok 3.7705 (4.0903)	LR 1.000e-03
0: TRAIN [1][530/1154]	Time 0.715 (0.458)	Data 9.80e-05 (9.50e-05)	Tok/s 14612 (13114)	Loss/tok 4.1488 (4.0881)	LR 1.000e-03
0: TRAIN [1][540/1154]	Time 0.586 (0.460)	Data 9.35e-05 (9.51e-05)	Tok/s 13617 (13118)	Loss/tok 3.9670 (4.0850)	LR 1.000e-03
0: TRAIN [1][550/1154]	Time 0.578 (0.460)	Data 9.37e-05 (9.51e-05)	Tok/s 14045 (13112)	Loss/tok 3.9710 (4.0800)	LR 1.000e-03
0: TRAIN [1][560/1154]	Time 0.282 (0.460)	Data 1.08e-04 (9.51e-05)	Tok/s 12629 (13114)	Loss/tok 3.4321 (4.0765)	LR 1.000e-03
0: TRAIN [1][570/1154]	Time 0.705 (0.459)	Data 8.92e-05 (9.50e-05)	Tok/s 14722 (13106)	Loss/tok 4.1860 (4.0723)	LR 1.000e-03
0: TRAIN [1][580/1154]	Time 0.282 (0.459)	Data 1.06e-04 (9.50e-05)	Tok/s 12096 (13106)	Loss/tok 3.4317 (4.0694)	LR 5.000e-04
0: TRAIN [1][590/1154]	Time 0.571 (0.459)	Data 8.75e-05 (9.50e-05)	Tok/s 14257 (13106)	Loss/tok 4.0045 (4.0662)	LR 5.000e-04
0: TRAIN [1][600/1154]	Time 0.691 (0.460)	Data 9.27e-05 (9.50e-05)	Tok/s 15063 (13112)	Loss/tok 4.1023 (4.0638)	LR 5.000e-04
0: TRAIN [1][610/1154]	Time 0.589 (0.461)	Data 9.35e-05 (9.52e-05)	Tok/s 13743 (13119)	Loss/tok 4.0491 (4.0610)	LR 5.000e-04
0: TRAIN [1][620/1154]	Time 0.277 (0.461)	Data 1.08e-04 (9.52e-05)	Tok/s 12297 (13122)	Loss/tok 3.4473 (4.0577)	LR 5.000e-04
0: TRAIN [1][630/1154]	Time 0.575 (0.461)	Data 1.05e-04 (9.52e-05)	Tok/s 13929 (13123)	Loss/tok 3.8407 (4.0541)	LR 5.000e-04
0: TRAIN [1][640/1154]	Time 0.454 (0.459)	Data 8.70e-05 (9.51e-05)	Tok/s 12682 (13115)	Loss/tok 3.6538 (4.0494)	LR 5.000e-04
0: TRAIN [1][650/1154]	Time 0.279 (0.459)	Data 1.12e-04 (9.51e-05)	Tok/s 12019 (13112)	Loss/tok 3.2984 (4.0453)	LR 5.000e-04
0: TRAIN [1][660/1154]	Time 0.456 (0.458)	Data 9.08e-05 (9.51e-05)	Tok/s 12497 (13110)	Loss/tok 3.5598 (4.0414)	LR 5.000e-04
0: TRAIN [1][670/1154]	Time 0.576 (0.459)	Data 9.70e-05 (9.52e-05)	Tok/s 14010 (13115)	Loss/tok 3.8167 (4.0377)	LR 5.000e-04
0: TRAIN [1][680/1154]	Time 0.278 (0.457)	Data 9.08e-05 (9.51e-05)	Tok/s 12602 (13106)	Loss/tok 3.5543 (4.0336)	LR 5.000e-04
0: TRAIN [1][690/1154]	Time 0.461 (0.459)	Data 1.10e-04 (9.52e-05)	Tok/s 12574 (13113)	Loss/tok 3.7176 (4.0319)	LR 5.000e-04
0: TRAIN [1][700/1154]	Time 0.148 (0.460)	Data 1.10e-04 (9.52e-05)	Tok/s 11933 (13123)	Loss/tok 3.2505 (4.0311)	LR 5.000e-04
0: TRAIN [1][710/1154]	Time 0.456 (0.460)	Data 9.39e-05 (9.52e-05)	Tok/s 12766 (13125)	Loss/tok 3.6361 (4.0283)	LR 5.000e-04
0: TRAIN [1][720/1154]	Time 0.149 (0.460)	Data 9.18e-05 (9.52e-05)	Tok/s 11369 (13122)	Loss/tok 3.2851 (4.0248)	LR 5.000e-04
0: TRAIN [1][730/1154]	Time 0.280 (0.459)	Data 9.89e-05 (9.52e-05)	Tok/s 12579 (13119)	Loss/tok 3.4065 (4.0210)	LR 5.000e-04
0: TRAIN [1][740/1154]	Time 0.467 (0.459)	Data 9.75e-05 (9.52e-05)	Tok/s 12264 (13119)	Loss/tok 3.7495 (4.0188)	LR 5.000e-04
0: TRAIN [1][750/1154]	Time 0.687 (0.460)	Data 8.94e-05 (9.53e-05)	Tok/s 15130 (13121)	Loss/tok 4.1459 (4.0160)	LR 5.000e-04
0: TRAIN [1][760/1154]	Time 0.706 (0.460)	Data 9.39e-05 (9.53e-05)	Tok/s 14747 (13122)	Loss/tok 4.0476 (4.0133)	LR 5.000e-04
0: TRAIN [1][770/1154]	Time 0.455 (0.460)	Data 1.11e-04 (9.53e-05)	Tok/s 12652 (13120)	Loss/tok 3.7070 (4.0104)	LR 2.500e-04
0: TRAIN [1][780/1154]	Time 0.282 (0.459)	Data 9.37e-05 (9.52e-05)	Tok/s 12488 (13116)	Loss/tok 3.4962 (4.0073)	LR 2.500e-04
0: TRAIN [1][790/1154]	Time 0.447 (0.459)	Data 9.82e-05 (9.52e-05)	Tok/s 12838 (13115)	Loss/tok 3.6558 (4.0042)	LR 2.500e-04
0: TRAIN [1][800/1154]	Time 0.285 (0.459)	Data 8.85e-05 (9.53e-05)	Tok/s 12240 (13119)	Loss/tok 3.3321 (4.0027)	LR 2.500e-04
0: TRAIN [1][810/1154]	Time 0.465 (0.460)	Data 9.30e-05 (9.53e-05)	Tok/s 12451 (13120)	Loss/tok 3.5770 (4.0005)	LR 2.500e-04
0: TRAIN [1][820/1154]	Time 0.452 (0.459)	Data 8.68e-05 (9.53e-05)	Tok/s 12879 (13119)	Loss/tok 3.6083 (3.9970)	LR 2.500e-04
0: TRAIN [1][830/1154]	Time 0.577 (0.459)	Data 9.51e-05 (9.53e-05)	Tok/s 13815 (13117)	Loss/tok 3.8011 (3.9936)	LR 2.500e-04
0: TRAIN [1][840/1154]	Time 0.598 (0.459)	Data 9.56e-05 (9.53e-05)	Tok/s 13497 (13116)	Loss/tok 4.0752 (3.9916)	LR 2.500e-04
0: TRAIN [1][850/1154]	Time 0.688 (0.458)	Data 9.68e-05 (9.52e-05)	Tok/s 15344 (13114)	Loss/tok 4.0731 (3.9892)	LR 2.500e-04
0: TRAIN [1][860/1154]	Time 0.572 (0.458)	Data 9.01e-05 (9.52e-05)	Tok/s 14233 (13115)	Loss/tok 3.8091 (3.9861)	LR 2.500e-04
0: TRAIN [1][870/1154]	Time 0.566 (0.458)	Data 1.07e-04 (9.52e-05)	Tok/s 14156 (13112)	Loss/tok 3.9544 (3.9837)	LR 2.500e-04
0: TRAIN [1][880/1154]	Time 0.451 (0.458)	Data 1.06e-04 (9.52e-05)	Tok/s 12738 (13108)	Loss/tok 3.6095 (3.9807)	LR 2.500e-04
0: TRAIN [1][890/1154]	Time 0.469 (0.458)	Data 9.89e-05 (9.52e-05)	Tok/s 12406 (13106)	Loss/tok 3.7269 (3.9778)	LR 2.500e-04
0: TRAIN [1][900/1154]	Time 0.691 (0.458)	Data 8.27e-05 (9.52e-05)	Tok/s 15021 (13110)	Loss/tok 3.9807 (3.9761)	LR 2.500e-04
0: TRAIN [1][910/1154]	Time 0.276 (0.458)	Data 1.07e-04 (9.52e-05)	Tok/s 12621 (13109)	Loss/tok 3.4807 (3.9730)	LR 2.500e-04
0: TRAIN [1][920/1154]	Time 0.466 (0.458)	Data 1.08e-04 (9.52e-05)	Tok/s 12472 (13111)	Loss/tok 3.5973 (3.9702)	LR 2.500e-04
0: TRAIN [1][930/1154]	Time 0.580 (0.458)	Data 9.49e-05 (9.52e-05)	Tok/s 14131 (13109)	Loss/tok 3.8637 (3.9673)	LR 2.500e-04
0: TRAIN [1][940/1154]	Time 0.462 (0.458)	Data 1.05e-04 (9.52e-05)	Tok/s 12558 (13114)	Loss/tok 3.7118 (3.9666)	LR 2.500e-04
0: TRAIN [1][950/1154]	Time 0.674 (0.459)	Data 8.94e-05 (9.52e-05)	Tok/s 15605 (13118)	Loss/tok 4.0879 (3.9647)	LR 2.500e-04
0: TRAIN [1][960/1154]	Time 0.460 (0.460)	Data 1.03e-04 (9.52e-05)	Tok/s 12710 (13117)	Loss/tok 3.5063 (3.9623)	LR 1.250e-04
0: TRAIN [1][970/1154]	Time 0.695 (0.460)	Data 9.89e-05 (9.52e-05)	Tok/s 14943 (13122)	Loss/tok 3.8845 (3.9607)	LR 1.250e-04
0: TRAIN [1][980/1154]	Time 0.700 (0.460)	Data 8.87e-05 (9.52e-05)	Tok/s 14855 (13118)	Loss/tok 4.0524 (3.9585)	LR 1.250e-04
0: TRAIN [1][990/1154]	Time 0.286 (0.461)	Data 9.35e-05 (9.52e-05)	Tok/s 12334 (13122)	Loss/tok 3.4451 (3.9573)	LR 1.250e-04
0: TRAIN [1][1000/1154]	Time 0.473 (0.461)	Data 9.37e-05 (9.52e-05)	Tok/s 12234 (13120)	Loss/tok 3.5356 (3.9548)	LR 1.250e-04
0: TRAIN [1][1010/1154]	Time 0.275 (0.460)	Data 8.32e-05 (9.53e-05)	Tok/s 12841 (13120)	Loss/tok 3.4416 (3.9530)	LR 1.250e-04
0: TRAIN [1][1020/1154]	Time 0.457 (0.460)	Data 9.16e-05 (9.52e-05)	Tok/s 12634 (13117)	Loss/tok 3.6315 (3.9505)	LR 1.250e-04
0: TRAIN [1][1030/1154]	Time 0.448 (0.460)	Data 8.77e-05 (9.52e-05)	Tok/s 12847 (13119)	Loss/tok 3.4900 (3.9491)	LR 1.250e-04
0: TRAIN [1][1040/1154]	Time 0.701 (0.461)	Data 8.82e-05 (9.52e-05)	Tok/s 14763 (13123)	Loss/tok 4.1128 (3.9482)	LR 1.250e-04
0: TRAIN [1][1050/1154]	Time 0.141 (0.461)	Data 8.82e-05 (9.53e-05)	Tok/s 12149 (13124)	Loss/tok 3.2194 (3.9471)	LR 1.250e-04
0: TRAIN [1][1060/1154]	Time 0.147 (0.461)	Data 1.07e-04 (9.53e-05)	Tok/s 11323 (13126)	Loss/tok 3.2168 (3.9459)	LR 1.250e-04
0: TRAIN [1][1070/1154]	Time 0.706 (0.462)	Data 9.89e-05 (9.53e-05)	Tok/s 14851 (13130)	Loss/tok 3.9771 (3.9445)	LR 1.250e-04
0: TRAIN [1][1080/1154]	Time 0.455 (0.462)	Data 1.14e-04 (9.53e-05)	Tok/s 12516 (13130)	Loss/tok 3.5294 (3.9426)	LR 1.250e-04
0: TRAIN [1][1090/1154]	Time 0.455 (0.462)	Data 9.16e-05 (9.53e-05)	Tok/s 12644 (13131)	Loss/tok 3.6308 (3.9412)	LR 1.250e-04
0: TRAIN [1][1100/1154]	Time 0.142 (0.461)	Data 8.87e-05 (9.53e-05)	Tok/s 12530 (13126)	Loss/tok 3.3636 (3.9387)	LR 1.250e-04
0: TRAIN [1][1110/1154]	Time 0.466 (0.461)	Data 1.00e-04 (9.53e-05)	Tok/s 12318 (13124)	Loss/tok 3.5417 (3.9365)	LR 1.250e-04
0: TRAIN [1][1120/1154]	Time 0.575 (0.462)	Data 9.20e-05 (9.53e-05)	Tok/s 14084 (13127)	Loss/tok 3.8381 (3.9351)	LR 1.250e-04
0: TRAIN [1][1130/1154]	Time 0.704 (0.463)	Data 8.92e-05 (9.53e-05)	Tok/s 14880 (13130)	Loss/tok 4.0599 (3.9337)	LR 1.250e-04
0: TRAIN [1][1140/1154]	Time 0.289 (0.463)	Data 9.32e-05 (9.53e-05)	Tok/s 12253 (13130)	Loss/tok 3.4581 (3.9320)	LR 1.250e-04
0: TRAIN [1][1150/1154]	Time 0.573 (0.463)	Data 1.10e-03 (9.62e-05)	Tok/s 13947 (13131)	Loss/tok 3.7672 (3.9304)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/160]	Time 0.189 (0.000)	Data 9.70e-04 (0.00e+00)	Tok/s 30339 (0)	Loss/tok 5.4603 (5.4603)
0: VALIDATION [1][10/160]	Time 0.094 (0.107)	Data 8.26e-04 (8.28e-04)	Tok/s 36823 (36571)	Loss/tok 4.9943 (5.1665)
0: VALIDATION [1][20/160]	Time 0.077 (0.095)	Data 7.76e-04 (8.10e-04)	Tok/s 38177 (37138)	Loss/tok 4.9134 (5.1019)
0: VALIDATION [1][30/160]	Time 0.063 (0.086)	Data 7.56e-04 (7.96e-04)	Tok/s 41127 (38344)	Loss/tok 5.2469 (5.0526)
0: VALIDATION [1][40/160]	Time 0.056 (0.079)	Data 7.57e-04 (7.88e-04)	Tok/s 41843 (39197)	Loss/tok 4.6813 (5.0182)
0: VALIDATION [1][50/160]	Time 0.051 (0.074)	Data 7.50e-04 (7.80e-04)	Tok/s 41677 (39720)	Loss/tok 4.8072 (4.9750)
0: VALIDATION [1][60/160]	Time 0.046 (0.070)	Data 7.41e-04 (7.75e-04)	Tok/s 42167 (40045)	Loss/tok 4.4881 (4.9438)
0: VALIDATION [1][70/160]	Time 0.043 (0.066)	Data 7.37e-04 (7.72e-04)	Tok/s 42107 (40151)	Loss/tok 4.3524 (4.9194)
0: VALIDATION [1][80/160]	Time 0.045 (0.064)	Data 7.34e-04 (7.68e-04)	Tok/s 36336 (39737)	Loss/tok 4.7879 (4.8968)
0: VALIDATION [1][90/160]	Time 0.042 (0.061)	Data 7.38e-04 (7.64e-04)	Tok/s 35325 (39297)	Loss/tok 4.5832 (4.8791)
0: VALIDATION [1][100/160]	Time 0.034 (0.059)	Data 7.32e-04 (7.61e-04)	Tok/s 39089 (39093)	Loss/tok 4.6623 (4.8647)
0: VALIDATION [1][110/160]	Time 0.029 (0.057)	Data 7.21e-04 (7.59e-04)	Tok/s 41517 (39137)	Loss/tok 4.6143 (4.8462)
0: VALIDATION [1][120/160]	Time 0.028 (0.054)	Data 7.40e-04 (7.58e-04)	Tok/s 38458 (39197)	Loss/tok 4.4968 (4.8338)
0: VALIDATION [1][130/160]	Time 0.024 (0.052)	Data 7.30e-04 (7.56e-04)	Tok/s 39903 (39138)	Loss/tok 4.3942 (4.8192)
0: VALIDATION [1][140/160]	Time 0.022 (0.050)	Data 7.28e-04 (7.54e-04)	Tok/s 36035 (39046)	Loss/tok 4.2690 (4.8075)
0: VALIDATION [1][150/160]	Time 0.018 (0.048)	Data 7.53e-04 (7.52e-04)	Tok/s 36178 (38832)	Loss/tok 4.4085 (4.7935)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/94]	Time 0.6903 (0.7827)	Decoder iters 149.0 (139.3)	Tok/s 4476 (4890)
0: TEST [1][19/94]	Time 0.3864 (0.6451)	Decoder iters 54.0 (118.0)	Tok/s 6608 (5249)
0: TEST [1][29/94]	Time 0.5481 (0.5827)	Decoder iters 149.0 (111.0)	Tok/s 4224 (5316)
0: TEST [1][39/94]	Time 0.2960 (0.5273)	Decoder iters 52.0 (100.9)	Tok/s 6480 (5475)
0: TEST [1][49/94]	Time 0.2592 (0.4903)	Decoder iters 45.0 (96.1)	Tok/s 6578 (5533)
0: TEST [1][59/94]	Time 0.2155 (0.4555)	Decoder iters 37.0 (90.3)	Tok/s 6818 (5613)
0: TEST [1][69/94]	Time 0.4505 (0.4270)	Decoder iters 149.0 (85.8)	Tok/s 2997 (5646)
0: TEST [1][79/94]	Time 0.1579 (0.3949)	Decoder iters 30.0 (79.1)	Tok/s 6325 (5749)
0: TEST [1][89/94]	Time 0.1305 (0.3659)	Decoder iters 30.0 (73.2)	Tok/s 5450 (5821)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 3.9299	Validation Loss: 4.7822	Test BLEU: 8.56
0: Performance: Epoch: 1	Training: 13130 Tok/s	Validation: 38274 Tok/s
0: Finished epoch 1
0: Total training time 1185 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 128|                      8.56|                      13182.6|                         19.75|
DONE!
