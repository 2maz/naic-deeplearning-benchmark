The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_d10y2z60/none_42oblxjo
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_d10y2z60/none_42oblxjo/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_d10y2z60/none_42oblxjo/attempt_0/1/error.json
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
1: thread affinity: {1, 33, 5, 37, 9, 41, 13, 45}0: thread affinity: {0, 32, 4, 36, 8, 40, 12, 44}

1: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA RTX A4500
GPU 1: NVIDIA RTX A4500
GPU 2: NVIDIA RTX A4500
GPU 3: NVIDIA RTX A4500
GPU 4: NVIDIA RTX A4500
GPU 5: NVIDIA RTX A4500
GPU 6: NVIDIA RTX A4500
GPU 7: NVIDIA RTX A4500

Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA RTX A4500
GPU 1: NVIDIA RTX A4500
GPU 2: NVIDIA RTX A4500
GPU 3: NVIDIA RTX A4500
GPU 4: NVIDIA RTX A4500
GPU 5: NVIDIA RTX A4500
GPU 6: NVIDIA RTX A4500
GPU 7: NVIDIA RTX A4500

Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
1: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 614
1: Scheduler decay interval: 77
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 614
0: Scheduler decay interval: 77
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/461]	Time 0.328 (0.000)	Data 1.57e-01 (0.00e+00)	Tok/s 13231 (0)	Loss/tok 10.5928 (10.5928)	LR 2.047e-05
1: TRAIN [0][0/461]	Time 0.327 (0.000)	Data 1.62e-01 (0.00e+00)	Tok/s 13313 (0)	Loss/tok 10.6227 (10.6227)	LR 2.047e-05
1: TRAIN [0][10/461]	Time 0.311 (0.254)	Data 1.10e-04 (1.16e-04)	Tok/s 32316 (28919)	Loss/tok 9.7113 (10.1005)	LR 2.576e-05
0: TRAIN [0][10/461]	Time 0.311 (0.254)	Data 1.10e-04 (1.19e-04)	Tok/s 32494 (28866)	Loss/tok 9.7133 (10.1044)	LR 2.576e-05
0: TRAIN [0][20/461]	Time 0.244 (0.269)	Data 1.09e-04 (1.21e-04)	Tok/s 29417 (29294)	Loss/tok 9.1436 (9.7343)	LR 3.244e-05
1: TRAIN [0][20/461]	Time 0.244 (0.269)	Data 1.12e-04 (1.18e-04)	Tok/s 29655 (29378)	Loss/tok 9.1820 (9.7446)	LR 3.244e-05
0: TRAIN [0][30/461]	Time 0.312 (0.272)	Data 1.13e-04 (1.22e-04)	Tok/s 32515 (29658)	Loss/tok 8.9610 (9.5021)	LR 4.083e-05
1: TRAIN [0][30/461]	Time 0.311 (0.272)	Data 1.14e-04 (1.16e-04)	Tok/s 32868 (29637)	Loss/tok 8.9854 (9.5144)	LR 4.083e-05
0: TRAIN [0][40/461]	Time 0.184 (0.272)	Data 1.15e-04 (1.22e-04)	Tok/s 23386 (29461)	Loss/tok 8.5876 (9.3407)	LR 5.141e-05
1: TRAIN [0][40/461]	Time 0.184 (0.272)	Data 1.13e-04 (1.16e-04)	Tok/s 23233 (29433)	Loss/tok 8.5473 (9.3522)	LR 5.141e-05
1: TRAIN [0][50/461]	Time 0.184 (0.263)	Data 1.21e-04 (1.18e-04)	Tok/s 23952 (28993)	Loss/tok 8.3446 (9.2231)	LR 6.472e-05
0: TRAIN [0][50/461]	Time 0.184 (0.263)	Data 1.05e-04 (1.23e-04)	Tok/s 24022 (29027)	Loss/tok 8.2907 (9.2144)	LR 6.472e-05
0: TRAIN [0][60/461]	Time 0.244 (0.263)	Data 1.19e-04 (1.23e-04)	Tok/s 29516 (29194)	Loss/tok 8.1829 (9.0761)	LR 8.148e-05
1: TRAIN [0][60/461]	Time 0.244 (0.263)	Data 1.16e-04 (1.18e-04)	Tok/s 29482 (29156)	Loss/tok 8.2542 (9.0862)	LR 8.148e-05
0: TRAIN [0][70/461]	Time 0.183 (0.257)	Data 1.16e-04 (1.22e-04)	Tok/s 23557 (28709)	Loss/tok 7.9397 (8.9791)	LR 1.026e-04
1: TRAIN [0][70/461]	Time 0.183 (0.257)	Data 1.19e-04 (1.20e-04)	Tok/s 24132 (28677)	Loss/tok 7.8659 (8.9896)	LR 1.026e-04
1: TRAIN [0][80/461]	Time 0.244 (0.260)	Data 1.38e-04 (1.20e-04)	Tok/s 29701 (28843)	Loss/tok 7.8751 (8.8562)	LR 1.291e-04
0: TRAIN [0][80/461]	Time 0.244 (0.260)	Data 1.17e-04 (1.22e-04)	Tok/s 29778 (28844)	Loss/tok 7.8228 (8.8493)	LR 1.291e-04
1: TRAIN [0][90/461]	Time 0.246 (0.264)	Data 1.19e-04 (1.19e-04)	Tok/s 28950 (28990)	Loss/tok 7.7604 (8.7421)	LR 1.626e-04
0: TRAIN [0][90/461]	Time 0.246 (0.264)	Data 1.15e-04 (1.23e-04)	Tok/s 29158 (29009)	Loss/tok 7.7412 (8.7343)	LR 1.626e-04
1: TRAIN [0][100/461]	Time 0.397 (0.271)	Data 1.03e-04 (1.18e-04)	Tok/s 32969 (29153)	Loss/tok 7.9539 (8.6350)	LR 2.047e-04
0: TRAIN [0][100/461]	Time 0.397 (0.271)	Data 1.18e-04 (1.24e-04)	Tok/s 32903 (29186)	Loss/tok 7.9547 (8.6263)	LR 2.047e-04
1: TRAIN [0][110/461]	Time 0.185 (0.266)	Data 1.18e-04 (1.19e-04)	Tok/s 22907 (28919)	Loss/tok 7.4399 (8.5670)	LR 2.576e-04
0: TRAIN [0][110/461]	Time 0.185 (0.266)	Data 1.20e-04 (1.23e-04)	Tok/s 22986 (28952)	Loss/tok 7.4451 (8.5575)	LR 2.576e-04
0: TRAIN [0][120/461]	Time 0.185 (0.264)	Data 1.16e-04 (1.23e-04)	Tok/s 23659 (28868)	Loss/tok 7.3367 (8.4970)	LR 3.244e-04
1: TRAIN [0][120/461]	Time 0.185 (0.264)	Data 1.12e-04 (1.18e-04)	Tok/s 24176 (28850)	Loss/tok 7.2463 (8.5054)	LR 3.244e-04
1: TRAIN [0][130/461]	Time 0.399 (0.264)	Data 1.10e-04 (1.18e-04)	Tok/s 32796 (28790)	Loss/tok 8.0122 (8.4470)	LR 4.083e-04
0: TRAIN [0][130/461]	Time 0.399 (0.264)	Data 1.16e-04 (1.23e-04)	Tok/s 33132 (28812)	Loss/tok 8.0409 (8.4419)	LR 4.083e-04
1: TRAIN [0][140/461]	Time 0.315 (0.261)	Data 1.14e-04 (1.18e-04)	Tok/s 31920 (28593)	Loss/tok 7.8082 (8.3980)	LR 5.141e-04
0: TRAIN [0][140/461]	Time 0.315 (0.261)	Data 1.13e-04 (1.22e-04)	Tok/s 31401 (28601)	Loss/tok 7.7887 (8.3926)	LR 5.141e-04
1: TRAIN [0][150/461]	Time 0.246 (0.261)	Data 1.15e-04 (1.18e-04)	Tok/s 28701 (28619)	Loss/tok 7.6731 (8.3483)	LR 6.472e-04
0: TRAIN [0][150/461]	Time 0.246 (0.261)	Data 1.14e-04 (1.23e-04)	Tok/s 28796 (28626)	Loss/tok 7.6348 (8.3426)	LR 6.472e-04
1: TRAIN [0][160/461]	Time 0.314 (0.259)	Data 1.12e-04 (1.18e-04)	Tok/s 32081 (28498)	Loss/tok 8.0789 (8.3093)	LR 8.148e-04
0: TRAIN [0][160/461]	Time 0.314 (0.259)	Data 1.04e-04 (1.23e-04)	Tok/s 32118 (28509)	Loss/tok 8.0749 (8.3037)	LR 8.148e-04
1: TRAIN [0][170/461]	Time 0.316 (0.258)	Data 1.22e-04 (1.18e-04)	Tok/s 31877 (28402)	Loss/tok 7.7509 (8.2748)	LR 1.026e-03
0: TRAIN [0][170/461]	Time 0.316 (0.258)	Data 1.14e-04 (1.23e-04)	Tok/s 32004 (28406)	Loss/tok 7.6996 (8.2685)	LR 1.026e-03
1: TRAIN [0][180/461]	Time 0.315 (0.258)	Data 1.24e-04 (1.18e-04)	Tok/s 32011 (28451)	Loss/tok 7.6680 (8.2390)	LR 1.291e-03
0: TRAIN [0][180/461]	Time 0.315 (0.258)	Data 1.42e-04 (1.24e-04)	Tok/s 32001 (28452)	Loss/tok 7.6402 (8.2306)	LR 1.291e-03
1: TRAIN [0][190/461]	Time 0.315 (0.260)	Data 1.46e-04 (1.18e-04)	Tok/s 31679 (28518)	Loss/tok 7.5876 (8.1998)	LR 1.626e-03
0: TRAIN [0][190/461]	Time 0.315 (0.260)	Data 1.25e-04 (1.24e-04)	Tok/s 32050 (28524)	Loss/tok 7.5904 (8.1925)	LR 1.626e-03
1: TRAIN [0][200/461]	Time 0.316 (0.262)	Data 1.17e-04 (1.18e-04)	Tok/s 31601 (28557)	Loss/tok 7.3910 (8.1607)	LR 2.000e-03
0: TRAIN [0][200/461]	Time 0.316 (0.262)	Data 1.59e-04 (1.25e-04)	Tok/s 31700 (28564)	Loss/tok 7.3929 (8.1537)	LR 2.000e-03
1: TRAIN [0][210/461]	Time 0.318 (0.263)	Data 1.57e-04 (1.18e-04)	Tok/s 31640 (28579)	Loss/tok 7.3660 (8.1203)	LR 2.000e-03
0: TRAIN [0][210/461]	Time 0.318 (0.263)	Data 1.26e-04 (1.26e-04)	Tok/s 31735 (28595)	Loss/tok 7.3813 (8.1144)	LR 2.000e-03
1: TRAIN [0][220/461]	Time 0.184 (0.263)	Data 1.23e-04 (1.19e-04)	Tok/s 22962 (28575)	Loss/tok 6.8629 (8.0789)	LR 2.000e-03
0: TRAIN [0][220/461]	Time 0.184 (0.263)	Data 1.22e-04 (1.27e-04)	Tok/s 24130 (28601)	Loss/tok 6.7679 (8.0724)	LR 2.000e-03
1: TRAIN [0][230/461]	Time 0.315 (0.263)	Data 1.48e-04 (1.19e-04)	Tok/s 31909 (28601)	Loss/tok 7.0249 (8.0332)	LR 2.000e-03
0: TRAIN [0][230/461]	Time 0.315 (0.263)	Data 1.54e-04 (1.27e-04)	Tok/s 31893 (28627)	Loss/tok 7.1234 (8.0273)	LR 2.000e-03
1: TRAIN [0][240/461]	Time 0.316 (0.263)	Data 1.39e-04 (1.19e-04)	Tok/s 31615 (28640)	Loss/tok 6.9590 (7.9858)	LR 2.000e-03
0: TRAIN [0][240/461]	Time 0.316 (0.263)	Data 1.63e-04 (1.28e-04)	Tok/s 32073 (28668)	Loss/tok 6.9041 (7.9801)	LR 2.000e-03
1: TRAIN [0][250/461]	Time 0.245 (0.262)	Data 1.13e-04 (1.20e-04)	Tok/s 29355 (28576)	Loss/tok 6.6772 (7.9442)	LR 2.000e-03
0: TRAIN [0][250/461]	Time 0.245 (0.262)	Data 1.59e-04 (1.29e-04)	Tok/s 29396 (28607)	Loss/tok 6.6756 (7.9384)	LR 2.000e-03
1: TRAIN [0][260/461]	Time 0.398 (0.265)	Data 1.41e-04 (1.20e-04)	Tok/s 32800 (28659)	Loss/tok 6.8328 (7.8883)	LR 2.000e-03
0: TRAIN [0][260/461]	Time 0.399 (0.265)	Data 1.27e-04 (1.30e-04)	Tok/s 32787 (28691)	Loss/tok 6.9805 (7.8840)	LR 2.000e-03
1: TRAIN [0][270/461]	Time 0.246 (0.265)	Data 1.18e-04 (1.21e-04)	Tok/s 29388 (28682)	Loss/tok 6.4419 (7.8403)	LR 2.000e-03
0: TRAIN [0][270/461]	Time 0.246 (0.265)	Data 1.47e-04 (1.30e-04)	Tok/s 29185 (28707)	Loss/tok 6.4096 (7.8367)	LR 2.000e-03
1: TRAIN [0][280/461]	Time 0.186 (0.264)	Data 1.15e-04 (1.21e-04)	Tok/s 23595 (28646)	Loss/tok 6.0635 (7.7965)	LR 2.000e-03
0: TRAIN [0][280/461]	Time 0.186 (0.264)	Data 1.61e-04 (1.31e-04)	Tok/s 23246 (28668)	Loss/tok 5.9692 (7.7932)	LR 2.000e-03
1: TRAIN [0][290/461]	Time 0.185 (0.263)	Data 1.18e-04 (1.21e-04)	Tok/s 23797 (28626)	Loss/tok 6.0258 (7.7541)	LR 2.000e-03
0: TRAIN [0][290/461]	Time 0.183 (0.263)	Data 1.82e-04 (1.31e-04)	Tok/s 23628 (28647)	Loss/tok 5.9314 (7.7502)	LR 2.000e-03
1: TRAIN [0][300/461]	Time 0.187 (0.264)	Data 1.35e-04 (1.21e-04)	Tok/s 23165 (28691)	Loss/tok 5.8823 (7.7021)	LR 2.000e-03
0: TRAIN [0][300/461]	Time 0.187 (0.264)	Data 1.65e-04 (1.32e-04)	Tok/s 23212 (28709)	Loss/tok 5.9847 (7.6984)	LR 2.000e-03
1: TRAIN [0][310/461]	Time 0.318 (0.263)	Data 1.17e-04 (1.21e-04)	Tok/s 31548 (28650)	Loss/tok 6.4564 (7.6618)	LR 2.000e-03
0: TRAIN [0][310/461]	Time 0.318 (0.263)	Data 1.71e-04 (1.33e-04)	Tok/s 31715 (28667)	Loss/tok 6.4202 (7.6574)	LR 2.000e-03
1: TRAIN [0][320/461]	Time 0.397 (0.265)	Data 1.18e-04 (1.21e-04)	Tok/s 32849 (28729)	Loss/tok 6.4559 (7.6085)	LR 2.000e-03
0: TRAIN [0][320/461]	Time 0.397 (0.265)	Data 1.62e-04 (1.33e-04)	Tok/s 32891 (28744)	Loss/tok 6.4398 (7.6034)	LR 2.000e-03
1: TRAIN [0][330/461]	Time 0.185 (0.267)	Data 1.18e-04 (1.21e-04)	Tok/s 22987 (28796)	Loss/tok 5.8354 (7.5547)	LR 2.000e-03
0: TRAIN [0][330/461]	Time 0.185 (0.267)	Data 1.57e-04 (1.34e-04)	Tok/s 23475 (28814)	Loss/tok 5.8156 (7.5496)	LR 2.000e-03
1: TRAIN [0][340/461]	Time 0.248 (0.267)	Data 1.33e-04 (1.21e-04)	Tok/s 29131 (28787)	Loss/tok 5.9158 (7.5117)	LR 2.000e-03
0: TRAIN [0][340/461]	Time 0.248 (0.267)	Data 1.84e-04 (1.34e-04)	Tok/s 28941 (28802)	Loss/tok 5.9111 (7.5074)	LR 2.000e-03
1: TRAIN [0][350/461]	Time 0.246 (0.266)	Data 1.12e-04 (1.21e-04)	Tok/s 29393 (28745)	Loss/tok 5.8930 (7.4728)	LR 2.000e-03
0: TRAIN [0][350/461]	Time 0.246 (0.266)	Data 1.70e-04 (1.34e-04)	Tok/s 29374 (28759)	Loss/tok 5.8016 (7.4690)	LR 2.000e-03
1: TRAIN [0][360/461]	Time 0.183 (0.267)	Data 1.16e-04 (1.21e-04)	Tok/s 23591 (28738)	Loss/tok 5.5276 (7.4301)	LR 2.000e-03
0: TRAIN [0][360/461]	Time 0.183 (0.267)	Data 1.33e-04 (1.35e-04)	Tok/s 24111 (28755)	Loss/tok 5.4646 (7.4249)	LR 2.000e-03
1: TRAIN [0][370/461]	Time 0.248 (0.267)	Data 1.20e-04 (1.21e-04)	Tok/s 28904 (28738)	Loss/tok 5.6918 (7.3884)	LR 2.000e-03
0: TRAIN [0][370/461]	Time 0.248 (0.267)	Data 1.73e-04 (1.35e-04)	Tok/s 29007 (28755)	Loss/tok 5.7282 (7.3835)	LR 2.000e-03
1: TRAIN [0][380/461]	Time 0.398 (0.266)	Data 1.17e-04 (1.21e-04)	Tok/s 33296 (28684)	Loss/tok 6.1367 (7.3525)	LR 2.000e-03
0: TRAIN [0][380/461]	Time 0.398 (0.266)	Data 1.70e-04 (1.35e-04)	Tok/s 32659 (28691)	Loss/tok 6.0238 (7.3475)	LR 2.000e-03
1: TRAIN [0][390/461]	Time 0.185 (0.265)	Data 1.24e-04 (1.21e-04)	Tok/s 23359 (28593)	Loss/tok 5.2446 (7.3218)	LR 2.000e-03
0: TRAIN [0][390/461]	Time 0.185 (0.265)	Data 1.77e-04 (1.36e-04)	Tok/s 23239 (28591)	Loss/tok 5.3929 (7.3169)	LR 2.000e-03
1: TRAIN [0][400/461]	Time 0.244 (0.265)	Data 1.18e-04 (1.22e-04)	Tok/s 30028 (28579)	Loss/tok 5.7029 (7.2834)	LR 2.000e-03
0: TRAIN [0][400/461]	Time 0.244 (0.265)	Data 1.49e-04 (1.36e-04)	Tok/s 29660 (28576)	Loss/tok 5.6274 (7.2777)	LR 2.000e-03
1: TRAIN [0][410/461]	Time 0.318 (0.265)	Data 1.19e-04 (1.22e-04)	Tok/s 31532 (28583)	Loss/tok 5.6980 (7.2424)	LR 2.000e-03
0: TRAIN [0][410/461]	Time 0.318 (0.265)	Data 1.67e-04 (1.36e-04)	Tok/s 31524 (28580)	Loss/tok 5.6330 (7.2363)	LR 2.000e-03
1: TRAIN [0][420/461]	Time 0.124 (0.264)	Data 1.14e-04 (1.22e-04)	Tok/s 17224 (28514)	Loss/tok 4.5627 (7.2104)	LR 2.000e-03
0: TRAIN [0][420/461]	Time 0.124 (0.264)	Data 1.66e-04 (1.36e-04)	Tok/s 17363 (28510)	Loss/tok 4.9550 (7.2050)	LR 2.000e-03
1: TRAIN [0][430/461]	Time 0.317 (0.263)	Data 1.13e-04 (1.22e-04)	Tok/s 32006 (28512)	Loss/tok 5.6830 (7.1738)	LR 2.000e-03
0: TRAIN [0][430/461]	Time 0.317 (0.263)	Data 1.46e-04 (1.37e-04)	Tok/s 31499 (28509)	Loss/tok 5.6700 (7.1681)	LR 2.000e-03
1: TRAIN [0][440/461]	Time 0.185 (0.263)	Data 1.14e-04 (1.22e-04)	Tok/s 23646 (28527)	Loss/tok 4.8729 (7.1336)	LR 2.000e-03
0: TRAIN [0][440/461]	Time 0.185 (0.263)	Data 1.65e-04 (1.37e-04)	Tok/s 22981 (28525)	Loss/tok 5.0960 (7.1275)	LR 2.000e-03
1: TRAIN [0][450/461]	Time 0.319 (0.264)	Data 1.19e-04 (1.22e-04)	Tok/s 31293 (28528)	Loss/tok 5.4754 (7.0947)	LR 2.000e-03
0: TRAIN [0][450/461]	Time 0.318 (0.264)	Data 1.72e-04 (1.37e-04)	Tok/s 31616 (28527)	Loss/tok 5.4131 (7.0879)	LR 2.000e-03
1: TRAIN [0][460/461]	Time 0.184 (0.263)	Data 4.34e-05 (1.23e-04)	Tok/s 23930 (28481)	Loss/tok 4.8768 (7.0617)	LR 2.000e-03
0: TRAIN [0][460/461]	Time 0.184 (0.263)	Data 4.79e-05 (1.39e-04)	Tok/s 23626 (28481)	Loss/tok 4.8360 (7.0551)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.067 (0.000)	Data 1.21e-03 (0.00e+00)	Tok/s 70662 (0)	Loss/tok 6.7878 (6.7878)
0: VALIDATION [0][0/80]	Time 0.101 (0.000)	Data 1.31e-03 (0.00e+00)	Tok/s 56603 (0)	Loss/tok 6.8104 (6.8104)
1: VALIDATION [0][10/80]	Time 0.036 (0.043)	Data 9.16e-04 (9.52e-04)	Tok/s 80634 (79194)	Loss/tok 6.6359 (6.6567)
0: VALIDATION [0][10/80]	Time 0.037 (0.045)	Data 9.55e-04 (9.78e-04)	Tok/s 79598 (76530)	Loss/tok 6.4505 (6.6210)
1: VALIDATION [0][20/80]	Time 0.029 (0.037)	Data 9.16e-04 (9.35e-04)	Tok/s 80522 (79757)	Loss/tok 6.2426 (6.5425)
0: VALIDATION [0][20/80]	Time 0.030 (0.039)	Data 9.06e-04 (9.57e-04)	Tok/s 78511 (77157)	Loss/tok 6.2036 (6.5419)
1: VALIDATION [0][30/80]	Time 0.024 (0.034)	Data 9.06e-04 (9.27e-04)	Tok/s 79328 (79978)	Loss/tok 6.0544 (6.4549)
0: VALIDATION [0][30/80]	Time 0.025 (0.035)	Data 9.01e-04 (9.42e-04)	Tok/s 79328 (77664)	Loss/tok 5.9727 (6.4605)
1: VALIDATION [0][40/80]	Time 0.021 (0.031)	Data 9.00e-04 (9.20e-04)	Tok/s 77671 (79607)	Loss/tok 5.9279 (6.4058)
0: VALIDATION [0][40/80]	Time 0.021 (0.032)	Data 8.85e-04 (9.32e-04)	Tok/s 76168 (77493)	Loss/tok 6.1664 (6.3996)
1: VALIDATION [0][50/80]	Time 0.016 (0.028)	Data 8.83e-04 (9.14e-04)	Tok/s 80443 (79573)	Loss/tok 5.5947 (6.3544)
0: VALIDATION [0][50/80]	Time 0.017 (0.029)	Data 8.95e-04 (9.24e-04)	Tok/s 79561 (77682)	Loss/tok 6.0756 (6.3590)
1: VALIDATION [0][60/80]	Time 0.014 (0.026)	Data 8.65e-04 (9.09e-04)	Tok/s 77490 (79343)	Loss/tok 6.0340 (6.3112)
0: VALIDATION [0][60/80]	Time 0.014 (0.027)	Data 8.79e-04 (9.17e-04)	Tok/s 74661 (77541)	Loss/tok 5.8807 (6.3271)
1: VALIDATION [0][70/80]	Time 0.011 (0.024)	Data 8.66e-04 (9.05e-04)	Tok/s 73239 (78703)	Loss/tok 5.5963 (6.2770)
0: VALIDATION [0][70/80]	Time 0.012 (0.025)	Data 8.78e-04 (9.11e-04)	Tok/s 69118 (76803)	Loss/tok 5.6207 (6.2851)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [0][9/47]	Time 0.3970 (0.4758)	Decoder iters 149.0 (149.0)	Tok/s 5692 (6236)
0: TEST [0][9/47]	Time 0.3971 (0.4758)	Decoder iters 149.0 (149.0)	Tok/s 5923 (6394)
0: TEST [0][19/47]	Time 0.3554 (0.4268)	Decoder iters 149.0 (149.0)	Tok/s 5375 (5911)
1: TEST [0][19/47]	Time 0.3554 (0.4268)	Decoder iters 149.0 (149.0)	Tok/s 4651 (5896)
0: TEST [0][29/47]	Time 0.3249 (0.3980)	Decoder iters 149.0 (149.0)	Tok/s 4482 (5564)
1: TEST [0][29/47]	Time 0.3247 (0.3980)	Decoder iters 149.0 (145.6)	Tok/s 4207 (5506)
0: TEST [0][39/47]	Time 0.1571 (0.3680)	Decoder iters 67.0 (142.9)	Tok/s 5957 (5281)
1: TEST [0][39/47]	Time 0.1569 (0.3680)	Decoder iters 39.0 (138.8)	Tok/s 6017 (5258)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.0584	Validation Loss: 6.2475	Test BLEU: 2.05
0: Performance: Epoch: 0	Training: 56962 Tok/s	Validation: 151902 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
1: Sampler for epoch 1 uses seed 1323436024
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: Sampler for epoch 1 uses seed 1323436024
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
1: TRAIN [1][0/461]	Time 0.335 (0.000)	Data 9.52e-02 (0.00e+00)	Tok/s 21514 (0)	Loss/tok 5.1152 (5.1152)	LR 2.000e-03
0: TRAIN [1][0/461]	Time 0.336 (0.000)	Data 1.12e-01 (0.00e+00)	Tok/s 21445 (0)	Loss/tok 5.0418 (5.0418)	LR 2.000e-03
1: TRAIN [1][10/461]	Time 0.182 (0.235)	Data 1.22e-04 (1.25e-04)	Tok/s 22969 (27345)	Loss/tok 4.5765 (5.0077)	LR 2.000e-03
0: TRAIN [1][10/461]	Time 0.182 (0.235)	Data 1.20e-04 (1.30e-04)	Tok/s 23602 (27591)	Loss/tok 4.6227 (4.9696)	LR 2.000e-03
1: TRAIN [1][20/461]	Time 0.244 (0.270)	Data 1.19e-04 (1.28e-04)	Tok/s 29676 (28836)	Loss/tok 4.8958 (5.1320)	LR 2.000e-03
0: TRAIN [1][20/461]	Time 0.244 (0.270)	Data 1.32e-04 (1.35e-04)	Tok/s 29761 (29047)	Loss/tok 4.9810 (5.1496)	LR 2.000e-03
1: TRAIN [1][30/461]	Time 0.398 (0.274)	Data 1.22e-04 (1.26e-04)	Tok/s 33159 (29224)	Loss/tok 5.3645 (5.1126)	LR 2.000e-03
0: TRAIN [1][30/461]	Time 0.398 (0.274)	Data 1.38e-04 (1.33e-04)	Tok/s 33049 (29264)	Loss/tok 5.3321 (5.1073)	LR 2.000e-03
1: TRAIN [1][40/461]	Time 0.314 (0.266)	Data 1.18e-04 (1.24e-04)	Tok/s 32094 (29004)	Loss/tok 5.1360 (5.0553)	LR 2.000e-03
0: TRAIN [1][40/461]	Time 0.314 (0.266)	Data 1.21e-04 (1.30e-04)	Tok/s 32176 (28983)	Loss/tok 5.1237 (5.0477)	LR 2.000e-03
1: TRAIN [1][50/461]	Time 0.247 (0.267)	Data 1.22e-04 (1.23e-04)	Tok/s 29023 (29009)	Loss/tok 4.8487 (5.0394)	LR 2.000e-03
0: TRAIN [1][50/461]	Time 0.247 (0.267)	Data 1.32e-04 (1.30e-04)	Tok/s 29445 (28991)	Loss/tok 4.7592 (5.0296)	LR 2.000e-03
1: TRAIN [1][60/461]	Time 0.245 (0.264)	Data 1.37e-04 (1.22e-04)	Tok/s 29570 (28971)	Loss/tok 4.7069 (5.0077)	LR 2.000e-03
0: TRAIN [1][60/461]	Time 0.245 (0.264)	Data 1.18e-04 (1.30e-04)	Tok/s 29646 (28939)	Loss/tok 4.5561 (4.9926)	LR 2.000e-03
1: TRAIN [1][70/461]	Time 0.184 (0.260)	Data 1.13e-04 (1.22e-04)	Tok/s 23837 (28813)	Loss/tok 4.3229 (4.9800)	LR 2.000e-03
0: TRAIN [1][70/461]	Time 0.184 (0.260)	Data 1.20e-04 (1.29e-04)	Tok/s 24066 (28772)	Loss/tok 4.3299 (4.9653)	LR 2.000e-03
1: TRAIN [1][80/461]	Time 0.245 (0.260)	Data 1.23e-04 (1.21e-04)	Tok/s 28901 (28798)	Loss/tok 4.6468 (4.9644)	LR 2.000e-03
0: TRAIN [1][80/461]	Time 0.245 (0.260)	Data 1.30e-04 (1.29e-04)	Tok/s 29620 (28760)	Loss/tok 4.6692 (4.9512)	LR 2.000e-03
1: TRAIN [1][90/461]	Time 0.244 (0.258)	Data 1.08e-04 (1.21e-04)	Tok/s 29638 (28640)	Loss/tok 4.6315 (4.9453)	LR 2.000e-03
0: TRAIN [1][90/461]	Time 0.244 (0.258)	Data 1.14e-04 (1.31e-04)	Tok/s 29501 (28614)	Loss/tok 4.5871 (4.9348)	LR 2.000e-03
1: TRAIN [1][100/461]	Time 0.185 (0.257)	Data 1.14e-04 (1.20e-04)	Tok/s 23223 (28485)	Loss/tok 4.3050 (4.9281)	LR 2.000e-03
0: TRAIN [1][100/461]	Time 0.185 (0.257)	Data 1.42e-04 (1.32e-04)	Tok/s 23561 (28455)	Loss/tok 4.1994 (4.9146)	LR 2.000e-03
1: TRAIN [1][110/461]	Time 0.246 (0.255)	Data 1.08e-04 (1.20e-04)	Tok/s 29391 (28413)	Loss/tok 4.5457 (4.9016)	LR 2.000e-03
0: TRAIN [1][110/461]	Time 0.246 (0.255)	Data 1.36e-04 (1.32e-04)	Tok/s 28989 (28392)	Loss/tok 4.6640 (4.8920)	LR 2.000e-03
1: TRAIN [1][120/461]	Time 0.123 (0.255)	Data 1.21e-04 (1.21e-04)	Tok/s 17739 (28399)	Loss/tok 4.0863 (4.8858)	LR 2.000e-03
0: TRAIN [1][120/461]	Time 0.123 (0.255)	Data 1.54e-04 (1.33e-04)	Tok/s 17421 (28401)	Loss/tok 3.7947 (4.8740)	LR 2.000e-03
1: TRAIN [1][130/461]	Time 0.316 (0.255)	Data 1.22e-04 (1.20e-04)	Tok/s 31918 (28381)	Loss/tok 4.8497 (4.8697)	LR 2.000e-03
0: TRAIN [1][130/461]	Time 0.316 (0.255)	Data 1.42e-04 (1.34e-04)	Tok/s 31755 (28397)	Loss/tok 4.7183 (4.8584)	LR 2.000e-03
1: TRAIN [1][140/461]	Time 0.316 (0.259)	Data 1.17e-04 (1.20e-04)	Tok/s 31894 (28579)	Loss/tok 4.6598 (4.8534)	LR 2.000e-03
0: TRAIN [1][140/461]	Time 0.316 (0.259)	Data 1.29e-04 (1.34e-04)	Tok/s 32075 (28616)	Loss/tok 4.6477 (4.8465)	LR 2.000e-03
1: TRAIN [1][150/461]	Time 0.315 (0.261)	Data 1.18e-04 (1.20e-04)	Tok/s 32430 (28646)	Loss/tok 4.7720 (4.8402)	LR 2.000e-03
0: TRAIN [1][150/461]	Time 0.315 (0.261)	Data 1.30e-04 (1.34e-04)	Tok/s 31743 (28670)	Loss/tok 4.7456 (4.8357)	LR 2.000e-03
1: TRAIN [1][160/461]	Time 0.247 (0.263)	Data 1.21e-04 (1.20e-04)	Tok/s 29036 (28688)	Loss/tok 4.3731 (4.8292)	LR 1.000e-03
0: TRAIN [1][160/461]	Time 0.247 (0.263)	Data 1.14e-04 (1.34e-04)	Tok/s 29553 (28712)	Loss/tok 4.2656 (4.8234)	LR 1.000e-03
1: TRAIN [1][170/461]	Time 0.317 (0.264)	Data 1.26e-04 (1.20e-04)	Tok/s 32235 (28724)	Loss/tok 4.6071 (4.8111)	LR 1.000e-03
0: TRAIN [1][170/461]	Time 0.317 (0.264)	Data 1.35e-04 (1.34e-04)	Tok/s 32166 (28742)	Loss/tok 4.4303 (4.8029)	LR 1.000e-03
1: TRAIN [1][180/461]	Time 0.183 (0.264)	Data 1.19e-04 (1.21e-04)	Tok/s 23482 (28725)	Loss/tok 4.0526 (4.7915)	LR 1.000e-03
0: TRAIN [1][180/461]	Time 0.183 (0.264)	Data 1.29e-04 (1.34e-04)	Tok/s 23310 (28744)	Loss/tok 4.0013 (4.7823)	LR 1.000e-03
1: TRAIN [1][190/461]	Time 0.398 (0.265)	Data 1.21e-04 (1.22e-04)	Tok/s 32710 (28745)	Loss/tok 4.7229 (4.7749)	LR 1.000e-03
0: TRAIN [1][190/461]	Time 0.398 (0.265)	Data 1.23e-04 (1.34e-04)	Tok/s 32959 (28766)	Loss/tok 4.6901 (4.7669)	LR 1.000e-03
1: TRAIN [1][200/461]	Time 0.396 (0.266)	Data 1.22e-04 (1.22e-04)	Tok/s 32895 (28789)	Loss/tok 4.7072 (4.7565)	LR 1.000e-03
0: TRAIN [1][200/461]	Time 0.396 (0.266)	Data 1.31e-04 (1.34e-04)	Tok/s 33161 (28804)	Loss/tok 4.6072 (4.7480)	LR 1.000e-03
1: TRAIN [1][210/461]	Time 0.183 (0.265)	Data 1.26e-04 (1.22e-04)	Tok/s 23312 (28792)	Loss/tok 3.9084 (4.7349)	LR 1.000e-03
0: TRAIN [1][210/461]	Time 0.183 (0.265)	Data 1.56e-04 (1.33e-04)	Tok/s 23611 (28811)	Loss/tok 3.8989 (4.7267)	LR 1.000e-03
1: TRAIN [1][220/461]	Time 0.248 (0.265)	Data 1.21e-04 (1.22e-04)	Tok/s 29685 (28803)	Loss/tok 4.1516 (4.7181)	LR 1.000e-03
0: TRAIN [1][220/461]	Time 0.248 (0.265)	Data 1.34e-04 (1.34e-04)	Tok/s 28831 (28825)	Loss/tok 4.1480 (4.7103)	LR 1.000e-03
1: TRAIN [1][230/461]	Time 0.314 (0.266)	Data 1.80e-04 (1.23e-04)	Tok/s 32284 (28851)	Loss/tok 4.4472 (4.6995)	LR 5.000e-04
0: TRAIN [1][230/461]	Time 0.315 (0.266)	Data 1.23e-04 (1.34e-04)	Tok/s 31900 (28886)	Loss/tok 4.2216 (4.6914)	LR 5.000e-04
1: TRAIN [1][240/461]	Time 0.183 (0.264)	Data 1.19e-04 (1.23e-04)	Tok/s 23826 (28757)	Loss/tok 3.9791 (4.6826)	LR 5.000e-04
0: TRAIN [1][240/461]	Time 0.183 (0.264)	Data 1.40e-04 (1.34e-04)	Tok/s 23918 (28793)	Loss/tok 3.8742 (4.6740)	LR 5.000e-04
0: TRAIN [1][250/461]	Time 0.184 (0.264)	Data 1.22e-04 (1.34e-04)	Tok/s 23344 (28770)	Loss/tok 3.9453 (4.6579)	LR 5.000e-04
1: TRAIN [1][250/461]	Time 0.185 (0.264)	Data 1.86e-04 (1.23e-04)	Tok/s 23453 (28739)	Loss/tok 3.6181 (4.6651)	LR 5.000e-04
1: TRAIN [1][260/461]	Time 0.186 (0.265)	Data 1.82e-04 (1.24e-04)	Tok/s 23950 (28767)	Loss/tok 3.9918 (4.6505)	LR 5.000e-04
0: TRAIN [1][260/461]	Time 0.186 (0.265)	Data 1.32e-04 (1.34e-04)	Tok/s 23589 (28788)	Loss/tok 3.9934 (4.6450)	LR 5.000e-04
1: TRAIN [1][270/461]	Time 0.244 (0.265)	Data 1.25e-04 (1.23e-04)	Tok/s 28998 (28801)	Loss/tok 4.1995 (4.6344)	LR 5.000e-04
0: TRAIN [1][270/461]	Time 0.244 (0.265)	Data 1.29e-04 (1.35e-04)	Tok/s 29440 (28830)	Loss/tok 4.1377 (4.6278)	LR 5.000e-04
1: TRAIN [1][280/461]	Time 0.246 (0.265)	Data 1.30e-04 (1.23e-04)	Tok/s 28727 (28797)	Loss/tok 3.9732 (4.6171)	LR 5.000e-04
0: TRAIN [1][280/461]	Time 0.246 (0.265)	Data 1.33e-04 (1.35e-04)	Tok/s 29460 (28834)	Loss/tok 3.9868 (4.6105)	LR 5.000e-04
1: TRAIN [1][290/461]	Time 0.314 (0.265)	Data 1.12e-04 (1.24e-04)	Tok/s 32023 (28804)	Loss/tok 4.4461 (4.6038)	LR 5.000e-04
0: TRAIN [1][290/461]	Time 0.314 (0.265)	Data 1.33e-04 (1.35e-04)	Tok/s 31619 (28835)	Loss/tok 4.3180 (4.5958)	LR 5.000e-04
1: TRAIN [1][300/461]	Time 0.247 (0.266)	Data 1.80e-04 (1.24e-04)	Tok/s 29005 (28800)	Loss/tok 4.0145 (4.5929)	LR 5.000e-04
0: TRAIN [1][300/461]	Time 0.247 (0.266)	Data 1.64e-04 (1.36e-04)	Tok/s 29848 (28843)	Loss/tok 4.1894 (4.5863)	LR 5.000e-04
1: TRAIN [1][310/461]	Time 0.248 (0.266)	Data 1.99e-04 (1.24e-04)	Tok/s 29229 (28764)	Loss/tok 3.9961 (4.5794)	LR 2.500e-04
0: TRAIN [1][310/461]	Time 0.248 (0.266)	Data 1.09e-04 (1.36e-04)	Tok/s 29482 (28802)	Loss/tok 3.9017 (4.5738)	LR 2.500e-04
1: TRAIN [1][320/461]	Time 0.317 (0.267)	Data 1.25e-04 (1.24e-04)	Tok/s 31677 (28822)	Loss/tok 4.2327 (4.5681)	LR 2.500e-04
0: TRAIN [1][320/461]	Time 0.317 (0.267)	Data 1.29e-04 (1.36e-04)	Tok/s 31524 (28853)	Loss/tok 4.1934 (4.5601)	LR 2.500e-04
1: TRAIN [1][330/461]	Time 0.396 (0.267)	Data 1.89e-04 (1.25e-04)	Tok/s 33009 (28809)	Loss/tok 4.5214 (4.5558)	LR 2.500e-04
0: TRAIN [1][330/461]	Time 0.396 (0.267)	Data 1.31e-04 (1.36e-04)	Tok/s 32734 (28838)	Loss/tok 4.3845 (4.5475)	LR 2.500e-04
1: TRAIN [1][340/461]	Time 0.313 (0.268)	Data 1.25e-04 (1.25e-04)	Tok/s 32076 (28857)	Loss/tok 4.3470 (4.5468)	LR 2.500e-04
0: TRAIN [1][340/461]	Time 0.313 (0.268)	Data 1.26e-04 (1.36e-04)	Tok/s 31641 (28887)	Loss/tok 4.1741 (4.5380)	LR 2.500e-04
1: TRAIN [1][350/461]	Time 0.314 (0.269)	Data 1.24e-04 (1.25e-04)	Tok/s 32077 (28848)	Loss/tok 4.3002 (4.5380)	LR 2.500e-04
0: TRAIN [1][350/461]	Time 0.314 (0.269)	Data 1.30e-04 (1.37e-04)	Tok/s 32197 (28881)	Loss/tok 4.1829 (4.5282)	LR 2.500e-04
1: TRAIN [1][360/461]	Time 0.247 (0.268)	Data 1.86e-04 (1.26e-04)	Tok/s 29452 (28783)	Loss/tok 3.9395 (4.5271)	LR 2.500e-04
0: TRAIN [1][360/461]	Time 0.247 (0.268)	Data 1.53e-04 (1.37e-04)	Tok/s 29123 (28813)	Loss/tok 3.9814 (4.5176)	LR 2.500e-04
1: TRAIN [1][370/461]	Time 0.316 (0.267)	Data 1.26e-04 (1.26e-04)	Tok/s 31812 (28759)	Loss/tok 4.2321 (4.5159)	LR 2.500e-04
0: TRAIN [1][370/461]	Time 0.315 (0.267)	Data 1.64e-04 (1.37e-04)	Tok/s 31657 (28787)	Loss/tok 4.2364 (4.5064)	LR 2.500e-04
1: TRAIN [1][380/461]	Time 0.246 (0.266)	Data 1.25e-04 (1.26e-04)	Tok/s 28670 (28731)	Loss/tok 3.9449 (4.5048)	LR 2.500e-04
0: TRAIN [1][380/461]	Time 0.245 (0.266)	Data 1.32e-04 (1.38e-04)	Tok/s 29312 (28765)	Loss/tok 3.9999 (4.4961)	LR 2.500e-04
1: TRAIN [1][390/461]	Time 0.184 (0.266)	Data 1.22e-04 (1.26e-04)	Tok/s 23220 (28725)	Loss/tok 3.7532 (4.4931)	LR 1.250e-04
0: TRAIN [1][390/461]	Time 0.184 (0.266)	Data 1.25e-04 (1.37e-04)	Tok/s 22988 (28759)	Loss/tok 3.6109 (4.4848)	LR 1.250e-04
0: TRAIN [1][400/461]	Time 0.246 (0.265)	Data 1.28e-04 (1.38e-04)	Tok/s 29337 (28692)	Loss/tok 3.9820 (4.4744)	LR 1.250e-04
1: TRAIN [1][400/461]	Time 0.247 (0.265)	Data 1.83e-04 (1.26e-04)	Tok/s 29567 (28655)	Loss/tok 3.8645 (4.4827)	LR 1.250e-04
1: TRAIN [1][410/461]	Time 0.183 (0.263)	Data 1.29e-04 (1.26e-04)	Tok/s 24306 (28530)	Loss/tok 3.6656 (4.4744)	LR 1.250e-04
0: TRAIN [1][410/461]	Time 0.183 (0.263)	Data 1.24e-04 (1.37e-04)	Tok/s 23410 (28570)	Loss/tok 3.6683 (4.4656)	LR 1.250e-04
1: TRAIN [1][420/461]	Time 0.245 (0.262)	Data 1.24e-04 (1.26e-04)	Tok/s 28828 (28527)	Loss/tok 3.9144 (4.4641)	LR 1.250e-04
0: TRAIN [1][420/461]	Time 0.245 (0.262)	Data 1.30e-04 (1.37e-04)	Tok/s 29185 (28567)	Loss/tok 3.8621 (4.4550)	LR 1.250e-04
1: TRAIN [1][430/461]	Time 0.315 (0.263)	Data 1.18e-04 (1.26e-04)	Tok/s 31881 (28577)	Loss/tok 4.3124 (4.4552)	LR 1.250e-04
0: TRAIN [1][430/461]	Time 0.315 (0.263)	Data 1.28e-04 (1.37e-04)	Tok/s 32327 (28616)	Loss/tok 4.1619 (4.4458)	LR 1.250e-04
0: TRAIN [1][440/461]	Time 0.182 (0.263)	Data 1.44e-04 (1.38e-04)	Tok/s 24124 (28613)	Loss/tok 3.8055 (4.4378)	LR 1.250e-04
1: TRAIN [1][440/461]	Time 0.183 (0.263)	Data 1.19e-04 (1.26e-04)	Tok/s 24007 (28578)	Loss/tok 3.6658 (4.4460)	LR 1.250e-04
1: TRAIN [1][450/461]	Time 0.397 (0.263)	Data 1.70e-04 (1.27e-04)	Tok/s 32697 (28547)	Loss/tok 4.4584 (4.4397)	LR 1.250e-04
0: TRAIN [1][450/461]	Time 0.397 (0.263)	Data 1.15e-04 (1.38e-04)	Tok/s 32879 (28584)	Loss/tok 4.3493 (4.4312)	LR 1.250e-04
1: TRAIN [1][460/461]	Time 0.183 (0.262)	Data 4.46e-05 (1.28e-04)	Tok/s 23979 (28523)	Loss/tok 3.7158 (4.4321)	LR 1.250e-04
0: TRAIN [1][460/461]	Time 0.183 (0.262)	Data 4.51e-05 (1.39e-04)	Tok/s 23389 (28557)	Loss/tok 3.8031 (4.4234)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.067 (0.000)	Data 1.27e-03 (0.00e+00)	Tok/s 70769 (0)	Loss/tok 5.6114 (5.6114)
0: VALIDATION [1][0/80]	Time 0.101 (0.000)	Data 1.27e-03 (0.00e+00)	Tok/s 56891 (0)	Loss/tok 5.7628 (5.7628)
1: VALIDATION [1][10/80]	Time 0.036 (0.043)	Data 9.82e-04 (1.01e-03)	Tok/s 80286 (79393)	Loss/tok 5.3314 (5.4191)
0: VALIDATION [1][10/80]	Time 0.037 (0.045)	Data 9.72e-04 (9.96e-04)	Tok/s 79641 (76577)	Loss/tok 5.2335 (5.4129)
1: VALIDATION [1][20/80]	Time 0.029 (0.037)	Data 9.51e-04 (9.92e-04)	Tok/s 79929 (79879)	Loss/tok 4.9954 (5.3119)
0: VALIDATION [1][20/80]	Time 0.030 (0.039)	Data 9.35e-04 (9.75e-04)	Tok/s 78108 (77173)	Loss/tok 4.9525 (5.3540)
1: VALIDATION [1][30/80]	Time 0.024 (0.034)	Data 9.33e-04 (9.74e-04)	Tok/s 79253 (79944)	Loss/tok 4.8269 (5.2327)
0: VALIDATION [1][30/80]	Time 0.024 (0.035)	Data 9.10e-04 (9.58e-04)	Tok/s 80043 (77682)	Loss/tok 4.8186 (5.2782)
1: VALIDATION [1][40/80]	Time 0.021 (0.031)	Data 9.25e-04 (9.61e-04)	Tok/s 77445 (79568)	Loss/tok 4.8448 (5.2017)
0: VALIDATION [1][40/80]	Time 0.021 (0.032)	Data 9.14e-04 (9.47e-04)	Tok/s 76067 (77484)	Loss/tok 5.1290 (5.2236)
1: VALIDATION [1][50/80]	Time 0.016 (0.028)	Data 9.17e-04 (9.52e-04)	Tok/s 80688 (79560)	Loss/tok 4.5548 (5.1645)
0: VALIDATION [1][50/80]	Time 0.017 (0.029)	Data 8.81e-04 (9.38e-04)	Tok/s 79667 (77697)	Loss/tok 5.0520 (5.1923)
1: VALIDATION [1][60/80]	Time 0.014 (0.026)	Data 9.01e-04 (9.44e-04)	Tok/s 77138 (79318)	Loss/tok 4.9486 (5.1310)
0: VALIDATION [1][60/80]	Time 0.014 (0.027)	Data 8.84e-04 (9.29e-04)	Tok/s 74500 (77539)	Loss/tok 4.6837 (5.1641)
1: VALIDATION [1][70/80]	Time 0.011 (0.024)	Data 8.94e-04 (9.37e-04)	Tok/s 72982 (78659)	Loss/tok 4.5243 (5.1067)
0: VALIDATION [1][70/80]	Time 0.012 (0.025)	Data 8.70e-04 (9.22e-04)	Tok/s 69032 (76780)	Loss/tok 4.4722 (5.1291)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [1][9/47]	Time 0.3874 (0.4493)	Decoder iters 74.0 (137.3)	Tok/s 6665 (7260)
0: TEST [1][9/47]	Time 0.3872 (0.4492)	Decoder iters 149.0 (144.2)	Tok/s 7012 (7627)
0: TEST [1][19/47]	Time 0.2374 (0.3845)	Decoder iters 63.0 (124.7)	Tok/s 8327 (7538)
1: TEST [1][19/47]	Time 0.2377 (0.3845)	Decoder iters 89.0 (121.1)	Tok/s 7809 (7238)
0: TEST [1][29/47]	Time 0.1361 (0.3442)	Decoder iters 42.0 (106.2)	Tok/s 10772 (7444)
1: TEST [1][29/47]	Time 0.1360 (0.3442)	Decoder iters 41.0 (113.8)	Tok/s 10553 (7230)
1: TEST [1][39/47]	Time 0.1785 (0.3052)	Decoder iters 49.0 (95.3)	Tok/s 5820 (7333)
0: TEST [1][39/47]	Time 0.1787 (0.3052)	Decoder iters 78.0 (99.3)	Tok/s 5793 (7513)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.4278	Validation Loss: 5.0914	Test BLEU: 7.01
0: Performance: Epoch: 1	Training: 57079 Tok/s	Validation: 151912 Tok/s
0: Finished epoch 1
1: Total training time 304 s
0: Total training time 304 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 160|                      7.01|               57020.50390625|             5.073804227511088|
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003883838653564453 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "5259", "role": "default", "hostname": "a8f36d361477", "state": "SUCCEEDED", "total_run_time": 310, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "5260", "role": "default", "hostname": "a8f36d361477", "state": "SUCCEEDED", "total_run_time": 310, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "a8f36d361477", "state": "SUCCEEDED", "total_run_time": 310, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
