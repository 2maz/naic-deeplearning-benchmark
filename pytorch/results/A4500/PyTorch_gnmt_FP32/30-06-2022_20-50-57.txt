The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_r_nj4o5o/none_bv38643e
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_r_nj4o5o/none_bv38643e/attempt_0/0/error.json
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
0: thread affinity: {0, 32, 4, 36, 8, 40, 12, 44}
0: Collecting environment information...
0: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA RTX A4500
GPU 1: NVIDIA RTX A4500
GPU 2: NVIDIA RTX A4500
GPU 3: NVIDIA RTX A4500
GPU 4: NVIDIA RTX A4500
GPU 5: NVIDIA RTX A4500
GPU 6: NVIDIA RTX A4500
GPU 7: NVIDIA RTX A4500

Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1228
0: Scheduler decay interval: 154
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/922]	Time 0.515 (0.000)	Data 1.72e-01 (0.00e+00)	Tok/s 25391 (0)	Loss/tok 10.7108 (10.7108)	LR 2.047e-05
0: TRAIN [0][10/922]	Time 0.208 (0.203)	Data 1.25e-04 (1.27e-04)	Tok/s 34315 (33686)	Loss/tok 9.6671 (10.1935)	LR 2.576e-05
0: TRAIN [0][20/922]	Time 0.145 (0.236)	Data 1.25e-04 (1.24e-04)	Tok/s 29775 (34327)	Loss/tok 9.1409 (9.7946)	LR 3.244e-05
0: TRAIN [0][30/922]	Time 0.273 (0.236)	Data 2.35e-04 (1.32e-04)	Tok/s 37038 (34488)	Loss/tok 8.9309 (9.5616)	LR 4.083e-05
0: TRAIN [0][40/922]	Time 0.145 (0.230)	Data 1.25e-04 (1.28e-04)	Tok/s 30088 (34160)	Loss/tok 8.5617 (9.4013)	LR 5.141e-05
0: TRAIN [0][50/922]	Time 0.210 (0.231)	Data 1.16e-04 (1.28e-04)	Tok/s 34193 (34171)	Loss/tok 8.4794 (9.2482)	LR 6.472e-05
0: TRAIN [0][60/922]	Time 0.277 (0.232)	Data 1.13e-04 (1.26e-04)	Tok/s 36564 (34311)	Loss/tok 8.3872 (9.1128)	LR 8.148e-05
0: TRAIN [0][70/922]	Time 0.278 (0.231)	Data 1.05e-04 (1.25e-04)	Tok/s 36186 (34154)	Loss/tok 8.1151 (8.9942)	LR 1.026e-04
0: TRAIN [0][80/922]	Time 0.208 (0.232)	Data 1.20e-04 (1.23e-04)	Tok/s 34710 (34152)	Loss/tok 7.9050 (8.8763)	LR 1.291e-04
0: TRAIN [0][90/922]	Time 0.209 (0.229)	Data 1.12e-04 (1.22e-04)	Tok/s 34756 (34012)	Loss/tok 7.8598 (8.7930)	LR 1.626e-04
0: TRAIN [0][100/922]	Time 0.145 (0.228)	Data 1.09e-04 (1.21e-04)	Tok/s 29672 (33947)	Loss/tok 7.4841 (8.7082)	LR 2.047e-04
0: TRAIN [0][110/922]	Time 0.145 (0.226)	Data 1.11e-04 (1.20e-04)	Tok/s 30142 (33865)	Loss/tok 7.3957 (8.6319)	LR 2.576e-04
0: TRAIN [0][120/922]	Time 0.086 (0.223)	Data 1.15e-04 (1.20e-04)	Tok/s 25608 (33757)	Loss/tok 7.3253 (8.5678)	LR 3.244e-04
0: TRAIN [0][130/922]	Time 0.145 (0.228)	Data 1.13e-04 (1.19e-04)	Tok/s 30041 (33871)	Loss/tok 7.4346 (8.4960)	LR 4.083e-04
0: TRAIN [0][140/922]	Time 0.146 (0.229)	Data 1.19e-04 (1.19e-04)	Tok/s 29362 (33842)	Loss/tok 7.5332 (8.4425)	LR 5.141e-04
0: TRAIN [0][150/922]	Time 0.209 (0.229)	Data 1.13e-04 (1.18e-04)	Tok/s 34264 (33811)	Loss/tok 7.6421 (8.4018)	LR 6.472e-04
0: TRAIN [0][160/922]	Time 0.146 (0.230)	Data 1.44e-04 (1.18e-04)	Tok/s 29680 (33856)	Loss/tok 7.2697 (8.3554)	LR 8.148e-04
0: TRAIN [0][170/922]	Time 0.145 (0.228)	Data 1.12e-04 (1.18e-04)	Tok/s 29550 (33765)	Loss/tok 7.3473 (8.3209)	LR 1.026e-03
0: TRAIN [0][180/922]	Time 0.210 (0.227)	Data 1.09e-04 (1.18e-04)	Tok/s 34373 (33679)	Loss/tok 7.6867 (8.2887)	LR 1.291e-03
0: TRAIN [0][190/922]	Time 0.361 (0.227)	Data 1.13e-04 (1.17e-04)	Tok/s 36442 (33630)	Loss/tok 7.8665 (8.2560)	LR 1.626e-03
0: TRAIN [0][200/922]	Time 0.280 (0.228)	Data 1.06e-04 (1.17e-04)	Tok/s 36242 (33644)	Loss/tok 7.5915 (8.2232)	LR 2.000e-03
0: TRAIN [0][210/922]	Time 0.145 (0.229)	Data 1.72e-04 (1.18e-04)	Tok/s 30910 (33646)	Loss/tok 7.0055 (8.1865)	LR 2.000e-03
0: TRAIN [0][220/922]	Time 0.211 (0.228)	Data 1.23e-04 (1.18e-04)	Tok/s 33906 (33586)	Loss/tok 7.5452 (8.1549)	LR 2.000e-03
0: TRAIN [0][230/922]	Time 0.211 (0.228)	Data 1.10e-04 (1.18e-04)	Tok/s 34403 (33600)	Loss/tok 7.2439 (8.1234)	LR 2.000e-03
0: TRAIN [0][240/922]	Time 0.147 (0.227)	Data 1.12e-04 (1.18e-04)	Tok/s 29908 (33562)	Loss/tok 7.1125 (8.0918)	LR 2.000e-03
0: TRAIN [0][250/922]	Time 0.146 (0.225)	Data 1.14e-04 (1.18e-04)	Tok/s 30273 (33462)	Loss/tok 6.7057 (8.0643)	LR 2.000e-03
0: TRAIN [0][260/922]	Time 0.281 (0.225)	Data 1.14e-04 (1.18e-04)	Tok/s 35078 (33425)	Loss/tok 7.2393 (8.0330)	LR 2.000e-03
0: TRAIN [0][270/922]	Time 0.147 (0.225)	Data 1.16e-04 (1.19e-04)	Tok/s 29599 (33426)	Loss/tok 6.5902 (7.9959)	LR 2.000e-03
0: TRAIN [0][280/922]	Time 0.142 (0.225)	Data 2.08e-04 (1.19e-04)	Tok/s 29402 (33388)	Loss/tok 6.4724 (7.9623)	LR 2.000e-03
0: TRAIN [0][290/922]	Time 0.284 (0.224)	Data 1.13e-04 (1.20e-04)	Tok/s 35181 (33309)	Loss/tok 7.0886 (7.9321)	LR 2.000e-03
0: TRAIN [0][300/922]	Time 0.281 (0.224)	Data 1.17e-04 (1.20e-04)	Tok/s 35959 (33270)	Loss/tok 6.8945 (7.8992)	LR 2.000e-03
0: TRAIN [0][310/922]	Time 0.362 (0.223)	Data 2.07e-04 (1.20e-04)	Tok/s 35672 (33233)	Loss/tok 6.9965 (7.8667)	LR 2.000e-03
0: TRAIN [0][320/922]	Time 0.211 (0.224)	Data 1.16e-04 (1.21e-04)	Tok/s 33554 (33236)	Loss/tok 6.6727 (7.8323)	LR 2.000e-03
0: TRAIN [0][330/922]	Time 0.282 (0.223)	Data 1.17e-04 (1.21e-04)	Tok/s 35317 (33208)	Loss/tok 6.8648 (7.8012)	LR 2.000e-03
0: TRAIN [0][340/922]	Time 0.211 (0.222)	Data 1.21e-04 (1.21e-04)	Tok/s 33930 (33155)	Loss/tok 6.5399 (7.7727)	LR 2.000e-03
0: TRAIN [0][350/922]	Time 0.279 (0.223)	Data 1.13e-04 (1.21e-04)	Tok/s 35832 (33165)	Loss/tok 6.7509 (7.7387)	LR 2.000e-03
0: TRAIN [0][360/922]	Time 0.146 (0.221)	Data 1.19e-04 (1.21e-04)	Tok/s 30129 (33116)	Loss/tok 6.0615 (7.7098)	LR 2.000e-03
0: TRAIN [0][370/922]	Time 0.281 (0.222)	Data 1.18e-04 (1.21e-04)	Tok/s 35964 (33140)	Loss/tok 6.5835 (7.6764)	LR 2.000e-03
0: TRAIN [0][380/922]	Time 0.146 (0.222)	Data 1.16e-04 (1.22e-04)	Tok/s 30014 (33152)	Loss/tok 6.1584 (7.6442)	LR 2.000e-03
0: TRAIN [0][390/922]	Time 0.285 (0.222)	Data 1.15e-04 (1.22e-04)	Tok/s 35167 (33174)	Loss/tok 6.4879 (7.6108)	LR 2.000e-03
0: TRAIN [0][400/922]	Time 0.214 (0.222)	Data 1.21e-04 (1.22e-04)	Tok/s 34676 (33202)	Loss/tok 6.1832 (7.5777)	LR 2.000e-03
0: TRAIN [0][410/922]	Time 0.282 (0.222)	Data 1.13e-04 (1.22e-04)	Tok/s 35858 (33202)	Loss/tok 6.3947 (7.5478)	LR 2.000e-03
0: TRAIN [0][420/922]	Time 0.284 (0.222)	Data 1.13e-04 (1.23e-04)	Tok/s 35244 (33199)	Loss/tok 6.4096 (7.5180)	LR 2.000e-03
0: TRAIN [0][430/922]	Time 0.364 (0.223)	Data 1.13e-04 (1.23e-04)	Tok/s 35578 (33200)	Loss/tok 6.5152 (7.4867)	LR 2.000e-03
0: TRAIN [0][440/922]	Time 0.364 (0.223)	Data 1.17e-04 (1.23e-04)	Tok/s 35895 (33198)	Loss/tok 6.3918 (7.4565)	LR 2.000e-03
0: TRAIN [0][450/922]	Time 0.147 (0.222)	Data 1.30e-04 (1.23e-04)	Tok/s 29842 (33163)	Loss/tok 5.7627 (7.4328)	LR 2.000e-03
0: TRAIN [0][460/922]	Time 0.280 (0.222)	Data 2.54e-04 (1.23e-04)	Tok/s 36614 (33168)	Loss/tok 6.2081 (7.4029)	LR 2.000e-03
0: TRAIN [0][470/922]	Time 0.282 (0.223)	Data 1.16e-04 (1.24e-04)	Tok/s 35803 (33185)	Loss/tok 6.1305 (7.3706)	LR 2.000e-03
0: TRAIN [0][480/922]	Time 0.282 (0.224)	Data 1.39e-04 (1.24e-04)	Tok/s 35800 (33206)	Loss/tok 6.0445 (7.3381)	LR 2.000e-03
0: TRAIN [0][490/922]	Time 0.367 (0.225)	Data 1.14e-04 (1.24e-04)	Tok/s 35341 (33229)	Loss/tok 6.1491 (7.3055)	LR 2.000e-03
0: TRAIN [0][500/922]	Time 0.211 (0.225)	Data 1.13e-04 (1.25e-04)	Tok/s 33960 (33252)	Loss/tok 5.6359 (7.2751)	LR 2.000e-03
0: TRAIN [0][510/922]	Time 0.145 (0.224)	Data 2.08e-04 (1.25e-04)	Tok/s 30743 (33224)	Loss/tok 5.3758 (7.2524)	LR 2.000e-03
0: TRAIN [0][520/922]	Time 0.283 (0.224)	Data 1.19e-04 (1.25e-04)	Tok/s 35586 (33235)	Loss/tok 5.8944 (7.2241)	LR 2.000e-03
0: TRAIN [0][530/922]	Time 0.207 (0.225)	Data 2.28e-04 (1.26e-04)	Tok/s 35191 (33258)	Loss/tok 5.6797 (7.1951)	LR 2.000e-03
0: TRAIN [0][540/922]	Time 0.209 (0.224)	Data 1.27e-04 (1.26e-04)	Tok/s 33959 (33255)	Loss/tok 5.6110 (7.1691)	LR 2.000e-03
0: TRAIN [0][550/922]	Time 0.366 (0.224)	Data 1.12e-04 (1.26e-04)	Tok/s 35636 (33264)	Loss/tok 6.0497 (7.1409)	LR 2.000e-03
0: TRAIN [0][560/922]	Time 0.286 (0.224)	Data 1.20e-04 (1.26e-04)	Tok/s 35202 (33268)	Loss/tok 5.7939 (7.1130)	LR 2.000e-03
0: TRAIN [0][570/922]	Time 0.281 (0.224)	Data 1.15e-04 (1.26e-04)	Tok/s 36132 (33263)	Loss/tok 5.5817 (7.0873)	LR 2.000e-03
0: TRAIN [0][580/922]	Time 0.284 (0.224)	Data 1.17e-04 (1.26e-04)	Tok/s 35715 (33275)	Loss/tok 5.6562 (7.0598)	LR 2.000e-03
0: TRAIN [0][590/922]	Time 0.145 (0.224)	Data 2.20e-04 (1.27e-04)	Tok/s 29571 (33253)	Loss/tok 5.2322 (7.0369)	LR 2.000e-03
0: TRAIN [0][600/922]	Time 0.278 (0.224)	Data 2.04e-04 (1.27e-04)	Tok/s 36064 (33262)	Loss/tok 5.5671 (7.0108)	LR 2.000e-03
0: TRAIN [0][610/922]	Time 0.213 (0.223)	Data 1.16e-04 (1.28e-04)	Tok/s 33945 (33243)	Loss/tok 5.3077 (6.9887)	LR 2.000e-03
0: TRAIN [0][620/922]	Time 0.365 (0.224)	Data 1.13e-04 (1.28e-04)	Tok/s 36120 (33241)	Loss/tok 5.7285 (6.9627)	LR 2.000e-03
0: TRAIN [0][630/922]	Time 0.213 (0.223)	Data 1.13e-04 (1.28e-04)	Tok/s 33659 (33248)	Loss/tok 5.2249 (6.9376)	LR 2.000e-03
0: TRAIN [0][640/922]	Time 0.210 (0.223)	Data 1.15e-04 (1.28e-04)	Tok/s 33750 (33227)	Loss/tok 5.2509 (6.9156)	LR 2.000e-03
0: TRAIN [0][650/922]	Time 0.207 (0.223)	Data 2.12e-04 (1.28e-04)	Tok/s 34577 (33244)	Loss/tok 5.1016 (6.8875)	LR 2.000e-03
0: TRAIN [0][660/922]	Time 0.284 (0.223)	Data 1.13e-04 (1.28e-04)	Tok/s 35469 (33230)	Loss/tok 5.4008 (6.8648)	LR 2.000e-03
0: TRAIN [0][670/922]	Time 0.360 (0.223)	Data 1.20e-04 (1.28e-04)	Tok/s 36052 (33224)	Loss/tok 5.4859 (6.8415)	LR 2.000e-03
0: TRAIN [0][680/922]	Time 0.286 (0.223)	Data 1.24e-04 (1.28e-04)	Tok/s 35210 (33250)	Loss/tok 5.2771 (6.8128)	LR 2.000e-03
0: TRAIN [0][690/922]	Time 0.215 (0.224)	Data 1.25e-04 (1.29e-04)	Tok/s 33453 (33271)	Loss/tok 4.9748 (6.7840)	LR 2.000e-03
0: TRAIN [0][700/922]	Time 0.212 (0.224)	Data 1.40e-04 (1.29e-04)	Tok/s 34430 (33280)	Loss/tok 5.0284 (6.7600)	LR 2.000e-03
0: TRAIN [0][710/922]	Time 0.213 (0.224)	Data 1.12e-04 (1.29e-04)	Tok/s 34645 (33277)	Loss/tok 4.9746 (6.7374)	LR 2.000e-03
0: TRAIN [0][720/922]	Time 0.211 (0.224)	Data 1.14e-04 (1.29e-04)	Tok/s 33896 (33281)	Loss/tok 4.8324 (6.7144)	LR 2.000e-03
0: TRAIN [0][730/922]	Time 0.212 (0.225)	Data 1.18e-04 (1.29e-04)	Tok/s 34047 (33289)	Loss/tok 4.8511 (6.6903)	LR 2.000e-03
0: TRAIN [0][740/922]	Time 0.366 (0.225)	Data 1.12e-04 (1.29e-04)	Tok/s 35496 (33300)	Loss/tok 5.2934 (6.6662)	LR 2.000e-03
0: TRAIN [0][750/922]	Time 0.282 (0.225)	Data 2.07e-04 (1.29e-04)	Tok/s 35451 (33307)	Loss/tok 5.0578 (6.6418)	LR 2.000e-03
0: TRAIN [0][760/922]	Time 0.214 (0.225)	Data 1.15e-04 (1.30e-04)	Tok/s 34260 (33325)	Loss/tok 4.8529 (6.6169)	LR 2.000e-03
0: TRAIN [0][770/922]	Time 0.364 (0.225)	Data 1.28e-04 (1.29e-04)	Tok/s 35625 (33304)	Loss/tok 5.2226 (6.5973)	LR 2.000e-03
0: TRAIN [0][780/922]	Time 0.212 (0.225)	Data 1.28e-04 (1.29e-04)	Tok/s 33787 (33314)	Loss/tok 4.7791 (6.5745)	LR 2.000e-03
0: TRAIN [0][790/922]	Time 0.208 (0.226)	Data 1.35e-04 (1.29e-04)	Tok/s 34657 (33327)	Loss/tok 4.7864 (6.5508)	LR 2.000e-03
0: TRAIN [0][800/922]	Time 0.212 (0.226)	Data 2.09e-04 (1.30e-04)	Tok/s 33503 (33337)	Loss/tok 4.7255 (6.5280)	LR 2.000e-03
0: TRAIN [0][810/922]	Time 0.284 (0.226)	Data 1.21e-04 (1.30e-04)	Tok/s 35513 (33347)	Loss/tok 4.9783 (6.5052)	LR 2.000e-03
0: TRAIN [0][820/922]	Time 0.211 (0.226)	Data 1.18e-04 (1.30e-04)	Tok/s 34255 (33349)	Loss/tok 4.7522 (6.4843)	LR 2.000e-03
0: TRAIN [0][830/922]	Time 0.284 (0.226)	Data 1.13e-04 (1.30e-04)	Tok/s 35358 (33342)	Loss/tok 4.8232 (6.4642)	LR 2.000e-03
0: TRAIN [0][840/922]	Time 0.143 (0.227)	Data 2.10e-04 (1.30e-04)	Tok/s 29550 (33353)	Loss/tok 4.4141 (6.4423)	LR 2.000e-03
0: TRAIN [0][850/922]	Time 0.212 (0.227)	Data 1.15e-04 (1.30e-04)	Tok/s 33489 (33370)	Loss/tok 4.6156 (6.4199)	LR 2.000e-03
0: TRAIN [0][860/922]	Time 0.364 (0.227)	Data 1.18e-04 (1.30e-04)	Tok/s 35766 (33383)	Loss/tok 5.0323 (6.3990)	LR 2.000e-03
0: TRAIN [0][870/922]	Time 0.213 (0.227)	Data 1.14e-04 (1.30e-04)	Tok/s 33924 (33379)	Loss/tok 4.5268 (6.3800)	LR 2.000e-03
0: TRAIN [0][880/922]	Time 0.282 (0.227)	Data 1.11e-04 (1.30e-04)	Tok/s 35806 (33376)	Loss/tok 4.7782 (6.3611)	LR 2.000e-03
0: TRAIN [0][890/922]	Time 0.283 (0.227)	Data 1.15e-04 (1.30e-04)	Tok/s 35179 (33362)	Loss/tok 4.7073 (6.3441)	LR 2.000e-03
0: TRAIN [0][900/922]	Time 0.144 (0.227)	Data 1.15e-04 (1.30e-04)	Tok/s 30343 (33355)	Loss/tok 4.1947 (6.3261)	LR 2.000e-03
0: TRAIN [0][910/922]	Time 0.367 (0.227)	Data 1.15e-04 (1.30e-04)	Tok/s 35567 (33355)	Loss/tok 4.8741 (6.3076)	LR 2.000e-03
0: TRAIN [0][920/922]	Time 0.209 (0.227)	Data 4.29e-05 (1.32e-04)	Tok/s 34236 (33355)	Loss/tok 4.3841 (6.2894)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/160]	Time 0.100 (0.000)	Data 1.27e-03 (0.00e+00)	Tok/s 57271 (0)	Loss/tok 6.2722 (6.2722)
0: VALIDATION [0][10/160]	Time 0.046 (0.051)	Data 9.55e-04 (9.72e-04)	Tok/s 75562 (76611)	Loss/tok 5.9334 (6.0620)
0: VALIDATION [0][20/160]	Time 0.037 (0.046)	Data 9.26e-04 (9.54e-04)	Tok/s 80096 (77392)	Loss/tok 5.7775 (5.9943)
0: VALIDATION [0][30/160]	Time 0.034 (0.042)	Data 9.24e-04 (9.45e-04)	Tok/s 77889 (77848)	Loss/tok 6.0636 (5.9442)
0: VALIDATION [0][40/160]	Time 0.029 (0.039)	Data 8.98e-04 (9.37e-04)	Tok/s 80102 (78362)	Loss/tok 5.5308 (5.9065)
0: VALIDATION [0][50/160]	Time 0.027 (0.037)	Data 8.95e-04 (9.30e-04)	Tok/s 79652 (78588)	Loss/tok 5.6332 (5.8562)
0: VALIDATION [0][60/160]	Time 0.024 (0.035)	Data 8.94e-04 (9.24e-04)	Tok/s 81069 (78778)	Loss/tok 5.3327 (5.8178)
0: VALIDATION [0][70/160]	Time 0.023 (0.033)	Data 8.91e-04 (9.20e-04)	Tok/s 79104 (78652)	Loss/tok 5.3129 (5.7928)
0: VALIDATION [0][80/160]	Time 0.021 (0.032)	Data 8.86e-04 (9.17e-04)	Tok/s 76646 (78522)	Loss/tok 5.5589 (5.7651)
0: VALIDATION [0][90/160]	Time 0.018 (0.031)	Data 8.79e-04 (9.14e-04)	Tok/s 80203 (78650)	Loss/tok 5.4155 (5.7417)
0: VALIDATION [0][100/160]	Time 0.017 (0.029)	Data 8.81e-04 (9.11e-04)	Tok/s 79909 (78524)	Loss/tok 5.4771 (5.7247)
0: VALIDATION [0][110/160]	Time 0.015 (0.028)	Data 8.84e-04 (9.08e-04)	Tok/s 79640 (78434)	Loss/tok 5.4187 (5.7036)
0: VALIDATION [0][120/160]	Time 0.014 (0.027)	Data 8.93e-04 (9.06e-04)	Tok/s 75431 (78365)	Loss/tok 5.2048 (5.6874)
0: VALIDATION [0][130/160]	Time 0.012 (0.026)	Data 8.96e-04 (9.04e-04)	Tok/s 76967 (78113)	Loss/tok 5.0797 (5.6691)
0: VALIDATION [0][140/160]	Time 0.012 (0.025)	Data 8.70e-04 (9.02e-04)	Tok/s 70154 (77746)	Loss/tok 5.1023 (5.6556)
0: VALIDATION [0][150/160]	Time 0.009 (0.024)	Data 8.69e-04 (9.01e-04)	Tok/s 70287 (77246)	Loss/tok 5.0122 (5.6385)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/94]	Time 0.4739 (0.5670)	Decoder iters 149.0 (149.0)	Tok/s 8059 (8230)
0: TEST [0][19/94]	Time 0.3801 (0.4893)	Decoder iters 149.0 (149.0)	Tok/s 7121 (7861)
0: TEST [0][29/94]	Time 0.3734 (0.4504)	Decoder iters 149.0 (147.8)	Tok/s 6859 (7593)
0: TEST [0][39/94]	Time 0.3439 (0.4170)	Decoder iters 149.0 (142.8)	Tok/s 6098 (7486)
0: TEST [0][49/94]	Time 0.3295 (0.3854)	Decoder iters 149.0 (134.8)	Tok/s 5626 (7555)
0: TEST [0][59/94]	Time 0.1397 (0.3696)	Decoder iters 46.0 (133.4)	Tok/s 10504 (7357)
0: TEST [0][69/94]	Time 0.1242 (0.3483)	Decoder iters 42.0 (127.8)	Tok/s 9901 (7350)
0: TEST [0][79/94]	Time 0.1319 (0.3264)	Decoder iters 50.0 (120.9)	Tok/s 8357 (7458)
0: TEST [0][89/94]	Time 0.0798 (0.3095)	Decoder iters 27.0 (116.3)	Tok/s 9143 (7351)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 6.2872	Validation Loss: 5.6248	Test BLEU: 3.54
0: Performance: Epoch: 0	Training: 33358 Tok/s	Validation: 76079 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: Sampler for epoch 1 uses seed 3588440356
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: TRAIN [1][0/922]	Time 0.378 (0.000)	Data 1.12e-01 (0.00e+00)	Tok/s 26565 (0)	Loss/tok 4.6010 (4.6010)	LR 2.000e-03
0: TRAIN [1][10/922]	Time 0.210 (0.232)	Data 1.17e-04 (1.30e-04)	Tok/s 34574 (34023)	Loss/tok 4.2815 (4.4029)	LR 2.000e-03
0: TRAIN [1][20/922]	Time 0.210 (0.255)	Data 1.22e-04 (1.25e-04)	Tok/s 34907 (34502)	Loss/tok 4.1482 (4.4464)	LR 2.000e-03
0: TRAIN [1][30/922]	Time 0.279 (0.247)	Data 1.15e-04 (1.25e-04)	Tok/s 35888 (34421)	Loss/tok 4.3289 (4.4210)	LR 2.000e-03
0: TRAIN [1][40/922]	Time 0.209 (0.244)	Data 1.16e-04 (1.25e-04)	Tok/s 34748 (34292)	Loss/tok 4.3150 (4.4103)	LR 2.000e-03
0: TRAIN [1][50/922]	Time 0.145 (0.230)	Data 1.15e-04 (1.23e-04)	Tok/s 29765 (33705)	Loss/tok 3.9142 (4.3795)	LR 2.000e-03
0: TRAIN [1][60/922]	Time 0.144 (0.225)	Data 1.12e-04 (1.22e-04)	Tok/s 29977 (33470)	Loss/tok 3.9635 (4.3691)	LR 2.000e-03
0: TRAIN [1][70/922]	Time 0.364 (0.229)	Data 1.18e-04 (1.22e-04)	Tok/s 36189 (33605)	Loss/tok 4.6987 (4.3797)	LR 2.000e-03
0: TRAIN [1][80/922]	Time 0.360 (0.232)	Data 1.16e-04 (1.22e-04)	Tok/s 36269 (33747)	Loss/tok 4.6971 (4.3872)	LR 2.000e-03
0: TRAIN [1][90/922]	Time 0.277 (0.231)	Data 1.15e-04 (1.22e-04)	Tok/s 35818 (33758)	Loss/tok 4.3661 (4.3718)	LR 2.000e-03
0: TRAIN [1][100/922]	Time 0.282 (0.228)	Data 1.16e-04 (1.22e-04)	Tok/s 36107 (33684)	Loss/tok 4.3097 (4.3569)	LR 2.000e-03
0: TRAIN [1][110/922]	Time 0.212 (0.229)	Data 1.24e-04 (1.22e-04)	Tok/s 33582 (33700)	Loss/tok 4.2722 (4.3563)	LR 2.000e-03
0: TRAIN [1][120/922]	Time 0.281 (0.231)	Data 1.22e-04 (1.22e-04)	Tok/s 35819 (33781)	Loss/tok 4.3669 (4.3585)	LR 2.000e-03
0: TRAIN [1][130/922]	Time 0.208 (0.229)	Data 1.16e-04 (1.22e-04)	Tok/s 34621 (33703)	Loss/tok 4.1623 (4.3498)	LR 2.000e-03
0: TRAIN [1][140/922]	Time 0.211 (0.228)	Data 1.18e-04 (1.22e-04)	Tok/s 33850 (33675)	Loss/tok 4.1164 (4.3421)	LR 2.000e-03
0: TRAIN [1][150/922]	Time 0.280 (0.229)	Data 1.18e-04 (1.23e-04)	Tok/s 36160 (33652)	Loss/tok 4.4667 (4.3435)	LR 2.000e-03
0: TRAIN [1][160/922]	Time 0.144 (0.227)	Data 1.13e-04 (1.23e-04)	Tok/s 31283 (33648)	Loss/tok 3.8908 (4.3332)	LR 2.000e-03
0: TRAIN [1][170/922]	Time 0.281 (0.227)	Data 1.18e-04 (1.23e-04)	Tok/s 35884 (33618)	Loss/tok 4.3160 (4.3252)	LR 2.000e-03
0: TRAIN [1][180/922]	Time 0.145 (0.228)	Data 1.19e-04 (1.24e-04)	Tok/s 30563 (33643)	Loss/tok 3.8753 (4.3279)	LR 2.000e-03
0: TRAIN [1][190/922]	Time 0.084 (0.227)	Data 1.12e-04 (1.23e-04)	Tok/s 26013 (33571)	Loss/tok 3.5933 (4.3239)	LR 2.000e-03
0: TRAIN [1][200/922]	Time 0.146 (0.226)	Data 1.11e-04 (1.24e-04)	Tok/s 30873 (33505)	Loss/tok 3.8387 (4.3162)	LR 2.000e-03
0: TRAIN [1][210/922]	Time 0.370 (0.227)	Data 1.11e-04 (1.24e-04)	Tok/s 35477 (33579)	Loss/tok 4.5277 (4.3164)	LR 2.000e-03
0: TRAIN [1][220/922]	Time 0.365 (0.228)	Data 1.13e-04 (1.25e-04)	Tok/s 35708 (33580)	Loss/tok 4.5385 (4.3154)	LR 2.000e-03
0: TRAIN [1][230/922]	Time 0.279 (0.227)	Data 2.02e-04 (1.25e-04)	Tok/s 35642 (33511)	Loss/tok 4.2704 (4.3101)	LR 2.000e-03
0: TRAIN [1][240/922]	Time 0.280 (0.228)	Data 1.11e-04 (1.25e-04)	Tok/s 36224 (33524)	Loss/tok 4.2058 (4.3080)	LR 2.000e-03
0: TRAIN [1][250/922]	Time 0.146 (0.227)	Data 1.09e-04 (1.25e-04)	Tok/s 29759 (33511)	Loss/tok 3.7750 (4.3026)	LR 2.000e-03
0: TRAIN [1][260/922]	Time 0.212 (0.228)	Data 1.09e-04 (1.24e-04)	Tok/s 33694 (33516)	Loss/tok 4.1681 (4.2989)	LR 2.000e-03
0: TRAIN [1][270/922]	Time 0.283 (0.227)	Data 1.11e-04 (1.23e-04)	Tok/s 35426 (33499)	Loss/tok 4.4734 (4.2948)	LR 2.000e-03
0: TRAIN [1][280/922]	Time 0.211 (0.227)	Data 1.12e-04 (1.24e-04)	Tok/s 34276 (33496)	Loss/tok 4.0626 (4.2879)	LR 2.000e-03
0: TRAIN [1][290/922]	Time 0.207 (0.228)	Data 1.99e-04 (1.24e-04)	Tok/s 34628 (33535)	Loss/tok 3.9831 (4.2863)	LR 2.000e-03
0: TRAIN [1][300/922]	Time 0.281 (0.229)	Data 1.12e-04 (1.24e-04)	Tok/s 35488 (33574)	Loss/tok 4.1587 (4.2854)	LR 2.000e-03
0: TRAIN [1][310/922]	Time 0.146 (0.229)	Data 1.13e-04 (1.24e-04)	Tok/s 30202 (33549)	Loss/tok 3.7691 (4.2828)	LR 1.000e-03
0: TRAIN [1][320/922]	Time 0.213 (0.229)	Data 1.11e-04 (1.23e-04)	Tok/s 33816 (33521)	Loss/tok 3.9913 (4.2788)	LR 1.000e-03
0: TRAIN [1][330/922]	Time 0.213 (0.231)	Data 1.15e-04 (1.23e-04)	Tok/s 34173 (33576)	Loss/tok 3.9672 (4.2781)	LR 1.000e-03
0: TRAIN [1][340/922]	Time 0.280 (0.231)	Data 1.08e-04 (1.23e-04)	Tok/s 35568 (33572)	Loss/tok 4.1105 (4.2715)	LR 1.000e-03
0: TRAIN [1][350/922]	Time 0.209 (0.231)	Data 1.07e-04 (1.23e-04)	Tok/s 34949 (33595)	Loss/tok 3.9300 (4.2640)	LR 1.000e-03
0: TRAIN [1][360/922]	Time 0.281 (0.230)	Data 1.09e-04 (1.22e-04)	Tok/s 36341 (33601)	Loss/tok 4.1994 (4.2583)	LR 1.000e-03
0: TRAIN [1][370/922]	Time 0.212 (0.233)	Data 1.11e-04 (1.22e-04)	Tok/s 34111 (33638)	Loss/tok 3.9639 (4.2574)	LR 1.000e-03
0: TRAIN [1][380/922]	Time 0.209 (0.232)	Data 1.15e-04 (1.22e-04)	Tok/s 34123 (33619)	Loss/tok 3.9260 (4.2495)	LR 1.000e-03
0: TRAIN [1][390/922]	Time 0.280 (0.232)	Data 1.12e-04 (1.22e-04)	Tok/s 36313 (33645)	Loss/tok 4.1287 (4.2434)	LR 1.000e-03
0: TRAIN [1][400/922]	Time 0.280 (0.231)	Data 1.16e-04 (1.22e-04)	Tok/s 35253 (33614)	Loss/tok 4.0562 (4.2380)	LR 1.000e-03
0: TRAIN [1][410/922]	Time 0.210 (0.230)	Data 1.15e-04 (1.22e-04)	Tok/s 33968 (33601)	Loss/tok 3.7826 (4.2299)	LR 1.000e-03
0: TRAIN [1][420/922]	Time 0.147 (0.231)	Data 1.18e-04 (1.22e-04)	Tok/s 29244 (33596)	Loss/tok 3.6325 (4.2274)	LR 1.000e-03
0: TRAIN [1][430/922]	Time 0.283 (0.231)	Data 1.22e-04 (1.22e-04)	Tok/s 35283 (33609)	Loss/tok 3.9583 (4.2210)	LR 1.000e-03
0: TRAIN [1][440/922]	Time 0.209 (0.231)	Data 2.10e-04 (1.22e-04)	Tok/s 34572 (33578)	Loss/tok 3.8675 (4.2162)	LR 1.000e-03
0: TRAIN [1][450/922]	Time 0.282 (0.231)	Data 1.17e-04 (1.22e-04)	Tok/s 35584 (33584)	Loss/tok 4.0710 (4.2111)	LR 1.000e-03
0: TRAIN [1][460/922]	Time 0.285 (0.231)	Data 1.21e-04 (1.22e-04)	Tok/s 35254 (33590)	Loss/tok 4.0539 (4.2046)	LR 5.000e-04
0: TRAIN [1][470/922]	Time 0.209 (0.231)	Data 1.17e-04 (1.23e-04)	Tok/s 34929 (33588)	Loss/tok 3.6933 (4.1995)	LR 5.000e-04
0: TRAIN [1][480/922]	Time 0.208 (0.231)	Data 2.40e-04 (1.23e-04)	Tok/s 34601 (33593)	Loss/tok 3.7703 (4.1963)	LR 5.000e-04
0: TRAIN [1][490/922]	Time 0.360 (0.231)	Data 1.18e-04 (1.23e-04)	Tok/s 36101 (33550)	Loss/tok 4.1768 (4.1911)	LR 5.000e-04
0: TRAIN [1][500/922]	Time 0.282 (0.231)	Data 1.21e-04 (1.24e-04)	Tok/s 35494 (33565)	Loss/tok 3.9555 (4.1853)	LR 5.000e-04
0: TRAIN [1][510/922]	Time 0.143 (0.230)	Data 2.11e-04 (1.24e-04)	Tok/s 30646 (33555)	Loss/tok 3.5350 (4.1792)	LR 5.000e-04
0: TRAIN [1][520/922]	Time 0.281 (0.230)	Data 1.18e-04 (1.24e-04)	Tok/s 36020 (33549)	Loss/tok 3.9814 (4.1743)	LR 5.000e-04
0: TRAIN [1][530/922]	Time 0.144 (0.229)	Data 1.30e-04 (1.25e-04)	Tok/s 30716 (33512)	Loss/tok 3.5041 (4.1681)	LR 5.000e-04
0: TRAIN [1][540/922]	Time 0.145 (0.229)	Data 1.25e-04 (1.25e-04)	Tok/s 29713 (33506)	Loss/tok 3.5582 (4.1640)	LR 5.000e-04
0: TRAIN [1][550/922]	Time 0.147 (0.229)	Data 1.17e-04 (1.25e-04)	Tok/s 29217 (33498)	Loss/tok 3.5159 (4.1605)	LR 5.000e-04
0: TRAIN [1][560/922]	Time 0.284 (0.229)	Data 1.17e-04 (1.25e-04)	Tok/s 36019 (33507)	Loss/tok 4.0265 (4.1558)	LR 5.000e-04
0: TRAIN [1][570/922]	Time 0.207 (0.229)	Data 2.08e-04 (1.25e-04)	Tok/s 34572 (33518)	Loss/tok 3.8439 (4.1506)	LR 5.000e-04
0: TRAIN [1][580/922]	Time 0.146 (0.229)	Data 1.28e-04 (1.25e-04)	Tok/s 29918 (33505)	Loss/tok 3.5701 (4.1455)	LR 5.000e-04
0: TRAIN [1][590/922]	Time 0.280 (0.229)	Data 1.15e-04 (1.25e-04)	Tok/s 35915 (33483)	Loss/tok 3.9915 (4.1414)	LR 5.000e-04
0: TRAIN [1][600/922]	Time 0.283 (0.228)	Data 1.19e-04 (1.25e-04)	Tok/s 35198 (33473)	Loss/tok 3.9880 (4.1366)	LR 5.000e-04
0: TRAIN [1][610/922]	Time 0.086 (0.229)	Data 1.20e-04 (1.25e-04)	Tok/s 24850 (33468)	Loss/tok 3.4903 (4.1329)	LR 5.000e-04
0: TRAIN [1][620/922]	Time 0.211 (0.229)	Data 1.22e-04 (1.25e-04)	Tok/s 34392 (33473)	Loss/tok 3.7129 (4.1288)	LR 2.500e-04
0: TRAIN [1][630/922]	Time 0.210 (0.228)	Data 1.18e-04 (1.25e-04)	Tok/s 34394 (33470)	Loss/tok 3.5748 (4.1239)	LR 2.500e-04
0: TRAIN [1][640/922]	Time 0.282 (0.229)	Data 1.16e-04 (1.25e-04)	Tok/s 35671 (33484)	Loss/tok 3.8544 (4.1206)	LR 2.500e-04
0: TRAIN [1][650/922]	Time 0.364 (0.229)	Data 1.20e-04 (1.25e-04)	Tok/s 36292 (33479)	Loss/tok 4.1988 (4.1166)	LR 2.500e-04
0: TRAIN [1][660/922]	Time 0.084 (0.228)	Data 1.15e-04 (1.25e-04)	Tok/s 24879 (33471)	Loss/tok 3.3511 (4.1122)	LR 2.500e-04
0: TRAIN [1][670/922]	Time 0.282 (0.228)	Data 1.16e-04 (1.25e-04)	Tok/s 35371 (33470)	Loss/tok 4.0138 (4.1078)	LR 2.500e-04
0: TRAIN [1][680/922]	Time 0.282 (0.229)	Data 1.13e-04 (1.25e-04)	Tok/s 35356 (33471)	Loss/tok 3.9084 (4.1042)	LR 2.500e-04
0: TRAIN [1][690/922]	Time 0.211 (0.229)	Data 1.18e-04 (1.25e-04)	Tok/s 34208 (33491)	Loss/tok 3.5891 (4.0999)	LR 2.500e-04
0: TRAIN [1][700/922]	Time 0.366 (0.229)	Data 1.18e-04 (1.25e-04)	Tok/s 35540 (33485)	Loss/tok 4.1423 (4.0962)	LR 2.500e-04
0: TRAIN [1][710/922]	Time 0.281 (0.229)	Data 1.17e-04 (1.25e-04)	Tok/s 36018 (33496)	Loss/tok 3.8846 (4.0918)	LR 2.500e-04
0: TRAIN [1][720/922]	Time 0.145 (0.228)	Data 1.13e-04 (1.25e-04)	Tok/s 29630 (33474)	Loss/tok 3.5162 (4.0876)	LR 2.500e-04
0: TRAIN [1][730/922]	Time 0.280 (0.228)	Data 1.15e-04 (1.25e-04)	Tok/s 35812 (33478)	Loss/tok 3.8994 (4.0840)	LR 2.500e-04
0: TRAIN [1][740/922]	Time 0.282 (0.228)	Data 1.15e-04 (1.25e-04)	Tok/s 35664 (33460)	Loss/tok 3.8701 (4.0803)	LR 2.500e-04
0: TRAIN [1][750/922]	Time 0.366 (0.228)	Data 1.16e-04 (1.25e-04)	Tok/s 35575 (33468)	Loss/tok 4.0260 (4.0772)	LR 2.500e-04
0: TRAIN [1][760/922]	Time 0.144 (0.228)	Data 1.14e-04 (1.25e-04)	Tok/s 29795 (33470)	Loss/tok 3.4394 (4.0736)	LR 2.500e-04
0: TRAIN [1][770/922]	Time 0.144 (0.228)	Data 2.18e-04 (1.25e-04)	Tok/s 30901 (33467)	Loss/tok 3.5629 (4.0706)	LR 1.250e-04
0: TRAIN [1][780/922]	Time 0.144 (0.228)	Data 1.17e-04 (1.24e-04)	Tok/s 29488 (33454)	Loss/tok 3.4619 (4.0677)	LR 1.250e-04
0: TRAIN [1][790/922]	Time 0.144 (0.228)	Data 1.14e-04 (1.24e-04)	Tok/s 30107 (33452)	Loss/tok 3.3863 (4.0654)	LR 1.250e-04
0: TRAIN [1][800/922]	Time 0.279 (0.228)	Data 2.10e-04 (1.24e-04)	Tok/s 36332 (33462)	Loss/tok 3.8190 (4.0618)	LR 1.250e-04
0: TRAIN [1][810/922]	Time 0.364 (0.228)	Data 1.19e-04 (1.24e-04)	Tok/s 35848 (33464)	Loss/tok 4.0184 (4.0590)	LR 1.250e-04
0: TRAIN [1][820/922]	Time 0.284 (0.228)	Data 1.14e-04 (1.24e-04)	Tok/s 34862 (33473)	Loss/tok 3.8508 (4.0562)	LR 1.250e-04
0: TRAIN [1][830/922]	Time 0.145 (0.228)	Data 1.16e-04 (1.24e-04)	Tok/s 29483 (33454)	Loss/tok 3.3444 (4.0524)	LR 1.250e-04
0: TRAIN [1][840/922]	Time 0.281 (0.227)	Data 1.16e-04 (1.24e-04)	Tok/s 35318 (33433)	Loss/tok 3.9051 (4.0497)	LR 1.250e-04
0: TRAIN [1][850/922]	Time 0.145 (0.227)	Data 1.16e-04 (1.24e-04)	Tok/s 29621 (33417)	Loss/tok 3.4162 (4.0472)	LR 1.250e-04
0: TRAIN [1][860/922]	Time 0.208 (0.227)	Data 1.21e-04 (1.24e-04)	Tok/s 34794 (33409)	Loss/tok 3.7742 (4.0433)	LR 1.250e-04
0: TRAIN [1][870/922]	Time 0.281 (0.226)	Data 1.13e-04 (1.24e-04)	Tok/s 35702 (33413)	Loss/tok 3.9249 (4.0404)	LR 1.250e-04
0: TRAIN [1][880/922]	Time 0.360 (0.227)	Data 1.23e-04 (1.24e-04)	Tok/s 36342 (33403)	Loss/tok 4.1036 (4.0383)	LR 1.250e-04
0: TRAIN [1][890/922]	Time 0.209 (0.226)	Data 1.18e-04 (1.24e-04)	Tok/s 34266 (33403)	Loss/tok 3.6446 (4.0354)	LR 1.250e-04
0: TRAIN [1][900/922]	Time 0.145 (0.226)	Data 1.18e-04 (1.24e-04)	Tok/s 29757 (33400)	Loss/tok 3.5072 (4.0331)	LR 1.250e-04
0: TRAIN [1][910/922]	Time 0.212 (0.226)	Data 1.17e-04 (1.24e-04)	Tok/s 34524 (33397)	Loss/tok 3.6797 (4.0297)	LR 1.250e-04
0: TRAIN [1][920/922]	Time 0.363 (0.227)	Data 7.63e-05 (1.26e-04)	Tok/s 36097 (33406)	Loss/tok 3.9828 (4.0279)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/160]	Time 0.101 (0.000)	Data 1.19e-03 (0.00e+00)	Tok/s 56815 (0)	Loss/tok 5.4885 (5.4885)
0: VALIDATION [1][10/160]	Time 0.046 (0.052)	Data 9.51e-04 (9.73e-04)	Tok/s 74482 (75656)	Loss/tok 5.0534 (5.2132)
0: VALIDATION [1][20/160]	Time 0.037 (0.046)	Data 9.34e-04 (9.52e-04)	Tok/s 79096 (76363)	Loss/tok 4.9456 (5.1417)
0: VALIDATION [1][30/160]	Time 0.034 (0.043)	Data 9.17e-04 (9.43e-04)	Tok/s 76942 (76885)	Loss/tok 5.3575 (5.1008)
0: VALIDATION [1][40/160]	Time 0.030 (0.040)	Data 9.05e-04 (9.36e-04)	Tok/s 78809 (77401)	Loss/tok 4.7493 (5.0702)
0: VALIDATION [1][50/160]	Time 0.027 (0.037)	Data 8.92e-04 (9.30e-04)	Tok/s 78740 (77597)	Loss/tok 4.9110 (5.0290)
0: VALIDATION [1][60/160]	Time 0.024 (0.035)	Data 9.01e-04 (9.26e-04)	Tok/s 80065 (77806)	Loss/tok 4.5283 (4.9972)
0: VALIDATION [1][70/160]	Time 0.023 (0.034)	Data 8.95e-04 (9.22e-04)	Tok/s 78309 (77668)	Loss/tok 4.4269 (4.9767)
0: VALIDATION [1][80/160]	Time 0.022 (0.032)	Data 9.11e-04 (9.19e-04)	Tok/s 75401 (77541)	Loss/tok 4.8087 (4.9528)
0: VALIDATION [1][90/160]	Time 0.019 (0.031)	Data 8.90e-04 (9.16e-04)	Tok/s 78652 (77638)	Loss/tok 4.6111 (4.9351)
0: VALIDATION [1][100/160]	Time 0.017 (0.030)	Data 9.04e-04 (9.13e-04)	Tok/s 78928 (77519)	Loss/tok 4.8224 (4.9222)
0: VALIDATION [1][110/160]	Time 0.015 (0.029)	Data 8.79e-04 (9.11e-04)	Tok/s 78671 (77455)	Loss/tok 4.6279 (4.9033)
0: VALIDATION [1][120/160]	Time 0.015 (0.027)	Data 8.94e-04 (9.09e-04)	Tok/s 73860 (77376)	Loss/tok 4.4830 (4.8902)
0: VALIDATION [1][130/160]	Time 0.013 (0.026)	Data 8.92e-04 (9.07e-04)	Tok/s 76142 (77128)	Loss/tok 4.4103 (4.8753)
0: VALIDATION [1][140/160]	Time 0.012 (0.025)	Data 8.82e-04 (9.06e-04)	Tok/s 69144 (76777)	Loss/tok 4.2994 (4.8636)
0: VALIDATION [1][150/160]	Time 0.009 (0.024)	Data 8.81e-04 (9.04e-04)	Tok/s 69437 (76316)	Loss/tok 4.4332 (4.8491)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/94]	Time 0.4086 (0.4461)	Decoder iters 149.0 (141.9)	Tok/s 7885 (8409)
0: TEST [1][19/94]	Time 0.2188 (0.3845)	Decoder iters 62.0 (128.2)	Tok/s 11590 (8863)
0: TEST [1][29/94]	Time 0.2222 (0.3465)	Decoder iters 73.0 (118.2)	Tok/s 10039 (9093)
0: TEST [1][39/94]	Time 0.1843 (0.3124)	Decoder iters 62.0 (106.9)	Tok/s 10568 (9435)
0: TEST [1][49/94]	Time 0.1404 (0.2922)	Decoder iters 42.0 (101.6)	Tok/s 11981 (9469)
0: TEST [1][59/94]	Time 0.1200 (0.2799)	Decoder iters 37.0 (99.9)	Tok/s 12582 (9357)
0: TEST [1][69/94]	Time 0.1109 (0.2618)	Decoder iters 37.0 (94.1)	Tok/s 11081 (9491)
0: TEST [1][79/94]	Time 0.0868 (0.2451)	Decoder iters 28.0 (88.5)	Tok/s 12041 (9564)
0: TEST [1][89/94]	Time 0.0834 (0.2268)	Decoder iters 32.0 (81.8)	Tok/s 8585 (9700)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 4.0275	Validation Loss: 4.8384	Test BLEU: 8.19
0: Performance: Epoch: 1	Training: 33406 Tok/s	Validation: 75166 Tok/s
0: Finished epoch 1
0: Total training time 504 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 160|                      8.19|             33382.1228571669|             8.404330511887869|
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00046443939208984375 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "5266", "role": "default", "hostname": "92ebbc27b388", "state": "SUCCEEDED", "total_run_time": 510, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [1]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "92ebbc27b388", "state": "SUCCEEDED", "total_run_time": 510, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\"}", "agent_restarts": 0}}
DONE!
