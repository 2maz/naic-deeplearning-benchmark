The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_ipsr0cya/none_qz89cv0y
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_ipsr0cya/none_qz89cv0y/attempt_0/0/error.json
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
0: thread affinity: {0, 32, 4, 36, 8, 40, 12, 44}
0: Collecting environment information...
0: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA RTX A4500
GPU 1: NVIDIA RTX A4500
GPU 2: NVIDIA RTX A4500
GPU 3: NVIDIA RTX A4500
GPU 4: NVIDIA RTX A4500
GPU 5: NVIDIA RTX A4500
GPU 6: NVIDIA RTX A4500
GPU 7: NVIDIA RTX A4500

Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 756
0: Scheduler decay interval: 95
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/workspace/examples/gnmt/seq2seq/train/fp_optimizers.py:235: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.
  clip_grad_norm_(amp.master_params(optimizer), self.grad_clip)
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/568]	Time 0.297 (0.000)	Data 1.83e-01 (0.00e+00)	Tok/s 24045 (0)	Loss/tok 10.5003 (10.5003)	LR 2.047e-05
0: TRAIN [0][10/568]	Time 0.242 (0.231)	Data 1.38e-04 (1.46e-04)	Tok/s 67229 (65943)	Loss/tok 9.6835 (10.0835)	LR 2.576e-05
0: TRAIN [0][20/568]	Time 0.185 (0.194)	Data 1.33e-04 (1.45e-04)	Tok/s 63079 (62727)	Loss/tok 9.1923 (9.8148)	LR 3.244e-05
0: TRAIN [0][30/568]	Time 0.244 (0.190)	Data 1.26e-04 (1.44e-04)	Tok/s 66666 (61523)	Loss/tok 9.0290 (9.5918)	LR 4.083e-05
0: TRAIN [0][40/568]	Time 0.245 (0.190)	Data 1.30e-04 (1.42e-04)	Tok/s 66972 (61399)	Loss/tok 8.7334 (9.4024)	LR 5.141e-05
0: TRAIN [0][50/568]	Time 0.187 (0.193)	Data 1.37e-04 (1.44e-04)	Tok/s 62405 (61796)	Loss/tok 8.4488 (9.2350)	LR 6.472e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][60/568]	Time 0.315 (0.199)	Data 1.32e-04 (1.47e-04)	Tok/s 67759 (62482)	Loss/tok 8.7494 (9.0989)	LR 8.148e-05
0: TRAIN [0][70/568]	Time 0.244 (0.199)	Data 1.39e-04 (1.47e-04)	Tok/s 66858 (62450)	Loss/tok 8.2109 (8.9711)	LR 1.026e-04
0: TRAIN [0][80/568]	Time 0.128 (0.201)	Data 1.37e-04 (1.46e-04)	Tok/s 55169 (62575)	Loss/tok 7.7394 (8.8491)	LR 1.291e-04
0: TRAIN [0][90/568]	Time 0.246 (0.201)	Data 1.39e-04 (1.46e-04)	Tok/s 65378 (62549)	Loss/tok 7.8452 (8.7490)	LR 1.626e-04
0: TRAIN [0][100/568]	Time 0.314 (0.200)	Data 1.38e-04 (1.47e-04)	Tok/s 67324 (62451)	Loss/tok 7.9634 (8.6619)	LR 2.047e-04
0: TRAIN [0][110/568]	Time 0.188 (0.200)	Data 1.43e-04 (1.46e-04)	Tok/s 63005 (62319)	Loss/tok 7.6640 (8.5825)	LR 2.576e-04
0: TRAIN [0][120/568]	Time 0.129 (0.200)	Data 1.44e-04 (1.46e-04)	Tok/s 54188 (62294)	Loss/tok 7.5512 (8.5141)	LR 3.244e-04
0: TRAIN [0][130/568]	Time 0.129 (0.202)	Data 1.34e-04 (1.46e-04)	Tok/s 54294 (62264)	Loss/tok 7.3409 (8.4511)	LR 4.083e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][140/568]	Time 0.314 (0.203)	Data 1.41e-04 (1.47e-04)	Tok/s 67165 (62297)	Loss/tok 7.8597 (8.4048)	LR 5.141e-04
0: TRAIN [0][150/568]	Time 0.315 (0.206)	Data 1.46e-04 (1.47e-04)	Tok/s 67369 (62462)	Loss/tok 7.8447 (8.3580)	LR 6.472e-04
0: TRAIN [0][160/568]	Time 0.188 (0.204)	Data 2.56e-04 (1.48e-04)	Tok/s 62053 (62203)	Loss/tok 7.6181 (8.3186)	LR 8.148e-04
0: TRAIN [0][170/568]	Time 0.129 (0.203)	Data 1.36e-04 (1.48e-04)	Tok/s 54332 (62087)	Loss/tok 7.0521 (8.2807)	LR 1.026e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][180/568]	Time 0.188 (0.203)	Data 1.42e-04 (1.49e-04)	Tok/s 63382 (62162)	Loss/tok 7.5166 (8.2446)	LR 1.291e-03
0: TRAIN [0][190/568]	Time 0.132 (0.201)	Data 1.41e-04 (1.49e-04)	Tok/s 53376 (61991)	Loss/tok 7.2725 (8.2099)	LR 1.626e-03
0: TRAIN [0][200/568]	Time 0.130 (0.203)	Data 1.46e-04 (1.48e-04)	Tok/s 53900 (62074)	Loss/tok 7.1795 (8.1717)	LR 2.000e-03
0: TRAIN [0][210/568]	Time 0.250 (0.204)	Data 1.44e-04 (1.48e-04)	Tok/s 65235 (62182)	Loss/tok 7.3081 (8.1289)	LR 2.000e-03
0: TRAIN [0][220/568]	Time 0.247 (0.203)	Data 1.35e-04 (1.48e-04)	Tok/s 65941 (62075)	Loss/tok 7.2253 (8.0902)	LR 2.000e-03
0: TRAIN [0][230/568]	Time 0.189 (0.203)	Data 1.38e-04 (1.47e-04)	Tok/s 62114 (62094)	Loss/tok 6.9842 (8.0476)	LR 2.000e-03
0: TRAIN [0][240/568]	Time 0.318 (0.204)	Data 1.49e-04 (1.47e-04)	Tok/s 66717 (62104)	Loss/tok 7.2179 (8.0018)	LR 2.000e-03
0: TRAIN [0][250/568]	Time 0.130 (0.203)	Data 1.30e-04 (1.47e-04)	Tok/s 55409 (61953)	Loss/tok 6.5625 (7.9657)	LR 2.000e-03
0: TRAIN [0][260/568]	Time 0.186 (0.204)	Data 1.40e-04 (1.47e-04)	Tok/s 61862 (62034)	Loss/tok 6.6153 (7.9190)	LR 2.000e-03
0: TRAIN [0][270/568]	Time 0.189 (0.203)	Data 1.29e-04 (1.47e-04)	Tok/s 61983 (61854)	Loss/tok 6.7908 (7.8843)	LR 2.000e-03
0: TRAIN [0][280/568]	Time 0.317 (0.203)	Data 1.50e-04 (1.47e-04)	Tok/s 66973 (61850)	Loss/tok 6.8687 (7.8410)	LR 2.000e-03
0: TRAIN [0][290/568]	Time 0.187 (0.202)	Data 1.42e-04 (1.47e-04)	Tok/s 62419 (61775)	Loss/tok 6.4500 (7.8030)	LR 2.000e-03
0: TRAIN [0][300/568]	Time 0.316 (0.202)	Data 1.41e-04 (1.47e-04)	Tok/s 66906 (61745)	Loss/tok 6.7251 (7.7615)	LR 2.000e-03
0: TRAIN [0][310/568]	Time 0.246 (0.202)	Data 1.37e-04 (1.47e-04)	Tok/s 65875 (61681)	Loss/tok 6.5685 (7.7230)	LR 2.000e-03
0: TRAIN [0][320/568]	Time 0.317 (0.202)	Data 1.39e-04 (1.47e-04)	Tok/s 66304 (61699)	Loss/tok 6.5283 (7.6790)	LR 2.000e-03
0: TRAIN [0][330/568]	Time 0.136 (0.202)	Data 1.44e-04 (1.47e-04)	Tok/s 52777 (61689)	Loss/tok 5.7730 (7.6359)	LR 2.000e-03
0: TRAIN [0][340/568]	Time 0.127 (0.202)	Data 1.39e-04 (1.47e-04)	Tok/s 56628 (61704)	Loss/tok 5.8313 (7.5935)	LR 2.000e-03
0: TRAIN [0][350/568]	Time 0.124 (0.201)	Data 1.37e-04 (1.47e-04)	Tok/s 55565 (61570)	Loss/tok 5.6772 (7.5610)	LR 2.000e-03
0: TRAIN [0][360/568]	Time 0.188 (0.201)	Data 1.36e-04 (1.47e-04)	Tok/s 61900 (61549)	Loss/tok 6.0151 (7.5216)	LR 2.000e-03
0: TRAIN [0][370/568]	Time 0.245 (0.201)	Data 1.35e-04 (1.47e-04)	Tok/s 66175 (61466)	Loss/tok 6.1766 (7.4872)	LR 2.000e-03
0: TRAIN [0][380/568]	Time 0.314 (0.201)	Data 1.41e-04 (1.46e-04)	Tok/s 67543 (61485)	Loss/tok 6.1708 (7.4449)	LR 2.000e-03
0: TRAIN [0][390/568]	Time 0.190 (0.201)	Data 2.03e-04 (1.47e-04)	Tok/s 61961 (61474)	Loss/tok 5.7956 (7.4068)	LR 2.000e-03
0: TRAIN [0][400/568]	Time 0.130 (0.201)	Data 1.34e-04 (1.46e-04)	Tok/s 54205 (61434)	Loss/tok 5.4032 (7.3718)	LR 2.000e-03
0: TRAIN [0][410/568]	Time 0.249 (0.201)	Data 1.57e-04 (1.46e-04)	Tok/s 65247 (61482)	Loss/tok 5.8221 (7.3300)	LR 2.000e-03
0: TRAIN [0][420/568]	Time 0.127 (0.202)	Data 1.32e-04 (1.46e-04)	Tok/s 54465 (61490)	Loss/tok 5.3106 (7.2909)	LR 2.000e-03
0: TRAIN [0][430/568]	Time 0.246 (0.202)	Data 1.85e-04 (1.46e-04)	Tok/s 66526 (61520)	Loss/tok 5.7734 (7.2532)	LR 2.000e-03
0: TRAIN [0][440/568]	Time 0.187 (0.201)	Data 1.39e-04 (1.46e-04)	Tok/s 62081 (61402)	Loss/tok 5.5311 (7.2269)	LR 2.000e-03
0: TRAIN [0][450/568]	Time 0.127 (0.200)	Data 1.35e-04 (1.46e-04)	Tok/s 54918 (61391)	Loss/tok 5.0494 (7.1918)	LR 2.000e-03
0: TRAIN [0][460/568]	Time 0.191 (0.200)	Data 1.41e-04 (1.46e-04)	Tok/s 61585 (61387)	Loss/tok 5.3517 (7.1571)	LR 2.000e-03
0: TRAIN [0][470/568]	Time 0.190 (0.201)	Data 1.49e-04 (1.45e-04)	Tok/s 61990 (61425)	Loss/tok 5.3183 (7.1190)	LR 2.000e-03
0: TRAIN [0][480/568]	Time 0.189 (0.200)	Data 1.34e-04 (1.45e-04)	Tok/s 62720 (61401)	Loss/tok 5.3079 (7.0869)	LR 2.000e-03
0: TRAIN [0][490/568]	Time 0.316 (0.200)	Data 1.44e-04 (1.45e-04)	Tok/s 67451 (61384)	Loss/tok 5.6473 (7.0541)	LR 2.000e-03
0: TRAIN [0][500/568]	Time 0.247 (0.200)	Data 1.29e-04 (1.45e-04)	Tok/s 66843 (61393)	Loss/tok 5.5016 (7.0191)	LR 2.000e-03
0: TRAIN [0][510/568]	Time 0.129 (0.200)	Data 1.45e-04 (1.45e-04)	Tok/s 54588 (61389)	Loss/tok 4.7406 (6.9848)	LR 2.000e-03
0: TRAIN [0][520/568]	Time 0.249 (0.200)	Data 1.33e-04 (1.45e-04)	Tok/s 65462 (61355)	Loss/tok 5.3963 (6.9527)	LR 2.000e-03
0: TRAIN [0][530/568]	Time 0.130 (0.201)	Data 1.36e-04 (1.45e-04)	Tok/s 55446 (61410)	Loss/tok 4.6923 (6.9135)	LR 2.000e-03
0: TRAIN [0][540/568]	Time 0.188 (0.201)	Data 1.34e-04 (1.45e-04)	Tok/s 62260 (61400)	Loss/tok 4.8704 (6.8803)	LR 2.000e-03
0: TRAIN [0][550/568]	Time 0.245 (0.200)	Data 1.39e-04 (1.44e-04)	Tok/s 66903 (61377)	Loss/tok 5.1857 (6.8505)	LR 2.000e-03
0: TRAIN [0][560/568]	Time 0.189 (0.200)	Data 1.37e-04 (1.44e-04)	Tok/s 62011 (61373)	Loss/tok 4.8993 (6.8189)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/160]	Time 0.062 (0.000)	Data 1.24e-03 (0.00e+00)	Tok/s 92675 (0)	Loss/tok 6.5527 (6.5527)
0: VALIDATION [0][10/160]	Time 0.029 (0.032)	Data 9.50e-04 (9.75e-04)	Tok/s 120271 (121136)	Loss/tok 6.1993 (6.3616)
0: VALIDATION [0][20/160]	Time 0.023 (0.029)	Data 9.16e-04 (9.54e-04)	Tok/s 128138 (123052)	Loss/tok 6.1658 (6.3083)
0: VALIDATION [0][30/160]	Time 0.021 (0.027)	Data 9.17e-04 (9.42e-04)	Tok/s 122034 (123416)	Loss/tok 6.3696 (6.2642)
0: VALIDATION [0][40/160]	Time 0.019 (0.025)	Data 8.89e-04 (9.33e-04)	Tok/s 125930 (123879)	Loss/tok 5.8872 (6.2239)
0: VALIDATION [0][50/160]	Time 0.017 (0.023)	Data 8.77e-04 (9.25e-04)	Tok/s 125815 (124229)	Loss/tok 6.0051 (6.1806)
0: VALIDATION [0][60/160]	Time 0.015 (0.022)	Data 8.75e-04 (9.19e-04)	Tok/s 127534 (124314)	Loss/tok 5.7270 (6.1419)
0: VALIDATION [0][70/160]	Time 0.015 (0.021)	Data 8.82e-04 (9.14e-04)	Tok/s 123550 (124060)	Loss/tok 5.6232 (6.1159)
0: VALIDATION [0][80/160]	Time 0.014 (0.020)	Data 8.84e-04 (9.10e-04)	Tok/s 120260 (123779)	Loss/tok 5.9353 (6.0901)
0: VALIDATION [0][90/160]	Time 0.012 (0.019)	Data 8.69e-04 (9.07e-04)	Tok/s 124398 (123615)	Loss/tok 5.7555 (6.0676)
0: VALIDATION [0][100/160]	Time 0.011 (0.019)	Data 8.67e-04 (9.03e-04)	Tok/s 121094 (123085)	Loss/tok 5.7921 (6.0506)
0: VALIDATION [0][110/160]	Time 0.010 (0.018)	Data 8.60e-04 (9.00e-04)	Tok/s 119362 (122669)	Loss/tok 5.7110 (6.0294)
0: VALIDATION [0][120/160]	Time 0.010 (0.017)	Data 8.77e-04 (8.98e-04)	Tok/s 111435 (122076)	Loss/tok 5.6612 (6.0130)
0: VALIDATION [0][130/160]	Time 0.013 (0.017)	Data 8.68e-04 (8.96e-04)	Tok/s 70991 (120994)	Loss/tok 5.3180 (5.9937)
0: VALIDATION [0][140/160]	Time 0.008 (0.016)	Data 8.76e-04 (8.94e-04)	Tok/s 105506 (120161)	Loss/tok 5.3600 (5.9790)
0: VALIDATION [0][150/160]	Time 0.006 (0.015)	Data 8.86e-04 (8.92e-04)	Tok/s 102718 (119051)	Loss/tok 5.3552 (5.9621)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/94]	Time 0.3217 (0.3376)	Decoder iters 149.0 (149.0)	Tok/s 11982 (11906)
0: TEST [0][19/94]	Time 0.2941 (0.3184)	Decoder iters 149.0 (149.0)	Tok/s 8779 (11060)
0: TEST [0][29/94]	Time 0.2908 (0.3086)	Decoder iters 149.0 (149.0)	Tok/s 9799 (10468)
0: TEST [0][39/94]	Time 0.2763 (0.3013)	Decoder iters 149.0 (149.0)	Tok/s 7017 (9885)
0: TEST [0][49/94]	Time 0.2700 (0.2932)	Decoder iters 149.0 (147.4)	Tok/s 6133 (9517)
0: TEST [0][59/94]	Time 0.2608 (0.2859)	Decoder iters 149.0 (145.8)	Tok/s 6116 (9263)
0: TEST [0][69/94]	Time 0.2705 (0.2785)	Decoder iters 149.0 (143.4)	Tok/s 5842 (8996)
0: TEST [0][79/94]	Time 0.2583 (0.2705)	Decoder iters 149.0 (140.5)	Tok/s 4402 (8738)
0: TEST [0][89/94]	Time 0.0828 (0.2576)	Decoder iters 42.0 (134.4)	Tok/s 8957 (8642)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 6.7951	Validation Loss: 5.9481	Test BLEU: 2.57
0: Performance: Epoch: 0	Training: 61389 Tok/s	Validation: 117264 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: Sampler for epoch 1 uses seed 3588440356
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/568]	Time 0.430 (0.000)	Data 1.34e-01 (0.00e+00)	Tok/s 48958 (0)	Loss/tok 5.2286 (5.2286)	LR 2.000e-03
0: TRAIN [1][10/568]	Time 0.129 (0.177)	Data 1.36e-04 (1.64e-04)	Tok/s 53848 (59432)	Loss/tok 4.2814 (4.9979)	LR 2.000e-03
0: TRAIN [1][20/568]	Time 0.076 (0.185)	Data 1.55e-04 (1.59e-04)	Tok/s 46443 (60616)	Loss/tok 4.0609 (4.8951)	LR 2.000e-03
0: TRAIN [1][30/568]	Time 0.128 (0.196)	Data 1.42e-04 (1.58e-04)	Tok/s 55439 (61658)	Loss/tok 4.2906 (4.8633)	LR 2.000e-03
0: TRAIN [1][40/568]	Time 0.185 (0.196)	Data 1.35e-04 (1.55e-04)	Tok/s 63126 (61479)	Loss/tok 4.5215 (4.8253)	LR 2.000e-03
0: TRAIN [1][50/568]	Time 0.247 (0.201)	Data 1.46e-04 (1.53e-04)	Tok/s 65845 (61479)	Loss/tok 4.8068 (4.8341)	LR 2.000e-03
0: TRAIN [1][60/568]	Time 0.129 (0.198)	Data 1.34e-04 (1.51e-04)	Tok/s 54278 (61425)	Loss/tok 4.1950 (4.7944)	LR 2.000e-03
0: TRAIN [1][70/568]	Time 0.187 (0.197)	Data 1.32e-04 (1.51e-04)	Tok/s 63028 (61502)	Loss/tok 4.4384 (4.7649)	LR 2.000e-03
0: TRAIN [1][80/568]	Time 0.248 (0.198)	Data 1.34e-04 (1.51e-04)	Tok/s 65995 (61695)	Loss/tok 4.7174 (4.7434)	LR 2.000e-03
0: TRAIN [1][90/568]	Time 0.189 (0.197)	Data 1.31e-04 (1.50e-04)	Tok/s 61863 (61601)	Loss/tok 4.4387 (4.7235)	LR 2.000e-03
0: TRAIN [1][100/568]	Time 0.187 (0.195)	Data 1.31e-04 (1.49e-04)	Tok/s 61557 (61485)	Loss/tok 4.3817 (4.7007)	LR 2.000e-03
0: TRAIN [1][110/568]	Time 0.128 (0.194)	Data 1.33e-04 (1.48e-04)	Tok/s 56426 (61476)	Loss/tok 4.0989 (4.6848)	LR 2.000e-03
0: TRAIN [1][120/568]	Time 0.247 (0.195)	Data 1.37e-04 (1.47e-04)	Tok/s 65961 (61493)	Loss/tok 4.7551 (4.6754)	LR 2.000e-03
0: TRAIN [1][130/568]	Time 0.129 (0.196)	Data 1.35e-04 (1.46e-04)	Tok/s 54302 (61476)	Loss/tok 4.0384 (4.6660)	LR 2.000e-03
0: TRAIN [1][140/568]	Time 0.243 (0.194)	Data 1.33e-04 (1.46e-04)	Tok/s 67157 (61319)	Loss/tok 4.6614 (4.6556)	LR 2.000e-03
0: TRAIN [1][150/568]	Time 0.317 (0.197)	Data 1.31e-04 (1.46e-04)	Tok/s 67324 (61500)	Loss/tok 4.7995 (4.6520)	LR 2.000e-03
0: TRAIN [1][160/568]	Time 0.127 (0.196)	Data 1.32e-04 (1.46e-04)	Tok/s 54708 (61420)	Loss/tok 4.0203 (4.6348)	LR 2.000e-03
0: TRAIN [1][170/568]	Time 0.249 (0.197)	Data 1.44e-04 (1.45e-04)	Tok/s 66344 (61458)	Loss/tok 4.5507 (4.6237)	LR 2.000e-03
0: TRAIN [1][180/568]	Time 0.318 (0.195)	Data 1.42e-04 (1.45e-04)	Tok/s 66387 (61331)	Loss/tok 4.7037 (4.6108)	LR 2.000e-03
0: TRAIN [1][190/568]	Time 0.251 (0.195)	Data 1.37e-04 (1.45e-04)	Tok/s 66096 (61340)	Loss/tok 4.5432 (4.5977)	LR 1.000e-03
0: TRAIN [1][200/568]	Time 0.076 (0.196)	Data 1.37e-04 (1.46e-04)	Tok/s 45547 (61297)	Loss/tok 3.5859 (4.5893)	LR 1.000e-03
0: TRAIN [1][210/568]	Time 0.128 (0.198)	Data 1.39e-04 (1.45e-04)	Tok/s 54517 (61415)	Loss/tok 3.9365 (4.5826)	LR 1.000e-03
0: TRAIN [1][220/568]	Time 0.316 (0.198)	Data 1.34e-04 (1.46e-04)	Tok/s 67323 (61412)	Loss/tok 4.5590 (4.5669)	LR 1.000e-03
0: TRAIN [1][230/568]	Time 0.186 (0.198)	Data 1.35e-04 (1.46e-04)	Tok/s 62989 (61449)	Loss/tok 4.0840 (4.5532)	LR 1.000e-03
0: TRAIN [1][240/568]	Time 0.247 (0.198)	Data 1.36e-04 (1.45e-04)	Tok/s 66819 (61441)	Loss/tok 4.3579 (4.5395)	LR 1.000e-03
0: TRAIN [1][250/568]	Time 0.246 (0.198)	Data 1.33e-04 (1.45e-04)	Tok/s 65703 (61447)	Loss/tok 4.2332 (4.5247)	LR 1.000e-03
0: TRAIN [1][260/568]	Time 0.246 (0.197)	Data 1.39e-04 (1.44e-04)	Tok/s 66209 (61326)	Loss/tok 4.2743 (4.5117)	LR 1.000e-03
0: TRAIN [1][270/568]	Time 0.246 (0.197)	Data 1.33e-04 (1.44e-04)	Tok/s 66666 (61325)	Loss/tok 4.3503 (4.5003)	LR 1.000e-03
0: TRAIN [1][280/568]	Time 0.128 (0.196)	Data 1.36e-04 (1.44e-04)	Tok/s 55397 (61240)	Loss/tok 3.7591 (4.4867)	LR 1.000e-03
0: TRAIN [1][290/568]	Time 0.188 (0.197)	Data 1.38e-04 (1.44e-04)	Tok/s 61675 (61287)	Loss/tok 4.0700 (4.4767)	LR 5.000e-04
0: TRAIN [1][300/568]	Time 0.185 (0.198)	Data 2.51e-04 (1.45e-04)	Tok/s 62746 (61369)	Loss/tok 4.0452 (4.4665)	LR 5.000e-04
0: TRAIN [1][310/568]	Time 0.075 (0.197)	Data 1.42e-04 (1.44e-04)	Tok/s 46348 (61287)	Loss/tok 3.4694 (4.4570)	LR 5.000e-04
0: TRAIN [1][320/568]	Time 0.130 (0.198)	Data 1.49e-04 (1.45e-04)	Tok/s 55972 (61288)	Loss/tok 3.6685 (4.4490)	LR 5.000e-04
0: TRAIN [1][330/568]	Time 0.316 (0.197)	Data 1.37e-04 (1.44e-04)	Tok/s 67130 (61165)	Loss/tok 4.4310 (4.4390)	LR 5.000e-04
0: TRAIN [1][340/568]	Time 0.317 (0.197)	Data 1.38e-04 (1.44e-04)	Tok/s 66738 (61166)	Loss/tok 4.3816 (4.4281)	LR 5.000e-04
0: TRAIN [1][350/568]	Time 0.129 (0.197)	Data 1.51e-04 (1.44e-04)	Tok/s 53959 (61158)	Loss/tok 3.5607 (4.4184)	LR 5.000e-04
0: TRAIN [1][360/568]	Time 0.130 (0.197)	Data 1.37e-04 (1.45e-04)	Tok/s 54030 (61163)	Loss/tok 3.7097 (4.4072)	LR 5.000e-04
0: TRAIN [1][370/568]	Time 0.191 (0.197)	Data 1.34e-04 (1.44e-04)	Tok/s 61655 (61226)	Loss/tok 3.8248 (4.3983)	LR 5.000e-04
0: TRAIN [1][380/568]	Time 0.190 (0.197)	Data 1.32e-04 (1.44e-04)	Tok/s 62676 (61223)	Loss/tok 3.9213 (4.3876)	LR 2.500e-04
0: TRAIN [1][390/568]	Time 0.190 (0.198)	Data 1.45e-04 (1.45e-04)	Tok/s 61449 (61298)	Loss/tok 3.8131 (4.3779)	LR 2.500e-04
0: TRAIN [1][400/568]	Time 0.128 (0.198)	Data 1.37e-04 (1.45e-04)	Tok/s 54767 (61299)	Loss/tok 3.5563 (4.3688)	LR 2.500e-04
0: TRAIN [1][410/568]	Time 0.191 (0.199)	Data 1.41e-04 (1.45e-04)	Tok/s 61055 (61339)	Loss/tok 3.9198 (4.3619)	LR 2.500e-04
0: TRAIN [1][420/568]	Time 0.317 (0.200)	Data 1.41e-04 (1.45e-04)	Tok/s 66340 (61420)	Loss/tok 4.2894 (4.3535)	LR 2.500e-04
0: TRAIN [1][430/568]	Time 0.129 (0.200)	Data 1.40e-04 (1.45e-04)	Tok/s 54985 (61434)	Loss/tok 3.6653 (4.3448)	LR 2.500e-04
0: TRAIN [1][440/568]	Time 0.129 (0.200)	Data 1.35e-04 (1.45e-04)	Tok/s 54459 (61426)	Loss/tok 3.6562 (4.3374)	LR 2.500e-04
0: TRAIN [1][450/568]	Time 0.249 (0.200)	Data 1.41e-04 (1.45e-04)	Tok/s 65293 (61443)	Loss/tok 4.0390 (4.3303)	LR 2.500e-04
0: TRAIN [1][460/568]	Time 0.187 (0.200)	Data 1.36e-04 (1.44e-04)	Tok/s 63418 (61431)	Loss/tok 3.9284 (4.3218)	LR 2.500e-04
0: TRAIN [1][470/568]	Time 0.251 (0.200)	Data 1.36e-04 (1.44e-04)	Tok/s 64758 (61465)	Loss/tok 4.1324 (4.3155)	LR 2.500e-04
0: TRAIN [1][480/568]	Time 0.191 (0.200)	Data 1.42e-04 (1.44e-04)	Tok/s 62018 (61418)	Loss/tok 3.9213 (4.3092)	LR 1.250e-04
0: TRAIN [1][490/568]	Time 0.128 (0.200)	Data 1.29e-04 (1.44e-04)	Tok/s 55126 (61352)	Loss/tok 3.5450 (4.3034)	LR 1.250e-04
0: TRAIN [1][500/568]	Time 0.248 (0.200)	Data 1.37e-04 (1.44e-04)	Tok/s 65572 (61343)	Loss/tok 4.0386 (4.2963)	LR 1.250e-04
0: TRAIN [1][510/568]	Time 0.128 (0.199)	Data 1.51e-04 (1.44e-04)	Tok/s 54635 (61331)	Loss/tok 3.5374 (4.2896)	LR 1.250e-04
0: TRAIN [1][520/568]	Time 0.187 (0.199)	Data 1.31e-04 (1.45e-04)	Tok/s 62913 (61277)	Loss/tok 3.8752 (4.2847)	LR 1.250e-04
0: TRAIN [1][530/568]	Time 0.130 (0.199)	Data 1.38e-04 (1.45e-04)	Tok/s 54269 (61281)	Loss/tok 3.6226 (4.2785)	LR 1.250e-04
0: TRAIN [1][540/568]	Time 0.320 (0.200)	Data 1.38e-04 (1.45e-04)	Tok/s 66736 (61298)	Loss/tok 4.2829 (4.2740)	LR 1.250e-04
0: TRAIN [1][550/568]	Time 0.190 (0.200)	Data 1.39e-04 (1.45e-04)	Tok/s 62230 (61302)	Loss/tok 3.8346 (4.2687)	LR 1.250e-04
0: TRAIN [1][560/568]	Time 0.192 (0.200)	Data 1.38e-04 (1.45e-04)	Tok/s 61314 (61283)	Loss/tok 3.7643 (4.2620)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/160]	Time 0.062 (0.000)	Data 1.23e-03 (0.00e+00)	Tok/s 91944 (0)	Loss/tok 5.6816 (5.6816)
0: VALIDATION [1][10/160]	Time 0.029 (0.033)	Data 9.48e-04 (9.83e-04)	Tok/s 117901 (118745)	Loss/tok 5.2441 (5.3884)
0: VALIDATION [1][20/160]	Time 0.023 (0.029)	Data 9.18e-04 (9.59e-04)	Tok/s 125358 (120492)	Loss/tok 5.1038 (5.3217)
0: VALIDATION [1][30/160]	Time 0.022 (0.027)	Data 9.00e-04 (9.44e-04)	Tok/s 119506 (120754)	Loss/tok 5.4886 (5.2795)
0: VALIDATION [1][40/160]	Time 0.019 (0.025)	Data 8.86e-04 (9.34e-04)	Tok/s 122328 (121084)	Loss/tok 4.9225 (5.2452)
0: VALIDATION [1][50/160]	Time 0.017 (0.024)	Data 8.95e-04 (9.25e-04)	Tok/s 123498 (121455)	Loss/tok 5.0357 (5.2019)
0: VALIDATION [1][60/160]	Time 0.016 (0.023)	Data 8.74e-04 (9.17e-04)	Tok/s 125113 (121604)	Loss/tok 4.6936 (5.1666)
0: VALIDATION [1][70/160]	Time 0.015 (0.022)	Data 8.78e-04 (9.12e-04)	Tok/s 120620 (121378)	Loss/tok 4.6013 (5.1441)
0: VALIDATION [1][80/160]	Time 0.014 (0.021)	Data 8.70e-04 (9.07e-04)	Tok/s 119098 (121169)	Loss/tok 5.0202 (5.1200)
0: VALIDATION [1][90/160]	Time 0.012 (0.020)	Data 8.56e-04 (9.02e-04)	Tok/s 121538 (120990)	Loss/tok 4.7972 (5.1014)
0: VALIDATION [1][100/160]	Time 0.011 (0.019)	Data 8.59e-04 (8.98e-04)	Tok/s 119457 (120445)	Loss/tok 4.9322 (5.0871)
0: VALIDATION [1][110/160]	Time 0.010 (0.018)	Data 8.55e-04 (8.95e-04)	Tok/s 116570 (120074)	Loss/tok 4.7656 (5.0680)
0: VALIDATION [1][120/160]	Time 0.010 (0.018)	Data 8.57e-04 (8.92e-04)	Tok/s 109630 (119532)	Loss/tok 4.6604 (5.0548)
0: VALIDATION [1][130/160]	Time 0.015 (0.017)	Data 8.61e-04 (8.90e-04)	Tok/s 65384 (118520)	Loss/tok 4.5372 (5.0389)
0: VALIDATION [1][140/160]	Time 0.008 (0.016)	Data 8.52e-04 (8.88e-04)	Tok/s 104316 (117741)	Loss/tok 4.3795 (5.0261)
0: VALIDATION [1][150/160]	Time 0.006 (0.016)	Data 8.63e-04 (8.86e-04)	Tok/s 101547 (116732)	Loss/tok 4.4452 (5.0103)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/94]	Time 0.3044 (0.3260)	Decoder iters 149.0 (148.6)	Tok/s 10784 (11611)
0: TEST [1][19/94]	Time 0.1528 (0.2848)	Decoder iters 65.0 (133.3)	Tok/s 17209 (12029)
0: TEST [1][29/94]	Time 0.2821 (0.2798)	Decoder iters 149.0 (135.7)	Tok/s 8018 (11166)
0: TEST [1][39/94]	Time 0.1129 (0.2547)	Decoder iters 49.0 (123.9)	Tok/s 17117 (11672)
0: TEST [1][49/94]	Time 0.1139 (0.2372)	Decoder iters 54.0 (116.0)	Tok/s 14663 (11898)
0: TEST [1][59/94]	Time 0.0816 (0.2222)	Decoder iters 35.0 (109.1)	Tok/s 18185 (12008)
0: TEST [1][69/94]	Time 0.2596 (0.2122)	Decoder iters 149.0 (105.0)	Tok/s 4639 (11913)
0: TEST [1][79/94]	Time 0.0663 (0.2041)	Decoder iters 29.0 (101.7)	Tok/s 15558 (11725)
0: TEST [1][89/94]	Time 0.0592 (0.1938)	Decoder iters 28.0 (96.9)	Tok/s 11668 (11649)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 4.2590	Validation Loss: 4.9984	Test BLEU: 7.60
0: Performance: Epoch: 1	Training: 61318 Tok/s	Validation: 115060 Tok/s
0: Finished epoch 1
0: Total training time 304 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 260|                       7.6|            61353.82881242699|            5.0605419993400576|
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00037407875061035156 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "13624", "role": "default", "hostname": "92ebbc27b388", "state": "SUCCEEDED", "total_run_time": 305, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [1]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "92ebbc27b388", "state": "SUCCEEDED", "total_run_time": 305, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\"}", "agent_restarts": 0}}
DONE!
