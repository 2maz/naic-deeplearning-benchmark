The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 1
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_s__lu1k5/none_oerdnxfo
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0]
  role_ranks=[0]
  global_ranks=[0]
  role_world_sizes=[1]
  global_world_sizes=[1]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_s__lu1k5/none_oerdnxfo/attempt_0/0/error.json
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
0: thread affinity: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11}
0: Collecting environment information...
0: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.13.0-39-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: GPU 0: Quadro RTX 8000
Nvidia driver version: 510.60.02
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=576, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 339
0: Scheduler decay interval: 42
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/255]	Time 0.914 (0.000)	Data 2.05e-01 (0.00e+00)	Tok/s 16937 (0)	Loss/tok 10.6054 (10.6054)	LR 2.047e-05
0: TRAIN [0][10/255]	Time 2.041 (1.152)	Data 2.93e-04 (2.91e-04)	Tok/s 23167 (22837)	Loss/tok 9.8581 (10.1027)	LR 2.576e-05
0: TRAIN [0][20/255]	Time 1.152 (1.243)	Data 2.62e-04 (3.11e-04)	Tok/s 22544 (22650)	Loss/tok 9.1771 (9.7372)	LR 3.244e-05
0: TRAIN [0][30/255]	Time 0.746 (1.203)	Data 3.68e-04 (3.09e-04)	Tok/s 21347 (22403)	Loss/tok 8.7227 (9.5199)	LR 4.083e-05
0: TRAIN [0][40/255]	Time 0.759 (1.208)	Data 2.77e-04 (3.08e-04)	Tok/s 20555 (22266)	Loss/tok 8.5258 (9.3354)	LR 5.141e-05
0: TRAIN [0][50/255]	Time 1.176 (1.256)	Data 2.55e-04 (3.04e-04)	Tok/s 22003 (22269)	Loss/tok 8.4466 (9.1712)	LR 6.472e-05
0: TRAIN [0][60/255]	Time 1.628 (1.241)	Data 2.80e-04 (2.95e-04)	Tok/s 22190 (22113)	Loss/tok 8.3571 (9.0537)	LR 8.148e-05
0: TRAIN [0][70/255]	Time 1.190 (1.203)	Data 2.44e-04 (3.03e-04)	Tok/s 21781 (21996)	Loss/tok 8.0412 (8.9477)	LR 1.026e-04
0: TRAIN [0][80/255]	Time 0.765 (1.214)	Data 4.82e-04 (3.10e-04)	Tok/s 20428 (21943)	Loss/tok 7.6303 (8.8214)	LR 1.291e-04
0: TRAIN [0][90/255]	Time 1.643 (1.224)	Data 5.08e-04 (3.10e-04)	Tok/s 22029 (21877)	Loss/tok 7.9685 (8.7245)	LR 1.626e-04
0: TRAIN [0][100/255]	Time 1.203 (1.273)	Data 2.51e-04 (3.11e-04)	Tok/s 21797 (21873)	Loss/tok 7.7352 (8.6191)	LR 2.047e-04
0: TRAIN [0][110/255]	Time 1.196 (1.271)	Data 2.99e-04 (3.09e-04)	Tok/s 21807 (21831)	Loss/tok 7.5958 (8.5385)	LR 2.576e-04
0: TRAIN [0][120/255]	Time 1.640 (1.277)	Data 2.92e-04 (3.04e-04)	Tok/s 22014 (21807)	Loss/tok 7.7865 (8.4691)	LR 3.244e-04
0: TRAIN [0][130/255]	Time 1.192 (1.282)	Data 2.45e-04 (3.07e-04)	Tok/s 21783 (21795)	Loss/tok 7.5765 (8.4086)	LR 4.083e-04
0: TRAIN [0][140/255]	Time 1.641 (1.289)	Data 2.66e-04 (3.10e-04)	Tok/s 22208 (21783)	Loss/tok 7.7551 (8.3581)	LR 5.141e-04
0: TRAIN [0][150/255]	Time 2.135 (1.299)	Data 3.62e-04 (3.10e-04)	Tok/s 22039 (21763)	Loss/tok 7.7884 (8.3093)	LR 6.472e-04
0: TRAIN [0][160/255]	Time 1.640 (1.302)	Data 2.36e-04 (3.07e-04)	Tok/s 22224 (21741)	Loss/tok 7.7487 (8.2678)	LR 8.148e-04
0: TRAIN [0][170/255]	Time 0.767 (1.304)	Data 2.34e-04 (3.07e-04)	Tok/s 20223 (21725)	Loss/tok 7.0563 (8.2258)	LR 1.026e-03
0: TRAIN [0][180/255]	Time 1.190 (1.303)	Data 2.33e-04 (3.07e-04)	Tok/s 21851 (21717)	Loss/tok 7.3376 (8.1879)	LR 1.291e-03
0: TRAIN [0][190/255]	Time 0.758 (1.294)	Data 4.55e-04 (3.09e-04)	Tok/s 20529 (21701)	Loss/tok 6.9975 (8.1560)	LR 1.626e-03
0: TRAIN [0][200/255]	Time 1.186 (1.299)	Data 2.76e-04 (3.10e-04)	Tok/s 21953 (21701)	Loss/tok 7.2131 (8.1199)	LR 2.000e-03
0: TRAIN [0][210/255]	Time 1.629 (1.300)	Data 3.19e-04 (3.09e-04)	Tok/s 22307 (21703)	Loss/tok 7.2242 (8.0760)	LR 2.000e-03
0: TRAIN [0][220/255]	Time 1.195 (1.284)	Data 3.31e-04 (3.11e-04)	Tok/s 21834 (21685)	Loss/tok 7.0801 (8.0509)	LR 2.000e-03
0: TRAIN [0][230/255]	Time 0.766 (1.275)	Data 3.40e-04 (3.09e-04)	Tok/s 20295 (21662)	Loss/tok 6.5450 (8.0122)	LR 2.000e-03
0: TRAIN [0][240/255]	Time 1.632 (1.281)	Data 2.56e-04 (3.10e-04)	Tok/s 22232 (21667)	Loss/tok 6.8649 (7.9626)	LR 2.000e-03
0: TRAIN [0][250/255]	Time 2.130 (1.285)	Data 3.04e-04 (3.10e-04)	Tok/s 22065 (21672)	Loss/tok 6.9314 (7.9134)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/160]	Time 0.141 (0.000)	Data 1.45e-03 (0.00e+00)	Tok/s 40618 (0)	Loss/tok 7.7353 (7.7353)
0: VALIDATION [0][10/160]	Time 0.071 (0.080)	Data 1.25e-03 (1.49e-03)	Tok/s 48758 (49319)	Loss/tok 7.5217 (7.6498)
0: VALIDATION [0][20/160]	Time 0.056 (0.070)	Data 1.15e-03 (1.41e-03)	Tok/s 52475 (50169)	Loss/tok 7.5159 (7.6215)
0: VALIDATION [0][30/160]	Time 0.052 (0.065)	Data 1.34e-03 (1.34e-03)	Tok/s 49945 (50442)	Loss/tok 7.5745 (7.5881)
0: VALIDATION [0][40/160]	Time 0.045 (0.061)	Data 1.10e-03 (1.30e-03)	Tok/s 51732 (50727)	Loss/tok 7.3761 (7.5662)
0: VALIDATION [0][50/160]	Time 0.041 (0.057)	Data 1.02e-03 (1.27e-03)	Tok/s 51853 (50853)	Loss/tok 7.3799 (7.5373)
0: VALIDATION [0][60/160]	Time 0.037 (0.054)	Data 9.63e-04 (1.25e-03)	Tok/s 53212 (51021)	Loss/tok 7.2928 (7.5120)
0: VALIDATION [0][70/160]	Time 0.036 (0.052)	Data 1.10e-03 (1.23e-03)	Tok/s 50438 (50933)	Loss/tok 7.2500 (7.4968)
0: VALIDATION [0][80/160]	Time 0.034 (0.050)	Data 1.18e-03 (1.22e-03)	Tok/s 47851 (50720)	Loss/tok 7.3193 (7.4800)
0: VALIDATION [0][90/160]	Time 0.030 (0.048)	Data 1.23e-03 (1.24e-03)	Tok/s 49668 (50516)	Loss/tok 7.2545 (7.4626)
0: VALIDATION [0][100/160]	Time 0.027 (0.046)	Data 1.19e-03 (1.23e-03)	Tok/s 49542 (50311)	Loss/tok 7.2681 (7.4497)
0: VALIDATION [0][110/160]	Time 0.025 (0.044)	Data 1.20e-03 (1.23e-03)	Tok/s 48966 (50171)	Loss/tok 7.2319 (7.4352)
0: VALIDATION [0][120/160]	Time 0.024 (0.042)	Data 1.22e-03 (1.22e-03)	Tok/s 45806 (50005)	Loss/tok 7.2868 (7.4238)
0: VALIDATION [0][130/160]	Time 0.019 (0.041)	Data 1.07e-03 (1.21e-03)	Tok/s 50382 (49794)	Loss/tok 6.8377 (7.4077)
0: VALIDATION [0][140/160]	Time 0.018 (0.039)	Data 9.96e-04 (1.20e-03)	Tok/s 45966 (49653)	Loss/tok 6.9954 (7.3961)
0: VALIDATION [0][150/160]	Time 0.014 (0.037)	Data 9.79e-04 (1.20e-03)	Tok/s 46373 (49346)	Loss/tok 6.5381 (7.3810)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/94]	Time 0.9818 (1.0966)	Decoder iters 149.0 (149.0)	Tok/s 5243 (5418)
0: TEST [0][19/94]	Time 0.7955 (1.0031)	Decoder iters 149.0 (149.0)	Tok/s 5170 (5338)
0: TEST [0][29/94]	Time 0.7373 (0.9295)	Decoder iters 149.0 (149.0)	Tok/s 4679 (5269)
0: TEST [0][39/94]	Time 0.6413 (0.8733)	Decoder iters 149.0 (149.0)	Tok/s 4202 (5064)
0: TEST [0][49/94]	Time 0.5677 (0.8243)	Decoder iters 149.0 (149.0)	Tok/s 4194 (4878)
0: TEST [0][59/94]	Time 0.4698 (0.7753)	Decoder iters 149.0 (149.0)	Tok/s 3459 (4679)
0: TEST [0][69/94]	Time 0.4394 (0.7316)	Decoder iters 149.0 (149.0)	Tok/s 2861 (4471)
0: TEST [0][79/94]	Time 0.4337 (0.6949)	Decoder iters 149.0 (149.0)	Tok/s 2084 (4238)
0: TEST [0][89/94]	Time 0.1220 (0.6440)	Decoder iters 32.0 (141.1)	Tok/s 5272 (4239)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 7.8979	Validation Loss: 7.3655	Test BLEU: 0.17
0: Performance: Epoch: 0	Training: 21669 Tok/s	Validation: 48654 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: Sampler for epoch 1 uses seed 3588440356
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: TRAIN [1][0/255]	Time 0.946 (0.000)	Data 2.00e-01 (0.00e+00)	Tok/s 16832 (0)	Loss/tok 6.2725 (6.2725)	LR 2.000e-03
0: TRAIN [1][10/255]	Time 0.761 (1.190)	Data 2.56e-04 (1.57e-03)	Tok/s 20717 (21518)	Loss/tok 6.0832 (6.5033)	LR 2.000e-03
0: TRAIN [1][20/255]	Time 1.195 (1.380)	Data 2.36e-04 (9.47e-04)	Tok/s 21721 (21633)	Loss/tok 6.3601 (6.5476)	LR 2.000e-03
0: TRAIN [1][30/255]	Time 0.368 (1.295)	Data 2.37e-04 (7.29e-04)	Tok/s 20847 (21611)	Loss/tok 6.0298 (6.4803)	LR 2.000e-03
0: TRAIN [1][40/255]	Time 1.204 (1.286)	Data 3.04e-04 (6.17e-04)	Tok/s 21550 (21526)	Loss/tok 6.1932 (6.4364)	LR 2.000e-03
0: TRAIN [1][50/255]	Time 1.206 (1.264)	Data 3.50e-04 (5.56e-04)	Tok/s 21455 (21436)	Loss/tok 6.0904 (6.3743)	LR 2.000e-03
0: TRAIN [1][60/255]	Time 2.156 (1.266)	Data 2.46e-04 (5.19e-04)	Tok/s 21816 (21380)	Loss/tok 6.3896 (6.3433)	LR 2.000e-03
0: TRAIN [1][70/255]	Time 1.643 (1.303)	Data 2.43e-04 (4.90e-04)	Tok/s 22000 (21426)	Loss/tok 6.0860 (6.3058)	LR 2.000e-03
0: TRAIN [1][80/255]	Time 1.194 (1.285)	Data 3.23e-04 (4.69e-04)	Tok/s 21831 (21412)	Loss/tok 5.8407 (6.2598)	LR 2.000e-03
0: TRAIN [1][90/255]	Time 0.765 (1.291)	Data 2.62e-04 (4.46e-04)	Tok/s 20541 (21425)	Loss/tok 5.4060 (6.2143)	LR 1.000e-03
0: TRAIN [1][100/255]	Time 1.199 (1.295)	Data 2.39e-04 (4.40e-04)	Tok/s 21545 (21448)	Loss/tok 5.5802 (6.1649)	LR 1.000e-03
0: TRAIN [1][110/255]	Time 1.193 (1.300)	Data 2.99e-04 (4.34e-04)	Tok/s 21850 (21470)	Loss/tok 5.5380 (6.1222)	LR 1.000e-03
0: TRAIN [1][120/255]	Time 1.637 (1.308)	Data 3.57e-04 (4.24e-04)	Tok/s 22343 (21482)	Loss/tok 5.7129 (6.0864)	LR 1.000e-03
0: TRAIN [1][130/255]	Time 0.771 (1.303)	Data 2.27e-04 (4.14e-04)	Tok/s 20663 (21477)	Loss/tok 5.1266 (6.0460)	LR 5.000e-04
0: TRAIN [1][140/255]	Time 1.623 (1.299)	Data 4.87e-04 (4.13e-04)	Tok/s 22215 (21490)	Loss/tok 5.5785 (6.0076)	LR 5.000e-04
0: TRAIN [1][150/255]	Time 1.629 (1.299)	Data 3.06e-04 (4.07e-04)	Tok/s 22250 (21493)	Loss/tok 5.5497 (5.9699)	LR 5.000e-04
0: TRAIN [1][160/255]	Time 1.636 (1.312)	Data 2.59e-04 (3.99e-04)	Tok/s 22193 (21523)	Loss/tok 5.4731 (5.9347)	LR 5.000e-04
0: TRAIN [1][170/255]	Time 1.627 (1.306)	Data 2.36e-04 (3.98e-04)	Tok/s 22320 (21522)	Loss/tok 5.4697 (5.9009)	LR 2.500e-04
0: TRAIN [1][180/255]	Time 1.638 (1.302)	Data 2.62e-04 (3.91e-04)	Tok/s 22197 (21516)	Loss/tok 5.4511 (5.8690)	LR 2.500e-04
0: TRAIN [1][190/255]	Time 1.642 (1.291)	Data 2.32e-04 (3.87e-04)	Tok/s 22250 (21520)	Loss/tok 5.4390 (5.8426)	LR 2.500e-04
0: TRAIN [1][200/255]	Time 1.635 (1.291)	Data 4.51e-04 (3.86e-04)	Tok/s 22185 (21527)	Loss/tok 5.4183 (5.8155)	LR 2.500e-04
0: TRAIN [1][210/255]	Time 0.765 (1.293)	Data 2.80e-04 (3.81e-04)	Tok/s 20305 (21531)	Loss/tok 4.7725 (5.7880)	LR 1.250e-04
0: TRAIN [1][220/255]	Time 0.771 (1.291)	Data 2.40e-04 (3.78e-04)	Tok/s 20326 (21520)	Loss/tok 4.7924 (5.7641)	LR 1.250e-04
0: TRAIN [1][230/255]	Time 0.766 (1.296)	Data 2.32e-04 (3.74e-04)	Tok/s 20340 (21524)	Loss/tok 4.7805 (5.7434)	LR 1.250e-04
0: TRAIN [1][240/255]	Time 1.188 (1.296)	Data 2.62e-04 (3.72e-04)	Tok/s 21786 (21519)	Loss/tok 5.1042 (5.7225)	LR 1.250e-04
0: TRAIN [1][250/255]	Time 0.768 (1.288)	Data 3.88e-04 (3.71e-04)	Tok/s 20378 (21516)	Loss/tok 4.8092 (5.7018)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/160]	Time 0.145 (0.000)	Data 1.40e-03 (0.00e+00)	Tok/s 39513 (0)	Loss/tok 6.6658 (6.6658)
0: VALIDATION [1][10/160]	Time 0.072 (0.081)	Data 1.24e-03 (1.34e-03)	Tok/s 47735 (48245)	Loss/tok 6.3464 (6.4990)
0: VALIDATION [1][20/160]	Time 0.058 (0.072)	Data 1.23e-03 (1.29e-03)	Tok/s 50884 (49007)	Loss/tok 6.2702 (6.4445)
0: VALIDATION [1][30/160]	Time 0.053 (0.067)	Data 1.24e-03 (1.29e-03)	Tok/s 48900 (49189)	Loss/tok 6.4435 (6.3977)
0: VALIDATION [1][40/160]	Time 0.046 (0.062)	Data 1.05e-03 (1.26e-03)	Tok/s 50470 (49420)	Loss/tok 6.0276 (6.3602)
0: VALIDATION [1][50/160]	Time 0.042 (0.059)	Data 1.01e-03 (1.23e-03)	Tok/s 50687 (49532)	Loss/tok 6.1648 (6.3213)
0: VALIDATION [1][60/160]	Time 0.038 (0.056)	Data 1.03e-03 (1.22e-03)	Tok/s 51046 (49659)	Loss/tok 5.8883 (6.2863)
0: VALIDATION [1][70/160]	Time 0.036 (0.053)	Data 1.04e-03 (1.21e-03)	Tok/s 49643 (49539)	Loss/tok 5.8146 (6.2621)
0: VALIDATION [1][80/160]	Time 0.034 (0.051)	Data 1.17e-03 (1.21e-03)	Tok/s 47778 (49363)	Loss/tok 6.1435 (6.2394)
0: VALIDATION [1][90/160]	Time 0.031 (0.049)	Data 1.20e-03 (1.20e-03)	Tok/s 48064 (49191)	Loss/tok 5.8637 (6.2180)
0: VALIDATION [1][100/160]	Time 0.027 (0.047)	Data 9.96e-04 (1.19e-03)	Tok/s 49309 (48972)	Loss/tok 5.9590 (6.2034)
0: VALIDATION [1][110/160]	Time 0.026 (0.045)	Data 1.08e-03 (1.18e-03)	Tok/s 46673 (48824)	Loss/tok 5.9188 (6.1848)
0: VALIDATION [1][120/160]	Time 0.024 (0.043)	Data 1.17e-03 (1.17e-03)	Tok/s 44706 (48584)	Loss/tok 5.9154 (6.1699)
0: VALIDATION [1][130/160]	Time 0.019 (0.042)	Data 1.04e-03 (1.17e-03)	Tok/s 49495 (48417)	Loss/tok 5.5839 (6.1513)
0: VALIDATION [1][140/160]	Time 0.018 (0.040)	Data 1.19e-03 (1.16e-03)	Tok/s 44163 (48283)	Loss/tok 5.5566 (6.1380)
0: VALIDATION [1][150/160]	Time 0.015 (0.038)	Data 1.04e-03 (1.15e-03)	Tok/s 43282 (47963)	Loss/tok 5.4174 (6.1217)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/94]	Time 0.8474 (0.9437)	Decoder iters 149.0 (149.0)	Tok/s 4757 (5194)
0: TEST [1][19/94]	Time 0.6330 (0.8285)	Decoder iters 149.0 (149.0)	Tok/s 4446 (4955)
0: TEST [1][29/94]	Time 0.5533 (0.7513)	Decoder iters 149.0 (149.0)	Tok/s 4361 (4891)
0: TEST [1][39/94]	Time 0.5124 (0.6882)	Decoder iters 149.0 (146.0)	Tok/s 3817 (4805)
0: TEST [1][49/94]	Time 0.4632 (0.6314)	Decoder iters 149.0 (139.2)	Tok/s 3752 (4834)
0: TEST [1][59/94]	Time 0.2081 (0.5885)	Decoder iters 42.0 (134.4)	Tok/s 7180 (4826)
0: TEST [1][69/94]	Time 0.1785 (0.5424)	Decoder iters 37.0 (125.2)	Tok/s 6744 (4954)
0: TEST [1][79/94]	Time 0.1606 (0.4970)	Decoder iters 35.0 (114.7)	Tok/s 6313 (5133)
0: TEST [1][89/94]	Time 0.1039 (0.4576)	Decoder iters 23.0 (105.8)	Tok/s 6643 (5251)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 5.6935	Validation Loss: 6.1061	Test BLEU: 1.58
0: Performance: Epoch: 1	Training: 21519 Tok/s	Validation: 47288 Tok/s
0: Finished epoch 1
0: Total training time 802 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 576|                      1.58|           21594.119846778027|            13.359350391228993|
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.00048732757568359375 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "153", "role": "default", "hostname": "ba5e486eda38", "state": "SUCCEEDED", "total_run_time": 805, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [1]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "ba5e486eda38", "state": "SUCCEEDED", "total_run_time": 805, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\"}", "agent_restarts": 0}}
DONE!
