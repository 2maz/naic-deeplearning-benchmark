The module torch.distributed.launch is deprecated and going to be removed in future.Migrate to torch.distributed.run
WARNING:torch.distributed.run:--use_env is deprecated and will be removed in future releases.
 Please read local_rank from `os.environ('LOCAL_RANK')` instead.
INFO:torch.distributed.launcher.api:Starting elastic_operator with launch configs:
  entrypoint       : train.py
  min_nodes        : 1
  max_nodes        : 1
  nproc_per_node   : 2
  run_id           : none
  rdzv_backend     : static
  rdzv_endpoint    : 127.0.0.1:29500
  rdzv_configs     : {'rank': 0, 'timeout': 900}
  max_restarts     : 3
  monitor_interval : 5
  log_dir          : None
  metrics_cfg      : {}

INFO:torch.distributed.elastic.agent.server.local_elastic_agent:log directory set to: /tmp/torchelastic_j9trk3in/none_acaj13dk
INFO:torch.distributed.elastic.agent.server.api:[default] starting workers for entrypoint: python3
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous'ing worker group
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:52: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:[default] Rendezvous complete for workers. Result:
  restart_count=0
  master_addr=127.0.0.1
  master_port=29500
  group_rank=0
  group_world_size=1
  local_ranks=[0, 1]
  role_ranks=[0, 1]
  global_ranks=[0, 1]
  role_world_sizes=[2, 2]
  global_world_sizes=[2, 2]

INFO:torch.distributed.elastic.agent.server.api:[default] Starting worker group
INFO:torch.distributed.elastic.multiprocessing:Setting worker0 reply file to: /tmp/torchelastic_j9trk3in/none_acaj13dk/attempt_0/0/error.json
INFO:torch.distributed.elastic.multiprocessing:Setting worker1 reply file to: /tmp/torchelastic_j9trk3in/none_acaj13dk/attempt_0/1/error.json
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
train.py:41: UserWarning: PyProf is unavailable
  warnings.warn('PyProf is unavailable')
0: thread affinity: {0, 32, 4, 36, 8, 40, 12, 44}
1: thread affinity: {1, 33, 5, 37, 9, 41, 13, 45}
0: Collecting environment information...
1: Collecting environment information...
0: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA RTX A5500
GPU 1: NVIDIA RTX A5500
GPU 2: NVIDIA RTX A5500
GPU 3: NVIDIA RTX A5500
GPU 4: NVIDIA RTX A5500
GPU 5: NVIDIA RTX A5500
GPU 6: NVIDIA RTX A5500
GPU 7: NVIDIA RTX A5500

Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=200, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.10.0a0+ecc3718
Is debug build: False
CUDA used to build PyTorch: 11.4
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.2 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.21.0
Libc version: glibc-2.31

Python version: 3.8 (64-bit runtime)
Python platform: Linux-5.4.0-120-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.4.48
GPU models and configuration: 
GPU 0: NVIDIA RTX A5500
GPU 1: NVIDIA RTX A5500
GPU 2: NVIDIA RTX A5500
GPU 3: NVIDIA RTX A5500
GPU 4: NVIDIA RTX A5500
GPU 5: NVIDIA RTX A5500
GPU 6: NVIDIA RTX A5500
GPU 7: NVIDIA RTX A5500

Nvidia driver version: 510.73.05
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.2.2
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.2.2
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.21.1
[pip3] nvidia-dlprof-pytorch-nvtx==1.3.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.10.0a0+ecc3718
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.11.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.5                      281    conda-forge
[conda] mkl-include               2019.5                      281    conda-forge
[conda] numpy                     1.21.1           py38h9894fe3_0    conda-forge
[conda] nvidia-dlprof-pytorch-nvtx 1.3.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.10.0a0+ecc3718          pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.11.0a0                 pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=200, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
1: Worker 1 is using worker seed: 364522461
0: Worker 0 is using worker seed: 242886303
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
0: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
0: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 488
0: Scheduler decay interval: 61
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/_tensor.py:557: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /opt/pytorch/pytorch/aten/src/ATen/native/BinaryOps.cpp:461.)
  return torch.floor_divide(self, other)
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 488
1: Scheduler decay interval: 61
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/367]	Time 0.573 (0.000)	Data 2.19e-01 (0.00e+00)	Tok/s 28382 (0)	Loss/tok 10.7118 (10.7118)	LR 2.047e-05
1: TRAIN [0][0/367]	Time 0.574 (0.000)	Data 2.19e-01 (0.00e+00)	Tok/s 28322 (0)	Loss/tok 10.7165 (10.7165)	LR 2.047e-05
0: TRAIN [0][10/367]	Time 0.229 (0.246)	Data 1.20e-04 (1.27e-04)	Tok/s 39486 (38365)	Loss/tok 9.7021 (10.1473)	LR 2.576e-05
1: TRAIN [0][10/367]	Time 0.229 (0.246)	Data 1.15e-04 (1.21e-04)	Tok/s 39867 (38318)	Loss/tok 9.6957 (10.1503)	LR 2.576e-05
0: TRAIN [0][20/367]	Time 0.165 (0.255)	Data 1.27e-04 (1.35e-04)	Tok/s 32334 (39187)	Loss/tok 8.9798 (9.7985)	LR 3.244e-05
1: TRAIN [0][20/367]	Time 0.165 (0.255)	Data 1.22e-04 (1.23e-04)	Tok/s 33814 (39277)	Loss/tok 8.9699 (9.7822)	LR 3.244e-05
0: TRAIN [0][30/367]	Time 0.227 (0.247)	Data 1.24e-04 (1.35e-04)	Tok/s 39852 (38728)	Loss/tok 8.8351 (9.5724)	LR 4.083e-05
1: TRAIN [0][30/367]	Time 0.227 (0.247)	Data 1.18e-04 (1.23e-04)	Tok/s 39481 (38809)	Loss/tok 8.8318 (9.5647)	LR 4.083e-05
1: TRAIN [0][40/367]	Time 0.369 (0.257)	Data 1.26e-04 (1.24e-04)	Tok/s 44282 (39232)	Loss/tok 9.0815 (9.3753)	LR 5.141e-05
0: TRAIN [0][40/367]	Time 0.369 (0.257)	Data 1.35e-04 (1.34e-04)	Tok/s 44740 (39176)	Loss/tok 9.1035 (9.3859)	LR 5.141e-05
1: TRAIN [0][50/367]	Time 0.290 (0.257)	Data 1.25e-04 (1.23e-04)	Tok/s 43266 (39285)	Loss/tok 8.6204 (9.2175)	LR 6.472e-05
0: TRAIN [0][50/367]	Time 0.290 (0.257)	Data 1.37e-04 (1.40e-04)	Tok/s 43729 (39278)	Loss/tok 8.6136 (9.2286)	LR 6.472e-05
1: TRAIN [0][60/367]	Time 0.292 (0.253)	Data 1.26e-04 (1.24e-04)	Tok/s 43014 (39015)	Loss/tok 8.1925 (9.0865)	LR 8.148e-05
0: TRAIN [0][60/367]	Time 0.292 (0.253)	Data 1.42e-04 (1.40e-04)	Tok/s 43336 (38991)	Loss/tok 8.2629 (9.0959)	LR 8.148e-05
1: TRAIN [0][70/367]	Time 0.369 (0.252)	Data 1.26e-04 (1.24e-04)	Tok/s 44043 (38738)	Loss/tok 8.2449 (8.9620)	LR 1.026e-04
0: TRAIN [0][70/367]	Time 0.368 (0.252)	Data 1.66e-04 (1.43e-04)	Tok/s 44215 (38718)	Loss/tok 8.1765 (8.9679)	LR 1.026e-04
1: TRAIN [0][80/367]	Time 0.228 (0.252)	Data 1.22e-04 (1.26e-04)	Tok/s 38722 (38687)	Loss/tok 7.8591 (8.8511)	LR 1.291e-04
0: TRAIN [0][80/367]	Time 0.228 (0.252)	Data 1.66e-04 (1.43e-04)	Tok/s 38934 (38666)	Loss/tok 7.9739 (8.8611)	LR 1.291e-04
1: TRAIN [0][90/367]	Time 0.291 (0.253)	Data 1.25e-04 (1.26e-04)	Tok/s 43162 (38852)	Loss/tok 7.8535 (8.7415)	LR 1.626e-04
0: TRAIN [0][90/367]	Time 0.292 (0.253)	Data 2.15e-04 (1.43e-04)	Tok/s 43468 (38828)	Loss/tok 7.8826 (8.7501)	LR 1.626e-04
1: TRAIN [0][100/367]	Time 0.168 (0.252)	Data 1.16e-04 (1.26e-04)	Tok/s 33085 (38723)	Loss/tok 7.3571 (8.6515)	LR 2.047e-04
0: TRAIN [0][100/367]	Time 0.168 (0.252)	Data 1.26e-04 (1.43e-04)	Tok/s 32616 (38697)	Loss/tok 7.4025 (8.6605)	LR 2.047e-04
1: TRAIN [0][110/367]	Time 0.115 (0.248)	Data 1.24e-04 (1.25e-04)	Tok/s 23890 (38472)	Loss/tok 6.9653 (8.5908)	LR 2.576e-04
0: TRAIN [0][110/367]	Time 0.114 (0.247)	Data 1.46e-04 (1.44e-04)	Tok/s 23471 (38395)	Loss/tok 7.0048 (8.5997)	LR 2.576e-04
1: TRAIN [0][120/367]	Time 0.370 (0.247)	Data 1.99e-04 (1.26e-04)	Tok/s 44352 (38363)	Loss/tok 7.8895 (8.5241)	LR 3.244e-04
0: TRAIN [0][120/367]	Time 0.370 (0.247)	Data 1.32e-04 (1.44e-04)	Tok/s 44115 (38275)	Loss/tok 7.9053 (8.5344)	LR 3.244e-04
1: TRAIN [0][130/367]	Time 0.170 (0.242)	Data 1.40e-04 (1.26e-04)	Tok/s 32877 (38045)	Loss/tok 7.2609 (8.4713)	LR 4.083e-04
0: TRAIN [0][130/367]	Time 0.169 (0.242)	Data 1.40e-04 (1.44e-04)	Tok/s 31171 (37952)	Loss/tok 7.2251 (8.4803)	LR 4.083e-04
1: TRAIN [0][140/367]	Time 0.167 (0.242)	Data 1.31e-04 (1.28e-04)	Tok/s 33105 (38030)	Loss/tok 7.2783 (8.4165)	LR 5.141e-04
0: TRAIN [0][140/367]	Time 0.167 (0.242)	Data 1.36e-04 (1.45e-04)	Tok/s 33371 (37946)	Loss/tok 7.2633 (8.4244)	LR 5.141e-04
1: TRAIN [0][150/367]	Time 0.293 (0.245)	Data 2.04e-04 (1.30e-04)	Tok/s 42959 (38125)	Loss/tok 7.8268 (8.3655)	LR 6.472e-04
0: TRAIN [0][150/367]	Time 0.293 (0.245)	Data 1.30e-04 (1.46e-04)	Tok/s 43163 (38050)	Loss/tok 7.8058 (8.3746)	LR 6.472e-04
1: TRAIN [0][160/367]	Time 0.167 (0.242)	Data 1.31e-04 (1.30e-04)	Tok/s 32086 (37946)	Loss/tok 7.2874 (8.3271)	LR 8.148e-04
0: TRAIN [0][160/367]	Time 0.167 (0.242)	Data 1.44e-04 (1.46e-04)	Tok/s 32833 (37868)	Loss/tok 7.2643 (8.3373)	LR 8.148e-04
1: TRAIN [0][170/367]	Time 0.292 (0.243)	Data 1.35e-04 (1.31e-04)	Tok/s 43287 (38039)	Loss/tok 7.8987 (8.2916)	LR 1.026e-03
0: TRAIN [0][170/367]	Time 0.292 (0.243)	Data 1.39e-04 (1.46e-04)	Tok/s 42998 (37976)	Loss/tok 7.9093 (8.3012)	LR 1.026e-03
1: TRAIN [0][180/367]	Time 0.293 (0.245)	Data 1.39e-04 (1.31e-04)	Tok/s 42689 (38176)	Loss/tok 7.5618 (8.2504)	LR 1.291e-03
0: TRAIN [0][180/367]	Time 0.293 (0.245)	Data 1.40e-04 (1.46e-04)	Tok/s 42493 (38117)	Loss/tok 7.5763 (8.2577)	LR 1.291e-03
1: TRAIN [0][190/367]	Time 0.168 (0.244)	Data 1.97e-04 (1.32e-04)	Tok/s 32745 (38106)	Loss/tok 7.0361 (8.2143)	LR 1.626e-03
0: TRAIN [0][190/367]	Time 0.168 (0.244)	Data 1.44e-04 (1.46e-04)	Tok/s 32191 (38038)	Loss/tok 6.9504 (8.2207)	LR 1.626e-03
1: TRAIN [0][200/367]	Time 0.294 (0.245)	Data 1.37e-04 (1.33e-04)	Tok/s 42919 (38180)	Loss/tok 7.4208 (8.1747)	LR 2.000e-03
0: TRAIN [0][200/367]	Time 0.294 (0.245)	Data 1.37e-04 (1.47e-04)	Tok/s 42922 (38140)	Loss/tok 7.4836 (8.1806)	LR 2.000e-03
1: TRAIN [0][210/367]	Time 0.228 (0.244)	Data 1.34e-04 (1.34e-04)	Tok/s 39601 (38162)	Loss/tok 7.1464 (8.1354)	LR 2.000e-03
0: TRAIN [0][210/367]	Time 0.228 (0.244)	Data 1.46e-04 (1.46e-04)	Tok/s 40163 (38128)	Loss/tok 7.1687 (8.1419)	LR 2.000e-03
1: TRAIN [0][220/367]	Time 0.292 (0.245)	Data 1.34e-04 (1.35e-04)	Tok/s 42907 (38167)	Loss/tok 7.6198 (8.0944)	LR 2.000e-03
0: TRAIN [0][220/367]	Time 0.292 (0.245)	Data 1.37e-04 (1.47e-04)	Tok/s 42791 (38143)	Loss/tok 7.5969 (8.1000)	LR 2.000e-03
1: TRAIN [0][230/367]	Time 0.370 (0.245)	Data 2.17e-04 (1.36e-04)	Tok/s 44579 (38207)	Loss/tok 7.3174 (8.0521)	LR 2.000e-03
0: TRAIN [0][230/367]	Time 0.370 (0.245)	Data 2.32e-04 (1.47e-04)	Tok/s 44140 (38180)	Loss/tok 7.2819 (8.0569)	LR 2.000e-03
1: TRAIN [0][240/367]	Time 0.293 (0.247)	Data 2.09e-04 (1.36e-04)	Tok/s 43269 (38337)	Loss/tok 6.9687 (7.9982)	LR 2.000e-03
0: TRAIN [0][240/367]	Time 0.293 (0.247)	Data 1.32e-04 (1.47e-04)	Tok/s 42555 (38298)	Loss/tok 6.9576 (8.0033)	LR 2.000e-03
1: TRAIN [0][250/367]	Time 0.290 (0.248)	Data 2.13e-04 (1.37e-04)	Tok/s 43994 (38375)	Loss/tok 6.8936 (7.9495)	LR 2.000e-03
0: TRAIN [0][250/367]	Time 0.290 (0.248)	Data 1.29e-04 (1.47e-04)	Tok/s 43580 (38340)	Loss/tok 6.8835 (7.9547)	LR 2.000e-03
1: TRAIN [0][260/367]	Time 0.292 (0.247)	Data 1.93e-04 (1.38e-04)	Tok/s 42761 (38314)	Loss/tok 6.7925 (7.9088)	LR 2.000e-03
0: TRAIN [0][260/367]	Time 0.292 (0.247)	Data 1.44e-04 (1.47e-04)	Tok/s 42981 (38292)	Loss/tok 6.7917 (7.9140)	LR 2.000e-03
1: TRAIN [0][270/367]	Time 0.169 (0.248)	Data 1.42e-04 (1.38e-04)	Tok/s 31803 (38312)	Loss/tok 6.3013 (7.8640)	LR 2.000e-03
0: TRAIN [0][270/367]	Time 0.169 (0.248)	Data 1.47e-04 (1.48e-04)	Tok/s 31773 (38288)	Loss/tok 6.2198 (7.8675)	LR 2.000e-03
1: TRAIN [0][280/367]	Time 0.292 (0.247)	Data 2.11e-04 (1.39e-04)	Tok/s 43754 (38315)	Loss/tok 6.5687 (7.8183)	LR 2.000e-03
0: TRAIN [0][280/367]	Time 0.292 (0.247)	Data 1.37e-04 (1.48e-04)	Tok/s 42894 (38292)	Loss/tok 6.5781 (7.8209)	LR 2.000e-03
1: TRAIN [0][290/367]	Time 0.370 (0.247)	Data 1.94e-04 (1.40e-04)	Tok/s 43832 (38233)	Loss/tok 6.7653 (7.7782)	LR 2.000e-03
0: TRAIN [0][290/367]	Time 0.370 (0.247)	Data 1.39e-04 (1.49e-04)	Tok/s 44442 (38213)	Loss/tok 6.7517 (7.7815)	LR 2.000e-03
0: TRAIN [0][300/367]	Time 0.290 (0.246)	Data 1.33e-04 (1.49e-04)	Tok/s 43610 (38224)	Loss/tok 6.4041 (7.7376)	LR 2.000e-03
1: TRAIN [0][300/367]	Time 0.290 (0.246)	Data 1.37e-04 (1.41e-04)	Tok/s 43088 (38243)	Loss/tok 6.4921 (7.7342)	LR 2.000e-03
1: TRAIN [0][310/367]	Time 0.230 (0.246)	Data 2.00e-04 (1.42e-04)	Tok/s 39447 (38259)	Loss/tok 6.2087 (7.6894)	LR 2.000e-03
0: TRAIN [0][310/367]	Time 0.230 (0.246)	Data 1.55e-04 (1.50e-04)	Tok/s 39218 (38252)	Loss/tok 6.2108 (7.6911)	LR 2.000e-03
1: TRAIN [0][320/367]	Time 0.227 (0.245)	Data 2.13e-04 (1.42e-04)	Tok/s 39560 (38239)	Loss/tok 6.0364 (7.6466)	LR 2.000e-03
0: TRAIN [0][320/367]	Time 0.227 (0.245)	Data 1.38e-04 (1.49e-04)	Tok/s 39406 (38234)	Loss/tok 6.0652 (7.6484)	LR 2.000e-03
1: TRAIN [0][330/367]	Time 0.168 (0.245)	Data 2.65e-04 (1.43e-04)	Tok/s 32086 (38217)	Loss/tok 5.6549 (7.6051)	LR 2.000e-03
0: TRAIN [0][330/367]	Time 0.168 (0.245)	Data 1.37e-04 (1.49e-04)	Tok/s 32048 (38207)	Loss/tok 5.7658 (7.6067)	LR 2.000e-03
1: TRAIN [0][340/367]	Time 0.231 (0.244)	Data 1.90e-04 (1.43e-04)	Tok/s 38980 (38216)	Loss/tok 5.9283 (7.5629)	LR 2.000e-03
0: TRAIN [0][340/367]	Time 0.231 (0.244)	Data 1.41e-04 (1.50e-04)	Tok/s 39081 (38203)	Loss/tok 5.8733 (7.5635)	LR 2.000e-03
1: TRAIN [0][350/367]	Time 0.228 (0.244)	Data 2.05e-04 (1.44e-04)	Tok/s 40018 (38209)	Loss/tok 5.8475 (7.5193)	LR 2.000e-03
0: TRAIN [0][350/367]	Time 0.228 (0.244)	Data 1.29e-04 (1.50e-04)	Tok/s 39175 (38194)	Loss/tok 5.8301 (7.5196)	LR 2.000e-03
1: TRAIN [0][360/367]	Time 0.168 (0.243)	Data 1.87e-04 (1.44e-04)	Tok/s 31947 (38191)	Loss/tok 5.3338 (7.4767)	LR 2.000e-03
0: TRAIN [0][360/367]	Time 0.167 (0.243)	Data 1.40e-04 (1.49e-04)	Tok/s 32476 (38172)	Loss/tok 5.4135 (7.4776)	LR 2.000e-03
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [0][0/40]	Time 0.077 (0.000)	Data 2.22e-03 (0.00e+00)	Tok/s 112028 (0)	Loss/tok 7.0136 (7.0136)
0: VALIDATION [0][0/40]	Time 0.122 (0.000)	Data 2.26e-03 (0.00e+00)	Tok/s 85660 (0)	Loss/tok 7.1000 (7.1000)
1: VALIDATION [0][10/40]	Time 0.037 (0.048)	Data 1.75e-03 (2.09e-03)	Tok/s 122186 (120322)	Loss/tok 6.7579 (6.8446)
0: VALIDATION [0][10/40]	Time 0.037 (0.048)	Data 1.76e-03 (1.81e-03)	Tok/s 125057 (122739)	Loss/tok 6.5521 (6.8678)
1: VALIDATION [0][20/40]	Time 0.026 (0.039)	Data 1.70e-03 (1.90e-03)	Tok/s 122740 (121295)	Loss/tok 6.5119 (6.7266)
0: VALIDATION [0][20/40]	Time 0.027 (0.040)	Data 1.73e-03 (1.77e-03)	Tok/s 121547 (122516)	Loss/tok 6.4681 (6.7491)
1: VALIDATION [0][30/40]	Time 0.018 (0.033)	Data 1.68e-03 (1.83e-03)	Tok/s 117210 (120317)	Loss/tok 6.2020 (6.6591)
0: VALIDATION [0][30/40]	Time 0.018 (0.034)	Data 1.70e-03 (1.75e-03)	Tok/s 116073 (121133)	Loss/tok 6.4314 (6.6777)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
0: TEST [0][9/12]	Time 0.4199 (0.7238)	Decoder iters 39.0 (138.0)	Tok/s 10846 (12924)
1: TEST [0][9/12]	Time 0.4200 (0.7239)	Decoder iters 149.0 (149.0)	Tok/s 10232 (11872)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.4445	Validation Loss: 6.6090	Test BLEU: 0.96
0: Performance: Epoch: 0	Training: 76501 Tok/s	Validation: 233380 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
1: Sampler for epoch 1 uses seed 1323436024
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
0: Sampler for epoch 1 uses seed 1323436024
[W pthreadpool-cpp.cc:99] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)
1: TRAIN [1][0/367]	Time 0.368 (0.000)	Data 1.42e-01 (0.00e+00)	Tok/s 24421 (0)	Loss/tok 5.6674 (5.6674)	LR 2.000e-03
0: TRAIN [1][0/367]	Time 0.369 (0.000)	Data 1.58e-01 (0.00e+00)	Tok/s 24857 (0)	Loss/tok 5.5825 (5.5825)	LR 2.000e-03
1: TRAIN [1][10/367]	Time 0.227 (0.212)	Data 1.95e-04 (1.57e-04)	Tok/s 39470 (36420)	Loss/tok 5.5536 (5.6112)	LR 2.000e-03
0: TRAIN [1][10/367]	Time 0.227 (0.212)	Data 1.36e-04 (1.42e-04)	Tok/s 39582 (36653)	Loss/tok 5.5392 (5.5824)	LR 2.000e-03
1: TRAIN [1][20/367]	Time 0.228 (0.222)	Data 1.41e-04 (1.50e-04)	Tok/s 39246 (37151)	Loss/tok 5.4693 (5.6053)	LR 2.000e-03
0: TRAIN [1][20/367]	Time 0.228 (0.222)	Data 1.56e-04 (1.45e-04)	Tok/s 38607 (37383)	Loss/tok 5.3639 (5.6053)	LR 2.000e-03
1: TRAIN [1][30/367]	Time 0.228 (0.231)	Data 1.33e-04 (1.43e-04)	Tok/s 39736 (37026)	Loss/tok 5.4909 (5.6473)	LR 2.000e-03
0: TRAIN [1][30/367]	Time 0.228 (0.231)	Data 1.56e-04 (1.49e-04)	Tok/s 39200 (37209)	Loss/tok 5.5997 (5.6532)	LR 2.000e-03
1: TRAIN [1][40/367]	Time 0.168 (0.238)	Data 1.53e-04 (1.44e-04)	Tok/s 31843 (37519)	Loss/tok 5.0409 (5.6467)	LR 2.000e-03
0: TRAIN [1][40/367]	Time 0.168 (0.238)	Data 1.51e-04 (1.48e-04)	Tok/s 32443 (37739)	Loss/tok 4.9627 (5.6532)	LR 2.000e-03
0: TRAIN [1][50/367]	Time 0.166 (0.230)	Data 1.51e-04 (1.50e-04)	Tok/s 32931 (37150)	Loss/tok 5.0979 (5.6037)	LR 2.000e-03
1: TRAIN [1][50/367]	Time 0.166 (0.230)	Data 1.44e-04 (1.45e-04)	Tok/s 33510 (37053)	Loss/tok 4.9790 (5.6000)	LR 2.000e-03
1: TRAIN [1][60/367]	Time 0.172 (0.231)	Data 2.27e-04 (1.49e-04)	Tok/s 32661 (37257)	Loss/tok 4.9696 (5.5576)	LR 2.000e-03
0: TRAIN [1][60/367]	Time 0.172 (0.231)	Data 1.37e-04 (1.48e-04)	Tok/s 31795 (37281)	Loss/tok 4.8675 (5.5629)	LR 2.000e-03
1: TRAIN [1][70/367]	Time 0.227 (0.232)	Data 1.66e-04 (1.50e-04)	Tok/s 40462 (37547)	Loss/tok 5.1502 (5.5187)	LR 2.000e-03
0: TRAIN [1][70/367]	Time 0.227 (0.232)	Data 1.42e-04 (1.46e-04)	Tok/s 40201 (37570)	Loss/tok 5.0804 (5.5241)	LR 2.000e-03
1: TRAIN [1][80/367]	Time 0.228 (0.231)	Data 2.62e-04 (1.53e-04)	Tok/s 39446 (37611)	Loss/tok 5.1594 (5.4704)	LR 2.000e-03
0: TRAIN [1][80/367]	Time 0.228 (0.231)	Data 1.45e-04 (1.44e-04)	Tok/s 39501 (37596)	Loss/tok 5.0846 (5.4775)	LR 2.000e-03
1: TRAIN [1][90/367]	Time 0.166 (0.231)	Data 1.31e-04 (1.54e-04)	Tok/s 32901 (37486)	Loss/tok 4.6741 (5.4415)	LR 2.000e-03
0: TRAIN [1][90/367]	Time 0.166 (0.231)	Data 1.42e-04 (1.43e-04)	Tok/s 32383 (37494)	Loss/tok 4.6117 (5.4435)	LR 2.000e-03
1: TRAIN [1][100/367]	Time 0.168 (0.233)	Data 1.41e-04 (1.53e-04)	Tok/s 32512 (37631)	Loss/tok 4.7129 (5.4179)	LR 2.000e-03
0: TRAIN [1][100/367]	Time 0.168 (0.233)	Data 1.62e-04 (1.43e-04)	Tok/s 31727 (37641)	Loss/tok 4.6727 (5.4215)	LR 2.000e-03
1: TRAIN [1][110/367]	Time 0.229 (0.235)	Data 1.43e-04 (1.53e-04)	Tok/s 38837 (37724)	Loss/tok 4.9505 (5.3879)	LR 2.000e-03
0: TRAIN [1][110/367]	Time 0.229 (0.235)	Data 1.52e-04 (1.43e-04)	Tok/s 39446 (37730)	Loss/tok 4.9448 (5.3942)	LR 2.000e-03
1: TRAIN [1][120/367]	Time 0.230 (0.239)	Data 1.42e-04 (1.52e-04)	Tok/s 38282 (38058)	Loss/tok 4.7554 (5.3606)	LR 1.000e-03
0: TRAIN [1][120/367]	Time 0.230 (0.239)	Data 1.45e-04 (1.43e-04)	Tok/s 38881 (38066)	Loss/tok 4.9600 (5.3682)	LR 1.000e-03
1: TRAIN [1][130/367]	Time 0.291 (0.238)	Data 1.49e-04 (1.52e-04)	Tok/s 42811 (38083)	Loss/tok 5.0010 (5.3262)	LR 1.000e-03
0: TRAIN [1][130/367]	Time 0.291 (0.238)	Data 1.61e-04 (1.43e-04)	Tok/s 42831 (38075)	Loss/tok 4.9391 (5.3298)	LR 1.000e-03
1: TRAIN [1][140/367]	Time 0.167 (0.241)	Data 1.41e-04 (1.52e-04)	Tok/s 33164 (38171)	Loss/tok 4.5708 (5.3003)	LR 1.000e-03
0: TRAIN [1][140/367]	Time 0.167 (0.241)	Data 1.50e-04 (1.43e-04)	Tok/s 32448 (38159)	Loss/tok 4.4556 (5.3052)	LR 1.000e-03
1: TRAIN [1][150/367]	Time 0.292 (0.241)	Data 1.34e-04 (1.51e-04)	Tok/s 42941 (38155)	Loss/tok 4.9536 (5.2684)	LR 1.000e-03
0: TRAIN [1][150/367]	Time 0.291 (0.241)	Data 1.91e-04 (1.43e-04)	Tok/s 43351 (38130)	Loss/tok 4.9625 (5.2744)	LR 1.000e-03
1: TRAIN [1][160/367]	Time 0.295 (0.244)	Data 1.44e-04 (1.51e-04)	Tok/s 42577 (38383)	Loss/tok 4.8371 (5.2388)	LR 1.000e-03
0: TRAIN [1][160/367]	Time 0.295 (0.244)	Data 1.55e-04 (1.43e-04)	Tok/s 42799 (38353)	Loss/tok 4.9070 (5.2463)	LR 1.000e-03
1: TRAIN [1][170/367]	Time 0.229 (0.242)	Data 2.20e-04 (1.51e-04)	Tok/s 39342 (38267)	Loss/tok 4.6632 (5.2069)	LR 1.000e-03
0: TRAIN [1][170/367]	Time 0.229 (0.242)	Data 1.56e-04 (1.43e-04)	Tok/s 39324 (38229)	Loss/tok 4.6090 (5.2145)	LR 1.000e-03
1: TRAIN [1][180/367]	Time 0.372 (0.242)	Data 1.41e-04 (1.50e-04)	Tok/s 43822 (38246)	Loss/tok 5.0847 (5.1803)	LR 1.000e-03
0: TRAIN [1][180/367]	Time 0.372 (0.242)	Data 1.82e-04 (1.43e-04)	Tok/s 44090 (38217)	Loss/tok 4.9384 (5.1845)	LR 1.000e-03
1: TRAIN [1][190/367]	Time 0.166 (0.243)	Data 1.43e-04 (1.50e-04)	Tok/s 32039 (38322)	Loss/tok 4.1327 (5.1514)	LR 5.000e-04
0: TRAIN [1][190/367]	Time 0.166 (0.243)	Data 1.54e-04 (1.43e-04)	Tok/s 32640 (38294)	Loss/tok 4.1781 (5.1546)	LR 5.000e-04
1: TRAIN [1][200/367]	Time 0.229 (0.244)	Data 1.47e-04 (1.49e-04)	Tok/s 39577 (38388)	Loss/tok 4.4264 (5.1251)	LR 5.000e-04
0: TRAIN [1][200/367]	Time 0.229 (0.244)	Data 1.53e-04 (1.43e-04)	Tok/s 38941 (38354)	Loss/tok 4.4435 (5.1277)	LR 5.000e-04
1: TRAIN [1][210/367]	Time 0.167 (0.245)	Data 1.82e-04 (1.49e-04)	Tok/s 32875 (38513)	Loss/tok 4.2177 (5.0999)	LR 5.000e-04
0: TRAIN [1][210/367]	Time 0.167 (0.245)	Data 1.70e-04 (1.43e-04)	Tok/s 32589 (38469)	Loss/tok 4.0894 (5.1012)	LR 5.000e-04
1: TRAIN [1][220/367]	Time 0.294 (0.246)	Data 1.38e-04 (1.49e-04)	Tok/s 42823 (38549)	Loss/tok 4.6345 (5.0755)	LR 5.000e-04
0: TRAIN [1][220/367]	Time 0.294 (0.246)	Data 1.48e-04 (1.43e-04)	Tok/s 42575 (38501)	Loss/tok 4.7739 (5.0792)	LR 5.000e-04
1: TRAIN [1][230/367]	Time 0.168 (0.245)	Data 1.93e-04 (1.49e-04)	Tok/s 33116 (38525)	Loss/tok 4.1610 (5.0525)	LR 5.000e-04
0: TRAIN [1][230/367]	Time 0.168 (0.245)	Data 1.60e-04 (1.44e-04)	Tok/s 33182 (38476)	Loss/tok 4.1369 (5.0551)	LR 5.000e-04
1: TRAIN [1][240/367]	Time 0.167 (0.244)	Data 1.81e-04 (1.49e-04)	Tok/s 32214 (38460)	Loss/tok 4.1203 (5.0289)	LR 5.000e-04
0: TRAIN [1][240/367]	Time 0.167 (0.244)	Data 1.77e-04 (1.44e-04)	Tok/s 32200 (38410)	Loss/tok 4.0648 (5.0305)	LR 5.000e-04
1: TRAIN [1][250/367]	Time 0.291 (0.244)	Data 2.02e-04 (1.50e-04)	Tok/s 43486 (38468)	Loss/tok 4.6102 (5.0074)	LR 2.500e-04
0: TRAIN [1][250/367]	Time 0.291 (0.244)	Data 1.53e-04 (1.45e-04)	Tok/s 43200 (38405)	Loss/tok 4.4813 (5.0097)	LR 2.500e-04
0: TRAIN [1][260/367]	Time 0.230 (0.244)	Data 1.43e-04 (1.45e-04)	Tok/s 39384 (38431)	Loss/tok 4.4101 (4.9895)	LR 2.500e-04
1: TRAIN [1][260/367]	Time 0.230 (0.244)	Data 2.05e-04 (1.51e-04)	Tok/s 39527 (38480)	Loss/tok 4.2528 (4.9882)	LR 2.500e-04
1: TRAIN [1][270/367]	Time 0.373 (0.245)	Data 1.39e-04 (1.51e-04)	Tok/s 43637 (38540)	Loss/tok 4.8182 (4.9720)	LR 2.500e-04
0: TRAIN [1][270/367]	Time 0.373 (0.245)	Data 1.76e-04 (1.45e-04)	Tok/s 43900 (38491)	Loss/tok 4.6983 (4.9726)	LR 2.500e-04
1: TRAIN [1][280/367]	Time 0.115 (0.245)	Data 2.43e-04 (1.52e-04)	Tok/s 23741 (38428)	Loss/tok 3.8394 (4.9552)	LR 2.500e-04
0: TRAIN [1][280/367]	Time 0.115 (0.245)	Data 1.66e-04 (1.45e-04)	Tok/s 23441 (38393)	Loss/tok 3.6345 (4.9563)	LR 2.500e-04
1: TRAIN [1][290/367]	Time 0.371 (0.246)	Data 1.63e-04 (1.51e-04)	Tok/s 43402 (38502)	Loss/tok 4.8041 (4.9384)	LR 2.500e-04
0: TRAIN [1][290/367]	Time 0.371 (0.246)	Data 1.46e-04 (1.45e-04)	Tok/s 44001 (38469)	Loss/tok 4.7022 (4.9384)	LR 2.500e-04
1: TRAIN [1][300/367]	Time 0.294 (0.246)	Data 1.51e-04 (1.51e-04)	Tok/s 42807 (38514)	Loss/tok 4.5937 (4.9219)	LR 2.500e-04
0: TRAIN [1][300/367]	Time 0.294 (0.246)	Data 1.67e-04 (1.45e-04)	Tok/s 42948 (38477)	Loss/tok 4.5538 (4.9206)	LR 2.500e-04
1: TRAIN [1][310/367]	Time 0.294 (0.246)	Data 1.46e-04 (1.51e-04)	Tok/s 42932 (38426)	Loss/tok 4.5763 (4.9075)	LR 1.250e-04
0: TRAIN [1][310/367]	Time 0.294 (0.246)	Data 1.62e-04 (1.45e-04)	Tok/s 42608 (38389)	Loss/tok 4.5395 (4.9069)	LR 1.250e-04
1: TRAIN [1][320/367]	Time 0.293 (0.246)	Data 1.49e-04 (1.51e-04)	Tok/s 42913 (38439)	Loss/tok 4.5066 (4.8927)	LR 1.250e-04
0: TRAIN [1][320/367]	Time 0.293 (0.246)	Data 1.55e-04 (1.45e-04)	Tok/s 42693 (38393)	Loss/tok 4.5554 (4.8916)	LR 1.250e-04
1: TRAIN [1][330/367]	Time 0.168 (0.246)	Data 2.09e-04 (1.51e-04)	Tok/s 31636 (38420)	Loss/tok 4.0102 (4.8789)	LR 1.250e-04
0: TRAIN [1][330/367]	Time 0.168 (0.246)	Data 1.74e-04 (1.46e-04)	Tok/s 32138 (38372)	Loss/tok 4.0203 (4.8769)	LR 1.250e-04
1: TRAIN [1][340/367]	Time 0.228 (0.245)	Data 1.70e-04 (1.51e-04)	Tok/s 39002 (38388)	Loss/tok 4.3911 (4.8655)	LR 1.250e-04
0: TRAIN [1][340/367]	Time 0.228 (0.245)	Data 1.70e-04 (1.46e-04)	Tok/s 39417 (38338)	Loss/tok 4.2510 (4.8631)	LR 1.250e-04
1: TRAIN [1][350/367]	Time 0.168 (0.245)	Data 2.05e-04 (1.51e-04)	Tok/s 32644 (38410)	Loss/tok 4.0384 (4.8520)	LR 1.250e-04
0: TRAIN [1][350/367]	Time 0.168 (0.245)	Data 1.64e-04 (1.46e-04)	Tok/s 31561 (38359)	Loss/tok 3.9397 (4.8494)	LR 1.250e-04
1: TRAIN [1][360/367]	Time 0.371 (0.245)	Data 1.44e-04 (1.51e-04)	Tok/s 43903 (38378)	Loss/tok 4.6931 (4.8403)	LR 1.250e-04
0: TRAIN [1][360/367]	Time 0.371 (0.245)	Data 1.56e-04 (1.46e-04)	Tok/s 43912 (38325)	Loss/tok 4.6922 (4.8379)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [1][0/40]	Time 0.076 (0.000)	Data 2.29e-03 (0.00e+00)	Tok/s 112345 (0)	Loss/tok 5.8635 (5.8635)
0: VALIDATION [1][0/40]	Time 0.122 (0.000)	Data 2.24e-03 (0.00e+00)	Tok/s 86054 (0)	Loss/tok 6.0181 (6.0181)
1: VALIDATION [1][10/40]	Time 0.037 (0.047)	Data 1.81e-03 (1.86e-03)	Tok/s 122425 (121371)	Loss/tok 5.4504 (5.6525)
0: VALIDATION [1][10/40]	Time 0.037 (0.048)	Data 1.77e-03 (1.80e-03)	Tok/s 125254 (122838)	Loss/tok 5.3158 (5.6886)
1: VALIDATION [1][20/40]	Time 0.026 (0.039)	Data 1.72e-03 (1.80e-03)	Tok/s 122812 (121722)	Loss/tok 5.3057 (5.5399)
0: VALIDATION [1][20/40]	Time 0.026 (0.040)	Data 1.72e-03 (1.77e-03)	Tok/s 122977 (122638)	Loss/tok 5.3328 (5.5651)
1: VALIDATION [1][30/40]	Time 0.018 (0.033)	Data 1.70e-03 (1.77e-03)	Tok/s 117443 (120609)	Loss/tok 5.0658 (5.4739)
0: VALIDATION [1][30/40]	Time 0.018 (0.034)	Data 1.71e-03 (1.75e-03)	Tok/s 116015 (121239)	Loss/tok 5.1900 (5.4946)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [1][9/12]	Time 0.4163 (0.7051)	Decoder iters 59.0 (140.0)	Tok/s 10175 (12704)
0: TEST [1][9/12]	Time 0.4167 (0.7050)	Decoder iters 149.0 (139.1)	Tok/s 10820 (13679)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.8308	Validation Loss: 5.4281	Test BLEU: 4.58
0: Performance: Epoch: 1	Training: 76640 Tok/s	Validation: 233761 Tok/s
0: Finished epoch 1
1: Total training time 231 s
0: Total training time 231 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 200|                      4.58|                76570.0390625|            3.8504989703496295|
INFO:torch.distributed.elastic.agent.server.api:[default] worker group successfully finished. Waiting 300 seconds for other agents to finish.
INFO:torch.distributed.elastic.agent.server.api:Local worker group finished (SUCCEEDED). Waiting 300 seconds for other agents to finish
/opt/conda/lib/python3.8/site-packages/torch/distributed/elastic/utils/store.py:70: FutureWarning: This is an experimental API and will be changed in future.
  warnings.warn(
INFO:torch.distributed.elastic.agent.server.api:Done waiting for other agents. Elapsed: 0.0003972053527832031 seconds
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 0, "group_rank": 0, "worker_id": "150", "role": "default", "hostname": "a8cf658809de", "state": "SUCCEEDED", "total_run_time": 235, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [0], \"role_rank\": [0], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "WORKER", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": 1, "group_rank": 0, "worker_id": "151", "role": "default", "hostname": "a8cf658809de", "state": "SUCCEEDED", "total_run_time": 235, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\", \"local_rank\": [1], \"role_rank\": [1], \"role_world_size\": [2]}", "agent_restarts": 0}}
{"name": "torchelastic.worker.status.SUCCEEDED", "source": "AGENT", "timestamp": 0, "metadata": {"run_id": "none", "global_rank": null, "group_rank": 0, "worker_id": null, "role": "default", "hostname": "a8cf658809de", "state": "SUCCEEDED", "total_run_time": 235, "rdzv_backend": "static", "raw_error": null, "metadata": "{\"group_world_size\": 1, \"entry_point\": \"python3\"}", "agent_restarts": 0}}
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
