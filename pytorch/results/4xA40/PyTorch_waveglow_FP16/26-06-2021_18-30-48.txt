train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
DLL 2021-06-26 18:30:50.195336 - PARAMETER output : ./ 
DLL 2021-06-26 18:30:50.195409 - PARAMETER dataset_path : /data/tacotron2/LJSpeech-1.1 
DLL 2021-06-26 18:30:50.195436 - PARAMETER model_name : WaveGlow 
DLL 2021-06-26 18:30:50.195456 - PARAMETER log_file : nvlog.json 
DLL 2021-06-26 18:30:50.195475 - PARAMETER anneal_steps : None 
DLL 2021-06-26 18:30:50.195498 - PARAMETER anneal_factor : 0.1 
DLL 2021-06-26 18:30:50.195518 - PARAMETER epochs : 2 
DLL 2021-06-26 18:30:50.195537 - PARAMETER epochs_per_checkpoint : 50 
DLL 2021-06-26 18:30:50.195555 - PARAMETER checkpoint_path :  
DLL 2021-06-26 18:30:50.195575 - PARAMETER resume_from_last : False 
DLL 2021-06-26 18:30:50.195595 - PARAMETER dynamic_loss_scaling : True 
DLL 2021-06-26 18:30:50.195615 - PARAMETER amp : False 
DLL 2021-06-26 18:30:50.195635 - PARAMETER cudnn_enabled : True 
DLL 2021-06-26 18:30:50.195652 - PARAMETER cudnn_benchmark : True 
DLL 2021-06-26 18:30:50.195669 - PARAMETER disable_uniform_initialize_bn_weight : False 
DLL 2021-06-26 18:30:50.195686 - PARAMETER use_saved_learning_rate : False 
DLL 2021-06-26 18:30:50.195702 - PARAMETER learning_rate : 0.0 
DLL 2021-06-26 18:30:50.195722 - PARAMETER weight_decay : 0.0 
DLL 2021-06-26 18:30:50.195739 - PARAMETER grad_clip_thresh : 65504.0 
DLL 2021-06-26 18:30:50.195756 - PARAMETER batch_size : 32 
DLL 2021-06-26 18:30:50.195773 - PARAMETER grad_clip : 5.0 
DLL 2021-06-26 18:30:50.195790 - PARAMETER load_mel_from_disk : False 
DLL 2021-06-26 18:30:50.195807 - PARAMETER training_files : filelists/ljs_audio_text_train_subset_625_filelist.txt 
DLL 2021-06-26 18:30:50.195824 - PARAMETER validation_files : filelists/ljs_audio_text_val_filelist.txt 
DLL 2021-06-26 18:30:50.195840 - PARAMETER text_cleaners : ['english_cleaners'] 
DLL 2021-06-26 18:30:50.195860 - PARAMETER max_wav_value : 32768.0 
DLL 2021-06-26 18:30:50.195877 - PARAMETER sampling_rate : 22050 
DLL 2021-06-26 18:30:50.195893 - PARAMETER filter_length : 1024 
DLL 2021-06-26 18:30:50.195910 - PARAMETER hop_length : 256 
DLL 2021-06-26 18:30:50.195927 - PARAMETER win_length : 1024 
DLL 2021-06-26 18:30:50.195943 - PARAMETER mel_fmin : 0.0 
DLL 2021-06-26 18:30:50.195960 - PARAMETER mel_fmax : 8000.0 
DLL 2021-06-26 18:30:50.195976 - PARAMETER rank : 0 
DLL 2021-06-26 18:30:50.195992 - PARAMETER world_size : 4 
DLL 2021-06-26 18:30:50.196012 - PARAMETER dist_url : tcp://localhost:23456 
DLL 2021-06-26 18:30:50.196028 - PARAMETER group_name : group_name 
DLL 2021-06-26 18:30:50.196044 - PARAMETER dist_backend : nccl 
DLL 2021-06-26 18:30:50.196060 - PARAMETER bench_class :  
DLL 2021-06-26 18:30:50.196077 - PARAMETER model_name : Tacotron2_PyT 
Initializing Distributed
Done initializing distributed
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
DLL 2021-06-26 18:31:53.007969 - (0, 0) glob_iter/iters_per_epoch : 0/4 
DLL 2021-06-26 18:32:04.247186 - (0, 0) train_loss : 0.002195630921050906 
DLL 2021-06-26 18:32:15.283664 - (0, 0) train_items_per_sec : 45969.21051798852 
DLL 2021-06-26 18:32:15.283800 - (0, 0) train_iter_time : 22.27577955900051 
DLL 2021-06-26 18:32:15.288644 - (0, 1) glob_iter/iters_per_epoch : 1/4 
DLL 2021-06-26 18:32:16.486125 - (0, 1) train_loss : 0.0022632540203630924 
DLL 2021-06-26 18:32:18.391706 - (0, 1) train_items_per_sec : 329991.73423239193 
DLL 2021-06-26 18:32:18.391787 - (0, 1) train_iter_time : 3.103108028999486 
DLL 2021-06-26 18:32:18.393575 - (0, 2) glob_iter/iters_per_epoch : 2/4 
DLL 2021-06-26 18:32:20.671062 - (0, 2) train_loss : 0.0024182635825127363 
DLL 2021-06-26 18:32:22.534538 - (0, 2) train_items_per_sec : 247284.93481675923 
DLL 2021-06-26 18:32:22.534618 - (0, 2) train_iter_time : 4.140972035998857 
DLL 2021-06-26 18:32:22.536273 - (0, 3) glob_iter/iters_per_epoch : 3/4 
DLL 2021-06-26 18:32:24.813986 - (0, 3) train_loss : 0.0023004040122032166 
DLL 2021-06-26 18:32:26.680795 - (0, 3) train_items_per_sec : 247072.50562073037 
DLL 2021-06-26 18:32:26.680896 - (0, 3) train_iter_time : 4.144532381000317 
DLL 2021-06-26 18:32:26.766922 - (0,) train_items_per_sec : 217579.5962969675 
DLL 2021-06-26 18:32:26.766968 - (0,) train_loss : 0.0023004040122032166 
DLL 2021-06-26 18:32:26.766995 - (0,) train_epoch_time : 34.062843040999724 
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 151, in forward
    self.cond_layers[i](spect),
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 259, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 3; 44.56 GiB total capacity; 36.84 GiB already allocated; 76.06 MiB free; 42.40 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 2; 44.56 GiB total capacity; 36.89 GiB already allocated; 50.06 MiB free; 42.41 GiB reserved in total by PyTorch)


Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 151, in forward
    self.cond_layers[i](spect),
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 259, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 1; 44.56 GiB total capacity; 36.84 GiB already allocated; 76.06 MiB free; 42.40 GiB reserved in total by PyTorch)
DLL 2021-06-26 18:32:32.048163 - (0, 4, 0) val_items_per_sec : 160989.4517636193 
DLL 2021-06-26 18:32:32.163255 - (0,) val_loss : 0.001992255449295044 
DLL 2021-06-26 18:32:32.163362 - (0,) val_items_per_sec : 160989.4517636193 
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0.pt
DLL 2021-06-26 18:32:35.836355 - (1, 0) glob_iter/iters_per_epoch : 4/4 
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
RuntimeError: CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 44.56 GiB total capacity; 36.89 GiB already allocated; 58.06 MiB free; 42.41 GiB reserved in total by PyTorch)


DONE!
