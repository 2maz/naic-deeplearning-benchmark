:::NVLOGv0.2.2 Tacotron2_PyT 1593042616.214877605 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593042616.242105484 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593042616.259989977 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593042619.878053904 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593042619.885874987 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 72, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593042622.742172003 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1593042644.174830675 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593042644.176310539 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/2 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042652.276005268 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042662.815027714 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.97732162475586
:::NVLOGv0.2.2 Tacotron2_PyT 1593042664.932506800 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042664.933012724 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25814.64148992063
:::NVLOGv0.2.2 Tacotron2_PyT 1593042664.933380604 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 12.659017562866211
Batch: 1/2 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042664.939942598 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042666.404215574 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.22579574584961
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.406971216 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.408138514 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59561.23638375848
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.410047293 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.467582941055298
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.533276320 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.534638882 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 24753.195197145687
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.535660744 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 42687.93893683955
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.536634445 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.101558685302734
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.537589550 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 26.35797095298767
:::NVLOGv0.2.2 Tacotron2_PyT 1593042670.538563490 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593042672.430461884 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.11798858642578
:::NVLOGv0.2.2 Tacotron2_PyT 1593042672.431814671 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042672.755914450 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/2 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042674.518757105 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042678.794372797 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.681392669677734
:::NVLOGv0.2.2 Tacotron2_PyT 1593042681.517161131 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593042681.520823479 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 45747.32097386795
:::NVLOGv0.2.2 Tacotron2_PyT 1593042681.522677422 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 7.000781536102295
Batch: 1/2 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042681.537121534 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042684.954845190 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.865272521972656
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.494014502 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.495612383 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 47763.90038386991
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.498004198 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.957660436630249
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.600405216 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.602460384 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 41185.56407136365
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.604854107 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 46755.61067886893
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.606935740 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.273332595825195
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.608214617 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 15.845163583755493
:::NVLOGv0.2.2 Tacotron2_PyT 1593042688.609411478 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042690.587470055 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.116249084472656
:::NVLOGv0.2.2 Tacotron2_PyT 1593042690.588558912 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593042690.589742661 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 67.84689784049988
:::NVLOGv0.2.2 Tacotron2_PyT 1593042690.590185642 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 67.84689784049988
:::NVLOGv0.2.2 Tacotron2_PyT 1593042690.590618610 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 74.47584652900696
:::NVLOGv0.2.2 Tacotron2_PyT 1593042690.591011047 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
