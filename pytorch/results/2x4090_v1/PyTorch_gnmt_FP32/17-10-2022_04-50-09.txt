/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
1: thread affinity: {1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31, 33, 35, 37, 39, 41, 43, 45, 47, 49, 51, 53, 55, 57, 59, 61, 63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91, 93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117, 119, 121, 123, 125, 127}
0: thread affinity: {0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70, 72, 74, 76, 78, 80, 82, 84, 86, 88, 90, 92, 94, 96, 98, 100, 102, 104, 106, 108, 110, 112, 114, 116, 118, 120, 122, 124, 126}
0: Collecting environment information...
1: Collecting environment information...
0: PyTorch version: 1.13.0a0+d0d6b1f
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.2
Libc version: glibc-2.31

Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 520.56.06
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] functorch==0.3.0a0
[pip3] numpy==1.22.2
[pip3] pytorch-quantization==2.1.2
[pip3] torch==1.13.0a0+d0d6b1f
[pip3] torch-tensorrt==1.2.0a0
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.14.0a0
[conda] functorch                 0.3.0a0                  pypi_0    pypi
[conda] mkl                       2020.4             h726a3e6_304    conda-forge
[conda] mkl-include               2020.4             h726a3e6_304    conda-forge
[conda] numpy                     1.22.2           py38h6ae9a64_0    conda-forge
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] torch                     1.13.0a0+d0d6b1f          pypi_0    pypi
[conda] torch-tensorrt            1.2.0a0                  pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.14.0a0                 pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=288, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.13.0a0+d0d6b1f
Is debug build: False
CUDA used to build PyTorch: 11.8
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.5 LTS (x86_64)
GCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0
Clang version: Could not collect
CMake version: version 3.22.2
Libc version: glibc-2.31

Python version: 3.8.13 | packaged by conda-forge | (default, Mar 25 2022, 06:04:10)  [GCC 10.3.0] (64-bit runtime)
Python platform: Linux-5.15.0-50-generic-x86_64-with-glibc2.10
Is CUDA available: True
CUDA runtime version: 11.8.89
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 4090
GPU 1: NVIDIA GeForce RTX 4090

Nvidia driver version: 520.56.06
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.6.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.6.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

Versions of relevant libraries:
[pip3] functorch==0.3.0a0
[pip3] numpy==1.22.2
[pip3] pytorch-quantization==2.1.2
[pip3] torch==1.13.0a0+d0d6b1f
[pip3] torch-tensorrt==1.2.0a0
[pip3] torchtext==0.11.0a0
[pip3] torchvision==0.14.0a0
[conda] functorch                 0.3.0a0                  pypi_0    pypi
[conda] mkl                       2020.4             h726a3e6_304    conda-forge
[conda] mkl-include               2020.4             h726a3e6_304    conda-forge
[conda] numpy                     1.22.2           py38h6ae9a64_0    conda-forge
[conda] pytorch-quantization      2.1.2                    pypi_0    pypi
[conda] torch                     1.13.0a0+d0d6b1f          pypi_0    pypi
[conda] torch-tensorrt            1.2.0a0                  pypi_0    pypi
[conda] torchtext                 0.11.0a0                 pypi_0    pypi
[conda] torchvision               0.14.0a0                 pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=288, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
1: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.002
    maximize: False
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 339
0: Scheduler decay interval: 42
0: Scheduler decay factor: 0.5
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    eps: 1e-08
    foreach: None
    lr: 0.002
    maximize: False
    weight_decay: 0
)
0: Scheduler max decay steps: 4
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 339
1: Scheduler decay interval: 42
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
0: Starting epoch 0
1: Starting epoch 0
0: Executing preallocation
1: Executing preallocation
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/255]	Time 0.401 (0.000)	Data 1.63e-01 (0.00e+00)	Tok/s 20054 (0)	Loss/tok 10.6127 (10.6127)	LR 2.047e-05
1: TRAIN [0][0/255]	Time 0.401 (0.000)	Data 1.59e-01 (0.00e+00)	Tok/s 19630 (0)	Loss/tok 10.6187 (10.6187)	LR 2.047e-05
1: TRAIN [0][10/255]	Time 0.229 (0.281)	Data 8.89e-05 (1.21e-04)	Tok/s 34758 (45851)	Loss/tok 9.4487 (10.1083)	LR 2.576e-05
0: TRAIN [0][10/255]	Time 0.222 (0.283)	Data 9.92e-05 (1.04e-04)	Tok/s 35179 (44577)	Loss/tok 9.4724 (10.1093)	LR 2.576e-05
1: TRAIN [0][20/255]	Time 0.267 (0.302)	Data 8.99e-05 (1.27e-04)	Tok/s 48400 (48654)	Loss/tok 9.1717 (9.7394)	LR 3.244e-05
0: TRAIN [0][20/255]	Time 0.276 (0.304)	Data 9.80e-05 (1.23e-04)	Tok/s 47284 (47815)	Loss/tok 9.2342 (9.7346)	LR 3.244e-05
1: TRAIN [0][30/255]	Time 0.105 (0.288)	Data 9.44e-05 (1.27e-04)	Tok/s 36382 (47289)	Loss/tok 8.7200 (9.5338)	LR 4.083e-05
0: TRAIN [0][30/255]	Time 0.073 (0.289)	Data 1.04e-04 (1.16e-04)	Tok/s 52759 (47274)	Loss/tok 8.6659 (9.5324)	LR 4.083e-05
1: TRAIN [0][40/255]	Time 0.265 (0.289)	Data 9.51e-05 (1.19e-04)	Tok/s 49251 (46836)	Loss/tok 8.6709 (9.3614)	LR 5.141e-05
0: TRAIN [0][40/255]	Time 0.273 (0.290)	Data 1.03e-04 (1.12e-04)	Tok/s 47110 (46704)	Loss/tok 8.7222 (9.3631)	LR 5.141e-05
1: TRAIN [0][50/255]	Time 0.266 (0.285)	Data 9.78e-05 (1.14e-04)	Tok/s 48921 (46509)	Loss/tok 8.4302 (9.2172)	LR 6.472e-05
0: TRAIN [0][50/255]	Time 0.274 (0.286)	Data 1.02e-04 (1.12e-04)	Tok/s 47001 (46311)	Loss/tok 8.4641 (9.2175)	LR 6.472e-05
1: TRAIN [0][60/255]	Time 0.371 (0.285)	Data 9.78e-05 (1.11e-04)	Tok/s 63958 (46395)	Loss/tok 8.4989 (9.0860)	LR 8.148e-05
0: TRAIN [0][60/255]	Time 0.400 (0.286)	Data 1.13e-04 (1.14e-04)	Tok/s 58343 (46379)	Loss/tok 8.5180 (9.0867)	LR 8.148e-05
1: TRAIN [0][70/255]	Time 0.321 (0.289)	Data 2.09e-04 (1.11e-04)	Tok/s 56570 (47099)	Loss/tok 8.1666 (8.9450)	LR 1.026e-04
0: TRAIN [0][70/255]	Time 0.329 (0.290)	Data 2.17e-04 (1.15e-04)	Tok/s 54574 (47108)	Loss/tok 8.1486 (8.9392)	LR 1.026e-04
1: TRAIN [0][80/255]	Time 0.273 (0.287)	Data 1.06e-04 (1.10e-04)	Tok/s 48148 (46807)	Loss/tok 7.8856 (8.8354)	LR 1.291e-04
0: TRAIN [0][80/255]	Time 0.277 (0.288)	Data 1.16e-04 (1.15e-04)	Tok/s 46719 (46818)	Loss/tok 7.8511 (8.8305)	LR 1.291e-04
1: TRAIN [0][90/255]	Time 0.251 (0.288)	Data 9.92e-05 (1.13e-04)	Tok/s 31070 (46915)	Loss/tok 7.6046 (8.7293)	LR 1.626e-04
0: TRAIN [0][90/255]	Time 0.222 (0.288)	Data 1.03e-04 (1.17e-04)	Tok/s 35722 (46956)	Loss/tok 7.5808 (8.7227)	LR 1.626e-04
1: TRAIN [0][100/255]	Time 0.274 (0.288)	Data 1.06e-04 (1.14e-04)	Tok/s 46831 (47086)	Loss/tok 7.7408 (8.6348)	LR 2.047e-04
0: TRAIN [0][100/255]	Time 0.273 (0.289)	Data 1.08e-04 (1.18e-04)	Tok/s 47619 (47097)	Loss/tok 7.6841 (8.6306)	LR 2.047e-04
1: TRAIN [0][110/255]	Time 0.274 (0.289)	Data 1.78e-04 (1.15e-04)	Tok/s 47508 (47118)	Loss/tok 7.6034 (8.5559)	LR 2.576e-04
0: TRAIN [0][110/255]	Time 0.276 (0.290)	Data 1.02e-04 (1.18e-04)	Tok/s 47258 (47255)	Loss/tok 7.7017 (8.5525)	LR 2.576e-04
1: TRAIN [0][120/255]	Time 0.322 (0.290)	Data 9.23e-05 (1.17e-04)	Tok/s 56841 (47198)	Loss/tok 7.8233 (8.4899)	LR 3.244e-04
0: TRAIN [0][120/255]	Time 0.332 (0.291)	Data 9.56e-05 (1.17e-04)	Tok/s 54995 (47427)	Loss/tok 7.7946 (8.4881)	LR 3.244e-04
1: TRAIN [0][130/255]	Time 0.225 (0.290)	Data 9.23e-05 (1.17e-04)	Tok/s 35479 (47070)	Loss/tok 7.3563 (8.4286)	LR 4.083e-04
0: TRAIN [0][130/255]	Time 0.220 (0.290)	Data 2.16e-04 (1.17e-04)	Tok/s 36079 (47308)	Loss/tok 7.3401 (8.4272)	LR 4.083e-04
1: TRAIN [0][140/255]	Time 0.330 (0.289)	Data 9.61e-05 (1.15e-04)	Tok/s 54830 (47057)	Loss/tok 7.7539 (8.3793)	LR 5.141e-04
0: TRAIN [0][140/255]	Time 0.330 (0.290)	Data 1.09e-04 (1.16e-04)	Tok/s 54342 (47346)	Loss/tok 7.7554 (8.3778)	LR 5.141e-04
1: TRAIN [0][150/255]	Time 0.314 (0.289)	Data 9.44e-05 (1.16e-04)	Tok/s 57878 (47064)	Loss/tok 7.7456 (8.3297)	LR 6.472e-04
0: TRAIN [0][150/255]	Time 0.333 (0.290)	Data 9.94e-05 (1.16e-04)	Tok/s 54417 (47342)	Loss/tok 7.7698 (8.3286)	LR 6.472e-04
1: TRAIN [0][160/255]	Time 0.342 (0.291)	Data 1.01e-04 (1.17e-04)	Tok/s 53069 (47373)	Loss/tok 7.7021 (8.2808)	LR 8.148e-04
0: TRAIN [0][160/255]	Time 0.328 (0.291)	Data 1.04e-04 (1.16e-04)	Tok/s 55233 (47637)	Loss/tok 7.6168 (8.2798)	LR 8.148e-04
1: TRAIN [0][170/255]	Time 0.313 (0.290)	Data 9.82e-05 (1.18e-04)	Tok/s 57993 (47285)	Loss/tok 7.7200 (8.2442)	LR 1.026e-03
0: TRAIN [0][170/255]	Time 0.331 (0.290)	Data 9.54e-05 (1.16e-04)	Tok/s 54914 (47541)	Loss/tok 7.7287 (8.2429)	LR 1.026e-03
1: TRAIN [0][180/255]	Time 0.314 (0.289)	Data 9.66e-05 (1.17e-04)	Tok/s 58198 (47237)	Loss/tok 7.6049 (8.2036)	LR 1.291e-03
0: TRAIN [0][180/255]	Time 0.333 (0.290)	Data 9.63e-05 (1.15e-04)	Tok/s 54264 (47461)	Loss/tok 7.5862 (8.2031)	LR 1.291e-03
1: TRAIN [0][190/255]	Time 0.341 (0.288)	Data 1.05e-04 (1.17e-04)	Tok/s 53240 (46967)	Loss/tok 7.5526 (8.1715)	LR 1.626e-03
0: TRAIN [0][190/255]	Time 0.328 (0.288)	Data 2.19e-04 (1.15e-04)	Tok/s 55985 (47326)	Loss/tok 7.5660 (8.1712)	LR 1.626e-03
1: TRAIN [0][200/255]	Time 0.396 (0.288)	Data 9.47e-05 (1.17e-04)	Tok/s 45978 (46976)	Loss/tok 7.6918 (8.1342)	LR 2.000e-03
0: TRAIN [0][200/255]	Time 0.441 (0.288)	Data 8.99e-05 (1.15e-04)	Tok/s 40905 (47382)	Loss/tok 7.6660 (8.1331)	LR 2.000e-03
1: TRAIN [0][210/255]	Time 0.231 (0.288)	Data 1.02e-04 (1.17e-04)	Tok/s 33802 (47043)	Loss/tok 6.8867 (8.0956)	LR 2.000e-03
0: TRAIN [0][210/255]	Time 0.223 (0.289)	Data 1.09e-04 (1.15e-04)	Tok/s 34622 (47433)	Loss/tok 6.7876 (8.0946)	LR 2.000e-03
1: TRAIN [0][220/255]	Time 0.230 (0.288)	Data 9.92e-05 (1.16e-04)	Tok/s 33970 (46977)	Loss/tok 6.6816 (8.0558)	LR 2.000e-03
0: TRAIN [0][220/255]	Time 0.222 (0.288)	Data 1.10e-04 (1.15e-04)	Tok/s 35263 (47353)	Loss/tok 6.6713 (8.0547)	LR 2.000e-03
1: TRAIN [0][230/255]	Time 0.232 (0.289)	Data 9.35e-05 (1.16e-04)	Tok/s 33772 (47006)	Loss/tok 6.6347 (8.0105)	LR 2.000e-03
0: TRAIN [0][230/255]	Time 0.224 (0.289)	Data 9.73e-05 (1.15e-04)	Tok/s 34692 (47444)	Loss/tok 6.5006 (8.0085)	LR 2.000e-03
1: TRAIN [0][240/255]	Time 0.265 (0.289)	Data 9.42e-05 (1.16e-04)	Tok/s 49141 (47000)	Loss/tok 6.8590 (7.9643)	LR 2.000e-03
0: TRAIN [0][240/255]	Time 0.275 (0.289)	Data 9.06e-05 (1.15e-04)	Tok/s 46810 (47418)	Loss/tok 6.8022 (7.9615)	LR 2.000e-03
1: TRAIN [0][250/255]	Time 0.240 (0.288)	Data 9.89e-05 (1.16e-04)	Tok/s 32995 (46873)	Loss/tok 6.3481 (7.9213)	LR 2.000e-03
0: TRAIN [0][250/255]	Time 0.223 (0.288)	Data 1.02e-04 (1.15e-04)	Tok/s 34630 (47333)	Loss/tok 6.4786 (7.9186)	LR 2.000e-03
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [0][0/40]	Time 0.042 (0.000)	Data 1.43e-03 (0.00e+00)	Tok/s 206555 (0)	Loss/tok 7.6236 (7.6236)
0: VALIDATION [0][0/40]	Time 0.067 (0.000)	Data 1.46e-03 (0.00e+00)	Tok/s 156536 (0)	Loss/tok 7.6590 (7.6590)
1: VALIDATION [0][10/40]	Time 0.020 (0.026)	Data 1.10e-03 (1.15e-03)	Tok/s 227615 (224133)	Loss/tok 7.4817 (7.4990)
0: VALIDATION [0][10/40]	Time 0.020 (0.026)	Data 1.11e-03 (1.17e-03)	Tok/s 233430 (228053)	Loss/tok 7.3270 (7.5221)
1: VALIDATION [0][20/40]	Time 0.014 (0.021)	Data 1.05e-03 (1.11e-03)	Tok/s 226274 (224899)	Loss/tok 7.3079 (7.4215)
0: VALIDATION [0][20/40]	Time 0.015 (0.021)	Data 1.09e-03 (1.14e-03)	Tok/s 222200 (226888)	Loss/tok 7.1849 (7.4369)
1: VALIDATION [0][30/40]	Time 0.010 (0.018)	Data 1.04e-03 (1.10e-03)	Tok/s 210136 (221165)	Loss/tok 7.0091 (7.3729)
0: VALIDATION [0][30/40]	Time 0.010 (0.018)	Data 1.08e-03 (1.13e-03)	Tok/s 204773 (222955)	Loss/tok 7.2914 (7.3879)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
0: TEST [0][9/12]	Time 0.4927 (0.7828)	Decoder iters 149.0 (149.0)	Tok/s 25800 (24349)
1: TEST [0][9/12]	Time 0.4926 (0.7828)	Decoder iters 149.0 (149.0)	Tok/s 21688 (23282)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.9000	Validation Loss: 7.3338	Test BLEU: 0.10
0: Performance: Epoch: 0	Training: 94249 Tok/s	Validation: 427505 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/255]	Time 0.298 (0.000)	Data 6.03e-02 (0.00e+00)	Tok/s 43695 (0)	Loss/tok 6.5707 (6.5707)	LR 2.000e-03
0: TRAIN [1][0/255]	Time 0.299 (0.000)	Data 6.61e-02 (0.00e+00)	Tok/s 43520 (0)	Loss/tok 6.5490 (6.5490)	LR 2.000e-03
1: TRAIN [1][10/255]	Time 0.308 (0.241)	Data 8.94e-05 (1.21e-04)	Tok/s 25389 (39874)	Loss/tok 6.1290 (6.4401)	LR 2.000e-03
0: TRAIN [1][10/255]	Time 0.333 (0.240)	Data 9.51e-05 (1.22e-04)	Tok/s 23767 (42307)	Loss/tok 6.2608 (6.4744)	LR 2.000e-03
1: TRAIN [1][20/255]	Time 0.282 (0.276)	Data 9.47e-05 (1.21e-04)	Tok/s 45939 (41760)	Loss/tok 6.3515 (6.5010)	LR 2.000e-03
0: TRAIN [1][20/255]	Time 0.270 (0.276)	Data 2.23e-04 (1.33e-04)	Tok/s 48441 (42782)	Loss/tok 6.3639 (6.5160)	LR 2.000e-03
1: TRAIN [1][30/255]	Time 0.339 (0.284)	Data 2.23e-04 (1.25e-04)	Tok/s 53637 (44179)	Loss/tok 6.4495 (6.4768)	LR 2.000e-03
0: TRAIN [1][30/255]	Time 0.330 (0.284)	Data 1.04e-04 (1.27e-04)	Tok/s 54923 (44802)	Loss/tok 6.4536 (6.4912)	LR 2.000e-03
Traceback (most recent call last):
  File "train.py", line 659, in <module>
    main()
  File "train.py", line 582, in main
    train_loss, train_perf = trainer.optimize(train_loader)
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/trainer.py", line 352, in optimize
    output = self.feed_data(data_loader, training=True)
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/trainer.py", line 236, in feed_data
Traceback (most recent call last):
  File "train.py", line 659, in <module>
    stats = self.iterate(src, tgt, update, training=training)
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/trainer.py", line 191, in iterate
    self.fp_optimizer.step(loss, self.optimizer, self.scheduler,
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/fp_optimizers.py", line 181, in step
    main()
  File "train.py", line 582, in main
    loss.backward()    
train_loss, train_perf = trainer.optimize(train_loader)
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/trainer.py", line 352, in optimize
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 484, in backward
    output = self.feed_data(data_loader, training=True)
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/trainer.py", line 236, in feed_data
    stats = self.iterate(src, tgt, update, training=training)
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/trainer.py", line 191, in iterate
    self.fp_optimizer.step(loss, self.optimizer, self.scheduler,
  File "/workspace/benchmark/Translation/GNMT/seq2seq/train/fp_optimizers.py", line 181, in step
    loss.backward()
  File "/opt/conda/lib/python3.8/site-packages/torch/_tensor.py", line 484, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 191, in backward
    torch.autograd.backward(
  File "/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py", line 191, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda    .Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward passOutOfMemoryError
: CUDA out of memory. Tried to allocate 1.67 GiB (GPU 1; 23.69 GiB total capacity; 15.61 GiB already allocated; 1.67 GiB free; 21.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.67 GiB (GPU 0; 23.69 GiB total capacity; 15.60 GiB already allocated; 1.67 GiB free; 21.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 5775) of binary: /opt/conda/bin/python3
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-10-17_04:52:12
  host      : 774a59ba9d3c
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 5776)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-10-17_04:52:12
  host      : 774a59ba9d3c
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 5775)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE!
