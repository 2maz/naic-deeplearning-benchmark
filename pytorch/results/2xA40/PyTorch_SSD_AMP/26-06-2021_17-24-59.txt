Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 648k/97.8M [00:00<00:15, 6.57MB/s]  1%|          | 536k/97.8M [00:00<00:18, 5.42MB/s]  6%|▌         | 5.38M/97.8M [00:00<00:13, 7.40MB/s]  5%|▌         | 5.23M/97.8M [00:00<00:10, 8.86MB/s] 10%|█         | 10.0M/97.8M [00:00<00:07, 11.8MB/s] 10%|█         | 10.0M/97.8M [00:00<00:09, 9.91MB/s] 15%|█▍        | 14.6M/97.8M [00:00<00:05, 15.2MB/s] 15%|█▍        | 14.6M/97.8M [00:00<00:06, 13.0MB/s] 21%|██        | 20.1M/97.8M [00:00<00:04, 16.9MB/s] 20%|██        | 20.0M/97.8M [00:00<00:04, 19.5MB/s] 26%|██▋       | 25.7M/97.8M [00:00<00:03, 21.5MB/s] 26%|██▌       | 25.4M/97.8M [00:00<00:03, 24.2MB/s] 32%|███▏      | 31.1M/97.8M [00:00<00:02, 26.5MB/s] 32%|███▏      | 31.0M/97.8M [00:00<00:02, 29.4MB/s] 38%|███▊      | 36.7M/97.8M [00:00<00:02, 31.6MB/s] 37%|███▋      | 36.6M/97.8M [00:00<00:01, 34.5MB/s] 43%|████▎     | 42.0M/97.8M [00:00<00:01, 36.3MB/s] 43%|████▎     | 42.1M/97.8M [00:00<00:01, 39.3MB/s] 49%|████▊     | 47.6M/97.8M [00:01<00:01, 41.1MB/s] 49%|████▊     | 47.6M/97.8M [00:01<00:01, 43.4MB/s] 54%|█████▍    | 53.1M/97.8M [00:01<00:01, 44.8MB/s] 54%|█████▍    | 53.1M/97.8M [00:01<00:00, 46.9MB/s] 60%|█████▉    | 58.4M/97.8M [00:01<00:00, 47.6MB/s] 60%|██████    | 58.9M/97.8M [00:01<00:00, 50.2MB/s] 66%|██████▌   | 64.1M/97.8M [00:01<00:00, 50.6MB/s] 66%|██████▌   | 64.3M/97.8M [00:01<00:00, 51.8MB/s] 72%|███████▏  | 70.3M/97.8M [00:01<00:00, 54.6MB/s] 71%|███████   | 69.5M/97.8M [00:01<00:00, 51.2MB/s] 78%|███████▊  | 75.8M/97.8M [00:01<00:00, 55.4MB/s] 77%|███████▋  | 75.0M/97.8M [00:01<00:00, 53.0MB/s] 83%|████████▎ | 80.7M/97.8M [00:01<00:00, 54.8MB/s] 83%|████████▎ | 81.4M/97.8M [00:01<00:00, 55.0MB/s] 89%|████████▊ | 86.5M/97.8M [00:01<00:00, 56.4MB/s] 89%|████████▉ | 86.8M/97.8M [00:01<00:00, 54.9MB/s] 94%|█████████▍| 92.1M/97.8M [00:01<00:00, 56.3MB/s] 94%|█████████▍| 92.3M/97.8M [00:01<00:00, 55.8MB/s]100%|█████████▉| 97.7M/97.8M [00:01<00:00, 57.0MB/s]100%|█████████▉| 97.7M/97.8M [00:01<00:00, 55.9MB/s]100%|██████████| 97.8M/97.8M [00:01<00:00, 53.3MB/s]
100%|██████████| 97.8M/97.8M [00:01<00:00, 53.2MB/s]
DLL 2021-06-26 17:25:03.546610 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-06-26 17:25:03.550046 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 3674
Using seed = 3524
loading annotations into memory...
loading annotations into memory...
Done (t=0.37s)
creating index...
Done (t=0.37s)
creating index...
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
DLL 2021-06-26 17:27:03.642526 - () avg_img/sec : 175.27116557453195  med_img/sec : 175.2936117474043  min_img/sec : 174.5773203351975  max_img/sec : 175.5092666329294 
Done benchmarking. Total images: 10240	total time: 58.424	Average images/sec: 175.271	Median images/sec: 175.294
DLL 2021-06-26 17:27:03.642682 - () avg_img/sec : 175.27255318081134  med_img/sec : 175.2756276606264  min_img/sec : 174.631124862327  max_img/sec : 175.55804978414574 
Done benchmarking. Total images: 10240	total time: 58.423	Average images/sec: 175.273	Median images/sec: 175.276
Training performance = 350.5692443847656 FPS
DLL 2021-06-26 17:27:03.643149 - (0,) time : 104.413982629776 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-26 17:27:03.643397 - () total time : 104.413982629776 
DLL 2021-06-26 17:27:03.643414 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-06-26 17:27:03.643504 - () total time : 104.41424441337585 
DLL 2021-06-26 17:27:03.643529 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
