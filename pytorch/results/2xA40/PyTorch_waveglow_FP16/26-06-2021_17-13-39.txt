DLL 2021-06-26 17:13:41.829699 - PARAMETER output : ./ 
DLL 2021-06-26 17:13:41.829769 - PARAMETER dataset_path : /data/tacotron2/LJSpeech-1.1 
DLL 2021-06-26 17:13:41.829792 - PARAMETER model_name : WaveGlow 
DLL 2021-06-26 17:13:41.829810 - PARAMETER log_file : nvlog.json 
DLL 2021-06-26 17:13:41.829828 - PARAMETER anneal_steps : None 
DLL 2021-06-26 17:13:41.829850 - PARAMETER anneal_factor : 0.1 
DLL 2021-06-26 17:13:41.829867 - PARAMETER epochs : 2 
DLL 2021-06-26 17:13:41.829884 - PARAMETER epochs_per_checkpoint : 50 
DLL 2021-06-26 17:13:41.829900 - PARAMETER checkpoint_path :  
DLL 2021-06-26 17:13:41.829917 - PARAMETER resume_from_last : False 
DLL 2021-06-26 17:13:41.829934 - PARAMETER dynamic_loss_scaling : True 
DLL 2021-06-26 17:13:41.829952 - PARAMETER amp : False 
DLL 2021-06-26 17:13:41.829972 - PARAMETER cudnn_enabled : True 
DLL 2021-06-26 17:13:41.829988 - PARAMETER cudnn_benchmark : True 
DLL 2021-06-26 17:13:41.830003 - PARAMETER disable_uniform_initialize_bn_weight : False 
DLL 2021-06-26 17:13:41.830019 - PARAMETER use_saved_learning_rate : False 
DLL 2021-06-26 17:13:41.830034 - PARAMETER learning_rate : 0.0 
DLL 2021-06-26 17:13:41.830049 - PARAMETER weight_decay : 0.0 
DLL 2021-06-26 17:13:41.830063 - PARAMETER grad_clip_thresh : 65504.0 
DLL 2021-06-26 17:13:41.830079 - PARAMETER batch_size : 32 
DLL 2021-06-26 17:13:41.830094 - PARAMETER grad_clip : 5.0 
DLL 2021-06-26 17:13:41.830109 - PARAMETER load_mel_from_disk : False 
DLL 2021-06-26 17:13:41.830124 - PARAMETER training_files : filelists/ljs_audio_text_train_subset_625_filelist.txt 
DLL 2021-06-26 17:13:41.830139 - PARAMETER validation_files : filelists/ljs_audio_text_val_filelist.txt 
DLL 2021-06-26 17:13:41.830153 - PARAMETER text_cleaners : ['english_cleaners'] 
DLL 2021-06-26 17:13:41.830172 - PARAMETER max_wav_value : 32768.0 
DLL 2021-06-26 17:13:41.830188 - PARAMETER sampling_rate : 22050 
DLL 2021-06-26 17:13:41.830202 - PARAMETER filter_length : 1024 
DLL 2021-06-26 17:13:41.830217 - PARAMETER hop_length : 256 
DLL 2021-06-26 17:13:41.830231 - PARAMETER win_length : 1024 
DLL 2021-06-26 17:13:41.830246 - PARAMETER mel_fmin : 0.0 
DLL 2021-06-26 17:13:41.830261 - PARAMETER mel_fmax : 8000.0 
DLL 2021-06-26 17:13:41.830276 - PARAMETER rank : 0 
DLL 2021-06-26 17:13:41.830290 - PARAMETER world_size : 2 
DLL 2021-06-26 17:13:41.830309 - PARAMETER dist_url : tcp://localhost:23456 
DLL 2021-06-26 17:13:41.830325 - PARAMETER group_name : group_name 
DLL 2021-06-26 17:13:41.830339 - PARAMETER dist_backend : nccl 
DLL 2021-06-26 17:13:41.830354 - PARAMETER bench_class :  
DLL 2021-06-26 17:13:41.830368 - PARAMETER model_name : Tacotron2_PyT 
Initializing Distributed
Done initializing distributed
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
DLL 2021-06-26 17:14:18.702348 - (0, 0) glob_iter/iters_per_epoch : 0/9 
DLL 2021-06-26 17:14:29.760236 - (0, 0) train_loss : 0.002134077250957489 
DLL 2021-06-26 17:14:40.070122 - (0, 0) train_items_per_sec : 23961.251258334683 
DLL 2021-06-26 17:14:40.070233 - (0, 0) train_iter_time : 21.367832359001113 
DLL 2021-06-26 17:14:40.072175 - (0, 1) glob_iter/iters_per_epoch : 1/9 
DLL 2021-06-26 17:14:41.017369 - (0, 1) train_loss : 0.0024991093669086695 
DLL 2021-06-26 17:14:42.889679 - (0, 1) train_items_per_sec : 181721.53745173005 
DLL 2021-06-26 17:14:42.889818 - (0, 1) train_iter_time : 2.81749762400068 
DLL 2021-06-26 17:14:42.891675 - (0, 2) glob_iter/iters_per_epoch : 2/9 
DLL 2021-06-26 17:14:44.995358 - (0, 2) train_loss : 0.0021653883159160614 
DLL 2021-06-26 17:14:46.835778 - (0, 2) train_items_per_sec : 129813.50867108359 
DLL 2021-06-26 17:14:46.835870 - (0, 2) train_iter_time : 3.9441195700001117 
DLL 2021-06-26 17:14:46.837595 - (0, 3) glob_iter/iters_per_epoch : 3/9 
DLL 2021-06-26 17:14:48.972376 - (0, 3) train_loss : 0.0023593090008944273 
DLL 2021-06-26 17:14:50.812193 - (0, 3) train_items_per_sec : 128817.84456057409 
DLL 2021-06-26 17:14:50.812284 - (0, 3) train_iter_time : 3.974604618999365 
DLL 2021-06-26 17:14:50.813981 - (0, 4) glob_iter/iters_per_epoch : 4/9 
DLL 2021-06-26 17:14:52.914905 - (0, 4) train_loss : 0.0019766020122915506 
DLL 2021-06-26 17:14:54.757162 - (0, 4) train_items_per_sec : 129844.2789887067 
DLL 2021-06-26 17:14:54.757268 - (0, 4) train_iter_time : 3.943184898000254 
DLL 2021-06-26 17:14:54.759090 - (0, 5) glob_iter/iters_per_epoch : 5/9 
DLL 2021-06-26 17:14:57.014927 - (0, 5) train_loss : 0.002290839096531272 
DLL 2021-06-26 17:14:58.856658 - (0, 5) train_items_per_sec : 124951.96591402541 
DLL 2021-06-26 17:14:58.856739 - (0, 5) train_iter_time : 4.0975745859996096 
DLL 2021-06-26 17:14:58.858538 - (0, 6) glob_iter/iters_per_epoch : 6/9 
DLL 2021-06-26 17:15:01.040727 - (0, 6) train_loss : 0.0020375356543809175 
DLL 2021-06-26 17:15:02.879547 - (0, 6) train_items_per_sec : 127331.05620521857 
DLL 2021-06-26 17:15:02.879628 - (0, 6) train_iter_time : 4.021014317000663 
DLL 2021-06-26 17:15:02.881249 - (0, 7) glob_iter/iters_per_epoch : 7/9 
DLL 2021-06-26 17:15:05.050955 - (0, 7) train_loss : 0.0027163890190422535 
DLL 2021-06-26 17:15:06.889234 - (0, 7) train_items_per_sec : 127744.76863491365 
DLL 2021-06-26 17:15:06.889328 - (0, 7) train_iter_time : 4.00799191599981 
DLL 2021-06-26 17:15:06.891011 - (0, 8) glob_iter/iters_per_epoch : 8/9 
DLL 2021-06-26 17:15:09.210954 - (0, 8) train_loss : 0.002115691313520074 
DLL 2021-06-26 17:15:11.051832 - (0, 8) train_items_per_sec : 123052.45409639487 
DLL 2021-06-26 17:15:11.051913 - (0, 8) train_iter_time : 4.160827216001053 
DLL 2021-06-26 17:15:11.126963 - (0,) train_items_per_sec : 121915.40730899795 
DLL 2021-06-26 17:15:11.127017 - (0,) train_loss : 0.002115691313520074 
DLL 2021-06-26 17:15:11.127051 - (0,) train_epoch_time : 52.75720086799993 
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 151, in forward
    self.cond_layers[i](spect),
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 259, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 1; 44.56 GiB total capacity; 36.85 GiB already allocated; 110.06 MiB free; 42.40 GiB reserved in total by PyTorch)
DLL 2021-06-26 17:15:13.614069 - (0, 9, 0) val_items_per_sec : 232118.3857915828 
DLL 2021-06-26 17:15:18.009739 - (0, 9, 1) val_items_per_sec : 65546.0456636336 
DLL 2021-06-26 17:15:18.117310 - (0,) val_loss : 0.0022867294028401375 
DLL 2021-06-26 17:15:18.117403 - (0,) val_items_per_sec : 148832.2157276082 
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0.pt
DLL 2021-06-26 17:15:21.750099 - (1, 0) glob_iter/iters_per_epoch : 9/9 
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 151, in forward
    self.cond_layers[i](spect),
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 263, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py", line 259, in _conv_forward
    return F.conv1d(input, weight, bias, self.stride,
RuntimeError: CUDA out of memory. Tried to allocate 126.00 MiB (GPU 0; 44.56 GiB total capacity; 36.85 GiB already allocated; 110.06 MiB free; 42.40 GiB reserved in total by PyTorch)
DONE!
