1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Quadro RTX 6000
GPU 1: Quadro RTX 6000

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=288, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Quadro RTX 6000
GPU 1: Quadro RTX 6000

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=288, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
0: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
0: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 125
0: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
1: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 339
0: Scheduler decay interval: 42
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Executing preallocation
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 339
1: Scheduler decay interval: 42
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/255]	Time 0.640 (0.640)	Data 1.85e-01 (1.85e-01)	Tok/s 12566 (12566)	Loss/tok 10.6113 (10.6113)	LR 2.047e-05
1: TRAIN [0][0/255]	Time 0.632 (0.632)	Data 1.99e-01 (1.99e-01)	Tok/s 12452 (12452)	Loss/tok 10.6137 (10.6137)	LR 2.047e-05
1: TRAIN [0][10/255]	Time 0.446 (0.653)	Data 3.48e-04 (1.83e-02)	Tok/s 17853 (18771)	Loss/tok 9.4478 (10.1071)	LR 2.576e-05
0: TRAIN [0][10/255]	Time 0.448 (0.654)	Data 2.35e-04 (1.71e-02)	Tok/s 17397 (18646)	Loss/tok 9.4700 (10.1090)	LR 2.576e-05
1: TRAIN [0][20/255]	Time 0.653 (0.742)	Data 2.58e-04 (9.80e-03)	Tok/s 19775 (19412)	Loss/tok 9.1773 (9.7406)	LR 3.244e-05
0: TRAIN [0][20/255]	Time 0.653 (0.742)	Data 2.63e-04 (9.06e-03)	Tok/s 19956 (19322)	Loss/tok 9.2402 (9.7366)	LR 3.244e-05
0: TRAIN [0][30/255]	Time 0.256 (0.704)	Data 2.33e-04 (6.22e-03)	Tok/s 15045 (19198)	Loss/tok 8.6683 (9.5352)	LR 4.083e-05
1: TRAIN [0][30/255]	Time 0.256 (0.704)	Data 2.48e-04 (6.72e-03)	Tok/s 14915 (19234)	Loss/tok 8.7360 (9.5362)	LR 4.083e-05
0: TRAIN [0][40/255]	Time 0.659 (0.699)	Data 2.27e-04 (4.76e-03)	Tok/s 19555 (19229)	Loss/tok 8.7282 (9.3625)	LR 5.141e-05
1: TRAIN [0][40/255]	Time 0.658 (0.699)	Data 2.59e-04 (5.14e-03)	Tok/s 19839 (19266)	Loss/tok 8.6765 (9.3608)	LR 5.141e-05
1: TRAIN [0][50/255]	Time 0.659 (0.688)	Data 2.41e-04 (4.18e-03)	Tok/s 19728 (19251)	Loss/tok 8.4301 (9.2165)	LR 6.472e-05
0: TRAIN [0][50/255]	Time 0.660 (0.688)	Data 2.29e-04 (3.87e-03)	Tok/s 19490 (19184)	Loss/tok 8.4618 (9.2163)	LR 6.472e-05
1: TRAIN [0][60/255]	Time 1.133 (0.690)	Data 2.44e-04 (3.54e-03)	Tok/s 20920 (19173)	Loss/tok 8.4427 (9.0845)	LR 8.148e-05
0: TRAIN [0][60/255]	Time 1.133 (0.690)	Data 2.83e-04 (3.28e-03)	Tok/s 20601 (19125)	Loss/tok 8.4578 (9.0844)	LR 8.148e-05
0: TRAIN [0][70/255]	Time 0.886 (0.709)	Data 2.33e-04 (2.85e-03)	Tok/s 20274 (19235)	Loss/tok 8.1147 (8.9362)	LR 1.026e-04
1: TRAIN [0][70/255]	Time 0.888 (0.709)	Data 2.46e-04 (3.08e-03)	Tok/s 20459 (19274)	Loss/tok 8.1357 (8.9427)	LR 1.026e-04
1: TRAIN [0][80/255]	Time 0.668 (0.702)	Data 2.41e-04 (2.73e-03)	Tok/s 19689 (19228)	Loss/tok 7.9153 (8.8319)	LR 1.291e-04
0: TRAIN [0][80/255]	Time 0.668 (0.702)	Data 2.36e-04 (2.53e-03)	Tok/s 19332 (19194)	Loss/tok 7.8679 (8.8261)	LR 1.291e-04
1: TRAIN [0][90/255]	Time 0.455 (0.706)	Data 2.52e-04 (2.45e-03)	Tok/s 17112 (19230)	Loss/tok 7.6907 (8.7241)	LR 1.626e-04
0: TRAIN [0][90/255]	Time 0.455 (0.706)	Data 2.41e-04 (2.28e-03)	Tok/s 17392 (19210)	Loss/tok 7.6460 (8.7166)	LR 1.626e-04
0: TRAIN [0][100/255]	Time 0.668 (0.709)	Data 2.34e-04 (2.08e-03)	Tok/s 19490 (19229)	Loss/tok 7.6554 (8.6242)	LR 2.047e-04
1: TRAIN [0][100/255]	Time 0.668 (0.709)	Data 2.45e-04 (2.24e-03)	Tok/s 19203 (19250)	Loss/tok 7.7173 (8.6292)	LR 2.047e-04
0: TRAIN [0][110/255]	Time 0.668 (0.712)	Data 3.38e-04 (1.91e-03)	Tok/s 19529 (19220)	Loss/tok 7.6861 (8.5471)	LR 2.576e-04
1: TRAIN [0][110/255]	Time 0.670 (0.712)	Data 2.37e-04 (2.06e-03)	Tok/s 19435 (19244)	Loss/tok 7.5903 (8.5512)	LR 2.576e-04
1: TRAIN [0][120/255]	Time 0.890 (0.717)	Data 2.42e-04 (1.91e-03)	Tok/s 20563 (19237)	Loss/tok 7.7976 (8.4832)	LR 3.244e-04
0: TRAIN [0][120/255]	Time 0.891 (0.717)	Data 2.43e-04 (1.77e-03)	Tok/s 20522 (19212)	Loss/tok 7.7718 (8.4805)	LR 3.244e-04
0: TRAIN [0][130/255]	Time 0.456 (0.715)	Data 2.19e-04 (1.66e-03)	Tok/s 17440 (19192)	Loss/tok 7.3374 (8.4228)	LR 4.083e-04
1: TRAIN [0][130/255]	Time 0.456 (0.715)	Data 2.38e-04 (1.78e-03)	Tok/s 17475 (19211)	Loss/tok 7.3590 (8.4252)	LR 4.083e-04
1: TRAIN [0][140/255]	Time 0.887 (0.714)	Data 2.58e-04 (1.67e-03)	Tok/s 20421 (19194)	Loss/tok 7.7526 (8.3787)	LR 5.141e-04
0: TRAIN [0][140/255]	Time 0.887 (0.714)	Data 2.33e-04 (1.56e-03)	Tok/s 20230 (19177)	Loss/tok 7.7521 (8.3764)	LR 5.141e-04
0: TRAIN [0][150/255]	Time 0.888 (0.714)	Data 2.42e-04 (1.47e-03)	Tok/s 20371 (19176)	Loss/tok 7.7143 (8.3267)	LR 6.472e-04
1: TRAIN [0][150/255]	Time 0.891 (0.714)	Data 3.51e-04 (1.58e-03)	Tok/s 20368 (19186)	Loss/tok 7.6956 (8.3286)	LR 6.472e-04
0: TRAIN [0][160/255]	Time 0.883 (0.721)	Data 2.53e-04 (1.39e-03)	Tok/s 20538 (19227)	Loss/tok 7.6322 (8.2794)	LR 8.148e-04
1: TRAIN [0][160/255]	Time 0.884 (0.721)	Data 2.45e-04 (1.50e-03)	Tok/s 20553 (19237)	Loss/tok 7.7017 (8.2812)	LR 8.148e-04
1: TRAIN [0][170/255]	Time 0.886 (0.719)	Data 2.39e-04 (1.43e-03)	Tok/s 20510 (19220)	Loss/tok 7.6453 (8.2392)	LR 1.026e-03
0: TRAIN [0][170/255]	Time 0.886 (0.719)	Data 2.38e-04 (1.33e-03)	Tok/s 20501 (19215)	Loss/tok 7.6496 (8.2370)	LR 1.026e-03
1: TRAIN [0][180/255]	Time 0.891 (0.717)	Data 2.52e-04 (1.36e-03)	Tok/s 20535 (19204)	Loss/tok 7.9710 (8.2034)	LR 1.291e-03
0: TRAIN [0][180/255]	Time 0.891 (0.717)	Data 2.31e-04 (1.27e-03)	Tok/s 20299 (19195)	Loss/tok 7.9533 (8.2025)	LR 1.291e-03
1: TRAIN [0][190/255]	Time 0.889 (0.712)	Data 2.61e-04 (1.30e-03)	Tok/s 20419 (19145)	Loss/tok 7.6008 (8.1738)	LR 1.626e-03
0: TRAIN [0][190/255]	Time 0.889 (0.712)	Data 2.68e-04 (1.21e-03)	Tok/s 20665 (19140)	Loss/tok 7.6233 (8.1731)	LR 1.626e-03
0: TRAIN [0][200/255]	Time 0.891 (0.713)	Data 2.20e-04 (1.17e-03)	Tok/s 20255 (19129)	Loss/tok 7.4617 (8.1344)	LR 2.000e-03
1: TRAIN [0][200/255]	Time 0.934 (0.713)	Data 2.59e-04 (1.25e-03)	Tok/s 19524 (19119)	Loss/tok 7.4934 (8.1356)	LR 2.000e-03
1: TRAIN [0][210/255]	Time 0.461 (0.714)	Data 2.41e-04 (1.20e-03)	Tok/s 16964 (19132)	Loss/tok 6.8033 (8.0917)	LR 2.000e-03
0: TRAIN [0][210/255]	Time 0.461 (0.714)	Data 2.54e-04 (1.12e-03)	Tok/s 16711 (19133)	Loss/tok 6.7057 (8.0906)	LR 2.000e-03
1: TRAIN [0][220/255]	Time 0.462 (0.714)	Data 2.47e-04 (1.16e-03)	Tok/s 16946 (19114)	Loss/tok 6.7217 (8.0521)	LR 2.000e-03
0: TRAIN [0][220/255]	Time 0.463 (0.714)	Data 2.41e-04 (1.08e-03)	Tok/s 16937 (19118)	Loss/tok 6.6893 (8.0509)	LR 2.000e-03
1: TRAIN [0][230/255]	Time 0.460 (0.716)	Data 2.38e-04 (1.12e-03)	Tok/s 17005 (19108)	Loss/tok 6.6714 (8.0070)	LR 2.000e-03
0: TRAIN [0][230/255]	Time 0.460 (0.716)	Data 2.37e-04 (1.05e-03)	Tok/s 16887 (19115)	Loss/tok 6.5507 (8.0046)	LR 2.000e-03
1: TRAIN [0][240/255]	Time 0.668 (0.716)	Data 2.47e-04 (1.08e-03)	Tok/s 19524 (19102)	Loss/tok 6.7916 (7.9600)	LR 2.000e-03
0: TRAIN [0][240/255]	Time 0.668 (0.716)	Data 2.43e-04 (1.01e-03)	Tok/s 19250 (19111)	Loss/tok 6.7365 (7.9568)	LR 2.000e-03
0: TRAIN [0][250/255]	Time 0.458 (0.713)	Data 2.50e-04 (9.84e-04)	Tok/s 16873 (19088)	Loss/tok 6.4273 (7.9133)	LR 2.000e-03
1: TRAIN [0][250/255]	Time 0.458 (0.713)	Data 2.56e-04 (1.05e-03)	Tok/s 17276 (19081)	Loss/tok 6.2967 (7.9162)	LR 2.000e-03
0: Running validation on dev set
1: Running validation on dev set
0: Executing preallocation
1: Executing preallocation
1: VALIDATION [0][0/40]	Time 0.157 (0.157)	Data 4.22e-03 (4.22e-03)	Tok/s 54736 (54736)	Loss/tok 7.5920 (7.5920)
0: VALIDATION [0][0/40]	Time 0.243 (0.243)	Data 4.24e-03 (4.24e-03)	Tok/s 43105 (43105)	Loss/tok 7.6613 (7.6613)
1: VALIDATION [0][10/40]	Time 0.079 (0.106)	Data 3.75e-03 (3.91e-03)	Tok/s 57481 (56404)	Loss/tok 7.4950 (7.4954)
0: VALIDATION [0][10/40]	Time 0.082 (0.116)	Data 3.83e-03 (3.91e-03)	Tok/s 56508 (55571)	Loss/tok 7.3048 (7.5116)
1: VALIDATION [0][20/40]	Time 0.058 (0.088)	Data 3.70e-03 (3.84e-03)	Tok/s 54457 (56003)	Loss/tok 7.1870 (7.4291)
0: VALIDATION [0][20/40]	Time 0.059 (0.094)	Data 3.71e-03 (3.85e-03)	Tok/s 54521 (55419)	Loss/tok 7.2603 (7.4307)
1: VALIDATION [0][30/40]	Time 0.040 (0.075)	Data 3.65e-03 (3.79e-03)	Tok/s 52865 (55108)	Loss/tok 7.1068 (7.3805)
0: VALIDATION [0][30/40]	Time 0.042 (0.080)	Data 3.71e-03 (3.81e-03)	Tok/s 51038 (54523)	Loss/tok 7.1683 (7.3858)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [0][9/12]	Time 2.2261 (3.0794)	Decoder iters 149.0 (149.0)	Tok/s 4809 (6274)
0: TEST [0][9/12]	Time 2.2312 (3.0799)	Decoder iters 149.0 (149.0)	Tok/s 5464 (6690)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.8944	Validation Loss: 7.3373	Test BLEU: 0.10
0: Performance: Epoch: 0	Training: 38180 Tok/s	Validation: 106088 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/255]	Time 0.817 (0.817)	Data 1.65e-01 (1.65e-01)	Tok/s 15933 (15933)	Loss/tok 6.5688 (6.5688)	LR 2.000e-03
0: TRAIN [1][0/255]	Time 0.817 (0.817)	Data 1.73e-01 (1.73e-01)	Tok/s 15937 (15937)	Loss/tok 6.5567 (6.5567)	LR 2.000e-03
0: TRAIN [1][10/255]	Time 0.450 (0.544)	Data 2.27e-04 (1.59e-02)	Tok/s 17613 (17940)	Loss/tok 6.2132 (6.4450)	LR 2.000e-03
1: TRAIN [1][10/255]	Time 0.450 (0.544)	Data 2.73e-04 (1.52e-02)	Tok/s 17348 (17880)	Loss/tok 6.1029 (6.4111)	LR 2.000e-03
0: TRAIN [1][20/255]	Time 0.662 (0.615)	Data 3.19e-04 (8.47e-03)	Tok/s 19710 (18531)	Loss/tok 6.3612 (6.4780)	LR 2.000e-03
1: TRAIN [1][20/255]	Time 0.663 (0.615)	Data 2.58e-04 (8.08e-03)	Tok/s 19584 (18518)	Loss/tok 6.3383 (6.4630)	LR 2.000e-03
0: TRAIN [1][30/255]	Time 0.882 (0.659)	Data 2.67e-04 (5.82e-03)	Tok/s 20569 (18907)	Loss/tok 6.4224 (6.4554)	LR 2.000e-03
1: TRAIN [1][30/255]	Time 0.882 (0.659)	Data 2.64e-04 (5.56e-03)	Tok/s 20581 (18899)	Loss/tok 6.4051 (6.4390)	LR 2.000e-03
1: TRAIN [1][40/255]	Time 1.135 (0.712)	Data 2.56e-04 (4.27e-03)	Tok/s 20703 (19114)	Loss/tok 6.4399 (6.4240)	LR 2.000e-03
0: TRAIN [1][40/255]	Time 1.134 (0.712)	Data 3.14e-04 (4.47e-03)	Tok/s 20717 (19118)	Loss/tok 6.5234 (6.4407)	LR 2.000e-03
1: TRAIN [1][50/255]	Time 0.656 (0.704)	Data 2.54e-04 (3.48e-03)	Tok/s 19933 (19080)	Loss/tok 5.9911 (6.3691)	LR 2.000e-03
0: TRAIN [1][50/255]	Time 0.656 (0.704)	Data 2.55e-04 (3.65e-03)	Tok/s 20010 (19095)	Loss/tok 6.1375 (6.3920)	LR 2.000e-03
1: TRAIN [1][60/255]	Time 0.664 (0.684)	Data 2.44e-04 (2.95e-03)	Tok/s 19483 (18957)	Loss/tok 6.0240 (6.3127)	LR 2.000e-03
0: TRAIN [1][60/255]	Time 0.664 (0.684)	Data 2.41e-04 (3.09e-03)	Tok/s 19661 (18982)	Loss/tok 5.9943 (6.3277)	LR 2.000e-03
1: TRAIN [1][70/255]	Time 0.661 (0.685)	Data 2.90e-04 (2.57e-03)	Tok/s 19515 (18981)	Loss/tok 5.9060 (6.2650)	LR 2.000e-03
0: TRAIN [1][70/255]	Time 0.666 (0.685)	Data 2.53e-04 (2.69e-03)	Tok/s 19507 (19021)	Loss/tok 5.8570 (6.2800)	LR 2.000e-03
1: TRAIN [1][80/255]	Time 0.663 (0.694)	Data 2.68e-04 (2.29e-03)	Tok/s 19588 (19020)	Loss/tok 5.7447 (6.2228)	LR 2.000e-03
0: TRAIN [1][80/255]	Time 0.663 (0.694)	Data 2.91e-04 (2.39e-03)	Tok/s 19610 (19069)	Loss/tok 5.7800 (6.2368)	LR 2.000e-03
0: TRAIN [1][90/255]	Time 0.453 (0.699)	Data 2.70e-04 (2.16e-03)	Tok/s 17257 (19107)	Loss/tok 5.3659 (6.1895)	LR 1.000e-03
1: TRAIN [1][90/255]	Time 0.453 (0.699)	Data 2.53e-04 (2.06e-03)	Tok/s 17356 (19059)	Loss/tok 5.3092 (6.1754)	LR 1.000e-03
0: TRAIN [1][100/255]	Time 0.458 (0.695)	Data 2.37e-04 (1.97e-03)	Tok/s 17399 (19053)	Loss/tok 5.2081 (6.1406)	LR 1.000e-03
1: TRAIN [1][100/255]	Time 0.458 (0.695)	Data 2.36e-04 (1.88e-03)	Tok/s 17391 (19010)	Loss/tok 5.2825 (6.1279)	LR 1.000e-03
0: TRAIN [1][110/255]	Time 0.458 (0.699)	Data 2.51e-04 (1.81e-03)	Tok/s 17379 (19051)	Loss/tok 5.0523 (6.0967)	LR 1.000e-03
1: TRAIN [1][110/255]	Time 0.457 (0.699)	Data 3.08e-04 (1.74e-03)	Tok/s 17312 (19019)	Loss/tok 5.1434 (6.0866)	LR 1.000e-03
1: TRAIN [1][120/255]	Time 0.885 (0.695)	Data 2.42e-04 (1.61e-03)	Tok/s 20693 (19006)	Loss/tok 5.6471 (6.0386)	LR 1.000e-03
0: TRAIN [1][120/255]	Time 0.886 (0.695)	Data 2.38e-04 (1.68e-03)	Tok/s 20281 (19033)	Loss/tok 5.6450 (6.0488)	LR 1.000e-03
0: TRAIN [1][130/255]	Time 0.888 (0.706)	Data 2.28e-04 (1.57e-03)	Tok/s 20689 (19070)	Loss/tok 5.5874 (6.0123)	LR 5.000e-04
1: TRAIN [1][130/255]	Time 0.889 (0.706)	Data 2.40e-04 (1.51e-03)	Tok/s 20549 (19042)	Loss/tok 5.5118 (6.0009)	LR 5.000e-04
1: TRAIN [1][140/255]	Time 0.662 (0.704)	Data 2.38e-04 (1.42e-03)	Tok/s 19277 (19024)	Loss/tok 5.4020 (5.9607)	LR 5.000e-04
0: TRAIN [1][140/255]	Time 0.662 (0.704)	Data 2.32e-04 (1.48e-03)	Tok/s 19774 (19055)	Loss/tok 5.3638 (5.9718)	LR 5.000e-04
0: TRAIN [1][150/255]	Time 0.886 (0.710)	Data 2.60e-04 (1.40e-03)	Tok/s 20299 (19113)	Loss/tok 5.4755 (5.9312)	LR 5.000e-04
1: TRAIN [1][150/255]	Time 0.889 (0.710)	Data 2.54e-04 (1.34e-03)	Tok/s 20336 (19086)	Loss/tok 5.4410 (5.9213)	LR 5.000e-04
0: TRAIN [1][160/255]	Time 0.891 (0.708)	Data 2.47e-04 (1.33e-03)	Tok/s 20200 (19092)	Loss/tok 5.4639 (5.8974)	LR 5.000e-04
1: TRAIN [1][160/255]	Time 0.892 (0.708)	Data 2.54e-04 (1.28e-03)	Tok/s 20436 (19068)	Loss/tok 5.5363 (5.8877)	LR 5.000e-04
0: TRAIN [1][170/255]	Time 0.890 (0.704)	Data 2.53e-04 (1.27e-03)	Tok/s 20411 (19053)	Loss/tok 5.4643 (5.8658)	LR 2.500e-04
1: TRAIN [1][170/255]	Time 0.890 (0.704)	Data 2.52e-04 (1.22e-03)	Tok/s 20243 (19032)	Loss/tok 5.4074 (5.8551)	LR 2.500e-04
1: TRAIN [1][180/255]	Time 0.668 (0.701)	Data 2.45e-04 (1.16e-03)	Tok/s 19454 (19028)	Loss/tok 5.1960 (5.8208)	LR 2.500e-04
0: TRAIN [1][180/255]	Time 0.668 (0.701)	Data 2.52e-04 (1.21e-03)	Tok/s 19554 (19052)	Loss/tok 5.1620 (5.8296)	LR 2.500e-04
0: TRAIN [1][190/255]	Time 0.664 (0.705)	Data 2.48e-04 (1.16e-03)	Tok/s 19405 (19094)	Loss/tok 5.0577 (5.7976)	LR 2.500e-04
1: TRAIN [1][190/255]	Time 0.665 (0.705)	Data 2.58e-04 (1.11e-03)	Tok/s 19604 (19069)	Loss/tok 5.0968 (5.7904)	LR 2.500e-04
1: TRAIN [1][200/255]	Time 0.664 (0.707)	Data 2.45e-04 (1.07e-03)	Tok/s 19675 (19083)	Loss/tok 5.1233 (5.7627)	LR 2.500e-04
0: TRAIN [1][200/255]	Time 0.667 (0.707)	Data 2.53e-04 (1.12e-03)	Tok/s 19298 (19107)	Loss/tok 5.0712 (5.7681)	LR 2.500e-04
0: TRAIN [1][210/255]	Time 0.892 (0.707)	Data 2.52e-04 (1.08e-03)	Tok/s 20417 (19119)	Loss/tok 5.4098 (5.7393)	LR 1.250e-04
1: TRAIN [1][210/255]	Time 0.893 (0.707)	Data 2.72e-04 (1.03e-03)	Tok/s 20136 (19097)	Loss/tok 5.2953 (5.7361)	LR 1.250e-04
1: TRAIN [1][220/255]	Time 0.888 (0.701)	Data 2.48e-04 (9.97e-04)	Tok/s 20543 (19042)	Loss/tok 5.3506 (5.7128)	LR 1.250e-04
0: TRAIN [1][220/255]	Time 0.888 (0.701)	Data 2.44e-04 (1.04e-03)	Tok/s 20381 (19059)	Loss/tok 5.2799 (5.7160)	LR 1.250e-04
0: TRAIN [1][230/255]	Time 0.890 (0.701)	Data 2.51e-04 (1.00e-03)	Tok/s 20351 (19051)	Loss/tok 5.2567 (5.6924)	LR 1.250e-04
1: TRAIN [1][230/255]	Time 0.890 (0.701)	Data 2.62e-04 (9.65e-04)	Tok/s 20230 (19032)	Loss/tok 5.2946 (5.6903)	LR 1.250e-04
1: TRAIN [1][240/255]	Time 0.887 (0.706)	Data 2.57e-04 (9.36e-04)	Tok/s 20506 (19057)	Loss/tok 5.3321 (5.6712)	LR 1.250e-04
0: TRAIN [1][240/255]	Time 0.887 (0.706)	Data 2.77e-04 (9.73e-04)	Tok/s 20212 (19075)	Loss/tok 5.2626 (5.6721)	LR 1.250e-04
1: TRAIN [1][250/255]	Time 1.141 (0.712)	Data 2.48e-04 (9.09e-04)	Tok/s 20552 (19076)	Loss/tok 5.4929 (5.6533)	LR 1.250e-04
0: TRAIN [1][250/255]	Time 1.141 (0.712)	Data 2.48e-04 (9.45e-04)	Tok/s 20574 (19094)	Loss/tok 5.4658 (5.6540)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [1][0/40]	Time 0.155 (0.155)	Data 4.32e-03 (4.32e-03)	Tok/s 55299 (55299)	Loss/tok 6.5140 (6.5140)
0: VALIDATION [1][0/40]	Time 0.243 (0.243)	Data 4.30e-03 (4.30e-03)	Tok/s 43041 (43041)	Loss/tok 6.6049 (6.6049)
1: VALIDATION [1][10/40]	Time 0.079 (0.106)	Data 3.73e-03 (3.91e-03)	Tok/s 57537 (56469)	Loss/tok 6.2228 (6.3134)
0: VALIDATION [1][10/40]	Time 0.082 (0.117)	Data 3.90e-03 (3.95e-03)	Tok/s 56267 (55484)	Loss/tok 5.9858 (6.3282)
1: VALIDATION [1][20/40]	Time 0.058 (0.088)	Data 3.69e-03 (3.83e-03)	Tok/s 54791 (55965)	Loss/tok 5.8917 (6.2069)
0: VALIDATION [1][20/40]	Time 0.058 (0.094)	Data 3.76e-03 (3.86e-03)	Tok/s 55330 (55373)	Loss/tok 5.9713 (6.2081)
1: VALIDATION [1][30/40]	Time 0.041 (0.075)	Data 3.65e-03 (3.79e-03)	Tok/s 51102 (55014)	Loss/tok 5.7058 (6.1337)
0: VALIDATION [1][30/40]	Time 0.042 (0.080)	Data 3.74e-03 (3.82e-03)	Tok/s 50914 (54430)	Loss/tok 5.8402 (6.1438)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [1][9/12]	Time 0.8780 (1.7928)	Decoder iters 149.0 (129.8)	Tok/s 4886 (5463)
0: TEST [1][9/12]	Time 0.8772 (1.7928)	Decoder iters 41.0 (137.1)	Tok/s 5218 (6127)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 5.6473	Validation Loss: 6.0799	Test BLEU: 1.62
0: Performance: Epoch: 1	Training: 38181 Tok/s	Validation: 105927 Tok/s
0: Finished epoch 1
0: Total training time 476 s
1: Total training time 476 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 288|                      1.62|                      38180.4|                         7.938|
DONE!
