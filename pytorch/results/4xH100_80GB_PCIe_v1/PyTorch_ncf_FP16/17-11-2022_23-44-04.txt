/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DLL 2022-11-17 23:44:08.867147 - PARAMETER data : /data/ncf/cache/ml-20m  feature_spec_file : feature_spec.yaml  epochs : 2  batch_size : 64000000  valid_batch_size : 1048576  factors : 64  layers : [256, 256, 128, 64]  negative_samples : 4  learning_rate : 0.0045  topk : 10  seed : None  threshold : 1.0  beta1 : 0.25  beta2 : 0.5  eps : 1e-08  dropout : 0.5  checkpoint_dir :   load_checkpoint_path : None  mode : train  grads_accumulated : 1  amp : False  log_path : log.json  world_size : 4  distributed : True  local_rank : 0 
DistributedDataParallel(
  (module): NeuMF(
    (mf_user_embed): Embedding(138493, 64)
    (mf_item_embed): Embedding(26744, 64)
    (mlp_user_embed): Embedding(138493, 128)
    (mlp_item_embed): Embedding(26744, 128)
    (mlp): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (final): Linear(in_features=128, out_features=1, bias=True)
  )
)
31832577 parameters
DistributedDataParallel(
  (module): NeuMF(
    (mf_user_embed): Embedding(138493, 64)
    (mf_item_embed): Embedding(26744, 64)
    (mlp_user_embed): Embedding(138493, 128)
    (mlp_item_embed): Embedding(26744, 128)
    (mlp): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (final): Linear(in_features=128, out_features=1, bias=True)
  )
)
31832577 parameters
DistributedDataParallel(
  (module): NeuMF(
    (mf_user_embed): Embedding(138493, 64)
    (mf_item_embed): Embedding(26744, 64)
    (mlp_user_embed): Embedding(138493, 128)
    (mlp_item_embed): Embedding(26744, 128)
    (mlp): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (final): Linear(in_features=128, out_features=1, bias=True)
  )
)
31832577 parameters
DistributedDataParallel(
  (module): NeuMF(
    (mf_user_embed): Embedding(138493, 64)
    (mf_item_embed): Embedding(26744, 64)
    (mlp_user_embed): Embedding(138493, 128)
    (mlp_item_embed): Embedding(26744, 128)
    (mlp): ModuleList(
      (0): Linear(in_features=256, out_features=256, bias=True)
      (1): Linear(in_features=256, out_features=128, bias=True)
      (2): Linear(in_features=128, out_features=64, bias=True)
    )
    (final): Linear(in_features=128, out_features=1, bias=True)
  )
)
31832577 parameters
Traceback (most recent call last):
  File "ncf.py", line 383, in <module>
    main()
  File "ncf.py", line 310, in main
    outputs = model(user_batch, item_batch)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/benchmark/Recommendation/NCF/neumf.py", line 92, in forward
    xmlp = nn.functional.dropout(xmlp, p=self.dropout, training=self.training)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.26 GiB (GPU 0; 79.18 GiB total capacity; 62.53 GiB already allocated; 4.28 GiB free; 74.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "ncf.py", line 383, in <module>
    main()
  File "ncf.py", line 310, in main
    outputs = model(user_batch, item_batch)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/benchmark/Recommendation/NCF/neumf.py", line 92, in forward
    xmlp = nn.functional.dropout(xmlp, p=self.dropout, training=self.training)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 15.26 GiB (GPU 1; 79.18 GiB total capacity; 62.53 GiB already allocated; 4.28 GiB free; 74.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
Traceback (most recent call last):
  File "ncf.py", line 383, in <module>
  File "ncf.py", line 383, in <module>
        main()main()

  File "ncf.py", line 310, in main
  File "ncf.py", line 310, in main
        outputs = model(user_batch, item_batch)outputs = model(user_batch, item_batch)

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    return forward_call(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1185, in _call_impl
    return forward_call(*input, **kwargs)
  File "/workspace/benchmark/Recommendation/NCF/neumf.py", line 92, in forward
    return forward_call(*input, **kwargs)
  File "/workspace/benchmark/Recommendation/NCF/neumf.py", line 92, in forward
    xmlp = nn.functional.dropout(xmlp, p=self.dropout, training=self.training)
    xmlp = nn.functional.dropout(xmlp, p=self.dropout, training=self.training)  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout

  File "/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py", line 1252, in dropout
        return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)

torch.cudatorch.cuda..OutOfMemoryErrorOutOfMemoryError: : CUDA out of memory. Tried to allocate 15.26 GiB (GPU 2; 79.18 GiB total capacity; 62.53 GiB already allocated; 4.28 GiB free; 74.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
CUDA out of memory. Tried to allocate 15.26 GiB (GPU 3; 79.18 GiB total capacity; 62.53 GiB already allocated; 4.28 GiB free; 74.02 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 3544) of binary: /opt/conda/bin/python
Traceback (most recent call last):
  File "/opt/conda/lib/python3.8/runpy.py", line 194, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/opt/conda/lib/python3.8/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/opt/conda/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
ncf.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2022-11-17_23:44:16
  host      : 62540cf00283
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 3545)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[2]:
  time      : 2022-11-17_23:44:16
  host      : 62540cf00283
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 3546)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
[3]:
  time      : 2022-11-17_23:44:16
  host      : 62540cf00283
  rank      : 3 (local_rank: 3)
  exitcode  : 1 (pid: 3547)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2022-11-17_23:44:16
  host      : 62540cf00283
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 3544)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
