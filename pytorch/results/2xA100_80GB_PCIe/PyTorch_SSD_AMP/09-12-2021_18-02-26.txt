Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  0%|          | 0.00/97.8M [00:00<?, ?B/s]  3%|▎         | 3.38M/97.8M [00:00<00:03, 32.9MB/s]  4%|▍         | 3.94M/97.8M [00:00<00:02, 40.1MB/s]  6%|▌         | 6.06M/97.8M [00:00<00:03, 31.1MB/s]  7%|▋         | 6.45M/97.8M [00:00<00:02, 34.6MB/s]  8%|▊         | 7.99M/97.8M [00:00<00:03, 26.6MB/s]  9%|▉         | 8.86M/97.8M [00:00<00:03, 31.0MB/s] 13%|█▎        | 12.9M/97.8M [00:00<00:02, 33.6MB/s] 12%|█▏        | 11.7M/97.8M [00:00<00:03, 27.4MB/s] 17%|█▋        | 16.3M/97.8M [00:00<00:02, 34.2MB/s] 19%|█▉        | 19.0M/97.8M [00:00<00:02, 30.2MB/s] 14%|█▍        | 13.6M/97.8M [00:00<00:05, 15.6MB/s] 22%|██▏       | 21.6M/97.8M [00:00<00:02, 28.6MB/s] 25%|██▌       | 24.8M/97.8M [00:00<00:02, 29.7MB/s] 28%|██▊       | 27.5M/97.8M [00:00<00:02, 24.7MB/s] 16%|█▌        | 15.2M/97.8M [00:01<00:08, 9.90MB/s] 32%|███▏      | 31.4M/97.8M [00:01<00:02, 27.9MB/s] 36%|███▌      | 34.8M/97.8M [00:01<00:02, 29.9MB/s] 17%|█▋        | 16.5M/97.8M [00:01<00:11, 7.19MB/s] 39%|███▊      | 37.8M/97.8M [00:01<00:02, 26.9MB/s] 18%|█▊        | 17.6M/97.8M [00:01<00:10, 8.12MB/s] 41%|████▏     | 40.5M/97.8M [00:01<00:02, 26.3MB/s] 21%|██        | 20.4M/97.8M [00:01<00:07, 10.2MB/s] 44%|████▍     | 43.3M/97.8M [00:01<00:02, 26.2MB/s] 25%|██▌       | 24.5M/97.8M [00:01<00:05, 13.2MB/s] 48%|████▊     | 46.7M/97.8M [00:01<00:01, 28.3MB/s] 32%|███▏      | 30.9M/97.8M [00:01<00:04, 17.4MB/s] 51%|█████     | 49.5M/97.8M [00:01<00:01, 25.5MB/s] 38%|███▊      | 37.5M/97.8M [00:01<00:02, 22.3MB/s] 54%|█████▍    | 52.7M/97.8M [00:01<00:01, 27.4MB/s] 52%|█████▏    | 50.9M/97.8M [00:01<00:01, 29.8MB/s] 57%|█████▋    | 55.5M/97.8M [00:02<00:01, 28.0MB/s] 59%|█████▉    | 57.9M/97.8M [00:02<00:01, 32.7MB/s] 60%|█████▉    | 58.2M/97.8M [00:02<00:01, 24.1MB/s] 64%|██████▍   | 62.5M/97.8M [00:02<00:01, 28.0MB/s] 65%|██████▌   | 64.0M/97.8M [00:02<00:01, 31.9MB/s] 67%|██████▋   | 65.5M/97.8M [00:02<00:01, 28.5MB/s] 71%|███████   | 69.1M/97.8M [00:02<00:00, 32.4MB/s] 70%|███████   | 68.4M/97.8M [00:02<00:01, 29.0MB/s] 75%|███████▍  | 73.0M/97.8M [00:02<00:00, 32.9MB/s] 75%|███████▌  | 73.7M/97.8M [00:02<00:00, 32.7MB/s] 78%|███████▊  | 76.5M/97.8M [00:02<00:00, 32.2MB/s] 80%|███████▉  | 77.9M/97.8M [00:02<00:00, 34.1MB/s] 82%|████████▏ | 79.8M/97.8M [00:02<00:00, 30.7MB/s] 84%|████████▎ | 81.9M/97.8M [00:02<00:00, 34.3MB/s] 86%|████████▌ | 83.9M/97.8M [00:02<00:00, 33.6MB/s] 88%|████████▊ | 85.7M/97.8M [00:03<00:00, 32.3MB/s] 89%|████████▉ | 87.3M/97.8M [00:03<00:00, 33.4MB/s] 92%|█████████▏| 89.6M/97.8M [00:03<00:00, 34.4MB/s] 93%|█████████▎| 91.1M/97.8M [00:03<00:00, 35.2MB/s] 95%|█████████▌| 93.2M/97.8M [00:03<00:00, 35.0MB/s] 97%|█████████▋| 94.6M/97.8M [00:03<00:00, 35.4MB/s] 99%|█████████▉| 96.8M/97.8M [00:03<00:00, 34.9MB/s]100%|██████████| 97.8M/97.8M [00:03<00:00, 30.7MB/s]
100%|██████████| 97.8M/97.8M [00:03<00:00, 30.5MB/s]
DLL 2021-12-09 18:02:31.429482 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 448  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-12-09 18:02:31.438445 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 448  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 9663
Using seed = 4532
loading annotations into memory...
loading annotations into memory...
Done (t=0.64s)
creating index...
Done (t=0.65s)
creating index...
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `Uniform` is now deprecated. Use `random.Uniform` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/ops.py:532: DeprecationWarning: WARNING: `CoinFlip` is now deprecated. Use `random.CoinFlip` instead
  op_instances.append(_OperatorInstance(input_set, self, **kwargs))
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/nvidia/dali/pipeline.py:163: Warning: batch_size is deprecated, please use max_batch_size instead
  _show_deprecation_warning("batch_size", "max_batch_size")
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.8/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
DLL 2021-12-09 18:07:54.662589 - () avg_img/sec : 397.3930668273557  med_img/sec : 397.55001900428897  min_img/sec : 392.0577668428204  max_img/sec : 400.44979481470244 
Done benchmarking. Total images: 89600	total time: 225.469	Average images/sec: 397.393	Median images/sec: 397.550
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-12-09 18:07:54.663264 - () total time : 312.8000400066376 
DLL 2021-12-09 18:07:54.663295 - () 
DLL 2021-12-09 18:07:54.672339 - () avg_img/sec : 397.4086574552085  med_img/sec : 397.4829951168781  min_img/sec : 392.6985013889342  max_img/sec : 401.17094332959925 
Done benchmarking. Total images: 89600	total time: 225.461	Average images/sec: 397.409	Median images/sec: 397.483
Training performance = 795.0330200195312 FPS
DLL 2021-12-09 18:07:54.672944 - (0,) time : 312.8092339038849 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-12-09 18:07:54.673124 - () total time : 312.8092339038849 
DLL 2021-12-09 18:07:54.673148 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
