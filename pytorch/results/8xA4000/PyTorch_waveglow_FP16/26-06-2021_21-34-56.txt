train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
DLL 2021-06-26 21:34:58.325200 - PARAMETER output : ./ 
DLL 2021-06-26 21:34:58.325266 - PARAMETER dataset_path : /data/tacotron2/LJSpeech-1.1 
DLL 2021-06-26 21:34:58.325294 - PARAMETER model_name : WaveGlow 
DLL 2021-06-26 21:34:58.325316 - PARAMETER log_file : nvlog.json 
DLL 2021-06-26 21:34:58.325336 - PARAMETER anneal_steps : None 
DLL 2021-06-26 21:34:58.325359 - PARAMETER anneal_factor : 0.1 
DLL 2021-06-26 21:34:58.325379 - PARAMETER epochs : 2 
DLL 2021-06-26 21:34:58.325398 - PARAMETER epochs_per_checkpoint : 50 
DLL 2021-06-26 21:34:58.325417 - PARAMETER checkpoint_path :  
DLL 2021-06-26 21:34:58.325434 - PARAMETER resume_from_last : False 
DLL 2021-06-26 21:34:58.325453 - PARAMETER dynamic_loss_scaling : True 
DLL 2021-06-26 21:34:58.325475 - PARAMETER amp : False 
DLL 2021-06-26 21:34:58.325497 - PARAMETER cudnn_enabled : True 
DLL 2021-06-26 21:34:58.325514 - PARAMETER cudnn_benchmark : True 
DLL 2021-06-26 21:34:58.325531 - PARAMETER disable_uniform_initialize_bn_weight : False 
DLL 2021-06-26 21:34:58.325548 - PARAMETER use_saved_learning_rate : False 
DLL 2021-06-26 21:34:58.325564 - PARAMETER learning_rate : 0.0 
DLL 2021-06-26 21:34:58.325583 - PARAMETER weight_decay : 0.0 
DLL 2021-06-26 21:34:58.325599 - PARAMETER grad_clip_thresh : 65504.0 
DLL 2021-06-26 21:34:58.325617 - PARAMETER batch_size : 10 
DLL 2021-06-26 21:34:58.325634 - PARAMETER grad_clip : 5.0 
DLL 2021-06-26 21:34:58.325651 - PARAMETER load_mel_from_disk : False 
DLL 2021-06-26 21:34:58.325668 - PARAMETER training_files : filelists/ljs_audio_text_train_subset_625_filelist.txt 
DLL 2021-06-26 21:34:58.325685 - PARAMETER validation_files : filelists/ljs_audio_text_val_filelist.txt 
DLL 2021-06-26 21:34:58.325702 - PARAMETER text_cleaners : ['english_cleaners'] 
DLL 2021-06-26 21:34:58.325721 - PARAMETER max_wav_value : 32768.0 
DLL 2021-06-26 21:34:58.325739 - PARAMETER sampling_rate : 22050 
DLL 2021-06-26 21:34:58.325755 - PARAMETER filter_length : 1024 
DLL 2021-06-26 21:34:58.325773 - PARAMETER hop_length : 256 
DLL 2021-06-26 21:34:58.325789 - PARAMETER win_length : 1024 
DLL 2021-06-26 21:34:58.325805 - PARAMETER mel_fmin : 0.0 
DLL 2021-06-26 21:34:58.325822 - PARAMETER mel_fmax : 8000.0 
DLL 2021-06-26 21:34:58.325838 - PARAMETER rank : 0 
DLL 2021-06-26 21:34:58.325854 - PARAMETER world_size : 8 
DLL 2021-06-26 21:34:58.325873 - PARAMETER dist_url : tcp://localhost:23456 
DLL 2021-06-26 21:34:58.325890 - PARAMETER group_name : group_name 
DLL 2021-06-26 21:34:58.325906 - PARAMETER dist_backend : nccl 
DLL 2021-06-26 21:34:58.325923 - PARAMETER bench_class :  
DLL 2021-06-26 21:34:58.325940 - PARAMETER model_name : Tacotron2_PyT 
Initializing Distributed
Done initializing distributed
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 7; 15.74 GiB total capacity; 12.39 GiB already allocated; 32.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 5; 15.74 GiB total capacity; 12.39 GiB already allocated; 12.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 4; 15.74 GiB total capacity; 12.39 GiB already allocated; 12.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 3; 15.74 GiB total capacity; 12.39 GiB already allocated; 12.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
DLL 2021-06-26 21:36:58.361113 - (0, 0) glob_iter/iters_per_epoch : 0/7 
DLL 2021-06-26 21:37:17.992473 - (0, 0) train_loss : 0.002430995926260948 
DLL 2021-06-26 21:37:31.617321 - (0, 0) train_items_per_sec : 19244.45351582504 
DLL 2021-06-26 21:37:31.617403 - (0, 0) train_iter_time : 33.25633536300302 
DLL 2021-06-26 21:37:31.619310 - (0, 1) glob_iter/iters_per_epoch : 1/7 
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 15.74 GiB total capacity; 12.39 GiB already allocated; 16.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 2; 15.74 GiB total capacity; 12.39 GiB already allocated; 12.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 6; 15.74 GiB total capacity; 12.39 GiB already allocated; 12.69 MiB free; 13.79 GiB reserved in total by PyTorch)

/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 231, in forward
    output = self.WN[k]((audio_0, spect))
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/waveglow/model.py", line 149, in forward
    acts = fused_add_tanh_sigmoid_multiply(
RuntimeError: The following operation failed in the TorchScript interpreter.
Traceback of TorchScript (most recent call last):
  File "/workspace/examples/tacotron2/waveglow/model.py", line 35, in fused_add_tanh_sigmoid_multiply
def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):
    n_channels_int = n_channels[0]
    in_act = input_a + input_b
             ~~~~~~~~~~~~~~~~~ <--- HERE
    t_act = torch.tanh(in_act[:, :n_channels_int, :])
    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])
RuntimeError: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 1; 15.74 GiB total capacity; 12.39 GiB already allocated; 12.69 MiB free; 13.79 GiB reserved in total by PyTorch)

DONE!
