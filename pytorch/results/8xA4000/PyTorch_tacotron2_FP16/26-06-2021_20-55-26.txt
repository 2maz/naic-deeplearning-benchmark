train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/workspace/examples/tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return s in _symbol_to_id and s is not '_' and s is not '~'
/workspace/examples/tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return s in _symbol_to_id and s is not '_' and s is not '~'
DLL 2021-06-26 20:55:28.775645 - PARAMETER output : ./ 
DLL 2021-06-26 20:55:28.775715 - PARAMETER dataset_path : /data/tacotron2/LJSpeech-1.1 
DLL 2021-06-26 20:55:28.775742 - PARAMETER model_name : Tacotron2 
DLL 2021-06-26 20:55:28.775764 - PARAMETER log_file : nvlog.json 
DLL 2021-06-26 20:55:28.775783 - PARAMETER anneal_steps : None 
DLL 2021-06-26 20:55:28.775803 - PARAMETER anneal_factor : 0.1 
DLL 2021-06-26 20:55:28.775823 - PARAMETER epochs : 3 
DLL 2021-06-26 20:55:28.775843 - PARAMETER epochs_per_checkpoint : 50 
DLL 2021-06-26 20:55:28.775862 - PARAMETER checkpoint_path :  
DLL 2021-06-26 20:55:28.775880 - PARAMETER resume_from_last : False 
DLL 2021-06-26 20:55:28.775899 - PARAMETER dynamic_loss_scaling : True 
DLL 2021-06-26 20:55:28.775919 - PARAMETER amp : False 
DLL 2021-06-26 20:55:28.775938 - PARAMETER cudnn_enabled : True 
DLL 2021-06-26 20:55:28.775955 - PARAMETER cudnn_benchmark : False 
DLL 2021-06-26 20:55:28.775972 - PARAMETER disable_uniform_initialize_bn_weight : False 
DLL 2021-06-26 20:55:28.775989 - PARAMETER use_saved_learning_rate : False 
DLL 2021-06-26 20:55:28.776006 - PARAMETER learning_rate : 0.0 
DLL 2021-06-26 20:55:28.776023 - PARAMETER weight_decay : 1e-06 
DLL 2021-06-26 20:55:28.776042 - PARAMETER grad_clip_thresh : 1.0 
DLL 2021-06-26 20:55:28.776060 - PARAMETER batch_size : 100 
DLL 2021-06-26 20:55:28.776077 - PARAMETER grad_clip : 5.0 
DLL 2021-06-26 20:55:28.776094 - PARAMETER load_mel_from_disk : False 
DLL 2021-06-26 20:55:28.776111 - PARAMETER training_files : filelists/ljs_audio_text_train_subset_1250_filelist.txt 
DLL 2021-06-26 20:55:28.776128 - PARAMETER validation_files : filelists/ljs_audio_text_val_filelist.txt 
DLL 2021-06-26 20:55:28.776145 - PARAMETER text_cleaners : ['english_cleaners'] 
DLL 2021-06-26 20:55:28.776164 - PARAMETER max_wav_value : 32768.0 
DLL 2021-06-26 20:55:28.776181 - PARAMETER sampling_rate : 22050 
DLL 2021-06-26 20:55:28.776198 - PARAMETER filter_length : 1024 
DLL 2021-06-26 20:55:28.776215 - PARAMETER hop_length : 256 
DLL 2021-06-26 20:55:28.776232 - PARAMETER win_length : 1024 
DLL 2021-06-26 20:55:28.776249 - PARAMETER mel_fmin : 0.0 
DLL 2021-06-26 20:55:28.776266 - PARAMETER mel_fmax : 8000.0 
DLL 2021-06-26 20:55:28.776283 - PARAMETER rank : 0 
DLL 2021-06-26 20:55:28.776300 - PARAMETER world_size : 8 
DLL 2021-06-26 20:55:28.776317 - PARAMETER dist_url : tcp://localhost:23456 
DLL 2021-06-26 20:55:28.776333 - PARAMETER group_name : group_name 
DLL 2021-06-26 20:55:28.776349 - PARAMETER dist_backend : nccl 
DLL 2021-06-26 20:55:28.776365 - PARAMETER bench_class :  
DLL 2021-06-26 20:55:28.776382 - PARAMETER model_name : Tacotron2_PyT 
Initializing Distributed
Done initializing distributed
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/workspace/examples/tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return s in _symbol_to_id and s is not '_' and s is not '~'
/workspace/examples/tacotron2/tacotron2/text/__init__.py:74: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  return s in _symbol_to_id and s is not '_' and s is not '~'
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 417, in decode
    attention_hidden, attention_cell = self.attention_rnn(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 1053, in forward
    return _VF.lstm_cell(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 3; 15.74 GiB total capacity; 14.00 GiB already allocated; 2.69 MiB free; 14.06 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 417, in decode
    attention_hidden, attention_cell = self.attention_rnn(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 1053, in forward
    return _VF.lstm_cell(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 6; 15.74 GiB total capacity; 13.68 GiB already allocated; 2.69 MiB free; 14.06 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 417, in decode
    attention_hidden, attention_cell = self.attention_rnn(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 1053, in forward
    return _VF.lstm_cell(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 5; 15.74 GiB total capacity; 14.02 GiB already allocated; 2.69 MiB free; 14.06 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 425, in decode
    attention_context, attention_weights = self.attention_layer(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 107, in forward
    alignment = self.get_alignment_energies(
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 91, in get_alignment_energies
    processed_query + processed_attention_weights + processed_memory))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 4; 15.74 GiB total capacity; 14.01 GiB already allocated; 4.69 MiB free; 14.06 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 433, in decode
    decoder_hidden, decoder_cell = self.decoder_rnn(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 1053, in forward
    return _VF.lstm_cell(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 1; 15.74 GiB total capacity; 14.01 GiB already allocated; 2.69 MiB free; 14.06 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 425, in decode
    attention_context, attention_weights = self.attention_layer(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 107, in forward
    alignment = self.get_alignment_energies(
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 91, in get_alignment_energies
    processed_query + processed_attention_weights + processed_memory))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 2; 15.74 GiB total capacity; 13.60 GiB already allocated; 8.69 MiB free; 14.06 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 417, in decode
    attention_hidden, attention_cell = self.attention_rnn(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 1053, in forward
    return _VF.lstm_cell(
RuntimeError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 7; 15.74 GiB total capacity; 14.01 GiB already allocated; 2.69 MiB free; 14.08 GiB reserved in total by PyTorch)
DLL 2021-06-26 20:56:01.992481 - (0, 0) glob_iter/iters_per_epoch : 0/1 
Traceback (most recent call last):
  File "train.py", line 545, in <module>
    main()
  File "train.py", line 472, in main
    y_pred = model(x)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/apex/parallel/distributed.py", line 560, in forward
    result = self.module(*inputs, **kwargs)
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 667, in forward
    mel_outputs, gate_outputs, alignments = self.decoder(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 491, in forward
    attention_context) = self.decode(decoder_input,
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 425, in decode
    attention_context, attention_weights = self.attention_layer(
  File "/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 107, in forward
    alignment = self.get_alignment_energies(
  File "/workspace/examples/tacotron2/tacotron2/model.py", line 91, in get_alignment_energies
    processed_query + processed_attention_weights + processed_memory))
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.74 GiB total capacity; 14.01 GiB already allocated; 12.69 MiB free; 14.06 GiB reserved in total by PyTorch)
DONE!
