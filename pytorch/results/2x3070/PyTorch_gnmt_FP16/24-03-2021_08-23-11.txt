1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31800
0: Size of vocabulary: 31800
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
0: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1228
0: Scheduler decay interval: 154
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 1228
1: Scheduler decay interval: 154
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/922]	Time 0.374 (0.000)	Data 1.90e-01 (0.00e+00)	Tok/s 13375 (0)	Loss/tok 10.6615 (10.6615)	LR 2.047e-05
1: TRAIN [0][0/922]	Time 0.374 (0.000)	Data 1.59e-01 (0.00e+00)	Tok/s 13458 (0)	Loss/tok 10.6662 (10.6662)	LR 2.047e-05
0: TRAIN [0][10/922]	Time 0.188 (0.194)	Data 7.70e-05 (7.99e-05)	Tok/s 19156 (20144)	Loss/tok 9.6600 (10.1679)	LR 2.576e-05
1: TRAIN [0][10/922]	Time 0.188 (0.194)	Data 7.61e-05 (8.02e-05)	Tok/s 19397 (20240)	Loss/tok 9.6410 (10.1482)	LR 2.576e-05
0: TRAIN [0][20/922]	Time 0.188 (0.204)	Data 7.61e-05 (7.92e-05)	Tok/s 19355 (21197)	Loss/tok 9.1677 (9.7852)	LR 3.244e-05
1: TRAIN [0][20/922]	Time 0.188 (0.204)	Data 7.56e-05 (7.91e-05)	Tok/s 19667 (21265)	Loss/tok 9.1248 (9.7819)	LR 3.244e-05
0: TRAIN [0][30/922]	Time 0.214 (0.202)	Data 9.56e-05 (7.99e-05)	Tok/s 23330 (20801)	Loss/tok 8.9261 (9.5818)	LR 4.083e-05
1: TRAIN [0][30/922]	Time 0.214 (0.202)	Data 7.96e-05 (7.97e-05)	Tok/s 23490 (20882)	Loss/tok 9.0679 (9.5748)	LR 4.083e-05
0: TRAIN [0][40/922]	Time 0.188 (0.201)	Data 7.61e-05 (7.98e-05)	Tok/s 19451 (20584)	Loss/tok 8.6784 (9.4193)	LR 5.141e-05
1: TRAIN [0][40/922]	Time 0.187 (0.201)	Data 9.56e-05 (8.21e-05)	Tok/s 19319 (20604)	Loss/tok 8.7412 (9.4078)	LR 5.141e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
1: TRAIN [0][50/922]	Time 0.160 (0.194)	Data 7.75e-05 (8.17e-05)	Tok/s 13566 (19676)	Loss/tok 8.4450 (9.3046)	LR 6.472e-05
0: TRAIN [0][50/922]	Time 0.160 (0.194)	Data 8.20e-05 (7.93e-05)	Tok/s 13541 (19657)	Loss/tok 8.4638 (9.3082)	LR 6.472e-05
0: TRAIN [0][60/922]	Time 0.160 (0.192)	Data 7.92e-05 (7.95e-05)	Tok/s 12981 (19262)	Loss/tok 8.1192 (9.1877)	LR 8.148e-05
1: TRAIN [0][60/922]	Time 0.160 (0.192)	Data 8.39e-05 (8.19e-05)	Tok/s 14003 (19321)	Loss/tok 8.1423 (9.1834)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
1: TRAIN [0][70/922]	Time 0.251 (0.194)	Data 8.15e-05 (8.22e-05)	Tok/s 26157 (19626)	Loss/tok 8.3199 (9.0464)	LR 1.026e-04
0: TRAIN [0][70/922]	Time 0.251 (0.194)	Data 7.84e-05 (7.99e-05)	Tok/s 26400 (19594)	Loss/tok 8.3498 (9.0480)	LR 1.026e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
1: TRAIN [0][80/922]	Time 0.245 (0.195)	Data 9.16e-05 (8.25e-05)	Tok/s 26601 (19857)	Loss/tok 8.4586 (8.9240)	LR 1.291e-04
0: TRAIN [0][80/922]	Time 0.245 (0.195)	Data 7.80e-05 (8.02e-05)	Tok/s 26560 (19814)	Loss/tok 8.5468 (8.9254)	LR 1.291e-04
1: TRAIN [0][90/922]	Time 0.216 (0.194)	Data 8.25e-05 (8.23e-05)	Tok/s 22776 (19842)	Loss/tok 8.1014 (8.8261)	LR 1.626e-04
0: TRAIN [0][90/922]	Time 0.217 (0.194)	Data 7.94e-05 (8.00e-05)	Tok/s 23075 (19802)	Loss/tok 8.1435 (8.8293)	LR 1.626e-04
0: TRAIN [0][100/922]	Time 0.215 (0.193)	Data 7.99e-05 (8.01e-05)	Tok/s 23811 (19644)	Loss/tok 7.8373 (8.7409)	LR 2.047e-04
1: TRAIN [0][100/922]	Time 0.215 (0.193)	Data 8.01e-05 (8.20e-05)	Tok/s 23531 (19678)	Loss/tok 7.9350 (8.7400)	LR 2.047e-04
1: TRAIN [0][110/922]	Time 0.187 (0.193)	Data 8.39e-05 (8.20e-05)	Tok/s 18736 (19694)	Loss/tok 7.8086 (8.6578)	LR 2.576e-04
0: TRAIN [0][110/922]	Time 0.187 (0.193)	Data 8.01e-05 (8.02e-05)	Tok/s 19429 (19672)	Loss/tok 7.6663 (8.6551)	LR 2.576e-04
1: TRAIN [0][120/922]	Time 0.214 (0.194)	Data 9.49e-05 (8.20e-05)	Tok/s 23523 (19828)	Loss/tok 7.8051 (8.5795)	LR 3.244e-04
0: TRAIN [0][120/922]	Time 0.214 (0.194)	Data 9.70e-05 (8.06e-05)	Tok/s 23608 (19809)	Loss/tok 7.8273 (8.5768)	LR 3.244e-04
0: TRAIN [0][130/922]	Time 0.186 (0.194)	Data 8.03e-05 (8.05e-05)	Tok/s 19088 (19672)	Loss/tok 7.5919 (8.5173)	LR 4.083e-04
1: TRAIN [0][130/922]	Time 0.186 (0.194)	Data 7.92e-05 (8.21e-05)	Tok/s 19704 (19702)	Loss/tok 7.6643 (8.5188)	LR 4.083e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][140/922]	Time 0.188 (0.193)	Data 8.51e-05 (8.05e-05)	Tok/s 19116 (19665)	Loss/tok 7.6086 (8.4619)	LR 5.141e-04
1: TRAIN [0][140/922]	Time 0.188 (0.193)	Data 7.77e-05 (8.21e-05)	Tok/s 19023 (19674)	Loss/tok 7.6395 (8.4660)	LR 5.141e-04
1: TRAIN [0][150/922]	Time 0.215 (0.194)	Data 8.13e-05 (8.22e-05)	Tok/s 23692 (19666)	Loss/tok 7.9563 (8.4181)	LR 6.472e-04
0: TRAIN [0][150/922]	Time 0.215 (0.194)	Data 8.96e-05 (8.06e-05)	Tok/s 23433 (19663)	Loss/tok 7.8223 (8.4136)	LR 6.472e-04
1: TRAIN [0][160/922]	Time 0.160 (0.193)	Data 7.80e-05 (8.19e-05)	Tok/s 14290 (19605)	Loss/tok 7.3750 (8.3777)	LR 8.148e-04
0: TRAIN [0][160/922]	Time 0.160 (0.193)	Data 8.01e-05 (8.07e-05)	Tok/s 13811 (19592)	Loss/tok 7.3915 (8.3727)	LR 8.148e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
1: TRAIN [0][170/922]	Time 0.214 (0.192)	Data 7.68e-05 (8.18e-05)	Tok/s 23536 (19567)	Loss/tok 7.7729 (8.3447)	LR 1.026e-03
0: TRAIN [0][170/922]	Time 0.214 (0.192)	Data 7.84e-05 (8.05e-05)	Tok/s 23483 (19557)	Loss/tok 7.7641 (8.3387)	LR 1.026e-03
0: TRAIN [0][180/922]	Time 0.160 (0.193)	Data 8.54e-05 (8.05e-05)	Tok/s 13933 (19645)	Loss/tok 7.0820 (8.2961)	LR 1.291e-03
1: TRAIN [0][180/922]	Time 0.160 (0.193)	Data 8.51e-05 (8.18e-05)	Tok/s 13684 (19645)	Loss/tok 7.3164 (8.3040)	LR 1.291e-03
1: TRAIN [0][190/922]	Time 0.137 (0.193)	Data 7.87e-05 (8.17e-05)	Tok/s 7979 (19539)	Loss/tok 6.6190 (8.2712)	LR 1.626e-03
0: TRAIN [0][190/922]	Time 0.137 (0.193)	Data 8.96e-05 (8.06e-05)	Tok/s 7920 (19548)	Loss/tok 7.0172 (8.2634)	LR 1.626e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
0: TRAIN [0][200/922]	Time 0.160 (0.192)	Data 9.06e-05 (8.08e-05)	Tok/s 14086 (19446)	Loss/tok 7.3325 (8.2409)	LR 2.000e-03
1: TRAIN [0][200/922]	Time 0.164 (0.192)	Data 9.01e-05 (8.17e-05)	Tok/s 13816 (19442)	Loss/tok 7.3104 (8.2481)	LR 2.000e-03
1: TRAIN [0][210/922]	Time 0.249 (0.193)	Data 8.08e-05 (8.18e-05)	Tok/s 26403 (19567)	Loss/tok 7.6108 (8.2088)	LR 2.000e-03
0: TRAIN [0][210/922]	Time 0.249 (0.193)	Data 9.06e-05 (8.10e-05)	Tok/s 26316 (19565)	Loss/tok 7.6343 (8.2030)	LR 2.000e-03
1: TRAIN [0][220/922]	Time 0.250 (0.193)	Data 9.49e-05 (8.19e-05)	Tok/s 25774 (19591)	Loss/tok 7.6189 (8.1705)	LR 2.000e-03
0: TRAIN [0][220/922]	Time 0.250 (0.193)	Data 9.56e-05 (8.13e-05)	Tok/s 26398 (19586)	Loss/tok 7.3998 (8.1618)	LR 2.000e-03
1: TRAIN [0][230/922]	Time 0.214 (0.193)	Data 7.49e-05 (8.19e-05)	Tok/s 23156 (19498)	Loss/tok 7.2408 (8.1369)	LR 2.000e-03
0: TRAIN [0][230/922]	Time 0.214 (0.193)	Data 8.58e-05 (8.14e-05)	Tok/s 23298 (19492)	Loss/tok 7.2851 (8.1279)	LR 2.000e-03
0: TRAIN [0][240/922]	Time 0.215 (0.193)	Data 8.39e-05 (8.15e-05)	Tok/s 23528 (19529)	Loss/tok 7.2696 (8.0859)	LR 2.000e-03
1: TRAIN [0][240/922]	Time 0.218 (0.193)	Data 8.01e-05 (8.18e-05)	Tok/s 23340 (19533)	Loss/tok 7.2875 (8.0958)	LR 2.000e-03
1: TRAIN [0][250/922]	Time 0.159 (0.193)	Data 7.89e-05 (8.18e-05)	Tok/s 13708 (19518)	Loss/tok 6.6214 (8.0598)	LR 2.000e-03
0: TRAIN [0][250/922]	Time 0.159 (0.193)	Data 8.70e-05 (8.15e-05)	Tok/s 13523 (19511)	Loss/tok 6.6383 (8.0506)	LR 2.000e-03
0: TRAIN [0][260/922]	Time 0.188 (0.193)	Data 8.99e-05 (8.15e-05)	Tok/s 18898 (19523)	Loss/tok 6.9976 (8.0124)	LR 2.000e-03
1: TRAIN [0][260/922]	Time 0.188 (0.193)	Data 7.70e-05 (8.19e-05)	Tok/s 19152 (19532)	Loss/tok 6.9832 (8.0200)	LR 2.000e-03
0: TRAIN [0][270/922]	Time 0.215 (0.193)	Data 8.44e-05 (8.14e-05)	Tok/s 23419 (19496)	Loss/tok 7.1430 (7.9766)	LR 2.000e-03
1: TRAIN [0][270/922]	Time 0.215 (0.193)	Data 8.49e-05 (8.19e-05)	Tok/s 23304 (19511)	Loss/tok 7.1636 (7.9826)	LR 2.000e-03
1: TRAIN [0][280/922]	Time 0.186 (0.192)	Data 7.92e-05 (8.19e-05)	Tok/s 19219 (19474)	Loss/tok 6.7681 (7.9466)	LR 2.000e-03
0: TRAIN [0][280/922]	Time 0.186 (0.192)	Data 8.70e-05 (8.16e-05)	Tok/s 19644 (19465)	Loss/tok 6.7490 (7.9409)	LR 2.000e-03
0: TRAIN [0][290/922]	Time 0.187 (0.193)	Data 9.97e-05 (8.16e-05)	Tok/s 19622 (19543)	Loss/tok 6.7533 (7.9000)	LR 2.000e-03
1: TRAIN [0][290/922]	Time 0.187 (0.193)	Data 7.65e-05 (8.20e-05)	Tok/s 18832 (19550)	Loss/tok 6.7084 (7.9053)	LR 2.000e-03
1: TRAIN [0][300/922]	Time 0.215 (0.194)	Data 8.08e-05 (8.21e-05)	Tok/s 23232 (19632)	Loss/tok 6.7774 (7.8610)	LR 2.000e-03
0: TRAIN [0][300/922]	Time 0.216 (0.194)	Data 9.23e-05 (8.16e-05)	Tok/s 23066 (19629)	Loss/tok 6.7872 (7.8571)	LR 2.000e-03
1: TRAIN [0][310/922]	Time 0.160 (0.194)	Data 7.46e-05 (8.20e-05)	Tok/s 13835 (19614)	Loss/tok 6.2444 (7.8234)	LR 2.000e-03
0: TRAIN [0][310/922]	Time 0.160 (0.194)	Data 7.99e-05 (8.16e-05)	Tok/s 13822 (19616)	Loss/tok 6.1508 (7.8209)	LR 2.000e-03
1: TRAIN [0][320/922]	Time 0.187 (0.193)	Data 7.89e-05 (8.20e-05)	Tok/s 19226 (19575)	Loss/tok 6.6652 (7.7888)	LR 2.000e-03
0: TRAIN [0][320/922]	Time 0.187 (0.193)	Data 8.65e-05 (8.16e-05)	Tok/s 19175 (19582)	Loss/tok 6.5201 (7.7859)	LR 2.000e-03
0: TRAIN [0][330/922]	Time 0.188 (0.194)	Data 8.75e-05 (8.16e-05)	Tok/s 19471 (19698)	Loss/tok 6.4876 (7.7410)	LR 2.000e-03
1: TRAIN [0][330/922]	Time 0.188 (0.194)	Data 8.15e-05 (8.21e-05)	Tok/s 19276 (19689)	Loss/tok 6.4846 (7.7434)	LR 2.000e-03
0: TRAIN [0][340/922]	Time 0.216 (0.194)	Data 7.99e-05 (8.17e-05)	Tok/s 23280 (19681)	Loss/tok 6.5526 (7.7048)	LR 2.000e-03
1: TRAIN [0][340/922]	Time 0.216 (0.194)	Data 7.92e-05 (8.21e-05)	Tok/s 22797 (19673)	Loss/tok 6.6158 (7.7084)	LR 2.000e-03
0: TRAIN [0][350/922]	Time 0.186 (0.194)	Data 8.96e-05 (8.17e-05)	Tok/s 19492 (19690)	Loss/tok 6.2892 (7.6693)	LR 2.000e-03
1: TRAIN [0][350/922]	Time 0.186 (0.194)	Data 8.27e-05 (8.20e-05)	Tok/s 19625 (19683)	Loss/tok 6.2981 (7.6708)	LR 2.000e-03
1: TRAIN [0][360/922]	Time 0.215 (0.194)	Data 7.84e-05 (8.19e-05)	Tok/s 23662 (19681)	Loss/tok 6.4760 (7.6351)	LR 2.000e-03
0: TRAIN [0][360/922]	Time 0.215 (0.194)	Data 8.82e-05 (8.19e-05)	Tok/s 23646 (19691)	Loss/tok 6.4416 (7.6332)	LR 2.000e-03
1: TRAIN [0][370/922]	Time 0.187 (0.195)	Data 7.49e-05 (8.18e-05)	Tok/s 19240 (19789)	Loss/tok 6.2550 (7.5914)	LR 2.000e-03
0: TRAIN [0][370/922]	Time 0.187 (0.195)	Data 8.42e-05 (8.18e-05)	Tok/s 19468 (19801)	Loss/tok 6.2528 (7.5893)	LR 2.000e-03
0: TRAIN [0][380/922]	Time 0.186 (0.194)	Data 8.99e-05 (8.17e-05)	Tok/s 19282 (19740)	Loss/tok 6.2928 (7.5593)	LR 2.000e-03
1: TRAIN [0][380/922]	Time 0.186 (0.194)	Data 9.54e-05 (8.17e-05)	Tok/s 19089 (19727)	Loss/tok 6.0959 (7.5610)	LR 2.000e-03
1: TRAIN [0][390/922]	Time 0.217 (0.195)	Data 7.58e-05 (8.17e-05)	Tok/s 23585 (19751)	Loss/tok 6.2498 (7.5251)	LR 2.000e-03
0: TRAIN [0][390/922]	Time 0.215 (0.195)	Data 1.07e-04 (8.16e-05)	Tok/s 23480 (19768)	Loss/tok 6.4054 (7.5244)	LR 2.000e-03
0: TRAIN [0][400/922]	Time 0.212 (0.194)	Data 8.46e-05 (8.16e-05)	Tok/s 23344 (19727)	Loss/tok 6.2560 (7.4932)	LR 2.000e-03
1: TRAIN [0][400/922]	Time 0.212 (0.194)	Data 7.96e-05 (8.17e-05)	Tok/s 23070 (19713)	Loss/tok 6.3171 (7.4941)	LR 2.000e-03
1: TRAIN [0][410/922]	Time 0.187 (0.194)	Data 8.15e-05 (8.15e-05)	Tok/s 18853 (19663)	Loss/tok 5.8714 (7.4642)	LR 2.000e-03
0: TRAIN [0][410/922]	Time 0.187 (0.194)	Data 8.61e-05 (8.15e-05)	Tok/s 19259 (19680)	Loss/tok 5.9818 (7.4637)	LR 2.000e-03
0: TRAIN [0][420/922]	Time 0.159 (0.194)	Data 8.25e-05 (8.14e-05)	Tok/s 13704 (19702)	Loss/tok 5.5101 (7.4293)	LR 2.000e-03
1: TRAIN [0][420/922]	Time 0.159 (0.194)	Data 7.46e-05 (8.15e-05)	Tok/s 13462 (19686)	Loss/tok 5.6767 (7.4307)	LR 2.000e-03
1: TRAIN [0][430/922]	Time 0.214 (0.194)	Data 7.80e-05 (8.14e-05)	Tok/s 23360 (19706)	Loss/tok 5.9720 (7.3960)	LR 2.000e-03
0: TRAIN [0][430/922]	Time 0.214 (0.194)	Data 8.37e-05 (8.14e-05)	Tok/s 23247 (19724)	Loss/tok 6.0762 (7.3943)	LR 2.000e-03
0: TRAIN [0][440/922]	Time 0.187 (0.194)	Data 8.15e-05 (8.13e-05)	Tok/s 19087 (19683)	Loss/tok 5.8392 (7.3662)	LR 2.000e-03
1: TRAIN [0][440/922]	Time 0.187 (0.194)	Data 7.63e-05 (8.14e-05)	Tok/s 19495 (19665)	Loss/tok 5.8006 (7.3675)	LR 2.000e-03
1: TRAIN [0][450/922]	Time 0.212 (0.194)	Data 8.03e-05 (8.14e-05)	Tok/s 23521 (19675)	Loss/tok 6.1093 (7.3360)	LR 2.000e-03
0: TRAIN [0][450/922]	Time 0.212 (0.194)	Data 8.65e-05 (8.14e-05)	Tok/s 23718 (19696)	Loss/tok 5.9679 (7.3333)	LR 2.000e-03
1: TRAIN [0][460/922]	Time 0.212 (0.194)	Data 7.70e-05 (8.14e-05)	Tok/s 23616 (19677)	Loss/tok 6.0029 (7.3052)	LR 2.000e-03
0: TRAIN [0][460/922]	Time 0.212 (0.194)	Data 1.43e-04 (8.15e-05)	Tok/s 23791 (19701)	Loss/tok 5.9669 (7.3021)	LR 2.000e-03
0: TRAIN [0][470/922]	Time 0.185 (0.194)	Data 7.96e-05 (8.15e-05)	Tok/s 20108 (19705)	Loss/tok 5.7963 (7.2711)	LR 2.000e-03
1: TRAIN [0][470/922]	Time 0.185 (0.194)	Data 7.77e-05 (8.13e-05)	Tok/s 19391 (19679)	Loss/tok 5.5637 (7.2740)	LR 2.000e-03
1: TRAIN [0][480/922]	Time 0.187 (0.194)	Data 7.39e-05 (8.14e-05)	Tok/s 19144 (19705)	Loss/tok 5.5834 (7.2430)	LR 2.000e-03
0: TRAIN [0][480/922]	Time 0.187 (0.194)	Data 9.13e-05 (8.17e-05)	Tok/s 19209 (19729)	Loss/tok 5.6042 (7.2398)	LR 2.000e-03
1: TRAIN [0][490/922]	Time 0.245 (0.194)	Data 7.44e-05 (8.14e-05)	Tok/s 26589 (19647)	Loss/tok 6.0665 (7.2171)	LR 2.000e-03
0: TRAIN [0][490/922]	Time 0.245 (0.194)	Data 8.23e-05 (8.17e-05)	Tok/s 26318 (19667)	Loss/tok 6.0074 (7.2143)	LR 2.000e-03
1: TRAIN [0][500/922]	Time 0.216 (0.194)	Data 8.25e-05 (8.14e-05)	Tok/s 23013 (19666)	Loss/tok 5.8970 (7.1857)	LR 2.000e-03
0: TRAIN [0][500/922]	Time 0.216 (0.194)	Data 8.18e-05 (8.19e-05)	Tok/s 23370 (19687)	Loss/tok 5.7495 (7.1828)	LR 2.000e-03
0: TRAIN [0][510/922]	Time 0.160 (0.194)	Data 8.34e-05 (8.18e-05)	Tok/s 13987 (19653)	Loss/tok 5.4415 (7.1566)	LR 2.000e-03
1: TRAIN [0][510/922]	Time 0.160 (0.194)	Data 8.11e-05 (8.14e-05)	Tok/s 13520 (19631)	Loss/tok 5.1196 (7.1598)	LR 2.000e-03
0: TRAIN [0][520/922]	Time 0.216 (0.194)	Data 7.63e-05 (8.18e-05)	Tok/s 23544 (19651)	Loss/tok 5.7492 (7.1284)	LR 2.000e-03
1: TRAIN [0][520/922]	Time 0.216 (0.194)	Data 7.63e-05 (8.13e-05)	Tok/s 23456 (19626)	Loss/tok 5.8222 (7.1316)	LR 2.000e-03
0: TRAIN [0][530/922]	Time 0.160 (0.193)	Data 8.42e-05 (8.17e-05)	Tok/s 13808 (19581)	Loss/tok 5.2047 (7.1051)	LR 2.000e-03
1: TRAIN [0][530/922]	Time 0.160 (0.193)	Data 8.18e-05 (8.13e-05)	Tok/s 13958 (19554)	Loss/tok 4.9643 (7.1084)	LR 2.000e-03
0: TRAIN [0][540/922]	Time 0.160 (0.193)	Data 8.08e-05 (8.17e-05)	Tok/s 13112 (19579)	Loss/tok 4.9845 (7.0776)	LR 2.000e-03
1: TRAIN [0][540/922]	Time 0.160 (0.193)	Data 7.87e-05 (8.13e-05)	Tok/s 13871 (19553)	Loss/tok 5.1047 (7.0804)	LR 2.000e-03
0: TRAIN [0][550/922]	Time 0.160 (0.193)	Data 8.39e-05 (8.17e-05)	Tok/s 13862 (19582)	Loss/tok 4.8254 (7.0496)	LR 2.000e-03
1: TRAIN [0][550/922]	Time 0.160 (0.193)	Data 7.84e-05 (8.13e-05)	Tok/s 12966 (19551)	Loss/tok 5.0290 (7.0526)	LR 2.000e-03
0: TRAIN [0][560/922]	Time 0.213 (0.194)	Data 8.42e-05 (8.17e-05)	Tok/s 24406 (19591)	Loss/tok 5.6784 (7.0216)	LR 2.000e-03
1: TRAIN [0][560/922]	Time 0.213 (0.194)	Data 7.80e-05 (8.13e-05)	Tok/s 23665 (19561)	Loss/tok 5.7444 (7.0243)	LR 2.000e-03
0: TRAIN [0][570/922]	Time 0.184 (0.194)	Data 8.49e-05 (8.17e-05)	Tok/s 19279 (19604)	Loss/tok 5.1059 (6.9932)	LR 2.000e-03
1: TRAIN [0][570/922]	Time 0.184 (0.194)	Data 7.87e-05 (8.13e-05)	Tok/s 19448 (19572)	Loss/tok 5.5809 (6.9961)	LR 2.000e-03
0: TRAIN [0][580/922]	Time 0.160 (0.193)	Data 8.01e-05 (8.17e-05)	Tok/s 13775 (19582)	Loss/tok 5.0207 (6.9678)	LR 2.000e-03
1: TRAIN [0][580/922]	Time 0.160 (0.193)	Data 7.68e-05 (8.13e-05)	Tok/s 13593 (19547)	Loss/tok 4.9644 (6.9715)	LR 2.000e-03
1: TRAIN [0][590/922]	Time 0.212 (0.193)	Data 8.44e-05 (8.13e-05)	Tok/s 23433 (19512)	Loss/tok 5.4540 (6.9479)	LR 2.000e-03
0: TRAIN [0][590/922]	Time 0.212 (0.193)	Data 8.63e-05 (8.17e-05)	Tok/s 23870 (19548)	Loss/tok 5.5526 (6.9446)	LR 2.000e-03
0: TRAIN [0][600/922]	Time 0.215 (0.193)	Data 8.06e-05 (8.17e-05)	Tok/s 23005 (19531)	Loss/tok 5.4960 (6.9201)	LR 2.000e-03
1: TRAIN [0][600/922]	Time 0.215 (0.193)	Data 7.94e-05 (8.13e-05)	Tok/s 23374 (19497)	Loss/tok 5.4340 (6.9225)	LR 2.000e-03
1: TRAIN [0][610/922]	Time 0.136 (0.193)	Data 8.75e-05 (8.13e-05)	Tok/s 7831 (19505)	Loss/tok 4.6784 (6.8947)	LR 2.000e-03
0: TRAIN [0][610/922]	Time 0.136 (0.193)	Data 9.18e-05 (8.17e-05)	Tok/s 7854 (19540)	Loss/tok 4.5803 (6.8930)	LR 2.000e-03
0: TRAIN [0][620/922]	Time 0.186 (0.193)	Data 8.70e-05 (8.16e-05)	Tok/s 19341 (19547)	Loss/tok 5.2587 (6.8674)	LR 2.000e-03
1: TRAIN [0][620/922]	Time 0.186 (0.193)	Data 8.27e-05 (8.14e-05)	Tok/s 19724 (19516)	Loss/tok 5.2893 (6.8685)	LR 2.000e-03
0: TRAIN [0][630/922]	Time 0.188 (0.193)	Data 8.13e-05 (8.17e-05)	Tok/s 19195 (19535)	Loss/tok 4.8834 (6.8423)	LR 2.000e-03
1: TRAIN [0][630/922]	Time 0.188 (0.193)	Data 7.82e-05 (8.14e-05)	Tok/s 19230 (19501)	Loss/tok 5.0860 (6.8447)	LR 2.000e-03
0: TRAIN [0][640/922]	Time 0.215 (0.193)	Data 8.03e-05 (8.16e-05)	Tok/s 23267 (19567)	Loss/tok 5.2683 (6.8148)	LR 2.000e-03
1: TRAIN [0][640/922]	Time 0.215 (0.193)	Data 7.61e-05 (8.14e-05)	Tok/s 23553 (19533)	Loss/tok 5.1704 (6.8164)	LR 2.000e-03
0: TRAIN [0][650/922]	Time 0.245 (0.193)	Data 8.44e-05 (8.16e-05)	Tok/s 26884 (19555)	Loss/tok 5.5895 (6.7921)	LR 2.000e-03
1: TRAIN [0][650/922]	Time 0.245 (0.193)	Data 8.27e-05 (8.14e-05)	Tok/s 27006 (19522)	Loss/tok 5.5256 (6.7928)	LR 2.000e-03
0: TRAIN [0][660/922]	Time 0.135 (0.193)	Data 7.96e-05 (8.16e-05)	Tok/s 7641 (19537)	Loss/tok 4.3405 (6.7691)	LR 2.000e-03
1: TRAIN [0][660/922]	Time 0.135 (0.193)	Data 8.54e-05 (8.15e-05)	Tok/s 7760 (19503)	Loss/tok 4.4643 (6.7698)	LR 2.000e-03
1: TRAIN [0][670/922]	Time 0.215 (0.193)	Data 7.89e-05 (8.15e-05)	Tok/s 23255 (19494)	Loss/tok 5.3548 (6.7460)	LR 2.000e-03
0: TRAIN [0][670/922]	Time 0.215 (0.193)	Data 7.99e-05 (8.16e-05)	Tok/s 23135 (19525)	Loss/tok 5.2771 (6.7462)	LR 2.000e-03
0: TRAIN [0][680/922]	Time 0.215 (0.193)	Data 8.63e-05 (8.16e-05)	Tok/s 23053 (19540)	Loss/tok 5.1706 (6.7202)	LR 2.000e-03
1: TRAIN [0][680/922]	Time 0.215 (0.193)	Data 1.01e-04 (8.16e-05)	Tok/s 23387 (19509)	Loss/tok 5.1711 (6.7202)	LR 2.000e-03
1: TRAIN [0][690/922]	Time 0.187 (0.193)	Data 8.42e-05 (8.16e-05)	Tok/s 18907 (19536)	Loss/tok 4.7955 (6.6924)	LR 2.000e-03
0: TRAIN [0][690/922]	Time 0.188 (0.193)	Data 8.32e-05 (8.16e-05)	Tok/s 19619 (19570)	Loss/tok 4.7456 (6.6936)	LR 2.000e-03
0: TRAIN [0][700/922]	Time 0.250 (0.193)	Data 7.65e-05 (8.16e-05)	Tok/s 25903 (19566)	Loss/tok 5.4435 (6.6703)	LR 2.000e-03
1: TRAIN [0][700/922]	Time 0.250 (0.193)	Data 8.25e-05 (8.16e-05)	Tok/s 26100 (19534)	Loss/tok 5.2621 (6.6685)	LR 2.000e-03
1: TRAIN [0][710/922]	Time 0.216 (0.193)	Data 8.85e-05 (8.17e-05)	Tok/s 23150 (19539)	Loss/tok 5.1638 (6.6456)	LR 2.000e-03
0: TRAIN [0][710/922]	Time 0.216 (0.193)	Data 8.61e-05 (8.16e-05)	Tok/s 23688 (19572)	Loss/tok 4.9717 (6.6458)	LR 2.000e-03
1: TRAIN [0][720/922]	Time 0.158 (0.193)	Data 7.99e-05 (8.18e-05)	Tok/s 13572 (19500)	Loss/tok 4.5521 (6.6259)	LR 2.000e-03
0: TRAIN [0][720/922]	Time 0.158 (0.193)	Data 7.89e-05 (8.16e-05)	Tok/s 13587 (19533)	Loss/tok 4.7349 (6.6263)	LR 2.000e-03
0: TRAIN [0][730/922]	Time 0.212 (0.193)	Data 7.92e-05 (8.16e-05)	Tok/s 23932 (19538)	Loss/tok 5.1163 (6.6037)	LR 2.000e-03
1: TRAIN [0][730/922]	Time 0.212 (0.193)	Data 7.56e-05 (8.18e-05)	Tok/s 23352 (19507)	Loss/tok 5.0774 (6.6033)	LR 2.000e-03
0: TRAIN [0][740/922]	Time 0.216 (0.193)	Data 8.15e-05 (8.16e-05)	Tok/s 23185 (19512)	Loss/tok 5.1046 (6.5841)	LR 2.000e-03
1: TRAIN [0][740/922]	Time 0.216 (0.193)	Data 8.42e-05 (8.18e-05)	Tok/s 23415 (19482)	Loss/tok 5.0211 (6.5835)	LR 2.000e-03
0: TRAIN [0][750/922]	Time 0.249 (0.193)	Data 7.96e-05 (8.16e-05)	Tok/s 25656 (19523)	Loss/tok 5.2210 (6.5608)	LR 2.000e-03
1: TRAIN [0][750/922]	Time 0.249 (0.193)	Data 7.94e-05 (8.18e-05)	Tok/s 26512 (19495)	Loss/tok 5.1259 (6.5603)	LR 2.000e-03
0: TRAIN [0][760/922]	Time 0.160 (0.193)	Data 9.11e-05 (8.16e-05)	Tok/s 13215 (19519)	Loss/tok 4.5565 (6.5399)	LR 2.000e-03
1: TRAIN [0][760/922]	Time 0.159 (0.193)	Data 8.49e-05 (8.19e-05)	Tok/s 13733 (19492)	Loss/tok 4.4033 (6.5392)	LR 2.000e-03
0: TRAIN [0][770/922]	Time 0.160 (0.193)	Data 8.54e-05 (8.16e-05)	Tok/s 14045 (19519)	Loss/tok 4.5674 (6.5183)	LR 2.000e-03
1: TRAIN [0][770/922]	Time 0.160 (0.193)	Data 8.46e-05 (8.19e-05)	Tok/s 13770 (19490)	Loss/tok 4.4888 (6.5175)	LR 2.000e-03
0: TRAIN [0][780/922]	Time 0.157 (0.193)	Data 8.37e-05 (8.16e-05)	Tok/s 13169 (19505)	Loss/tok 4.2862 (6.4983)	LR 2.000e-03
1: TRAIN [0][780/922]	Time 0.157 (0.193)	Data 8.18e-05 (8.19e-05)	Tok/s 13789 (19476)	Loss/tok 4.4420 (6.4972)	LR 2.000e-03
0: TRAIN [0][790/922]	Time 0.159 (0.193)	Data 7.56e-05 (8.16e-05)	Tok/s 13552 (19504)	Loss/tok 4.3666 (6.4785)	LR 2.000e-03
1: TRAIN [0][790/922]	Time 0.159 (0.193)	Data 7.77e-05 (8.19e-05)	Tok/s 13744 (19474)	Loss/tok 4.3720 (6.4761)	LR 2.000e-03
1: TRAIN [0][800/922]	Time 0.214 (0.193)	Data 7.99e-05 (8.19e-05)	Tok/s 23623 (19485)	Loss/tok 4.8390 (6.4545)	LR 2.000e-03
0: TRAIN [0][800/922]	Time 0.215 (0.193)	Data 8.54e-05 (8.16e-05)	Tok/s 23587 (19515)	Loss/tok 4.8261 (6.4567)	LR 2.000e-03
0: TRAIN [0][810/922]	Time 0.249 (0.193)	Data 8.34e-05 (8.15e-05)	Tok/s 26382 (19520)	Loss/tok 5.0601 (6.4365)	LR 2.000e-03
1: TRAIN [0][810/922]	Time 0.249 (0.193)	Data 7.65e-05 (8.20e-05)	Tok/s 26054 (19490)	Loss/tok 5.0301 (6.4338)	LR 2.000e-03
1: TRAIN [0][820/922]	Time 0.217 (0.193)	Data 8.18e-05 (8.21e-05)	Tok/s 22904 (19510)	Loss/tok 4.8066 (6.4113)	LR 2.000e-03
0: TRAIN [0][820/922]	Time 0.217 (0.193)	Data 7.56e-05 (8.15e-05)	Tok/s 22636 (19541)	Loss/tok 4.8631 (6.4135)	LR 2.000e-03
1: TRAIN [0][830/922]	Time 0.160 (0.193)	Data 8.11e-05 (8.21e-05)	Tok/s 13877 (19473)	Loss/tok 4.2252 (6.3947)	LR 2.000e-03
0: TRAIN [0][830/922]	Time 0.160 (0.193)	Data 8.30e-05 (8.15e-05)	Tok/s 12883 (19502)	Loss/tok 4.1597 (6.3971)	LR 2.000e-03
1: TRAIN [0][840/922]	Time 0.214 (0.193)	Data 8.08e-05 (8.21e-05)	Tok/s 22999 (19445)	Loss/tok 4.8179 (6.3777)	LR 2.000e-03
0: TRAIN [0][840/922]	Time 0.214 (0.193)	Data 9.23e-05 (8.16e-05)	Tok/s 23330 (19471)	Loss/tok 4.7941 (6.3806)	LR 2.000e-03
1: TRAIN [0][850/922]	Time 0.157 (0.192)	Data 7.51e-05 (8.21e-05)	Tok/s 13670 (19419)	Loss/tok 4.1472 (6.3611)	LR 2.000e-03
0: TRAIN [0][850/922]	Time 0.157 (0.192)	Data 7.96e-05 (8.16e-05)	Tok/s 13695 (19445)	Loss/tok 4.2607 (6.3643)	LR 2.000e-03
1: TRAIN [0][860/922]	Time 0.187 (0.192)	Data 8.06e-05 (8.21e-05)	Tok/s 19655 (19394)	Loss/tok 4.7184 (6.3443)	LR 2.000e-03
0: TRAIN [0][860/922]	Time 0.187 (0.192)	Data 8.42e-05 (8.15e-05)	Tok/s 19022 (19418)	Loss/tok 4.5718 (6.3475)	LR 2.000e-03
1: TRAIN [0][870/922]	Time 0.212 (0.192)	Data 8.20e-05 (8.22e-05)	Tok/s 23230 (19393)	Loss/tok 4.8573 (6.3259)	LR 2.000e-03
0: TRAIN [0][870/922]	Time 0.212 (0.192)	Data 8.27e-05 (8.15e-05)	Tok/s 24094 (19419)	Loss/tok 4.7894 (6.3287)	LR 2.000e-03
1: TRAIN [0][880/922]	Time 0.249 (0.192)	Data 8.15e-05 (8.21e-05)	Tok/s 26148 (19386)	Loss/tok 5.1104 (6.3077)	LR 2.000e-03
0: TRAIN [0][880/922]	Time 0.249 (0.192)	Data 7.96e-05 (8.15e-05)	Tok/s 26444 (19415)	Loss/tok 4.8222 (6.3102)	LR 2.000e-03
1: TRAIN [0][890/922]	Time 0.186 (0.192)	Data 8.87e-05 (8.22e-05)	Tok/s 19479 (19386)	Loss/tok 4.5371 (6.2889)	LR 2.000e-03
0: TRAIN [0][890/922]	Time 0.187 (0.192)	Data 8.32e-05 (8.15e-05)	Tok/s 18970 (19412)	Loss/tok 4.4576 (6.2922)	LR 2.000e-03
1: TRAIN [0][900/922]	Time 0.160 (0.192)	Data 7.80e-05 (8.22e-05)	Tok/s 13594 (19385)	Loss/tok 4.2901 (6.2710)	LR 2.000e-03
0: TRAIN [0][900/922]	Time 0.160 (0.192)	Data 7.77e-05 (8.15e-05)	Tok/s 13480 (19411)	Loss/tok 4.3474 (6.2736)	LR 2.000e-03
1: TRAIN [0][910/922]	Time 0.187 (0.192)	Data 8.65e-05 (8.22e-05)	Tok/s 19428 (19370)	Loss/tok 4.4597 (6.2544)	LR 2.000e-03
0: TRAIN [0][910/922]	Time 0.188 (0.192)	Data 7.80e-05 (8.14e-05)	Tok/s 19539 (19397)	Loss/tok 4.5163 (6.2566)	LR 2.000e-03
1: TRAIN [0][920/922]	Time 0.250 (0.192)	Data 4.29e-05 (8.32e-05)	Tok/s 26060 (19389)	Loss/tok 4.8227 (6.2349)	LR 2.000e-03
0: TRAIN [0][920/922]	Time 0.251 (0.192)	Data 3.12e-05 (8.25e-05)	Tok/s 26300 (19416)	Loss/tok 4.8202 (6.2368)	LR 2.000e-03
0: Running validation on dev set
1: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [0][0/106]	Time 0.051 (0.000)	Data 1.52e-03 (0.00e+00)	Tok/s 72275 (0)	Loss/tok 6.2362 (6.2362)
0: VALIDATION [0][0/107]	Time 0.072 (0.000)	Data 1.85e-03 (0.00e+00)	Tok/s 61879 (0)	Loss/tok 6.2658 (6.2658)
1: VALIDATION [0][10/106]	Time 0.032 (0.037)	Data 1.30e-03 (1.28e-03)	Tok/s 72980 (73511)	Loss/tok 5.8158 (5.9557)
0: VALIDATION [0][10/107]	Time 0.032 (0.038)	Data 1.49e-03 (1.59e-03)	Tok/s 72658 (72107)	Loss/tok 6.0842 (6.0150)
1: VALIDATION [0][20/106]	Time 0.026 (0.033)	Data 1.27e-03 (1.27e-03)	Tok/s 74329 (73435)	Loss/tok 5.5764 (5.9189)
0: VALIDATION [0][20/107]	Time 0.028 (0.034)	Data 1.43e-03 (1.51e-03)	Tok/s 68959 (72243)	Loss/tok 5.8999 (5.8886)
1: VALIDATION [0][30/106]	Time 0.024 (0.030)	Data 1.23e-03 (1.26e-03)	Tok/s 70454 (73031)	Loss/tok 5.4473 (5.8429)
0: VALIDATION [0][30/107]	Time 0.023 (0.031)	Data 1.33e-03 (1.47e-03)	Tok/s 71141 (72116)	Loss/tok 5.3495 (5.8392)
1: VALIDATION [0][40/106]	Time 0.021 (0.028)	Data 1.23e-03 (1.25e-03)	Tok/s 69227 (72534)	Loss/tok 5.3696 (5.7876)
0: VALIDATION [0][40/107]	Time 0.021 (0.029)	Data 1.30e-03 (1.43e-03)	Tok/s 69063 (71800)	Loss/tok 5.5880 (5.7825)
1: VALIDATION [0][50/106]	Time 0.019 (0.026)	Data 1.21e-03 (1.25e-03)	Tok/s 66863 (71815)	Loss/tok 5.3301 (5.7509)
0: VALIDATION [0][50/107]	Time 0.020 (0.027)	Data 1.29e-03 (1.40e-03)	Tok/s 63334 (71030)	Loss/tok 5.3085 (5.7383)
1: VALIDATION [0][60/106]	Time 0.017 (0.025)	Data 1.24e-03 (1.24e-03)	Tok/s 65884 (70897)	Loss/tok 5.3667 (5.7186)
0: VALIDATION [0][60/107]	Time 0.016 (0.025)	Data 1.23e-03 (1.38e-03)	Tok/s 67365 (70479)	Loss/tok 5.3999 (5.7087)
1: VALIDATION [0][70/106]	Time 0.014 (0.024)	Data 1.22e-03 (1.24e-03)	Tok/s 66801 (69996)	Loss/tok 5.2203 (5.6891)
0: VALIDATION [0][70/107]	Time 0.015 (0.024)	Data 1.24e-03 (1.36e-03)	Tok/s 63067 (69527)	Loss/tok 5.0781 (5.6803)
1: VALIDATION [0][80/106]	Time 0.013 (0.022)	Data 1.21e-03 (1.24e-03)	Tok/s 61942 (69119)	Loss/tok 5.2816 (5.6614)
0: VALIDATION [0][80/107]	Time 0.013 (0.023)	Data 1.24e-03 (1.35e-03)	Tok/s 60356 (68741)	Loss/tok 5.2520 (5.6580)
1: VALIDATION [0][90/106]	Time 0.011 (0.021)	Data 1.21e-03 (1.24e-03)	Tok/s 57941 (68118)	Loss/tok 4.5922 (5.6317)
0: VALIDATION [0][90/107]	Time 0.011 (0.022)	Data 1.21e-03 (1.33e-03)	Tok/s 58769 (67694)	Loss/tok 5.5527 (5.6367)
1: VALIDATION [0][100/106]	Time 0.009 (0.020)	Data 1.19e-03 (1.23e-03)	Tok/s 49900 (66739)	Loss/tok 4.5928 (5.6036)
0: VALIDATION [0][100/107]	Time 0.009 (0.020)	Data 1.18e-03 (1.32e-03)	Tok/s 52020 (66347)	Loss/tok 4.9999 (5.6090)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
0: TEST [0][9/63]	Time 0.3811 (0.4101)	Decoder iters 149.0 (145.3)	Tok/s 5561 (6430)
1: TEST [0][9/63]	Time 0.3809 (0.4102)	Decoder iters 135.0 (147.6)	Tok/s 5259 (6170)
0: TEST [0][19/63]	Time 0.1781 (0.3651)	Decoder iters 62.0 (119.8)	Tok/s 9221 (6239)
1: TEST [0][19/63]	Time 0.1780 (0.3651)	Decoder iters 54.0 (131.6)	Tok/s 9165 (6114)
0: TEST [0][29/63]	Time 0.1593 (0.3131)	Decoder iters 48.0 (101.3)	Tok/s 8261 (6697)
1: TEST [0][29/63]	Time 0.1593 (0.3131)	Decoder iters 58.0 (113.1)	Tok/s 8131 (6606)
0: TEST [0][39/63]	Time 0.1107 (0.2848)	Decoder iters 38.0 (92.0)	Tok/s 9604 (6676)
1: TEST [0][39/63]	Time 0.1109 (0.2849)	Decoder iters 33.0 (99.4)	Tok/s 9694 (6612)
1: TEST [0][49/63]	Time 0.1569 (0.2620)	Decoder iters 33.0 (90.0)	Tok/s 4934 (6544)
0: TEST [0][49/63]	Time 0.1570 (0.2619)	Decoder iters 61.0 (83.6)	Tok/s 5382 (6620)
0: TEST [0][59/63]	Time 0.0902 (0.2415)	Decoder iters 33.0 (76.8)	Tok/s 6255 (6548)
1: TEST [0][59/63]	Time 0.0900 (0.2415)	Decoder iters 25.0 (81.6)	Tok/s 5912 (6447)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.2341	Validation Loss: 5.5930	Test BLEU: 4.83
0: Performance: Epoch: 0	Training: 38805 Tok/s	Validation: 130837 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
1: TRAIN [1][0/922]	Time 0.358 (0.000)	Data 1.53e-01 (0.00e+00)	Tok/s 13874 (0)	Loss/tok 4.3289 (4.3289)	LR 2.000e-03
0: TRAIN [1][0/922]	Time 0.358 (0.000)	Data 1.74e-01 (0.00e+00)	Tok/s 13884 (0)	Loss/tok 4.4464 (4.4464)	LR 2.000e-03
1: TRAIN [1][10/922]	Time 0.159 (0.196)	Data 8.32e-05 (8.29e-05)	Tok/s 13667 (20850)	Loss/tok 3.9016 (4.4436)	LR 2.000e-03
0: TRAIN [1][10/922]	Time 0.159 (0.196)	Data 7.77e-05 (8.11e-05)	Tok/s 14089 (20876)	Loss/tok 3.9374 (4.3996)	LR 2.000e-03
0: TRAIN [1][20/922]	Time 0.159 (0.196)	Data 7.56e-05 (8.03e-05)	Tok/s 13889 (20333)	Loss/tok 3.9626 (4.3972)	LR 2.000e-03
1: TRAIN [1][20/922]	Time 0.159 (0.196)	Data 8.08e-05 (8.27e-05)	Tok/s 14113 (20427)	Loss/tok 3.9355 (4.4453)	LR 2.000e-03
1: TRAIN [1][30/922]	Time 0.215 (0.190)	Data 7.65e-05 (8.10e-05)	Tok/s 23433 (19273)	Loss/tok 4.4907 (4.4043)	LR 2.000e-03
0: TRAIN [1][30/922]	Time 0.215 (0.190)	Data 7.82e-05 (7.97e-05)	Tok/s 23491 (19251)	Loss/tok 4.5748 (4.3724)	LR 2.000e-03
1: TRAIN [1][40/922]	Time 0.160 (0.191)	Data 7.80e-05 (8.06e-05)	Tok/s 13470 (19450)	Loss/tok 3.9981 (4.4062)	LR 2.000e-03
0: TRAIN [1][40/922]	Time 0.160 (0.191)	Data 7.80e-05 (7.98e-05)	Tok/s 13843 (19468)	Loss/tok 3.6639 (4.3741)	LR 2.000e-03
1: TRAIN [1][50/922]	Time 0.135 (0.189)	Data 7.77e-05 (8.10e-05)	Tok/s 7989 (19006)	Loss/tok 3.4619 (4.3910)	LR 2.000e-03
0: TRAIN [1][50/922]	Time 0.135 (0.189)	Data 8.11e-05 (7.99e-05)	Tok/s 8149 (19098)	Loss/tok 3.6218 (4.3515)	LR 2.000e-03
1: TRAIN [1][60/922]	Time 0.187 (0.190)	Data 7.49e-05 (8.07e-05)	Tok/s 19305 (19071)	Loss/tok 4.0313 (4.3858)	LR 2.000e-03
0: TRAIN [1][60/922]	Time 0.187 (0.190)	Data 7.89e-05 (7.96e-05)	Tok/s 19021 (19162)	Loss/tok 4.1953 (4.3501)	LR 2.000e-03
1: TRAIN [1][70/922]	Time 0.185 (0.192)	Data 7.72e-05 (8.02e-05)	Tok/s 19312 (19402)	Loss/tok 4.2073 (4.3927)	LR 2.000e-03
0: TRAIN [1][70/922]	Time 0.185 (0.192)	Data 7.84e-05 (7.97e-05)	Tok/s 19268 (19453)	Loss/tok 4.2205 (4.3649)	LR 2.000e-03
1: TRAIN [1][80/922]	Time 0.186 (0.193)	Data 8.73e-05 (8.07e-05)	Tok/s 19032 (19570)	Loss/tok 4.0820 (4.3850)	LR 2.000e-03
0: TRAIN [1][80/922]	Time 0.186 (0.193)	Data 7.82e-05 (7.95e-05)	Tok/s 19393 (19603)	Loss/tok 4.1216 (4.3580)	LR 2.000e-03
0: TRAIN [1][90/922]	Time 0.250 (0.192)	Data 8.06e-05 (7.95e-05)	Tok/s 26001 (19472)	Loss/tok 4.6730 (4.3436)	LR 2.000e-03
1: TRAIN [1][90/922]	Time 0.250 (0.192)	Data 7.94e-05 (8.09e-05)	Tok/s 26137 (19456)	Loss/tok 4.7104 (4.3656)	LR 2.000e-03
1: TRAIN [1][100/922]	Time 0.185 (0.190)	Data 8.77e-05 (8.13e-05)	Tok/s 19258 (19235)	Loss/tok 4.0740 (4.3544)	LR 2.000e-03
0: TRAIN [1][100/922]	Time 0.185 (0.190)	Data 7.75e-05 (7.94e-05)	Tok/s 18838 (19235)	Loss/tok 4.1275 (4.3258)	LR 2.000e-03
0: TRAIN [1][110/922]	Time 0.186 (0.189)	Data 7.56e-05 (7.95e-05)	Tok/s 19368 (18994)	Loss/tok 4.1284 (4.3047)	LR 2.000e-03
1: TRAIN [1][110/922]	Time 0.186 (0.189)	Data 9.01e-05 (8.18e-05)	Tok/s 19638 (19009)	Loss/tok 4.1588 (4.3347)	LR 2.000e-03
0: TRAIN [1][120/922]	Time 0.245 (0.190)	Data 8.08e-05 (7.95e-05)	Tok/s 26634 (19145)	Loss/tok 4.5482 (4.3106)	LR 2.000e-03
1: TRAIN [1][120/922]	Time 0.245 (0.189)	Data 7.99e-05 (8.19e-05)	Tok/s 27018 (19166)	Loss/tok 4.5438 (4.3338)	LR 2.000e-03
1: TRAIN [1][130/922]	Time 0.186 (0.191)	Data 8.56e-05 (8.21e-05)	Tok/s 19711 (19425)	Loss/tok 4.0630 (4.3412)	LR 2.000e-03
0: TRAIN [1][130/922]	Time 0.186 (0.191)	Data 8.18e-05 (7.96e-05)	Tok/s 19408 (19406)	Loss/tok 4.1962 (4.3241)	LR 2.000e-03
1: TRAIN [1][140/922]	Time 0.187 (0.191)	Data 8.32e-05 (8.23e-05)	Tok/s 19343 (19394)	Loss/tok 4.1462 (4.3302)	LR 2.000e-03
0: TRAIN [1][140/922]	Time 0.187 (0.191)	Data 8.15e-05 (7.97e-05)	Tok/s 19295 (19372)	Loss/tok 4.1540 (4.3136)	LR 2.000e-03
0: TRAIN [1][150/922]	Time 0.183 (0.190)	Data 8.75e-05 (7.98e-05)	Tok/s 19871 (19255)	Loss/tok 4.0989 (4.3005)	LR 2.000e-03
1: TRAIN [1][150/922]	Time 0.183 (0.190)	Data 8.06e-05 (8.26e-05)	Tok/s 19387 (19270)	Loss/tok 4.1773 (4.3156)	LR 2.000e-03
1: TRAIN [1][160/922]	Time 0.187 (0.190)	Data 8.46e-05 (8.26e-05)	Tok/s 19522 (19208)	Loss/tok 4.0991 (4.3077)	LR 2.000e-03
0: TRAIN [1][160/922]	Time 0.187 (0.190)	Data 8.23e-05 (8.00e-05)	Tok/s 19224 (19195)	Loss/tok 4.1092 (4.2955)	LR 2.000e-03
1: TRAIN [1][170/922]	Time 0.158 (0.190)	Data 1.20e-04 (8.27e-05)	Tok/s 13792 (19169)	Loss/tok 3.9709 (4.3091)	LR 2.000e-03
0: TRAIN [1][170/922]	Time 0.159 (0.190)	Data 7.99e-05 (8.03e-05)	Tok/s 13884 (19153)	Loss/tok 3.7002 (4.2988)	LR 2.000e-03
0: TRAIN [1][180/922]	Time 0.157 (0.190)	Data 8.65e-05 (8.07e-05)	Tok/s 14240 (19192)	Loss/tok 3.6709 (4.2966)	LR 2.000e-03
1: TRAIN [1][180/922]	Time 0.157 (0.190)	Data 8.94e-05 (8.29e-05)	Tok/s 14039 (19199)	Loss/tok 3.9360 (4.3070)	LR 2.000e-03
1: TRAIN [1][190/922]	Time 0.160 (0.190)	Data 8.44e-05 (8.30e-05)	Tok/s 13461 (19171)	Loss/tok 3.8118 (4.3069)	LR 2.000e-03
0: TRAIN [1][190/922]	Time 0.160 (0.190)	Data 8.20e-05 (8.10e-05)	Tok/s 13844 (19172)	Loss/tok 3.8931 (4.2950)	LR 2.000e-03
1: TRAIN [1][200/922]	Time 0.185 (0.190)	Data 8.51e-05 (8.30e-05)	Tok/s 19358 (19140)	Loss/tok 4.0023 (4.2981)	LR 2.000e-03
0: TRAIN [1][200/922]	Time 0.184 (0.190)	Data 7.82e-05 (8.11e-05)	Tok/s 19825 (19141)	Loss/tok 4.0277 (4.2873)	LR 2.000e-03
0: TRAIN [1][210/922]	Time 0.217 (0.190)	Data 8.25e-05 (8.12e-05)	Tok/s 23652 (19204)	Loss/tok 4.2937 (4.2872)	LR 2.000e-03
1: TRAIN [1][210/922]	Time 0.217 (0.190)	Data 7.58e-05 (8.30e-05)	Tok/s 23047 (19201)	Loss/tok 4.2184 (4.2962)	LR 2.000e-03
1: TRAIN [1][220/922]	Time 0.187 (0.191)	Data 8.30e-05 (8.31e-05)	Tok/s 19254 (19333)	Loss/tok 3.9515 (4.2957)	LR 2.000e-03
0: TRAIN [1][220/922]	Time 0.187 (0.191)	Data 8.46e-05 (8.13e-05)	Tok/s 19617 (19331)	Loss/tok 4.1662 (4.2870)	LR 2.000e-03
1: TRAIN [1][230/922]	Time 0.186 (0.192)	Data 8.46e-05 (8.33e-05)	Tok/s 19462 (19508)	Loss/tok 4.2077 (4.2993)	LR 2.000e-03
0: TRAIN [1][230/922]	Time 0.186 (0.192)	Data 8.63e-05 (8.14e-05)	Tok/s 19306 (19502)	Loss/tok 4.1434 (4.2880)	LR 2.000e-03
1: TRAIN [1][240/922]	Time 0.187 (0.192)	Data 8.42e-05 (8.35e-05)	Tok/s 19229 (19529)	Loss/tok 4.0222 (4.2934)	LR 2.000e-03
0: TRAIN [1][240/922]	Time 0.187 (0.192)	Data 8.89e-05 (8.16e-05)	Tok/s 19425 (19521)	Loss/tok 4.2194 (4.2838)	LR 2.000e-03
1: TRAIN [1][250/922]	Time 0.159 (0.193)	Data 9.25e-05 (8.35e-05)	Tok/s 12857 (19588)	Loss/tok 3.8444 (4.2922)	LR 2.000e-03
0: TRAIN [1][250/922]	Time 0.159 (0.193)	Data 8.61e-05 (8.17e-05)	Tok/s 13501 (19586)	Loss/tok 3.6427 (4.2813)	LR 2.000e-03
1: TRAIN [1][260/922]	Time 0.157 (0.192)	Data 8.75e-05 (8.36e-05)	Tok/s 14303 (19489)	Loss/tok 3.7312 (4.2875)	LR 2.000e-03
0: TRAIN [1][260/922]	Time 0.157 (0.192)	Data 8.03e-05 (8.18e-05)	Tok/s 13886 (19487)	Loss/tok 3.8196 (4.2774)	LR 2.000e-03
0: TRAIN [1][270/922]	Time 0.159 (0.192)	Data 8.44e-05 (8.19e-05)	Tok/s 13861 (19462)	Loss/tok 3.8205 (4.2716)	LR 2.000e-03
1: TRAIN [1][270/922]	Time 0.159 (0.192)	Data 9.13e-05 (8.36e-05)	Tok/s 13759 (19461)	Loss/tok 3.8457 (4.2820)	LR 2.000e-03
1: TRAIN [1][280/922]	Time 0.248 (0.192)	Data 9.01e-05 (8.38e-05)	Tok/s 26434 (19466)	Loss/tok 4.6149 (4.2799)	LR 2.000e-03
0: TRAIN [1][280/922]	Time 0.248 (0.192)	Data 8.32e-05 (8.19e-05)	Tok/s 26327 (19469)	Loss/tok 4.7119 (4.2715)	LR 2.000e-03
1: TRAIN [1][290/922]	Time 0.159 (0.192)	Data 8.73e-05 (8.39e-05)	Tok/s 14017 (19521)	Loss/tok 3.6939 (4.2789)	LR 2.000e-03
0: TRAIN [1][290/922]	Time 0.159 (0.192)	Data 7.92e-05 (8.21e-05)	Tok/s 13284 (19523)	Loss/tok 3.7555 (4.2708)	LR 2.000e-03
0: TRAIN [1][300/922]	Time 0.188 (0.192)	Data 8.42e-05 (8.21e-05)	Tok/s 19184 (19458)	Loss/tok 4.0405 (4.2667)	LR 2.000e-03
1: TRAIN [1][300/922]	Time 0.188 (0.192)	Data 8.70e-05 (8.39e-05)	Tok/s 19094 (19454)	Loss/tok 4.0644 (4.2743)	LR 2.000e-03
0: TRAIN [1][310/922]	Time 0.189 (0.192)	Data 8.01e-05 (8.22e-05)	Tok/s 18592 (19396)	Loss/tok 4.1811 (4.2636)	LR 1.000e-03
1: TRAIN [1][310/922]	Time 0.189 (0.192)	Data 9.04e-05 (8.41e-05)	Tok/s 18740 (19396)	Loss/tok 4.0691 (4.2716)	LR 1.000e-03
1: TRAIN [1][320/922]	Time 0.216 (0.192)	Data 8.54e-05 (8.41e-05)	Tok/s 23505 (19469)	Loss/tok 4.2519 (4.2693)	LR 1.000e-03
0: TRAIN [1][320/922]	Time 0.216 (0.192)	Data 8.11e-05 (8.22e-05)	Tok/s 23366 (19467)	Loss/tok 4.2843 (4.2615)	LR 1.000e-03
1: TRAIN [1][330/922]	Time 0.156 (0.192)	Data 8.03e-05 (8.41e-05)	Tok/s 13242 (19392)	Loss/tok 3.9420 (4.2642)	LR 1.000e-03
0: TRAIN [1][330/922]	Time 0.156 (0.192)	Data 8.25e-05 (8.23e-05)	Tok/s 13758 (19394)	Loss/tok 3.5851 (4.2557)	LR 1.000e-03
1: TRAIN [1][340/922]	Time 0.214 (0.192)	Data 1.33e-04 (8.42e-05)	Tok/s 23251 (19390)	Loss/tok 4.1416 (4.2580)	LR 1.000e-03
0: TRAIN [1][340/922]	Time 0.214 (0.192)	Data 8.77e-05 (8.24e-05)	Tok/s 23379 (19390)	Loss/tok 4.0583 (4.2497)	LR 1.000e-03
1: TRAIN [1][350/922]	Time 0.183 (0.192)	Data 9.20e-05 (8.43e-05)	Tok/s 19083 (19379)	Loss/tok 3.8096 (4.2510)	LR 1.000e-03
0: TRAIN [1][350/922]	Time 0.183 (0.192)	Data 8.08e-05 (8.24e-05)	Tok/s 19644 (19378)	Loss/tok 3.8155 (4.2424)	LR 1.000e-03
0: TRAIN [1][360/922]	Time 0.184 (0.192)	Data 8.61e-05 (8.24e-05)	Tok/s 19394 (19411)	Loss/tok 3.9111 (4.2395)	LR 1.000e-03
1: TRAIN [1][360/922]	Time 0.184 (0.192)	Data 9.23e-05 (8.44e-05)	Tok/s 19709 (19414)	Loss/tok 3.9823 (4.2483)	LR 1.000e-03
1: TRAIN [1][370/922]	Time 0.187 (0.192)	Data 9.25e-05 (8.44e-05)	Tok/s 19205 (19381)	Loss/tok 3.8678 (4.2441)	LR 1.000e-03
0: TRAIN [1][370/922]	Time 0.187 (0.192)	Data 9.58e-05 (8.25e-05)	Tok/s 18998 (19376)	Loss/tok 3.9460 (4.2364)	LR 1.000e-03
1: TRAIN [1][380/922]	Time 0.136 (0.191)	Data 8.25e-05 (8.45e-05)	Tok/s 7976 (19342)	Loss/tok 3.1650 (4.2373)	LR 1.000e-03
0: TRAIN [1][380/922]	Time 0.136 (0.191)	Data 8.15e-05 (8.26e-05)	Tok/s 8108 (19343)	Loss/tok 3.3348 (4.2290)	LR 1.000e-03
1: TRAIN [1][390/922]	Time 0.216 (0.191)	Data 8.70e-05 (8.46e-05)	Tok/s 23112 (19327)	Loss/tok 4.1270 (4.2292)	LR 1.000e-03
0: TRAIN [1][390/922]	Time 0.212 (0.191)	Data 1.01e-04 (8.27e-05)	Tok/s 23513 (19327)	Loss/tok 4.1775 (4.2222)	LR 1.000e-03
1: TRAIN [1][400/922]	Time 0.160 (0.191)	Data 8.39e-05 (8.47e-05)	Tok/s 13345 (19334)	Loss/tok 3.7255 (4.2241)	LR 1.000e-03
0: TRAIN [1][400/922]	Time 0.160 (0.191)	Data 8.03e-05 (8.27e-05)	Tok/s 13521 (19335)	Loss/tok 3.5131 (4.2161)	LR 1.000e-03
1: TRAIN [1][410/922]	Time 0.187 (0.191)	Data 8.94e-05 (8.47e-05)	Tok/s 19332 (19315)	Loss/tok 3.8454 (4.2175)	LR 1.000e-03
0: TRAIN [1][410/922]	Time 0.187 (0.191)	Data 8.27e-05 (8.27e-05)	Tok/s 19595 (19314)	Loss/tok 3.9133 (4.2096)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
1: TRAIN [1][420/922]	Time 0.212 (0.191)	Data 8.32e-05 (8.47e-05)	Tok/s 23745 (19355)	Loss/tok 3.9907 (4.2126)	LR 1.000e-03
0: TRAIN [1][420/922]	Time 0.212 (0.191)	Data 8.32e-05 (8.26e-05)	Tok/s 23682 (19360)	Loss/tok 4.0719 (4.2045)	LR 1.000e-03
1: TRAIN [1][430/922]	Time 0.215 (0.191)	Data 9.13e-05 (8.48e-05)	Tok/s 23338 (19306)	Loss/tok 4.0026 (4.2082)	LR 1.000e-03
0: TRAIN [1][430/922]	Time 0.215 (0.191)	Data 7.94e-05 (8.26e-05)	Tok/s 23425 (19312)	Loss/tok 4.1908 (4.2000)	LR 1.000e-03
0: TRAIN [1][440/922]	Time 0.184 (0.191)	Data 8.06e-05 (8.27e-05)	Tok/s 19648 (19319)	Loss/tok 3.7987 (4.1943)	LR 1.000e-03
1: TRAIN [1][440/922]	Time 0.184 (0.191)	Data 8.18e-05 (8.48e-05)	Tok/s 19880 (19311)	Loss/tok 3.6895 (4.2014)	LR 1.000e-03
1: TRAIN [1][450/922]	Time 0.214 (0.191)	Data 8.34e-05 (8.49e-05)	Tok/s 23944 (19319)	Loss/tok 3.9130 (4.1976)	LR 1.000e-03
0: TRAIN [1][450/922]	Time 0.214 (0.191)	Data 8.23e-05 (8.28e-05)	Tok/s 23612 (19325)	Loss/tok 3.9974 (4.1894)	LR 1.000e-03
0: TRAIN [1][460/922]	Time 0.157 (0.191)	Data 8.23e-05 (8.28e-05)	Tok/s 13425 (19281)	Loss/tok 3.8118 (4.1844)	LR 5.000e-04
1: TRAIN [1][460/922]	Time 0.157 (0.191)	Data 8.32e-05 (8.50e-05)	Tok/s 14054 (19273)	Loss/tok 3.5571 (4.1918)	LR 5.000e-04
1: TRAIN [1][470/922]	Time 0.187 (0.191)	Data 8.06e-05 (8.49e-05)	Tok/s 19345 (19325)	Loss/tok 3.9434 (4.1890)	LR 5.000e-04
0: TRAIN [1][470/922]	Time 0.187 (0.191)	Data 8.15e-05 (8.28e-05)	Tok/s 19408 (19332)	Loss/tok 3.7313 (4.1805)	LR 5.000e-04
1: TRAIN [1][480/922]	Time 0.187 (0.191)	Data 7.99e-05 (8.48e-05)	Tok/s 19152 (19291)	Loss/tok 3.7343 (4.1824)	LR 5.000e-04
0: TRAIN [1][480/922]	Time 0.187 (0.191)	Data 8.27e-05 (8.28e-05)	Tok/s 19276 (19300)	Loss/tok 3.9273 (4.1751)	LR 5.000e-04
0: TRAIN [1][490/922]	Time 0.156 (0.191)	Data 8.01e-05 (8.27e-05)	Tok/s 13375 (19331)	Loss/tok 3.6050 (4.1715)	LR 5.000e-04
1: TRAIN [1][490/922]	Time 0.156 (0.191)	Data 8.46e-05 (8.49e-05)	Tok/s 13820 (19326)	Loss/tok 3.4858 (4.1782)	LR 5.000e-04
1: TRAIN [1][500/922]	Time 0.183 (0.191)	Data 8.30e-05 (8.48e-05)	Tok/s 19759 (19323)	Loss/tok 3.6643 (4.1729)	LR 5.000e-04
0: TRAIN [1][500/922]	Time 0.183 (0.191)	Data 7.94e-05 (8.27e-05)	Tok/s 19629 (19327)	Loss/tok 3.9735 (4.1675)	LR 5.000e-04
0: TRAIN [1][510/922]	Time 0.216 (0.191)	Data 8.06e-05 (8.27e-05)	Tok/s 22816 (19306)	Loss/tok 4.0849 (4.1625)	LR 5.000e-04
1: TRAIN [1][510/922]	Time 0.216 (0.191)	Data 8.61e-05 (8.47e-05)	Tok/s 22968 (19300)	Loss/tok 4.0423 (4.1679)	LR 5.000e-04
0: TRAIN [1][520/922]	Time 0.159 (0.191)	Data 7.99e-05 (8.27e-05)	Tok/s 13510 (19313)	Loss/tok 3.7443 (4.1565)	LR 5.000e-04
1: TRAIN [1][520/922]	Time 0.159 (0.191)	Data 8.13e-05 (8.47e-05)	Tok/s 13655 (19304)	Loss/tok 3.6926 (4.1618)	LR 5.000e-04
1: TRAIN [1][530/922]	Time 0.184 (0.191)	Data 8.39e-05 (8.47e-05)	Tok/s 19611 (19301)	Loss/tok 3.8911 (4.1566)	LR 5.000e-04
0: TRAIN [1][530/922]	Time 0.184 (0.191)	Data 8.06e-05 (8.27e-05)	Tok/s 19703 (19313)	Loss/tok 3.6419 (4.1523)	LR 5.000e-04
0: TRAIN [1][540/922]	Time 0.186 (0.191)	Data 7.99e-05 (8.26e-05)	Tok/s 19035 (19298)	Loss/tok 3.8348 (4.1481)	LR 5.000e-04
1: TRAIN [1][540/922]	Time 0.186 (0.191)	Data 8.27e-05 (8.47e-05)	Tok/s 19307 (19288)	Loss/tok 3.7673 (4.1513)	LR 5.000e-04
1: TRAIN [1][550/922]	Time 0.213 (0.190)	Data 8.32e-05 (8.46e-05)	Tok/s 23883 (19264)	Loss/tok 4.0608 (4.1460)	LR 5.000e-04
0: TRAIN [1][550/922]	Time 0.213 (0.190)	Data 7.75e-05 (8.25e-05)	Tok/s 23628 (19271)	Loss/tok 4.1017 (4.1429)	LR 5.000e-04
1: TRAIN [1][560/922]	Time 0.249 (0.191)	Data 8.27e-05 (8.45e-05)	Tok/s 26358 (19299)	Loss/tok 4.1742 (4.1418)	LR 5.000e-04
0: TRAIN [1][560/922]	Time 0.249 (0.191)	Data 7.92e-05 (8.25e-05)	Tok/s 26380 (19307)	Loss/tok 4.0857 (4.1388)	LR 5.000e-04
0: TRAIN [1][570/922]	Time 0.214 (0.191)	Data 7.72e-05 (8.24e-05)	Tok/s 22929 (19303)	Loss/tok 3.8626 (4.1343)	LR 5.000e-04
1: TRAIN [1][570/922]	Time 0.214 (0.191)	Data 8.08e-05 (8.46e-05)	Tok/s 23519 (19293)	Loss/tok 4.1336 (4.1386)	LR 5.000e-04
1: TRAIN [1][580/922]	Time 0.247 (0.191)	Data 8.23e-05 (8.45e-05)	Tok/s 26194 (19339)	Loss/tok 4.1800 (4.1355)	LR 5.000e-04
0: TRAIN [1][580/922]	Time 0.247 (0.191)	Data 7.92e-05 (8.24e-05)	Tok/s 26452 (19352)	Loss/tok 4.2701 (4.1308)	LR 5.000e-04
0: TRAIN [1][590/922]	Time 0.184 (0.191)	Data 7.96e-05 (8.24e-05)	Tok/s 19843 (19304)	Loss/tok 3.7562 (4.1247)	LR 5.000e-04
1: TRAIN [1][590/922]	Time 0.184 (0.191)	Data 8.27e-05 (8.45e-05)	Tok/s 19717 (19292)	Loss/tok 3.5248 (4.1295)	LR 5.000e-04
1: TRAIN [1][600/922]	Time 0.187 (0.191)	Data 8.01e-05 (8.45e-05)	Tok/s 19274 (19301)	Loss/tok 3.7259 (4.1251)	LR 5.000e-04
0: TRAIN [1][600/922]	Time 0.190 (0.191)	Data 7.89e-05 (8.23e-05)	Tok/s 19173 (19314)	Loss/tok 3.7883 (4.1205)	LR 5.000e-04
0: TRAIN [1][610/922]	Time 0.213 (0.191)	Data 7.72e-05 (8.23e-05)	Tok/s 23890 (19329)	Loss/tok 3.9601 (4.1157)	LR 5.000e-04
1: TRAIN [1][610/922]	Time 0.213 (0.191)	Data 8.20e-05 (8.44e-05)	Tok/s 24038 (19318)	Loss/tok 3.9463 (4.1204)	LR 5.000e-04
0: TRAIN [1][620/922]	Time 0.212 (0.191)	Data 7.65e-05 (8.23e-05)	Tok/s 24122 (19321)	Loss/tok 3.8936 (4.1111)	LR 2.500e-04
1: TRAIN [1][620/922]	Time 0.213 (0.191)	Data 8.39e-05 (8.45e-05)	Tok/s 24056 (19313)	Loss/tok 3.9146 (4.1156)	LR 2.500e-04
0: TRAIN [1][630/922]	Time 0.245 (0.191)	Data 8.08e-05 (8.23e-05)	Tok/s 26294 (19315)	Loss/tok 4.2235 (4.1068)	LR 2.500e-04
1: TRAIN [1][630/922]	Time 0.245 (0.191)	Data 8.77e-05 (8.44e-05)	Tok/s 26975 (19312)	Loss/tok 4.1670 (4.1107)	LR 2.500e-04
0: TRAIN [1][640/922]	Time 0.158 (0.191)	Data 7.99e-05 (8.22e-05)	Tok/s 13637 (19304)	Loss/tok 3.4853 (4.1031)	LR 2.500e-04
1: TRAIN [1][640/922]	Time 0.158 (0.191)	Data 7.92e-05 (8.44e-05)	Tok/s 13367 (19302)	Loss/tok 3.5413 (4.1060)	LR 2.500e-04
0: TRAIN [1][650/922]	Time 0.184 (0.190)	Data 7.92e-05 (8.22e-05)	Tok/s 20027 (19251)	Loss/tok 3.8362 (4.0981)	LR 2.500e-04
1: TRAIN [1][650/922]	Time 0.185 (0.190)	Data 8.30e-05 (8.43e-05)	Tok/s 19893 (19249)	Loss/tok 3.6479 (4.1009)	LR 2.500e-04
1: TRAIN [1][660/922]	Time 0.246 (0.191)	Data 7.80e-05 (8.43e-05)	Tok/s 26544 (19291)	Loss/tok 4.1162 (4.0981)	LR 2.500e-04
0: TRAIN [1][660/922]	Time 0.246 (0.191)	Data 8.18e-05 (8.22e-05)	Tok/s 26969 (19295)	Loss/tok 4.1588 (4.0954)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
1: TRAIN [1][670/922]	Time 0.185 (0.191)	Data 8.25e-05 (8.43e-05)	Tok/s 27678 (19322)	Loss/tok 3.9745 (4.0942)	LR 2.500e-04
0: TRAIN [1][670/922]	Time 0.185 (0.191)	Data 8.42e-05 (8.22e-05)	Tok/s 27676 (19325)	Loss/tok 3.9285 (4.0909)	LR 2.500e-04
1: TRAIN [1][680/922]	Time 0.250 (0.191)	Data 8.25e-05 (8.43e-05)	Tok/s 25641 (19322)	Loss/tok 4.2315 (4.0907)	LR 2.500e-04
0: TRAIN [1][680/922]	Time 0.250 (0.191)	Data 8.23e-05 (8.23e-05)	Tok/s 26430 (19326)	Loss/tok 4.0884 (4.0874)	LR 2.500e-04
1: TRAIN [1][690/922]	Time 0.156 (0.191)	Data 8.11e-05 (8.42e-05)	Tok/s 14004 (19337)	Loss/tok 3.3980 (4.0865)	LR 2.500e-04
0: TRAIN [1][690/922]	Time 0.156 (0.191)	Data 8.44e-05 (8.23e-05)	Tok/s 13724 (19340)	Loss/tok 3.4231 (4.0831)	LR 2.500e-04
0: TRAIN [1][700/922]	Time 0.185 (0.191)	Data 8.27e-05 (8.23e-05)	Tok/s 19264 (19332)	Loss/tok 3.7313 (4.0791)	LR 2.500e-04
1: TRAIN [1][700/922]	Time 0.185 (0.191)	Data 7.89e-05 (8.42e-05)	Tok/s 19454 (19328)	Loss/tok 3.5524 (4.0827)	LR 2.500e-04
1: TRAIN [1][710/922]	Time 0.156 (0.191)	Data 8.39e-05 (8.42e-05)	Tok/s 12958 (19325)	Loss/tok 3.3830 (4.0797)	LR 2.500e-04
0: TRAIN [1][710/922]	Time 0.156 (0.191)	Data 8.13e-05 (8.24e-05)	Tok/s 13784 (19330)	Loss/tok 3.5704 (4.0756)	LR 2.500e-04
1: TRAIN [1][720/922]	Time 0.187 (0.191)	Data 8.27e-05 (8.42e-05)	Tok/s 19128 (19335)	Loss/tok 3.6682 (4.0762)	LR 2.500e-04
0: TRAIN [1][720/922]	Time 0.187 (0.191)	Data 8.68e-05 (8.24e-05)	Tok/s 18825 (19340)	Loss/tok 3.7514 (4.0717)	LR 2.500e-04
0: TRAIN [1][730/922]	Time 0.187 (0.191)	Data 8.30e-05 (8.25e-05)	Tok/s 19098 (19369)	Loss/tok 3.6303 (4.0683)	LR 2.500e-04
1: TRAIN [1][730/922]	Time 0.188 (0.191)	Data 8.61e-05 (8.42e-05)	Tok/s 19551 (19366)	Loss/tok 3.6154 (4.0728)	LR 2.500e-04
0: TRAIN [1][740/922]	Time 0.156 (0.191)	Data 8.11e-05 (8.25e-05)	Tok/s 14320 (19370)	Loss/tok 3.5063 (4.0659)	LR 2.500e-04
1: TRAIN [1][740/922]	Time 0.156 (0.191)	Data 8.37e-05 (8.42e-05)	Tok/s 13977 (19368)	Loss/tok 3.5785 (4.0696)	LR 2.500e-04
1: TRAIN [1][750/922]	Time 0.159 (0.191)	Data 8.61e-05 (8.42e-05)	Tok/s 13025 (19353)	Loss/tok 3.3559 (4.0654)	LR 2.500e-04
0: TRAIN [1][750/922]	Time 0.159 (0.191)	Data 8.58e-05 (8.25e-05)	Tok/s 13663 (19356)	Loss/tok 3.5788 (4.0628)	LR 2.500e-04
1: TRAIN [1][760/922]	Time 0.184 (0.191)	Data 8.42e-05 (8.41e-05)	Tok/s 19704 (19369)	Loss/tok 3.6631 (4.0620)	LR 2.500e-04
0: TRAIN [1][760/922]	Time 0.184 (0.191)	Data 8.08e-05 (8.26e-05)	Tok/s 19829 (19372)	Loss/tok 3.7834 (4.0600)	LR 2.500e-04
0: TRAIN [1][770/922]	Time 0.184 (0.191)	Data 8.13e-05 (8.26e-05)	Tok/s 19430 (19374)	Loss/tok 3.7904 (4.0574)	LR 1.250e-04
1: TRAIN [1][770/922]	Time 0.184 (0.191)	Data 7.89e-05 (8.41e-05)	Tok/s 19257 (19367)	Loss/tok 3.6272 (4.0593)	LR 1.250e-04
1: TRAIN [1][780/922]	Time 0.159 (0.191)	Data 8.11e-05 (8.41e-05)	Tok/s 13623 (19380)	Loss/tok 3.3408 (4.0561)	LR 1.250e-04
0: TRAIN [1][780/922]	Time 0.159 (0.191)	Data 8.46e-05 (8.27e-05)	Tok/s 13980 (19388)	Loss/tok 3.2921 (4.0552)	LR 1.250e-04
1: TRAIN [1][790/922]	Time 0.247 (0.191)	Data 8.08e-05 (8.41e-05)	Tok/s 26367 (19388)	Loss/tok 4.1205 (4.0533)	LR 1.250e-04
0: TRAIN [1][790/922]	Time 0.247 (0.191)	Data 8.39e-05 (8.27e-05)	Tok/s 26406 (19394)	Loss/tok 4.0922 (4.0521)	LR 1.250e-04
1: TRAIN [1][800/922]	Time 0.157 (0.191)	Data 8.65e-05 (8.41e-05)	Tok/s 14127 (19367)	Loss/tok 3.4980 (4.0498)	LR 1.250e-04
0: TRAIN [1][800/922]	Time 0.157 (0.191)	Data 8.18e-05 (8.27e-05)	Tok/s 13696 (19372)	Loss/tok 3.4427 (4.0490)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
1: TRAIN [1][810/922]	Time 0.249 (0.191)	Data 8.56e-05 (8.41e-05)	Tok/s 26437 (19401)	Loss/tok 4.0593 (4.0478)	LR 1.250e-04
0: TRAIN [1][810/922]	Time 0.244 (0.191)	Data 1.24e-04 (8.28e-05)	Tok/s 26648 (19406)	Loss/tok 4.0402 (4.0460)	LR 1.250e-04
1: TRAIN [1][820/922]	Time 0.216 (0.191)	Data 8.85e-05 (8.41e-05)	Tok/s 23363 (19426)	Loss/tok 3.8012 (4.0450)	LR 1.250e-04
0: TRAIN [1][820/922]	Time 0.216 (0.191)	Data 8.37e-05 (8.29e-05)	Tok/s 22935 (19430)	Loss/tok 3.9738 (4.0440)	LR 1.250e-04
1: TRAIN [1][830/922]	Time 0.186 (0.191)	Data 8.20e-05 (8.41e-05)	Tok/s 19821 (19445)	Loss/tok 3.6593 (4.0425)	LR 1.250e-04
0: TRAIN [1][830/922]	Time 0.186 (0.191)	Data 8.01e-05 (8.29e-05)	Tok/s 19861 (19448)	Loss/tok 3.6084 (4.0412)	LR 1.250e-04
1: TRAIN [1][840/922]	Time 0.157 (0.191)	Data 8.13e-05 (8.41e-05)	Tok/s 14160 (19447)	Loss/tok 3.3300 (4.0388)	LR 1.250e-04
0: TRAIN [1][840/922]	Time 0.157 (0.191)	Data 8.44e-05 (8.29e-05)	Tok/s 14049 (19448)	Loss/tok 3.3182 (4.0378)	LR 1.250e-04
0: TRAIN [1][850/922]	Time 0.184 (0.191)	Data 8.61e-05 (8.29e-05)	Tok/s 19308 (19431)	Loss/tok 3.6636 (4.0339)	LR 1.250e-04
1: TRAIN [1][850/922]	Time 0.184 (0.191)	Data 8.18e-05 (8.41e-05)	Tok/s 19941 (19430)	Loss/tok 3.6774 (4.0354)	LR 1.250e-04
0: TRAIN [1][860/922]	Time 0.213 (0.191)	Data 8.03e-05 (8.29e-05)	Tok/s 23236 (19462)	Loss/tok 3.8239 (4.0313)	LR 1.250e-04
1: TRAIN [1][860/922]	Time 0.213 (0.191)	Data 8.27e-05 (8.42e-05)	Tok/s 23617 (19460)	Loss/tok 4.0371 (4.0329)	LR 1.250e-04
1: TRAIN [1][870/922]	Time 0.216 (0.191)	Data 8.51e-05 (8.42e-05)	Tok/s 23280 (19465)	Loss/tok 4.0203 (4.0309)	LR 1.250e-04
0: TRAIN [1][870/922]	Time 0.216 (0.191)	Data 8.70e-05 (8.30e-05)	Tok/s 23545 (19465)	Loss/tok 3.8783 (4.0291)	LR 1.250e-04
1: TRAIN [1][880/922]	Time 0.214 (0.191)	Data 8.77e-05 (8.42e-05)	Tok/s 22951 (19461)	Loss/tok 3.9149 (4.0281)	LR 1.250e-04
0: TRAIN [1][880/922]	Time 0.214 (0.191)	Data 8.20e-05 (8.31e-05)	Tok/s 23432 (19461)	Loss/tok 3.9282 (4.0263)	LR 1.250e-04
1: TRAIN [1][890/922]	Time 0.184 (0.191)	Data 8.01e-05 (8.42e-05)	Tok/s 20207 (19436)	Loss/tok 3.8838 (4.0249)	LR 1.250e-04
0: TRAIN [1][890/922]	Time 0.184 (0.191)	Data 8.03e-05 (8.31e-05)	Tok/s 19659 (19435)	Loss/tok 3.7279 (4.0228)	LR 1.250e-04
1: TRAIN [1][900/922]	Time 0.246 (0.191)	Data 8.37e-05 (8.42e-05)	Tok/s 26610 (19438)	Loss/tok 4.1667 (4.0229)	LR 1.250e-04
0: TRAIN [1][900/922]	Time 0.246 (0.191)	Data 8.54e-05 (8.31e-05)	Tok/s 26677 (19436)	Loss/tok 3.9947 (4.0206)	LR 1.250e-04
0: TRAIN [1][910/922]	Time 0.184 (0.191)	Data 8.39e-05 (8.31e-05)	Tok/s 19570 (19448)	Loss/tok 3.6684 (4.0184)	LR 1.250e-04
1: TRAIN [1][910/922]	Time 0.184 (0.191)	Data 7.94e-05 (8.42e-05)	Tok/s 19564 (19448)	Loss/tok 3.8207 (4.0208)	LR 1.250e-04
1: TRAIN [1][920/922]	Time 0.247 (0.191)	Data 3.41e-05 (8.53e-05)	Tok/s 26319 (19480)	Loss/tok 4.0732 (4.0190)	LR 1.250e-04
0: TRAIN [1][920/922]	Time 0.247 (0.191)	Data 3.55e-05 (8.42e-05)	Tok/s 26488 (19477)	Loss/tok 4.0493 (4.0166)	LR 1.250e-04
1: Running validation on dev set
1: Executing preallocation
0: Running validation on dev set
0: Executing preallocation
1: VALIDATION [1][0/106]	Time 0.051 (0.000)	Data 1.53e-03 (0.00e+00)	Tok/s 72876 (0)	Loss/tok 5.4925 (5.4925)
0: VALIDATION [1][0/107]	Time 0.071 (0.000)	Data 1.48e-03 (0.00e+00)	Tok/s 62797 (0)	Loss/tok 5.5984 (5.5984)
1: VALIDATION [1][10/106]	Time 0.032 (0.037)	Data 1.28e-03 (1.31e-03)	Tok/s 72193 (73667)	Loss/tok 5.0013 (5.1787)
0: VALIDATION [1][10/107]	Time 0.032 (0.038)	Data 1.28e-03 (1.31e-03)	Tok/s 74387 (73002)	Loss/tok 5.2121 (5.2272)
1: VALIDATION [1][20/106]	Time 0.026 (0.033)	Data 1.27e-03 (1.29e-03)	Tok/s 73447 (73628)	Loss/tok 4.7990 (5.1389)
0: VALIDATION [1][20/107]	Time 0.028 (0.033)	Data 1.26e-03 (1.29e-03)	Tok/s 70906 (72868)	Loss/tok 5.1601 (5.0905)
1: VALIDATION [1][30/106]	Time 0.023 (0.030)	Data 1.24e-03 (1.27e-03)	Tok/s 71150 (73256)	Loss/tok 4.6749 (5.0667)
0: VALIDATION [1][30/107]	Time 0.023 (0.031)	Data 1.23e-03 (1.28e-03)	Tok/s 71415 (72714)	Loss/tok 4.4828 (5.0521)
1: VALIDATION [1][40/106]	Time 0.021 (0.028)	Data 1.24e-03 (1.26e-03)	Tok/s 70392 (72752)	Loss/tok 4.6715 (5.0187)
0: VALIDATION [1][40/107]	Time 0.021 (0.028)	Data 1.23e-03 (1.27e-03)	Tok/s 69381 (72318)	Loss/tok 4.7635 (5.0058)
1: VALIDATION [1][50/106]	Time 0.019 (0.026)	Data 1.21e-03 (1.25e-03)	Tok/s 68302 (72010)	Loss/tok 4.7264 (4.9817)
0: VALIDATION [1][50/107]	Time 0.020 (0.027)	Data 1.23e-03 (1.26e-03)	Tok/s 63839 (71480)	Loss/tok 4.5078 (4.9660)
1: VALIDATION [1][60/106]	Time 0.017 (0.025)	Data 1.22e-03 (1.25e-03)	Tok/s 64833 (71244)	Loss/tok 4.6488 (4.9536)
0: VALIDATION [1][60/107]	Time 0.016 (0.025)	Data 1.20e-03 (1.26e-03)	Tok/s 67634 (70900)	Loss/tok 4.5938 (4.9395)
1: VALIDATION [1][70/106]	Time 0.014 (0.023)	Data 1.21e-03 (1.24e-03)	Tok/s 67558 (70312)	Loss/tok 4.5449 (4.9261)
0: VALIDATION [1][70/107]	Time 0.015 (0.024)	Data 1.20e-03 (1.25e-03)	Tok/s 62923 (69888)	Loss/tok 4.4528 (4.9179)
1: VALIDATION [1][80/106]	Time 0.013 (0.022)	Data 1.21e-03 (1.24e-03)	Tok/s 61849 (69396)	Loss/tok 4.3729 (4.9009)
0: VALIDATION [1][80/107]	Time 0.013 (0.023)	Data 1.20e-03 (1.25e-03)	Tok/s 61233 (69067)	Loss/tok 4.5261 (4.9000)
1: VALIDATION [1][90/106]	Time 0.011 (0.021)	Data 1.20e-03 (1.23e-03)	Tok/s 56828 (68344)	Loss/tok 3.9859 (4.8772)
0: VALIDATION [1][90/107]	Time 0.011 (0.021)	Data 1.22e-03 (1.24e-03)	Tok/s 58772 (67998)	Loss/tok 4.8310 (4.8843)
1: VALIDATION [1][100/106]	Time 0.009 (0.020)	Data 1.20e-03 (1.23e-03)	Tok/s 49142 (66927)	Loss/tok 4.0190 (4.8540)
0: VALIDATION [1][100/107]	Time 0.009 (0.020)	Data 1.18e-03 (1.24e-03)	Tok/s 52009 (66636)	Loss/tok 4.3062 (4.8607)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [1][9/63]	Time 0.3794 (0.3843)	Decoder iters 149.0 (131.2)	Tok/s 5702 (6674)
0: TEST [1][9/63]	Time 0.3796 (0.3842)	Decoder iters 149.0 (128.4)	Tok/s 5999 (6870)
0: TEST [1][19/63]	Time 0.1692 (0.3414)	Decoder iters 59.0 (102.8)	Tok/s 10255 (6869)
1: TEST [1][19/63]	Time 0.1690 (0.3414)	Decoder iters 48.0 (119.2)	Tok/s 9665 (6795)
1: TEST [1][29/63]	Time 0.1307 (0.3211)	Decoder iters 38.0 (109.2)	Tok/s 10391 (6624)
0: TEST [1][29/63]	Time 0.1309 (0.3211)	Decoder iters 45.0 (96.4)	Tok/s 10081 (6706)
0: TEST [1][39/63]	Time 0.1029 (0.2926)	Decoder iters 35.0 (89.6)	Tok/s 10813 (6907)
1: TEST [1][39/63]	Time 0.1028 (0.2926)	Decoder iters 32.0 (97.7)	Tok/s 10598 (6852)
1: TEST [1][49/63]	Time 0.0940 (0.2723)	Decoder iters 27.0 (90.5)	Tok/s 8796 (6778)
0: TEST [1][49/63]	Time 0.0942 (0.2723)	Decoder iters 32.0 (81.0)	Tok/s 9339 (6851)
1: TEST [1][59/63]	Time 0.0708 (0.2486)	Decoder iters 20.0 (81.8)	Tok/s 7434 (6859)
0: TEST [1][59/63]	Time 0.0709 (0.2485)	Decoder iters 24.0 (73.6)	Tok/s 7970 (6971)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.0174	Validation Loss: 4.8464	Test BLEU: 8.35
0: Performance: Epoch: 1	Training: 38966 Tok/s	Validation: 131307 Tok/s
0: Finished epoch 1
1: Total training time 423 s
0: Total training time 423 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                  80|                      8.35|                      38885.2|                         7.054|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
