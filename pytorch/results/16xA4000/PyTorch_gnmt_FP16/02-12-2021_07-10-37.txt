6: thread affinity: {22, 30, 6, 14}
2: thread affinity: {18, 26, 2, 10}
11: thread affinity: {43, 35, 51, 59}
13: thread affinity: {53, 61, 45, 37}
4: thread affinity: {20, 28, 4, 12}
7: thread affinity: {31, 15, 23, 7}
10: thread affinity: {34, 58, 42, 50}
14: thread affinity: {46, 54, 38, 62}
3: thread affinity: {11, 19, 3, 27}
5: thread affinity: {29, 13, 21, 5}
0: thread affinity: {0, 8, 16, 24}
9: thread affinity: {33, 49, 57, 41}
15: thread affinity: {55, 63, 47, 39}
8: thread affinity: {32, 56, 40, 48}
12: thread affinity: {36, 60, 44, 52}
1: thread affinity: {1, 25, 9, 17}
12: Collecting environment information...
8: Collecting environment information...
15: Collecting environment information...
0: Collecting environment information...
14: Collecting environment information...
2: Collecting environment information...
5: Collecting environment information...
6: Collecting environment information...
13: Collecting environment information...
10: Collecting environment information...
1: Collecting environment information...
9: Collecting environment information...
11: Collecting environment information...
7: Collecting environment information...
4: Collecting environment information...
3: Collecting environment information...
9: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
9: Saving results to: gnmt
9: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=9, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=9, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
9: Using master seed from command line: 2
11: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
11: Saving results to: gnmt
11: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=11, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=11, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
11: Using master seed from command line: 2
10: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
10: Saving results to: gnmt
10: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=10, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=10, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
10: Using master seed from command line: 2
13: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
13: Saving results to: gnmt
13: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=13, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=13, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
13: Using master seed from command line: 2
12: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
12: Saving results to: gnmt
12: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=12, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=12, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
12: Using master seed from command line: 2
8: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
8: Saving results to: gnmt
8: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=8, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=8, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
8: Using master seed from command line: 2
14: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
14: Saving results to: gnmt
14: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=14, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=14, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
14: Using master seed from command line: 2
6: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
6: Saving results to: gnmt
6: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=6, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=6, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
6: Using master seed from command line: 2
15: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
15: Saving results to: gnmt
15: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=15, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=15, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
15: Using master seed from command line: 2
0: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
7: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
7: Saving results to: gnmt
7: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=7, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=7, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
7: Using master seed from command line: 2
1: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
2: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
2: Saving results to: gnmt
2: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=2, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
2: Using master seed from command line: 2
3: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
3: Saving results to: gnmt
3: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=3, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
3: Using master seed from command line: 2
4: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
4: Saving results to: gnmt
4: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=4, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=4, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
4: Using master seed from command line: 2
5: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
5: Saving results to: gnmt
5: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=5, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=5, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=260, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
5: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
3: Worker 3 is using worker seed: 1323436024
2: Worker 2 is using worker seed: 3588440356
1: Worker 1 is using worker seed: 364522461
4: Worker 4 is using worker seed: 2602510382
7: Worker 7 is using worker seed: 117874757
5: Worker 5 is using worker seed: 2606193617
6: Worker 6 is using worker seed: 4077622522
8: Worker 8 is using worker seed: 1632151663
11: Worker 11 is using worker seed: 1014142328
13: Worker 13 is using worker seed: 1396478261
10: Worker 10 is using worker seed: 2407373688
15: Worker 15 is using worker seed: 3837860530
14: Worker 14 is using worker seed: 2191394736
9: Worker 9 is using worker seed: 2258090960
12: Worker 12 is using worker seed: 102469680
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
13: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
8: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
12: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
13: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
11: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
8: Size of vocabulary: 31800
10: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
12: Size of vocabulary: 31800
14: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
11: Size of vocabulary: 31800
10: Size of vocabulary: 31800
13: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
14: Size of vocabulary: 31800
6: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
8: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
12: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
6: Size of vocabulary: 31800
11: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
10: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
14: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
4: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
9: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
15: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
7: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Size of vocabulary: 31800
2: Size of vocabulary: 31800
4: Size of vocabulary: 31800
9: Size of vocabulary: 31800
15: Size of vocabulary: 31800
3: Size of vocabulary: 31800
5: Size of vocabulary: 31800
7: Size of vocabulary: 31800
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
9: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
15: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
13: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
8: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
12: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
10: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
11: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
14: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
15: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
9: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
13: Filtering data, min len: 0, max len: 50
8: Filtering data, min len: 0, max len: 50
12: Filtering data, min len: 0, max len: 50
10: Filtering data, min len: 0, max len: 50
11: Filtering data, min len: 0, max len: 50
14: Filtering data, min len: 0, max len: 50
6: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
4: Filtering data, min len: 0, max len: 50
5: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
7: Filtering data, min len: 0, max len: 50
15: Filtering data, min len: 0, max len: 50
9: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
13: Pairs before: 160078, after: 148120
12: Pairs before: 160078, after: 148120
8: Pairs before: 160078, after: 148120
10: Pairs before: 160078, after: 148120
11: Pairs before: 160078, after: 148120
14: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
4: Pairs before: 160078, after: 148120
13: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Pairs before: 160078, after: 148120
7: Pairs before: 160078, after: 148120
12: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
8: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
15: Pairs before: 160078, after: 148120
9: Pairs before: 160078, after: 148120
11: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
10: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
14: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
15: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
9: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
13: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
12: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
8: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
11: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
10: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
14: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
9: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
15: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
12: Filtering data, min len: 0, max len: 125
13: Filtering data, min len: 0, max len: 125
8: Filtering data, min len: 0, max len: 125
11: Filtering data, min len: 0, max len: 125
12: Pairs before: 5100, after: 5100
13: Pairs before: 5100, after: 5100
10: Filtering data, min len: 0, max len: 125
14: Filtering data, min len: 0, max len: 125
8: Pairs before: 5100, after: 5100
11: Pairs before: 5100, after: 5100
14: Pairs before: 5100, after: 5100
6: Filtering data, min len: 0, max len: 125
10: Pairs before: 5100, after: 5100
6: Pairs before: 5100, after: 5100
2: Filtering data, min len: 0, max len: 125
4: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
9: Filtering data, min len: 0, max len: 125
3: Filtering data, min len: 0, max len: 125
5: Filtering data, min len: 0, max len: 125
15: Filtering data, min len: 0, max len: 125
7: Filtering data, min len: 0, max len: 125
2: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
4: Pairs before: 5100, after: 5100
9: Pairs before: 5100, after: 5100
3: Pairs before: 5100, after: 5100
15: Pairs before: 5100, after: 5100
5: Pairs before: 5100, after: 5100
7: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
12: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
13: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
8: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
14: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
11: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
10: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
6: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
9: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
15: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
12: Filtering data, min len: 0, max len: 150
13: Filtering data, min len: 0, max len: 150
12: Pairs before: 3003, after: 3003
8: Filtering data, min len: 0, max len: 150
13: Pairs before: 3003, after: 3003
8: Pairs before: 3003, after: 3003
14: Filtering data, min len: 0, max len: 150
14: Pairs before: 3003, after: 3003
11: Filtering data, min len: 0, max len: 150
11: Pairs before: 3003, after: 3003
10: Filtering data, min len: 0, max len: 150
10: Pairs before: 3003, after: 3003
6: Filtering data, min len: 0, max len: 150
6: Pairs before: 3003, after: 3003
9: Filtering data, min len: 0, max len: 150
9: Pairs before: 3003, after: 3003
2: Filtering data, min len: 0, max len: 150
4: Filtering data, min len: 0, max len: 150
1: Filtering data, min len: 0, max len: 150
4: Pairs before: 3003, after: 3003
2: Pairs before: 3003, after: 3003
1: Pairs before: 3003, after: 3003
15: Filtering data, min len: 0, max len: 150
3: Filtering data, min len: 0, max len: 150
5: Filtering data, min len: 0, max len: 150
15: Pairs before: 3003, after: 3003
3: Pairs before: 3003, after: 3003
7: Filtering data, min len: 0, max len: 150
5: Pairs before: 3003, after: 3003
7: Pairs before: 3003, after: 3003
8: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
8: Building LabelSmoothingLoss (smoothing: 0.1)
8: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
8: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
8: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
8: Saving state of the tokenizer
8: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
8: Scheduler warmup steps: 200
8: Scheduler remain steps: 42
8: Scheduler decay interval: 5
8: Scheduler decay factor: 0.5
8: Scheduler max decay steps: 4
8: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
8: Initializing amp optimizer
8: Starting epoch 0
8: Executing preallocation
13: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
13: Building LabelSmoothingLoss (smoothing: 0.1)
13: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
13: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
13: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
13: Saving state of the tokenizer
13: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
13: Scheduler warmup steps: 200
13: Scheduler remain steps: 42
13: Scheduler decay interval: 5
13: Scheduler decay factor: 0.5
13: Scheduler max decay steps: 4
13: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
2: Saving state of the tokenizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 42
2: Scheduler decay interval: 5
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
9: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
9: Building LabelSmoothingLoss (smoothing: 0.1)
9: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
9: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
9: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
13: Initializing amp optimizer
2: Initializing amp optimizer
9: Saving state of the tokenizer
9: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
9: Scheduler warmup steps: 200
9: Scheduler remain steps: 42
9: Scheduler decay interval: 5
9: Scheduler decay factor: 0.5
9: Scheduler max decay steps: 4
9: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
13: Starting epoch 0
13: Executing preallocation
12: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
12: Building LabelSmoothingLoss (smoothing: 0.1)
12: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
12: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
12: Number of parameters: 159605817
5: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
5: Building LabelSmoothingLoss (smoothing: 0.1)
14: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
14: Building LabelSmoothingLoss (smoothing: 0.1)
5: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
5: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
14: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
14: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Starting epoch 0
5: Number of parameters: 159605817
14: Number of parameters: 159605817
2: Executing preallocation
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
15: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
15: Building LabelSmoothingLoss (smoothing: 0.1)
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
15: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
15: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
15: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
9: Initializing amp optimizer
12: Saving state of the tokenizer
12: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
12: Scheduler warmup steps: 200
12: Scheduler remain steps: 42
12: Scheduler decay interval: 5
12: Scheduler decay factor: 0.5
12: Scheduler max decay steps: 4
12: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
14: Saving state of the tokenizer
14: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
14: Scheduler warmup steps: 200
5: Saving state of the tokenizer
14: Scheduler remain steps: 42
14: Scheduler decay interval: 5
14: Scheduler decay factor: 0.5
14: Scheduler max decay steps: 4
14: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
5: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
5: Scheduler warmup steps: 200
5: Scheduler remain steps: 42
5: Scheduler decay interval: 5
5: Scheduler decay factor: 0.5
5: Scheduler max decay steps: 4
5: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
6: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
15: Saving state of the tokenizer
6: Building LabelSmoothingLoss (smoothing: 0.1)
6: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
15: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
15: Scheduler warmup steps: 200
9: Starting epoch 0
15: Scheduler remain steps: 42
15: Scheduler decay interval: 5
15: Scheduler decay factor: 0.5
6: Number of parameters: 159605817
15: Scheduler max decay steps: 4
15: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
9: Executing preallocation
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
7: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: Building LabelSmoothingLoss (smoothing: 0.1)
7: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
7: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
7: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
6: Saving state of the tokenizer
6: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Scheduler warmup steps: 200
6: Scheduler remain steps: 42
6: Scheduler decay interval: 5
6: Scheduler decay factor: 0.5
6: Scheduler max decay steps: 4
6: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
11: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
11: Building LabelSmoothingLoss (smoothing: 0.1)
12: Initializing amp optimizer
11: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
11: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
11: Number of parameters: 159605817
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
14: Initializing amp optimizer
5: Initializing amp optimizer
15: Initializing amp optimizer
7: Saving state of the tokenizer
7: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
7: Scheduler warmup steps: 200
7: Scheduler remain steps: 42
7: Scheduler decay interval: 5
7: Scheduler decay factor: 0.5
7: Scheduler max decay steps: 4
10: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
10: Building LabelSmoothingLoss (smoothing: 0.1)
10: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
10: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
10: Number of parameters: 159605817
12: Starting epoch 0
12: Executing preallocation
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
11: Saving state of the tokenizer
11: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
11: Scheduler warmup steps: 200
11: Scheduler remain steps: 42
11: Scheduler decay interval: 5
11: Scheduler decay factor: 0.5
11: Scheduler max decay steps: 4
11: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
3: Saving state of the tokenizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 42
3: Scheduler decay interval: 5
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
14: Starting epoch 0
14: Executing preallocation
6: Initializing amp optimizer
5: Starting epoch 0
5: Executing preallocation
10: Saving state of the tokenizer
15: Starting epoch 0
15: Executing preallocation
10: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
10: Scheduler warmup steps: 200
10: Scheduler remain steps: 42
10: Scheduler decay interval: 5
10: Scheduler decay factor: 0.5
10: Scheduler max decay steps: 4
10: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
7: Initializing amp optimizer
6: Starting epoch 0
6: Executing preallocation
11: Initializing amp optimizer
4: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
4: Building LabelSmoothingLoss (smoothing: 0.1)
4: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
4: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
4: Number of parameters: 159605817
3: Initializing amp optimizer
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
7: Starting epoch 0
7: Executing preallocation
10: Initializing amp optimizer
11: Starting epoch 0
11: Executing preallocation
3: Starting epoch 0
4: Saving state of the tokenizer
3: Executing preallocation
4: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
4: Scheduler warmup steps: 200
4: Scheduler remain steps: 42
4: Scheduler decay interval: 5
4: Scheduler decay factor: 0.5
4: Scheduler max decay steps: 4
4: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
10: Starting epoch 0
10: Executing preallocation
4: Initializing amp optimizer
4: Starting epoch 0
4: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 42
0: Scheduler decay interval: 5
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 42
1: Scheduler decay interval: 5
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
1: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
8: Sampler for epoch 0 uses seed 3422057796
4: Sampler for epoch 0 uses seed 3422057796
5: Sampler for epoch 0 uses seed 3422057796
3: Sampler for epoch 0 uses seed 3422057796
2: Sampler for epoch 0 uses seed 3422057796
9: Sampler for epoch 0 uses seed 3422057796
0: Sampler for epoch 0 uses seed 3422057796
6: Sampler for epoch 0 uses seed 3422057796
7: Sampler for epoch 0 uses seed 3422057796
1: Sampler for epoch 0 uses seed 3422057796
13: Sampler for epoch 0 uses seed 3422057796
11: Sampler for epoch 0 uses seed 3422057796
10: Sampler for epoch 0 uses seed 3422057796
15: Sampler for epoch 0 uses seed 3422057796
14: Sampler for epoch 0 uses seed 3422057796
12: Sampler for epoch 0 uses seed 3422057796
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0


/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Seems like `optimizer.step()` has been overridden after learning rate scheduler "

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0

14: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 7.89e-01 (0.00e+00)	Tok/s 15791 (0)	Loss/tok 10.6802 (10.6802)	LR 2.232e-05
0: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 6.62e-01 (0.00e+00)	Tok/s 15788 (0)	Loss/tok 10.6755 (10.6755)	LR 2.232e-05
1: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 7.31e-01 (0.00e+00)	Tok/s 15926 (0)	Loss/tok 10.6786 (10.6786)	LR 2.232e-05
2: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 5.95e-01 (0.00e+00)	Tok/s 15800 (0)	Loss/tok 10.6765 (10.6765)	LR 2.232e-05
15: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 7.70e-01 (0.00e+00)	Tok/s 16009 (0)	Loss/tok 10.6759 (10.6759)	LR 2.232e-05
3: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 5.86e-01 (0.00e+00)	Tok/s 16015 (0)	Loss/tok 10.6797 (10.6797)	LR 2.232e-05
13: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 7.52e-01 (0.00e+00)	Tok/s 15776 (0)	Loss/tok 10.6830 (10.6830)	LR 2.232e-05
12: TRAIN [0][0/32]	Time 1.339 (0.000)	Data 9.23e-01 (0.00e+00)	Tok/s 15897 (0)	Loss/tok 10.6820 (10.6820)	LR 2.232e-05
11: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 7.67e-01 (0.00e+00)	Tok/s 15752 (0)	Loss/tok 10.6856 (10.6856)	LR 2.232e-05
9: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 6.54e-01 (0.00e+00)	Tok/s 15726 (0)	Loss/tok 10.6805 (10.6805)	LR 2.232e-05
8: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 4.17e-01 (0.00e+00)	Tok/s 15873 (0)	Loss/tok 10.6882 (10.6882)	LR 2.232e-05
4: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 5.59e-01 (0.00e+00)	Tok/s 15855 (0)	Loss/tok 10.6733 (10.6733)	LR 2.232e-05
5: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 5.64e-01 (0.00e+00)	Tok/s 15843 (0)	Loss/tok 10.6883 (10.6883)	LR 2.232e-05
10: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 7.68e-01 (0.00e+00)	Tok/s 15898 (0)	Loss/tok 10.6829 (10.6829)	LR 2.232e-05
7: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 7.14e-01 (0.00e+00)	Tok/s 15892 (0)	Loss/tok 10.6893 (10.6893)	LR 2.232e-05
6: TRAIN [0][0/32]	Time 1.340 (0.000)	Data 7.18e-01 (0.00e+00)	Tok/s 15921 (0)	Loss/tok 10.6894 (10.6894)	LR 2.232e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
14: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.82e-04 (1.56e-04)	Tok/s 43904 (36348)	Loss/tok 10.1137 (10.2620)	LR 6.681e-05
1: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.79e-04 (2.06e-04)	Tok/s 44592 (36451)	Loss/tok 10.1010 (10.2562)	LR 6.681e-05
12: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.74e-04 (1.55e-04)	Tok/s 44371 (36244)	Loss/tok 10.1194 (10.2593)	LR 6.681e-05
15: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.87e-04 (2.08e-04)	Tok/s 44521 (36282)	Loss/tok 10.1238 (10.2607)	LR 6.681e-05
2: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.00e-04 (2.37e-04)	Tok/s 44039 (36252)	Loss/tok 10.1172 (10.2587)	LR 6.681e-05
0: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 7.83e-03 (2.93e-03)	Tok/s 44426 (36293)	Loss/tok 10.1242 (10.2608)	LR 6.681e-05
13: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.94e-04 (1.15e-03)	Tok/s 44532 (36425)	Loss/tok 10.1080 (10.2588)	LR 6.681e-05
3: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.02e-04 (2.47e-04)	Tok/s 44215 (36307)	Loss/tok 10.1531 (10.2648)	LR 6.681e-05
4: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.89e-04 (2.69e-03)	Tok/s 44443 (36297)	Loss/tok 10.1073 (10.2599)	LR 6.681e-05
11: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.94e-04 (2.07e-04)	Tok/s 44137 (36191)	Loss/tok 10.1157 (10.2557)	LR 6.681e-05
9: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.81e-04 (1.06e-03)	Tok/s 43966 (36292)	Loss/tok 10.1352 (10.2638)	LR 6.681e-05
7: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.78e-04 (7.42e-04)	Tok/s 44120 (36347)	Loss/tok 10.1466 (10.2682)	LR 6.681e-05
10: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.78e-04 (3.57e-03)	Tok/s 44519 (36308)	Loss/tok 10.1160 (10.2623)	LR 6.681e-05
8: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.65e-04 (1.52e-03)	Tok/s 44478 (36398)	Loss/tok 10.1265 (10.2639)	LR 6.681e-05
5: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 2.79e-04 (1.59e-03)	Tok/s 43927 (36128)	Loss/tok 10.1146 (10.2663)	LR 6.681e-05
6: TRAIN [0][10/32]	Time 0.480 (0.298)	Data 1.69e-04 (3.33e-04)	Tok/s 44594 (36370)	Loss/tok 10.1132 (10.2630)	LR 6.681e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
9: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.55e-04 (6.07e-04)	Tok/s 29950 (37643)	Loss/tok 8.6737 (9.8967)	LR 2.000e-04
8: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.65e-04 (1.12e-03)	Tok/s 29903 (37706)	Loss/tok 8.6109 (9.8977)	LR 2.000e-04
11: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.61e-04 (6.95e-04)	Tok/s 29807 (37642)	Loss/tok 8.6181 (9.8944)	LR 2.000e-04
10: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.64e-04 (1.86e-03)	Tok/s 30171 (37642)	Loss/tok 8.6312 (9.8931)	LR 2.000e-04
2: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.48e-04 (1.98e-04)	Tok/s 29745 (37612)	Loss/tok 8.6195 (9.8984)	LR 2.000e-04
7: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 2.48e-04 (4.78e-04)	Tok/s 30263 (37771)	Loss/tok 8.6572 (9.9002)	LR 2.000e-04
12: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 7.07e-03 (4.98e-04)	Tok/s 29281 (37538)	Loss/tok 8.6710 (9.8923)	LR 2.000e-04
13: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.39e-04 (6.55e-04)	Tok/s 29314 (37737)	Loss/tok 8.6703 (9.8961)	LR 2.000e-04
1: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.61e-04 (2.06e-04)	Tok/s 30777 (37751)	Loss/tok 8.6102 (9.8932)	LR 2.000e-04
0: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.56e-04 (1.95e-03)	Tok/s 30257 (37659)	Loss/tok 8.6265 (9.8895)	LR 2.000e-04
14: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 1.47e-02 (2.71e-03)	Tok/s 29863 (37728)	Loss/tok 8.6309 (9.8980)	LR 2.000e-04
4: TRAIN [0][20/32]	Time 0.238 (0.312)	Data 1.59e-04 (1.43e-03)	Tok/s 29458 (37643)	Loss/tok 8.6695 (9.9009)	LR 2.000e-04
15: TRAIN [0][20/32]	Time 0.237 (0.312)	Data 3.35e-04 (2.13e-04)	Tok/s 29911 (37572)	Loss/tok 8.6507 (9.8995)	LR 2.000e-04
5: TRAIN [0][20/32]	Time 0.238 (0.312)	Data 2.32e-04 (8.99e-04)	Tok/s 29047 (37583)	Loss/tok 8.6206 (9.8994)	LR 2.000e-04
3: TRAIN [0][20/32]	Time 0.238 (0.312)	Data 1.67e-04 (2.09e-04)	Tok/s 29709 (37584)	Loss/tok 8.6732 (9.8947)	LR 2.000e-04
6: TRAIN [0][20/32]	Time 0.239 (0.312)	Data 1.62e-04 (2.50e-04)	Tok/s 29070 (37695)	Loss/tok 8.6588 (9.9010)	LR 2.000e-04
1: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 5.27e-05 (1.94e-04)	Tok/s 43118 (37878)	Loss/tok 8.2458 (9.4513)	LR 5.987e-04
13: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 4.53e-05 (4.85e-04)	Tok/s 43100 (37961)	Loss/tok 8.2352 (9.4510)	LR 5.987e-04
15: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 7.68e-05 (2.01e-04)	Tok/s 42677 (37702)	Loss/tok 8.1805 (9.4578)	LR 5.987e-04
12: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 7.08e-05 (3.91e-04)	Tok/s 42959 (37652)	Loss/tok 8.2063 (9.4513)	LR 5.987e-04
0: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 7.61e-05 (1.44e-03)	Tok/s 43322 (37867)	Loss/tok 8.2188 (9.4503)	LR 5.987e-04
11: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 8.49e-05 (5.12e-04)	Tok/s 42908 (37798)	Loss/tok 8.2489 (9.4489)	LR 5.987e-04
3: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 6.65e-05 (1.92e-04)	Tok/s 43019 (37734)	Loss/tok 8.2210 (9.4507)	LR 5.987e-04
4: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 8.13e-05 (1.00e-03)	Tok/s 42781 (37812)	Loss/tok 8.2234 (9.4547)	LR 5.987e-04
2: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 5.63e-05 (1.79e-04)	Tok/s 42992 (37823)	Loss/tok 8.1985 (9.4592)	LR 5.987e-04
9: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 7.01e-05 (4.50e-04)	Tok/s 43143 (37809)	Loss/tok 8.2469 (9.4539)	LR 5.987e-04
14: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 8.56e-05 (1.88e-03)	Tok/s 43016 (37915)	Loss/tok 8.2280 (9.4524)	LR 5.987e-04
5: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 5.25e-05 (6.55e-04)	Tok/s 43367 (37803)	Loss/tok 8.1777 (9.4535)	LR 5.987e-04
10: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 7.80e-05 (1.29e-03)	Tok/s 42381 (37817)	Loss/tok 8.2287 (9.4516)	LR 5.987e-04
8: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 7.30e-05 (7.92e-04)	Tok/s 43454 (37893)	Loss/tok 8.1743 (9.4586)	LR 5.987e-04
6: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 8.08e-05 (2.16e-04)	Tok/s 42799 (37800)	Loss/tok 8.2090 (9.4534)	LR 5.987e-04
7: TRAIN [0][30/32]	Time 0.381 (0.313)	Data 5.34e-05 (3.75e-04)	Tok/s 42912 (37868)	Loss/tok 8.1945 (9.4587)	LR 5.987e-04
3: Running validation on dev set
11: Running validation on dev set
1: Running validation on dev set
2: Running validation on dev set
8: Running validation on dev set
5: Running validation on dev set
12: Running validation on dev set
0: Running validation on dev set
6: Running validation on dev set
15: Running validation on dev set
1: Executing preallocation
11: Executing preallocation
2: Executing preallocation
3: Executing preallocation
8: Executing preallocation
5: Executing preallocation
12: Executing preallocation
14: Running validation on dev set
0: Executing preallocation
6: Executing preallocation
15: Executing preallocation
9: Running validation on dev set
14: Executing preallocation
7: Running validation on dev set
13: Running validation on dev set
4: Running validation on dev set
10: Running validation on dev set
9: Executing preallocation
13: Executing preallocation
7: Executing preallocation
4: Executing preallocation
10: Executing preallocation
1: VALIDATION [0][0/5]	Time 0.075 (0.000)	Data 2.13e-03 (0.00e+00)	Tok/s 114915 (0)	Loss/tok 9.0145 (9.0145)
10: VALIDATION [0][0/5]	Time 0.045 (0.000)	Data 2.14e-03 (0.00e+00)	Tok/s 128799 (0)	Loss/tok 8.9095 (8.9095)
3: VALIDATION [0][0/5]	Time 0.061 (0.000)	Data 2.12e-03 (0.00e+00)	Tok/s 122652 (0)	Loss/tok 8.9580 (8.9580)
15: VALIDATION [0][0/5]	Time 0.041 (0.000)	Data 2.07e-03 (0.00e+00)	Tok/s 125432 (0)	Loss/tok 8.7506 (8.7506)
12: VALIDATION [0][0/5]	Time 0.042 (0.000)	Data 2.00e-03 (0.00e+00)	Tok/s 130815 (0)	Loss/tok 8.7701 (8.7701)
7: VALIDATION [0][0/5]	Time 0.050 (0.000)	Data 2.12e-03 (0.00e+00)	Tok/s 126090 (0)	Loss/tok 8.9526 (8.9526)
14: VALIDATION [0][0/5]	Time 0.043 (0.000)	Data 2.06e-03 (0.00e+00)	Tok/s 123787 (0)	Loss/tok 8.8010 (8.8010)
13: VALIDATION [0][0/5]	Time 0.044 (0.000)	Data 2.12e-03 (0.00e+00)	Tok/s 122385 (0)	Loss/tok 8.8057 (8.8057)
11: VALIDATION [0][0/5]	Time 0.046 (0.000)	Data 2.06e-03 (0.00e+00)	Tok/s 122833 (0)	Loss/tok 8.8231 (8.8231)
9: VALIDATION [0][0/5]	Time 0.049 (0.000)	Data 1.99e-03 (0.00e+00)	Tok/s 122595 (0)	Loss/tok 8.8451 (8.8451)
8: VALIDATION [0][0/5]	Time 0.048 (0.000)	Data 2.10e-03 (0.00e+00)	Tok/s 126289 (0)	Loss/tok 8.8915 (8.8915)
4: VALIDATION [0][0/5]	Time 0.055 (0.000)	Data 2.04e-03 (0.00e+00)	Tok/s 128278 (0)	Loss/tok 8.9673 (8.9673)
6: VALIDATION [0][0/5]	Time 0.055 (0.000)	Data 2.18e-03 (0.00e+00)	Tok/s 118134 (0)	Loss/tok 8.9095 (8.9095)
5: VALIDATION [0][0/5]	Time 0.056 (0.000)	Data 2.16e-03 (0.00e+00)	Tok/s 121297 (0)	Loss/tok 8.9590 (8.9590)
2: VALIDATION [0][0/5]	Time 0.066 (0.000)	Data 2.03e-03 (0.00e+00)	Tok/s 119490 (0)	Loss/tok 9.0008 (9.0008)
0: VALIDATION [0][0/5]	Time 0.118 (0.000)	Data 2.15e-03 (0.00e+00)	Tok/s 88743 (0)	Loss/tok 9.0396 (9.0396)
0: Saving model to gnmt/model_best.pth
15: Running evaluation on test set
10: Running evaluation on test set
7: Running evaluation on test set
11: Running evaluation on test set
13: Running evaluation on test set
14: Running evaluation on test set
2: Running evaluation on test set
4: Running evaluation on test set
5: Running evaluation on test set
3: Running evaluation on test set
8: Running evaluation on test set
6: Running evaluation on test set
1: Running evaluation on test set
0: Running evaluation on test set
9: Running evaluation on test set
12: Running evaluation on test set
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
7: Finished evaluation on test set
9: Finished evaluation on test set
11: Finished evaluation on test set
12: Finished evaluation on test set
3: Finished evaluation on test set
1: Finished evaluation on test set
14: Finished evaluation on test set
8: Finished evaluation on test set
15: Finished evaluation on test set
4: Finished evaluation on test set
13: Finished evaluation on test set
6: Finished evaluation on test set
5: Finished evaluation on test set
10: Finished evaluation on test set
2: Finished evaluation on test set
0: Finished evaluation on test set
7: Finished epoch 0
11: Finished epoch 0
9: Finished epoch 0
3: Finished epoch 0
15: Finished epoch 0
2: Finished epoch 0
13: Finished epoch 0
1: Finished epoch 0
14: Finished epoch 0
12: Finished epoch 0
8: Finished epoch 0
10: Finished epoch 0
5: Finished epoch 0
4: Finished epoch 0
6: Finished epoch 0
7: Starting epoch 1
9: Starting epoch 1
11: Starting epoch 1
15: Starting epoch 1
3: Starting epoch 1
2: Starting epoch 1
13: Starting epoch 1
1: Starting epoch 1
14: Starting epoch 1
12: Starting epoch 1
8: Starting epoch 1
10: Starting epoch 1
5: Starting epoch 1
4: Starting epoch 1
6: Starting epoch 1
11: Executing preallocation
3: Executing preallocation
15: Executing preallocation
6: Executing preallocation
14: Executing preallocation
10: Executing preallocation
7: Executing preallocation
13: Executing preallocation
1: Executing preallocation
9: Executing preallocation
5: Executing preallocation
12: Executing preallocation
4: Executing preallocation
8: Executing preallocation
2: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 9.4046	Validation Loss: 8.6213	Test BLEU: 0.03
0: Performance: Epoch: 0	Training: 607609 Tok/s	Validation: 1865668 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
7: Sampler for epoch 1 uses seed 3276568223
15: Sampler for epoch 1 uses seed 3276568223
14: Sampler for epoch 1 uses seed 3276568223
4: Sampler for epoch 1 uses seed 3276568223
3: Sampler for epoch 1 uses seed 3276568223
8: Sampler for epoch 1 uses seed 3276568223
12: Sampler for epoch 1 uses seed 3276568223
0: Sampler for epoch 1 uses seed 3276568223
1: Sampler for epoch 1 uses seed 3276568223
2: Sampler for epoch 1 uses seed 3276568223
6: Sampler for epoch 1 uses seed 3276568223
5: Sampler for epoch 1 uses seed 3276568223
9: Sampler for epoch 1 uses seed 3276568223
11: Sampler for epoch 1 uses seed 3276568223
13: Sampler for epoch 1 uses seed 3276568223
10: Sampler for epoch 1 uses seed 3276568223
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0

7: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.14e-01 (0.00e+00)	Tok/s 21890 (0)	Loss/tok 7.7332 (7.7332)	LR 7.455e-04
9: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.30e-01 (0.00e+00)	Tok/s 22469 (0)	Loss/tok 7.6577 (7.6577)	LR 7.455e-04
8: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.13e-01 (0.00e+00)	Tok/s 22343 (0)	Loss/tok 7.7428 (7.7428)	LR 7.455e-04
10: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.38e-01 (0.00e+00)	Tok/s 22397 (0)	Loss/tok 7.7355 (7.7355)	LR 7.455e-04
5: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.23e-01 (0.00e+00)	Tok/s 22036 (0)	Loss/tok 7.7135 (7.7135)	LR 7.455e-04
6: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.16e-01 (0.00e+00)	Tok/s 21927 (0)	Loss/tok 7.7506 (7.7506)	LR 7.455e-04
4: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.09e-01 (0.00e+00)	Tok/s 22043 (0)	Loss/tok 7.7892 (7.7892)	LR 7.455e-04
11: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.27e-01 (0.00e+00)	Tok/s 21913 (0)	Loss/tok 7.8041 (7.8041)	LR 7.455e-04
12: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.16e-01 (0.00e+00)	Tok/s 21786 (0)	Loss/tok 7.7009 (7.7009)	LR 7.455e-04
14: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.10e-01 (0.00e+00)	Tok/s 21602 (0)	Loss/tok 7.7227 (7.7227)	LR 7.455e-04
13: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.34e-01 (0.00e+00)	Tok/s 21887 (0)	Loss/tok 7.7292 (7.7292)	LR 7.455e-04
0: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.15e-01 (0.00e+00)	Tok/s 21824 (0)	Loss/tok 7.7018 (7.7018)	LR 7.455e-04
15: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.10e-01 (0.00e+00)	Tok/s 22341 (0)	Loss/tok 7.7624 (7.7624)	LR 7.455e-04
3: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.14e-01 (0.00e+00)	Tok/s 21878 (0)	Loss/tok 7.6766 (7.6766)	LR 7.455e-04
2: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.19e-01 (0.00e+00)	Tok/s 21829 (0)	Loss/tok 7.6692 (7.6692)	LR 7.455e-04
1: TRAIN [1][0/32]	Time 0.320 (0.000)	Data 1.16e-01 (0.00e+00)	Tok/s 21857 (0)	Loss/tok 7.7522 (7.7522)	LR 7.455e-04
8: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 6.22e-04 (1.44e-03)	Tok/s 38488 (38522)	Loss/tok 7.8460 (8.0445)	LR 1.000e-03
7: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.61e-04 (1.66e-04)	Tok/s 38739 (38367)	Loss/tok 7.8124 (8.0353)	LR 1.000e-03
11: TRAIN [1][10/32]	Time 0.303 (0.331)	Data 2.89e-04 (2.11e-04)	Tok/s 38700 (38420)	Loss/tok 7.8368 (8.0385)	LR 1.000e-03
4: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 2.34e-04 (1.69e-04)	Tok/s 38343 (38421)	Loss/tok 7.7839 (8.0351)	LR 1.000e-03
12: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.50e-04 (1.54e-04)	Tok/s 39017 (38444)	Loss/tok 7.8738 (8.0548)	LR 1.000e-03
10: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.58e-04 (1.58e-04)	Tok/s 38017 (38417)	Loss/tok 7.8078 (8.0488)	LR 1.000e-03
2: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.67e-04 (1.70e-04)	Tok/s 38647 (38466)	Loss/tok 7.8565 (8.0412)	LR 1.000e-03
6: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.65e-04 (1.68e-04)	Tok/s 38659 (38352)	Loss/tok 7.8102 (8.0394)	LR 1.000e-03
5: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.60e-04 (2.25e-04)	Tok/s 38870 (38573)	Loss/tok 7.8687 (8.0583)	LR 1.000e-03
3: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.63e-04 (1.66e-04)	Tok/s 39030 (38478)	Loss/tok 7.8396 (8.0351)	LR 1.000e-03
15: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.82e-04 (2.08e-04)	Tok/s 38817 (38414)	Loss/tok 7.8712 (8.0595)	LR 1.000e-03
14: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.82e-04 (2.08e-04)	Tok/s 38602 (38492)	Loss/tok 7.8445 (8.0359)	LR 1.000e-03
9: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.65e-04 (3.18e-03)	Tok/s 38889 (38366)	Loss/tok 7.9128 (8.0396)	LR 1.000e-03
0: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.64e-04 (8.06e-04)	Tok/s 38383 (38253)	Loss/tok 7.8288 (8.0508)	LR 1.000e-03
1: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 1.76e-04 (2.17e-04)	Tok/s 37895 (38175)	Loss/tok 7.8598 (8.0613)	LR 1.000e-03
13: TRAIN [1][10/32]	Time 0.304 (0.331)	Data 2.50e-04 (2.43e-04)	Tok/s 38404 (38271)	Loss/tok 7.8135 (8.0463)	LR 1.000e-03
0: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.72e-04 (4.83e-04)	Tok/s 39229 (39804)	Loss/tok 7.5720 (7.9109)	LR 2.500e-04
1: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 8.80e-03 (1.62e-03)	Tok/s 38488 (39687)	Loss/tok 7.5427 (7.9067)	LR 2.500e-04
12: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.58e-04 (2.76e-04)	Tok/s 38208 (39911)	Loss/tok 7.6120 (7.9148)	LR 2.500e-04
13: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 2.35e-04 (6.13e-04)	Tok/s 38832 (39773)	Loss/tok 7.5875 (7.9017)	LR 2.500e-04
3: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.59e-04 (1.59e-04)	Tok/s 38668 (39922)	Loss/tok 7.5780 (7.9044)	LR 2.500e-04
15: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.62e-04 (2.08e-04)	Tok/s 38119 (39862)	Loss/tok 7.5716 (7.9099)	LR 2.500e-04
2: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.19e-02 (7.52e-04)	Tok/s 38843 (39975)	Loss/tok 7.5605 (7.9101)	LR 2.500e-04
11: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.52e-04 (6.54e-04)	Tok/s 38526 (39894)	Loss/tok 7.5863 (7.9017)	LR 2.500e-04
14: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 2.57e-04 (8.24e-04)	Tok/s 38430 (39931)	Loss/tok 7.5992 (7.9023)	LR 2.500e-04
4: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.64e-04 (1.68e-04)	Tok/s 38779 (39860)	Loss/tok 7.5705 (7.8902)	LR 2.500e-04
5: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.66e-04 (1.92e-04)	Tok/s 38482 (39842)	Loss/tok 7.5473 (7.9026)	LR 2.500e-04
8: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.69e-04 (8.11e-04)	Tok/s 38831 (39915)	Loss/tok 7.6281 (7.9070)	LR 2.500e-04
9: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.61e-04 (1.88e-03)	Tok/s 38648 (39846)	Loss/tok 7.5604 (7.9027)	LR 2.500e-04
7: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.57e-04 (1.59e-04)	Tok/s 38911 (39919)	Loss/tok 7.5913 (7.9013)	LR 2.500e-04
6: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.67e-04 (1.27e-03)	Tok/s 38506 (39879)	Loss/tok 7.5656 (7.9055)	LR 2.500e-04
10: TRAIN [1][20/32]	Time 0.304 (0.351)	Data 1.46e-04 (1.59e-04)	Tok/s 38363 (39858)	Loss/tok 7.5602 (7.9080)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0


Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0

Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8.0
12: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 6.15e-05 (2.32e-04)	Tok/s 46760 (39014)	Loss/tok 8.0345 (7.8461)	LR 1.250e-04
15: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 6.53e-05 (1.92e-04)	Tok/s 46525 (39010)	Loss/tok 7.9782 (7.8481)	LR 1.250e-04
11: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 4.94e-05 (1.79e-03)	Tok/s 46284 (39079)	Loss/tok 8.0554 (7.8381)	LR 1.250e-04
1: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 7.84e-05 (1.92e-03)	Tok/s 46257 (38878)	Loss/tok 7.9632 (7.8458)	LR 1.250e-04
13: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 6.79e-05 (4.58e-04)	Tok/s 46315 (38962)	Loss/tok 8.0287 (7.8400)	LR 1.250e-04
0: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 1.08e-04 (3.75e-04)	Tok/s 46590 (38945)	Loss/tok 8.0296 (7.8460)	LR 1.250e-04
10: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 5.89e-05 (3.85e-04)	Tok/s 46436 (39021)	Loss/tok 8.0150 (7.8422)	LR 1.250e-04
14: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 6.87e-05 (6.05e-04)	Tok/s 46791 (39047)	Loss/tok 8.0164 (7.8362)	LR 1.250e-04
9: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 5.22e-05 (1.30e-03)	Tok/s 46253 (38980)	Loss/tok 7.9833 (7.8367)	LR 1.250e-04
2: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 7.22e-05 (7.47e-04)	Tok/s 46748 (39084)	Loss/tok 7.9618 (7.8434)	LR 1.250e-04
3: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 1.09e-04 (1.55e-04)	Tok/s 46660 (39057)	Loss/tok 7.9594 (7.8411)	LR 1.250e-04
7: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 7.51e-05 (1.62e-04)	Tok/s 45906 (39007)	Loss/tok 8.0091 (7.8389)	LR 1.250e-04
4: TRAIN [1][30/32]	Time 0.352 (0.328)	Data 7.13e-05 (1.59e-04)	Tok/s 46752 (38955)	Loss/tok 7.9721 (7.8271)	LR 1.250e-04
8: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 8.18e-05 (5.90e-04)	Tok/s 46284 (38995)	Loss/tok 8.0288 (7.8462)	LR 1.250e-04
5: TRAIN [1][30/32]	Time 0.352 (0.328)	Data 7.94e-05 (1.76e-04)	Tok/s 46748 (38933)	Loss/tok 7.9876 (7.8432)	LR 1.250e-04
6: TRAIN [1][30/32]	Time 0.351 (0.328)	Data 8.34e-05 (8.90e-04)	Tok/s 46276 (39026)	Loss/tok 7.9697 (7.8386)	LR 1.250e-04
6: Running validation on dev set
5: Running validation on dev set
8: Running validation on dev set
12: Running validation on dev set
15: Running validation on dev set
14: Running validation on dev set
3: Running validation on dev set
5: Executing preallocation
6: Executing preallocation
8: Executing preallocation
12: Executing preallocation
15: Executing preallocation
14: Executing preallocation
13: Running validation on dev set
1: Running validation on dev set
3: Executing preallocation
2: Running validation on dev set
0: Running validation on dev set
13: Executing preallocation
1: Executing preallocation
4: Running validation on dev set
11: Running validation on dev set
9: Running validation on dev set
2: Executing preallocation
7: Running validation on dev set
10: Running validation on dev set
0: Executing preallocation
4: Executing preallocation
11: Executing preallocation
9: Executing preallocation
7: Executing preallocation
10: Executing preallocation
14: VALIDATION [1][0/5]	Time 0.043 (0.000)	Data 2.09e-03 (0.00e+00)	Tok/s 123296 (0)	Loss/tok 8.5962 (8.5962)
11: VALIDATION [1][0/5]	Time 0.046 (0.000)	Data 2.09e-03 (0.00e+00)	Tok/s 122689 (0)	Loss/tok 8.6430 (8.6430)
3: VALIDATION [1][0/5]	Time 0.061 (0.000)	Data 2.14e-03 (0.00e+00)	Tok/s 122674 (0)	Loss/tok 8.6651 (8.6651)
6: VALIDATION [1][0/5]	Time 0.056 (0.000)	Data 2.15e-03 (0.00e+00)	Tok/s 117139 (0)	Loss/tok 8.6766 (8.6766)
5: VALIDATION [1][0/5]	Time 0.056 (0.000)	Data 2.23e-03 (0.00e+00)	Tok/s 121641 (0)	Loss/tok 8.6694 (8.6694)
10: VALIDATION [1][0/5]	Time 0.045 (0.000)	Data 2.15e-03 (0.00e+00)	Tok/s 129733 (0)	Loss/tok 8.6991 (8.6991)
12: VALIDATION [1][0/5]	Time 0.043 (0.000)	Data 2.12e-03 (0.00e+00)	Tok/s 130395 (0)	Loss/tok 8.5504 (8.5504)
8: VALIDATION [1][0/5]	Time 0.048 (0.000)	Data 2.03e-03 (0.00e+00)	Tok/s 127872 (0)	Loss/tok 8.7028 (8.7028)
13: VALIDATION [1][0/5]	Time 0.044 (0.000)	Data 2.13e-03 (0.00e+00)	Tok/s 122382 (0)	Loss/tok 8.6069 (8.6069)
15: VALIDATION [1][0/5]	Time 0.041 (0.000)	Data 2.09e-03 (0.00e+00)	Tok/s 125110 (0)	Loss/tok 8.5680 (8.5680)
7: VALIDATION [1][0/5]	Time 0.051 (0.000)	Data 2.16e-03 (0.00e+00)	Tok/s 124937 (0)	Loss/tok 8.7248 (8.7248)
4: VALIDATION [1][0/5]	Time 0.055 (0.000)	Data 2.18e-03 (0.00e+00)	Tok/s 127686 (0)	Loss/tok 8.7008 (8.7008)
2: VALIDATION [1][0/5]	Time 0.066 (0.000)	Data 2.06e-03 (0.00e+00)	Tok/s 119377 (0)	Loss/tok 8.7011 (8.7011)
1: VALIDATION [1][0/5]	Time 0.076 (0.000)	Data 2.18e-03 (0.00e+00)	Tok/s 113465 (0)	Loss/tok 8.6766 (8.6766)
9: VALIDATION [1][0/5]	Time 0.048 (0.000)	Data 2.10e-03 (0.00e+00)	Tok/s 123534 (0)	Loss/tok 8.6558 (8.6558)
0: VALIDATION [1][0/5]	Time 0.119 (0.000)	Data 2.20e-03 (0.00e+00)	Tok/s 88325 (0)	Loss/tok 8.6346 (8.6346)
0: Saving model to gnmt/model_best.pth
8: Running evaluation on test set
11: Running evaluation on test set
15: Running evaluation on test set
10: Running evaluation on test set
14: Running evaluation on test set
2: Running evaluation on test set
3: Running evaluation on test set
4: Running evaluation on test set
5: Running evaluation on test set
6: Running evaluation on test set
12: Running evaluation on test set
13: Running evaluation on test set
9: Running evaluation on test set
7: Running evaluation on test set
1: Running evaluation on test set
0: Running evaluation on test set
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
7: Finished evaluation on test set
0: Finished evaluation on test set
12: Finished evaluation on test set
9: Finished evaluation on test set
1: Finished evaluation on test set
8: Finished evaluation on test set
2: Finished evaluation on test set
6: Finished evaluation on test set
15: Finished evaluation on test set
5: Finished evaluation on test set
10: Finished evaluation on test set
4: Finished evaluation on test set
14: Finished evaluation on test set
13: Finished evaluation on test set
3: Finished evaluation on test set
11: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 7.8375	Validation Loss: 8.4023	Test BLEU: 0.08
0: Performance: Epoch: 1	Training: 614038 Tok/s	Validation: 1854549 Tok/s
2: Finished epoch 1
0: Finished epoch 1
1: Finished epoch 1
8: Finished epoch 1
6: Finished epoch 1
9: Finished epoch 1
7: Finished epoch 1
5: Finished epoch 1
10: Finished epoch 1
4: Finished epoch 1
12: Finished epoch 1
15: Finished epoch 1
13: Finished epoch 1
14: Finished epoch 1
3: Finished epoch 1
11: Finished epoch 1
2: Total training time 122 s
8: Total training time 122 s
0: Total training time 122 s
7: Total training time 122 s
14: Total training time 122 s
13: Total training time 122 s
5: Total training time 122 s
15: Total training time 122 s
4: Total training time 122 s
1: Total training time 122 s
12: Total training time 122 s
9: Total training time 122 s
6: Total training time 122 s
10: Total training time 122 s
3: Total training time 122 s
11: Total training time 122 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|      16|                 260|                      0.08|                     610823.3|                         2.036|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
