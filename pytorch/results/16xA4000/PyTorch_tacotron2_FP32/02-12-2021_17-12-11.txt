train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
DLL 2021-12-02 17:12:13.665812 - PARAMETER output : ./ 
DLL 2021-12-02 17:12:13.665875 - PARAMETER dataset_path : /data/tacotron2/LJSpeech-1.1 
DLL 2021-12-02 17:12:13.665898 - PARAMETER model_name : Tacotron2 
DLL 2021-12-02 17:12:13.665918 - PARAMETER log_file : nvlog.json 
DLL 2021-12-02 17:12:13.665935 - PARAMETER anneal_steps : None 
DLL 2021-12-02 17:12:13.665952 - PARAMETER anneal_factor : 0.1 
DLL 2021-12-02 17:12:13.665970 - PARAMETER epochs : 2 
DLL 2021-12-02 17:12:13.665987 - PARAMETER epochs_per_checkpoint : 50 
DLL 2021-12-02 17:12:13.666003 - PARAMETER checkpoint_path :  
DLL 2021-12-02 17:12:13.666021 - PARAMETER resume_from_last : False 
DLL 2021-12-02 17:12:13.666038 - PARAMETER dynamic_loss_scaling : True 
DLL 2021-12-02 17:12:13.666055 - PARAMETER amp : False 
DLL 2021-12-02 17:12:13.666073 - PARAMETER cudnn_enabled : True 
DLL 2021-12-02 17:12:13.666092 - PARAMETER cudnn_benchmark : False 
DLL 2021-12-02 17:12:13.666109 - PARAMETER disable_uniform_initialize_bn_weight : False 
DLL 2021-12-02 17:12:13.666125 - PARAMETER use_saved_learning_rate : False 
DLL 2021-12-02 17:12:13.666141 - PARAMETER learning_rate : 0.0 
DLL 2021-12-02 17:12:13.666158 - PARAMETER weight_decay : 1e-06 
DLL 2021-12-02 17:12:13.666176 - PARAMETER grad_clip_thresh : 1.0 
DLL 2021-12-02 17:12:13.666192 - PARAMETER batch_size : 52 
DLL 2021-12-02 17:12:13.666208 - PARAMETER grad_clip : 5.0 
DLL 2021-12-02 17:12:13.666223 - PARAMETER load_mel_from_disk : False 
DLL 2021-12-02 17:12:13.666239 - PARAMETER training_files : filelists/ljs_audio_text_train_subset_1250_filelist.txt 
DLL 2021-12-02 17:12:13.666255 - PARAMETER validation_files : filelists/ljs_audio_text_val_filelist.txt 
DLL 2021-12-02 17:12:13.666270 - PARAMETER text_cleaners : ['english_cleaners'] 
DLL 2021-12-02 17:12:13.666291 - PARAMETER max_wav_value : 32768.0 
DLL 2021-12-02 17:12:13.666307 - PARAMETER sampling_rate : 22050 
DLL 2021-12-02 17:12:13.666323 - PARAMETER filter_length : 1024 
DLL 2021-12-02 17:12:13.666338 - PARAMETER hop_length : 256 
DLL 2021-12-02 17:12:13.666354 - PARAMETER win_length : 1024 
DLL 2021-12-02 17:12:13.666369 - PARAMETER mel_fmin : 0.0 
DLL 2021-12-02 17:12:13.666384 - PARAMETER mel_fmax : 8000.0 
DLL 2021-12-02 17:12:13.666400 - PARAMETER rank : 0 
DLL 2021-12-02 17:12:13.666416 - PARAMETER world_size : 16 
DLL 2021-12-02 17:12:13.666431 - PARAMETER dist_url : tcp://localhost:23456 
DLL 2021-12-02 17:12:13.666446 - PARAMETER group_name : group_name 
DLL 2021-12-02 17:12:13.666461 - PARAMETER dist_backend : nccl 
DLL 2021-12-02 17:12:13.666476 - PARAMETER bench_class :  
DLL 2021-12-02 17:12:13.666494 - PARAMETER model_name : Tacotron2_PyT 
Initializing Distributed
Done initializing distributed
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
train.py:402: SyntaxWarning: "is not" with a literal. Did you mean "!="?
  if args.checkpoint_path is not "":
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
DLL 2021-12-02 17:12:50.524933 - (0, 0) glob_iter/iters_per_epoch : 0/1 
DLL 2021-12-02 17:13:56.478063 - (0, 0) train_loss : 47.023048400878906 
DLL 2021-12-02 17:14:00.961460 - (0, 0) train_items_per_sec : 6676.092465271924 
DLL 2021-12-02 17:14:00.961551 - (0, 0) train_iter_time : 70.43656187300076 
DLL 2021-12-02 17:14:00.994521 - (0,) train_items_per_sec : 6676.092465271924 
DLL 2021-12-02 17:14:00.994621 - (0,) train_loss : 47.023048400878906 
DLL 2021-12-02 17:14:00.994816 - (0,) train_epoch_time : 72.80042469900218 
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.8/site-packages/torch/distributed/distributed_c10d.py:144: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
DLL 2021-12-02 17:14:01.973314 - (0, 1, 0) val_items_per_sec : 97036.67029896197 
DLL 2021-12-02 17:14:02.029609 - (0,) val_loss : 49.59562683105469 
DLL 2021-12-02 17:14:02.029768 - (0,) val_items_per_sec : 97036.67029896197 
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0.pt
|||| Updating symlink ./checkpoint_Tacotron2_last.pt to point to checkpoint_Tacotron2_0.pt
DLL 2021-12-02 17:14:03.973852 - (1, 0) glob_iter/iters_per_epoch : 1/1 
DLL 2021-12-02 17:14:08.428465 - (1, 0) train_loss : 47.04438781738281 
DLL 2021-12-02 17:14:10.426473 - (1, 0) train_items_per_sec : 72592.01666726 
DLL 2021-12-02 17:14:10.426564 - (1, 0) train_iter_time : 6.452665479002462 
DLL 2021-12-02 17:14:10.483675 - (1,) train_items_per_sec : 72592.01666726 
DLL 2021-12-02 17:14:10.483819 - (1,) train_loss : 47.04438781738281 
DLL 2021-12-02 17:14:10.483869 - (1,) train_epoch_time : 7.6119211400000495 
DLL 2021-12-02 17:14:11.527202 - (1, 2, 0) val_items_per_sec : 92294.47462180522 
DLL 2021-12-02 17:14:11.592643 - (1,) val_loss : 49.60627746582031 
DLL 2021-12-02 17:14:11.592796 - (1,) val_items_per_sec : 92294.47462180522 
DLL 2021-12-02 17:14:11.594262 - () run_time : 103.34265097899333 
DLL 2021-12-02 17:14:11.594338 - () val_loss : 49.60627746582031 
DLL 2021-12-02 17:14:11.594383 - () train_items_per_sec : 72592.01666726 
DONE!
