3: thread affinity: {11, 19, 3, 27}
4: thread affinity: {20, 28, 4, 12}
1: thread affinity: {1, 25, 9, 17}
0: thread affinity: {0, 8, 16, 24}
10: thread affinity: {34, 58, 42, 50}
7: thread affinity: {31, 15, 23, 7}
2: thread affinity: {18, 26, 2, 10}
11: thread affinity: {43, 35, 51, 59}
15: thread affinity: {55, 63, 47, 39}
5: thread affinity: {29, 13, 21, 5}
8: thread affinity: {32, 56, 40, 48}
12: thread affinity: {36, 60, 44, 52}
9: thread affinity: {33, 49, 57, 41}
13: thread affinity: {53, 61, 45, 37}
14: thread affinity: {46, 54, 38, 62}
6: thread affinity: {22, 30, 6, 14}
9: Collecting environment information...
0: Collecting environment information...
4: Collecting environment information...
3: Collecting environment information...
8: Collecting environment information...
2: Collecting environment information...
6: Collecting environment information...
15: Collecting environment information...
10: Collecting environment information...
14: Collecting environment information...
11: Collecting environment information...
1: Collecting environment information...
13: Collecting environment information...
7: Collecting environment information...
5: Collecting environment information...
12: Collecting environment information...
9: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
9: Saving results to: gnmt
9: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=9, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=9, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
9: Using master seed from command line: 2
8: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
8: Saving results to: gnmt
8: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=8, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=8, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
8: Using master seed from command line: 2
10: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
10: Saving results to: gnmt
10: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=10, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=10, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
10: Using master seed from command line: 2
14: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
14: Saving results to: gnmt
14: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=14, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=14, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
14: Using master seed from command line: 2
11: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
11: Saving results to: gnmt
11: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=11, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=11, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
11: Using master seed from command line: 2
15: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
15: Saving results to: gnmt
15: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=15, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=15, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
15: Using master seed from command line: 2
13: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
13: Saving results to: gnmt
13: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=13, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=13, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
13: Using master seed from command line: 2
12: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
12: Saving results to: gnmt
12: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=12, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=12, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
12: Using master seed from command line: 2
6: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
6: Saving results to: gnmt
6: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=6, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=6, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
6: Using master seed from command line: 2
1: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
7: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
7: Saving results to: gnmt
7: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=7, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=7, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
7: Using master seed from command line: 2
5: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
5: Saving results to: gnmt
5: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=5, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=5, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
5: Using master seed from command line: 2
2: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
2: Saving results to: gnmt
2: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=2, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
2: Using master seed from command line: 2
0: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
3: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
3: Saving results to: gnmt
3: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=3, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
3: Using master seed from command line: 2
4: PyTorch version: 1.9.0a0+df837d0
Is debug build: False
CUDA used to build PyTorch: 11.2
ROCM used to build PyTorch: N/A

OS: Ubuntu 20.04.1 LTS (x86_64)
GCC version: (Ubuntu 9.3.0-17ubuntu1~20.04) 9.3.0
Clang version: Could not collect
CMake version: version 3.19.6

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: 11.2.142
GPU models and configuration: 
GPU 0: NVIDIA RTX A4000
GPU 1: NVIDIA RTX A4000
GPU 2: NVIDIA RTX A4000
GPU 3: NVIDIA RTX A4000
GPU 4: NVIDIA RTX A4000
GPU 5: NVIDIA RTX A4000
GPU 6: NVIDIA RTX A4000
GPU 7: NVIDIA RTX A4000
GPU 8: NVIDIA RTX A4000
GPU 9: NVIDIA RTX A4000
GPU 10: NVIDIA RTX A4000
GPU 11: NVIDIA RTX A4000
GPU 12: NVIDIA RTX A4000
GPU 13: NVIDIA RTX A4000
GPU 14: NVIDIA RTX A4000
GPU 15: NVIDIA RTX A4000

Nvidia driver version: 470.63.01
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.1.1
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.1.1
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.2
[pip3] nvidia-dlprof-pytorch-nvtx==1.0.0
[pip3] pytorch-quantization==2.1.0
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.9.0a0+df837d0
[pip3] torchtext==0.9.0a0
[pip3] torchvision==0.9.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.4                      243  
[conda] mkl-include               2019.4                      243  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.2           py38h6163131_0  
[conda] numpy-base                1.19.2           py38h75fe3a5_0  
[conda] nvidia-dlprof-pytorch-nvtx 1.0.0                    pypi_0    pypi
[conda] pytorch-quantization      2.1.0                    pypi_0    pypi
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.9.0a0+df837d0          pypi_0    pypi
[conda] torchtext                 0.9.0a0                  pypi_0    pypi
[conda] torchvision               0.9.0a0                  pypi_0    pypi
4: Saving results to: gnmt
4: Run arguments: Namespace(affinity='socket_unique_interleaved', batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=4, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, profile=False, rank=4, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
4: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
2: Worker 2 is using worker seed: 3588440356
3: Worker 3 is using worker seed: 1323436024
4: Worker 4 is using worker seed: 2602510382
7: Worker 7 is using worker seed: 117874757
10: Worker 10 is using worker seed: 2407373688
9: Worker 9 is using worker seed: 2258090960
6: Worker 6 is using worker seed: 4077622522
8: Worker 8 is using worker seed: 1632151663
5: Worker 5 is using worker seed: 2606193617
11: Worker 11 is using worker seed: 1014142328
15: Worker 15 is using worker seed: 3837860530
12: Worker 12 is using worker seed: 102469680
14: Worker 14 is using worker seed: 2191394736
13: Worker 13 is using worker seed: 1396478261
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
10: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
8: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
10: Size of vocabulary: 31794
8: Size of vocabulary: 31794
14: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
10: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
8: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
14: Size of vocabulary: 31794
3: Size of vocabulary: 31794
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
14: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
9: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
4: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Size of vocabulary: 31794
13: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
12: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
11: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
15: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
9: Size of vocabulary: 31794
4: Size of vocabulary: 31794
7: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
6: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
13: Size of vocabulary: 31794
5: Size of vocabulary: 31794
12: Size of vocabulary: 31794
11: Size of vocabulary: 31794
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
15: Size of vocabulary: 31794
7: Size of vocabulary: 31794
6: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
9: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
13: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
12: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
11: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
15: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
10: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
8: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
14: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
9: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
13: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
12: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
11: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
15: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
10: Filtering data, min len: 0, max len: 50
8: Filtering data, min len: 0, max len: 50
14: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
4: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
9: Filtering data, min len: 0, max len: 50
5: Filtering data, min len: 0, max len: 50
13: Filtering data, min len: 0, max len: 50
12: Filtering data, min len: 0, max len: 50
11: Filtering data, min len: 0, max len: 50
7: Filtering data, min len: 0, max len: 50
6: Filtering data, min len: 0, max len: 50
15: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
8: Pairs before: 160078, after: 148120
10: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
14: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
8: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
10: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Pairs before: 160078, after: 148120
4: Pairs before: 160078, after: 148120
9: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
12: Pairs before: 160078, after: 148120
5: Pairs before: 160078, after: 148120
11: Pairs before: 160078, after: 148120
13: Pairs before: 160078, after: 148120
6: Pairs before: 160078, after: 148120
7: Pairs before: 160078, after: 148120
15: Pairs before: 160078, after: 148120
14: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
9: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
12: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
11: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
13: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
15: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
8: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
10: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
14: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
9: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
12: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
11: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
13: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
15: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
8: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
10: Filtering data, min len: 0, max len: 125
8: Pairs before: 5100, after: 5100
10: Pairs before: 5100, after: 5100
14: Filtering data, min len: 0, max len: 125
3: Filtering data, min len: 0, max len: 125
14: Pairs before: 5100, after: 5100
3: Pairs before: 5100, after: 5100
2: Filtering data, min len: 0, max len: 125
4: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
9: Filtering data, min len: 0, max len: 125
12: Filtering data, min len: 0, max len: 125
2: Pairs before: 5100, after: 5100
5: Filtering data, min len: 0, max len: 125
11: Filtering data, min len: 0, max len: 125
4: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
9: Pairs before: 5100, after: 5100
13: Filtering data, min len: 0, max len: 125
6: Filtering data, min len: 0, max len: 125
7: Filtering data, min len: 0, max len: 125
12: Pairs before: 5100, after: 5100
15: Filtering data, min len: 0, max len: 125
5: Pairs before: 5100, after: 5100
11: Pairs before: 5100, after: 5100
13: Pairs before: 5100, after: 5100
7: Pairs before: 5100, after: 5100
6: Pairs before: 5100, after: 5100
15: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
8: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
10: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
14: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Pairs before: 3003, after: 3003
8: Filtering data, min len: 0, max len: 150
10: Filtering data, min len: 0, max len: 150
8: Pairs before: 3003, after: 3003
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
10: Pairs before: 3003, after: 3003
4: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
9: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
12: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
11: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
13: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
15: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
14: Filtering data, min len: 0, max len: 150
14: Pairs before: 3003, after: 3003
2: Filtering data, min len: 0, max len: 150
4: Filtering data, min len: 0, max len: 150
2: Pairs before: 3003, after: 3003
9: Filtering data, min len: 0, max len: 150
4: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
3: Filtering data, min len: 0, max len: 150
9: Pairs before: 3003, after: 3003
1: Pairs before: 3003, after: 3003
12: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
12: Pairs before: 3003, after: 3003
5: Filtering data, min len: 0, max len: 150
5: Pairs before: 3003, after: 3003
11: Filtering data, min len: 0, max len: 150
11: Pairs before: 3003, after: 3003
13: Filtering data, min len: 0, max len: 150
13: Pairs before: 3003, after: 3003
7: Filtering data, min len: 0, max len: 150
6: Filtering data, min len: 0, max len: 150
15: Filtering data, min len: 0, max len: 150
7: Pairs before: 3003, after: 3003
6: Pairs before: 3003, after: 3003
15: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 73
0: Scheduler decay interval: 9
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
0: Starting epoch 0
0: Executing preallocation
10: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
10: Building LabelSmoothingLoss (smoothing: 0.1)
10: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
10: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
10: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
10: Saving state of the tokenizer
10: Initializing fp32 optimizer
10: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
10: Scheduler warmup steps: 200
10: Scheduler remain steps: 73
10: Scheduler decay interval: 9
10: Scheduler decay factor: 0.5
10: Scheduler max decay steps: 4
10: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
10: Starting epoch 0
10: Executing preallocation
5: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
5: Building LabelSmoothingLoss (smoothing: 0.1)
5: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
5: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
5: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
12: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
12: Building LabelSmoothingLoss (smoothing: 0.1)
12: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
12: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
12: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
5: Saving state of the tokenizer
5: Initializing fp32 optimizer
5: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
5: Scheduler warmup steps: 200
5: Scheduler remain steps: 73
5: Scheduler decay interval: 9
5: Scheduler decay factor: 0.5
5: Scheduler max decay steps: 4
5: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
12: Saving state of the tokenizer
12: Initializing fp32 optimizer
12: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
12: Scheduler warmup steps: 200
12: Scheduler remain steps: 73
12: Scheduler decay interval: 9
12: Scheduler decay factor: 0.5
12: Scheduler max decay steps: 4
12: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
5: Starting epoch 0
5: Executing preallocation
12: Starting epoch 0
12: Executing preallocation
11: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
11: Building LabelSmoothingLoss (smoothing: 0.1)
11: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
11: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
11: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
14: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
14: Building LabelSmoothingLoss (smoothing: 0.1)
14: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
14: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
14: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
11: Saving state of the tokenizer
11: Initializing fp32 optimizer
11: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
11: Scheduler warmup steps: 200
11: Scheduler remain steps: 73
11: Scheduler decay interval: 9
11: Scheduler decay factor: 0.5
11: Scheduler max decay steps: 4
11: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
14: Saving state of the tokenizer
14: Initializing fp32 optimizer
14: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
14: Scheduler warmup steps: 200
14: Scheduler remain steps: 73
14: Scheduler decay interval: 9
14: Scheduler decay factor: 0.5
14: Scheduler max decay steps: 4
14: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
11: Starting epoch 0
11: Executing preallocation
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159593523
14: Starting epoch 0
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
14: Executing preallocation
4: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
4: Building LabelSmoothingLoss (smoothing: 0.1)
3: Saving state of the tokenizer
4: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
4: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Initializing fp32 optimizer
4: Number of parameters: 159593523
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 73
3: Scheduler decay interval: 9
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
13: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
13: Building LabelSmoothingLoss (smoothing: 0.1)
13: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
13: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
13: Number of parameters: 159593523
3: Starting epoch 0
3: Executing preallocation
4: Saving state of the tokenizer
4: Initializing fp32 optimizer
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
4: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
4: Scheduler warmup steps: 200
4: Scheduler remain steps: 73
4: Scheduler decay interval: 9
4: Scheduler decay factor: 0.5
4: Scheduler max decay steps: 4
4: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
4: Starting epoch 0
4: Executing preallocation
13: Saving state of the tokenizer
13: Initializing fp32 optimizer
13: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
13: Scheduler warmup steps: 200
13: Scheduler remain steps: 73
13: Scheduler decay interval: 9
13: Scheduler decay factor: 0.5
13: Scheduler max decay steps: 4
13: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
13: Starting epoch 0
13: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
9: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
9: Building LabelSmoothingLoss (smoothing: 0.1)
9: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
9: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
9: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
15: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
15: Building LabelSmoothingLoss (smoothing: 0.1)
15: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
15: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
15: Number of parameters: 159593523
7: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: Building LabelSmoothingLoss (smoothing: 0.1)
7: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
7: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
7: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 73
1: Scheduler decay interval: 9
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
9: Saving state of the tokenizer
9: Initializing fp32 optimizer
9: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
9: Scheduler warmup steps: 200
9: Scheduler remain steps: 73
9: Scheduler decay interval: 9
9: Scheduler decay factor: 0.5
9: Scheduler max decay steps: 4
9: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
15: Saving state of the tokenizer
15: Initializing fp32 optimizer
15: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
15: Scheduler warmup steps: 200
15: Scheduler remain steps: 73
15: Scheduler decay interval: 9
15: Scheduler decay factor: 0.5
15: Scheduler max decay steps: 4
15: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
7: Saving state of the tokenizer
7: Initializing fp32 optimizer
7: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
7: Scheduler warmup steps: 200
7: Scheduler remain steps: 73
7: Scheduler decay interval: 9
7: Scheduler decay factor: 0.5
7: Scheduler max decay steps: 4
7: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
1: Starting epoch 0
1: Executing preallocation
9: Starting epoch 0
9: Executing preallocation
15: Starting epoch 0
15: Executing preallocation
7: Starting epoch 0
7: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
2: Saving state of the tokenizer
2: Initializing fp32 optimizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 73
2: Scheduler decay interval: 9
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
2: Starting epoch 0
2: Executing preallocation
8: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
8: Building LabelSmoothingLoss (smoothing: 0.1)
8: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
8: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
8: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
8: Saving state of the tokenizer
8: Initializing fp32 optimizer
8: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
8: Scheduler warmup steps: 200
8: Scheduler remain steps: 73
8: Scheduler decay interval: 9
8: Scheduler decay factor: 0.5
8: Scheduler max decay steps: 4
8: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
6: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
6: Building LabelSmoothingLoss (smoothing: 0.1)
6: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
6: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
6: Number of parameters: 159593523
/opt/conda/lib/python3.8/site-packages/torch/tensor.py:559: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.
To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:335.)
  return torch.floor_divide(self, other)
8: Starting epoch 0
8: Executing preallocation
6: Saving state of the tokenizer
6: Initializing fp32 optimizer
6: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Scheduler warmup steps: 200
6: Scheduler remain steps: 73
6: Scheduler decay interval: 9
6: Scheduler decay factor: 0.5
6: Scheduler max decay steps: 4
6: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
6: Starting epoch 0
6: Executing preallocation
0: Sampler for epoch 0 uses seed 3422057796
15: Sampler for epoch 0 uses seed 3422057796
14: Sampler for epoch 0 uses seed 3422057796
5: Sampler for epoch 0 uses seed 3422057796
6: Sampler for epoch 0 uses seed 3422057796
1: Sampler for epoch 0 uses seed 3422057796
4: Sampler for epoch 0 uses seed 3422057796
2: Sampler for epoch 0 uses seed 3422057796
12: Sampler for epoch 0 uses seed 3422057796
8: Sampler for epoch 0 uses seed 3422057796
13: Sampler for epoch 0 uses seed 3422057796
11: Sampler for epoch 0 uses seed 3422057796
9: Sampler for epoch 0 uses seed 3422057796
3: Sampler for epoch 0 uses seed 3422057796
7: Sampler for epoch 0 uses seed 3422057796
10: Sampler for epoch 0 uses seed 3422057796
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/opt/conda/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:129: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
0: TRAIN [0][0/55]	Time 1.410 (0.000)	Data 5.78e-01 (0.00e+00)	Tok/s 5097 (0)	Loss/tok 10.6787 (10.6787)	LR 2.130e-05
1: TRAIN [0][0/55]	Time 1.363 (0.000)	Data 6.73e-01 (0.00e+00)	Tok/s 5257 (0)	Loss/tok 10.6806 (10.6806)	LR 2.130e-05
13: TRAIN [0][0/55]	Time 1.297 (0.000)	Data 6.68e-01 (0.00e+00)	Tok/s 5517 (0)	Loss/tok 10.6712 (10.6712)	LR 2.130e-05
15: TRAIN [0][0/55]	Time 1.256 (0.000)	Data 5.07e-01 (0.00e+00)	Tok/s 5767 (0)	Loss/tok 10.6794 (10.6794)	LR 2.130e-05
14: TRAIN [0][0/55]	Time 1.257 (0.000)	Data 5.94e-01 (0.00e+00)	Tok/s 5734 (0)	Loss/tok 10.6819 (10.6819)	LR 2.130e-05
2: TRAIN [0][0/55]	Time 1.359 (0.000)	Data 6.82e-01 (0.00e+00)	Tok/s 5323 (0)	Loss/tok 10.6693 (10.6693)	LR 2.130e-05
12: TRAIN [0][0/55]	Time 1.298 (0.000)	Data 6.19e-01 (0.00e+00)	Tok/s 5572 (0)	Loss/tok 10.6611 (10.6611)	LR 2.130e-05
3: TRAIN [0][0/55]	Time 1.346 (0.000)	Data 7.61e-01 (0.00e+00)	Tok/s 5355 (0)	Loss/tok 10.6609 (10.6609)	LR 2.130e-05
11: TRAIN [0][0/55]	Time 1.298 (0.000)	Data 6.84e-01 (0.00e+00)	Tok/s 5568 (0)	Loss/tok 10.6670 (10.6670)	LR 2.130e-05
4: TRAIN [0][0/55]	Time 1.347 (0.000)	Data 6.75e-01 (0.00e+00)	Tok/s 5362 (0)	Loss/tok 10.6892 (10.6892)	LR 2.130e-05
5: TRAIN [0][0/55]	Time 1.346 (0.000)	Data 6.15e-01 (0.00e+00)	Tok/s 5370 (0)	Loss/tok 10.6860 (10.6860)	LR 2.130e-05
9: TRAIN [0][0/55]	Time 1.298 (0.000)	Data 7.33e-01 (0.00e+00)	Tok/s 5697 (0)	Loss/tok 10.6789 (10.6789)	LR 2.130e-05
8: TRAIN [0][0/55]	Time 1.298 (0.000)	Data 6.28e-01 (0.00e+00)	Tok/s 5442 (0)	Loss/tok 10.6769 (10.6769)	LR 2.130e-05
10: TRAIN [0][0/55]	Time 1.299 (0.000)	Data 7.61e-01 (0.00e+00)	Tok/s 5560 (0)	Loss/tok 10.6761 (10.6761)	LR 2.130e-05
6: TRAIN [0][0/55]	Time 1.298 (0.000)	Data 5.71e-01 (0.00e+00)	Tok/s 5570 (0)	Loss/tok 10.6931 (10.6931)	LR 2.130e-05
7: TRAIN [0][0/55]	Time 1.298 (0.000)	Data 7.53e-01 (0.00e+00)	Tok/s 5484 (0)	Loss/tok 10.6760 (10.6760)	LR 2.130e-05
3: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.40e-04 (1.32e-04)	Tok/s 14679 (12522)	Loss/tok 9.5942 (10.0381)	LR 4.003e-05
4: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.39e-04 (1.44e-04)	Tok/s 14773 (12488)	Loss/tok 9.5620 (10.0277)	LR 4.003e-05
12: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.45e-04 (1.83e-04)	Tok/s 14787 (12560)	Loss/tok 9.5814 (10.0329)	LR 4.003e-05
1: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.39e-04 (9.62e-04)	Tok/s 14871 (12502)	Loss/tok 9.5940 (10.0312)	LR 4.003e-05
0: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.39e-04 (1.05e-03)	Tok/s 14587 (12488)	Loss/tok 9.5687 (10.0295)	LR 4.003e-05
13: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.53e-04 (1.48e-04)	Tok/s 14792 (12542)	Loss/tok 9.5496 (10.0264)	LR 4.003e-05
11: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.45e-04 (1.46e-04)	Tok/s 14738 (12566)	Loss/tok 9.5518 (10.0259)	LR 4.003e-05
15: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 9.91e-03 (2.15e-03)	Tok/s 14473 (12529)	Loss/tok 9.5536 (10.0266)	LR 4.003e-05
5: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.25e-04 (1.16e-03)	Tok/s 14684 (12528)	Loss/tok 9.5584 (10.0230)	LR 4.003e-05
14: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.39e-04 (1.42e-04)	Tok/s 14727 (12593)	Loss/tok 9.5386 (10.0316)	LR 4.003e-05
9: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.47e-04 (2.05e-04)	Tok/s 14662 (12470)	Loss/tok 9.5570 (10.0287)	LR 4.003e-05
10: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.37e-04 (1.39e-04)	Tok/s 14607 (12461)	Loss/tok 9.5395 (10.0263)	LR 4.003e-05
8: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.48e-04 (2.73e-04)	Tok/s 14819 (12492)	Loss/tok 9.5546 (10.0284)	LR 4.003e-05
2: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 1.51e-04 (1.77e-04)	Tok/s 14619 (12585)	Loss/tok 9.5303 (10.0201)	LR 4.003e-05
6: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 2.48e-04 (4.69e-03)	Tok/s 14727 (12577)	Loss/tok 9.5922 (10.0304)	LR 4.003e-05
7: TRAIN [0][10/55]	Time 0.684 (0.581)	Data 2.35e-04 (2.27e-04)	Tok/s 14531 (12501)	Loss/tok 9.5750 (10.0302)	LR 4.003e-05
13: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.44e-04 (5.25e-04)	Tok/s 12207 (12591)	Loss/tok 8.9425 (9.6450)	LR 7.523e-05
0: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.40e-04 (5.94e-04)	Tok/s 12638 (12592)	Loss/tok 8.9547 (9.6483)	LR 7.523e-05
14: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.20e-04 (1.38e-04)	Tok/s 12397 (12661)	Loss/tok 9.0030 (9.6556)	LR 7.523e-05
2: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.46e-04 (1.62e-04)	Tok/s 12480 (12659)	Loss/tok 8.8813 (9.6366)	LR 7.523e-05
1: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.42e-04 (5.54e-04)	Tok/s 12472 (12657)	Loss/tok 8.9901 (9.6481)	LR 7.523e-05
15: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.36e-04 (1.52e-03)	Tok/s 12208 (12628)	Loss/tok 8.9309 (9.6414)	LR 7.523e-05
12: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.48e-04 (1.63e-04)	Tok/s 12448 (12616)	Loss/tok 8.9100 (9.6498)	LR 7.523e-05
3: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.43e-04 (1.38e-04)	Tok/s 12387 (12597)	Loss/tok 8.9553 (9.6496)	LR 7.523e-05
4: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.40e-04 (1.41e-04)	Tok/s 12340 (12595)	Loss/tok 8.9825 (9.6485)	LR 7.523e-05
11: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.50e-04 (3.85e-04)	Tok/s 12362 (12602)	Loss/tok 8.9086 (9.6342)	LR 7.523e-05
10: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.35e-04 (1.39e-04)	Tok/s 12565 (12609)	Loss/tok 8.9764 (9.6507)	LR 7.523e-05
5: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.23e-04 (6.47e-04)	Tok/s 12422 (12613)	Loss/tok 8.9540 (9.6505)	LR 7.523e-05
9: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.49e-04 (1.76e-04)	Tok/s 12279 (12575)	Loss/tok 8.9598 (9.6521)	LR 7.523e-05
6: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 2.15e-04 (4.40e-03)	Tok/s 12207 (12609)	Loss/tok 8.9393 (9.6508)	LR 7.523e-05
8: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 1.42e-04 (2.03e-04)	Tok/s 12312 (12582)	Loss/tok 8.9234 (9.6367)	LR 7.523e-05
7: TRAIN [0][20/55]	Time 0.581 (0.585)	Data 2.48e-04 (2.36e-04)	Tok/s 12476 (12583)	Loss/tok 8.9647 (9.6500)	LR 7.523e-05
3: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.51e-04 (1.40e-04)	Tok/s 10100 (12725)	Loss/tok 8.1497 (9.3394)	LR 1.414e-04
12: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.48e-04 (1.57e-04)	Tok/s 10242 (12726)	Loss/tok 8.3437 (9.3431)	LR 1.414e-04
13: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.41e-04 (3.98e-04)	Tok/s 9905 (12678)	Loss/tok 8.3336 (9.3441)	LR 1.414e-04
1: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.42e-04 (4.17e-04)	Tok/s 10267 (12726)	Loss/tok 8.3026 (9.3522)	LR 1.414e-04
11: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.40e-04 (3.04e-04)	Tok/s 10164 (12687)	Loss/tok 8.2830 (9.3360)	LR 1.414e-04
4: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.57e-04 (1.45e-04)	Tok/s 9794 (12684)	Loss/tok 8.1952 (9.3522)	LR 1.414e-04
0: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.42e-04 (4.42e-04)	Tok/s 10279 (12681)	Loss/tok 8.2393 (9.3444)	LR 1.414e-04
9: TRAIN [0][30/55]	Time 0.428 (0.596)	Data 5.61e-03 (3.47e-04)	Tok/s 10087 (12670)	Loss/tok 8.2554 (9.3486)	LR 1.414e-04
15: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.45e-04 (2.26e-03)	Tok/s 9762 (12701)	Loss/tok 8.2470 (9.3416)	LR 1.414e-04
14: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.53e-04 (1.37e-04)	Tok/s 10136 (12743)	Loss/tok 8.2827 (9.3491)	LR 1.414e-04
5: TRAIN [0][30/55]	Time 0.428 (0.596)	Data 1.35e-04 (4.79e-04)	Tok/s 10310 (12713)	Loss/tok 8.2855 (9.3449)	LR 1.414e-04
6: TRAIN [0][30/55]	Time 0.428 (0.596)	Data 1.43e-04 (3.32e-03)	Tok/s 9998 (12691)	Loss/tok 8.2588 (9.3429)	LR 1.414e-04
2: TRAIN [0][30/55]	Time 0.427 (0.596)	Data 1.42e-04 (1.58e-04)	Tok/s 10383 (12741)	Loss/tok 8.2963 (9.3381)	LR 1.414e-04
7: TRAIN [0][30/55]	Time 0.428 (0.596)	Data 2.31e-04 (2.28e-04)	Tok/s 10259 (12706)	Loss/tok 8.2694 (9.3492)	LR 1.414e-04
8: TRAIN [0][30/55]	Time 0.428 (0.596)	Data 1.44e-04 (1.81e-04)	Tok/s 10080 (12688)	Loss/tok 8.2377 (9.3353)	LR 1.414e-04
10: TRAIN [0][30/55]	Time 0.428 (0.596)	Data 2.25e-04 (1.42e-04)	Tok/s 10022 (12683)	Loss/tok 8.2569 (9.3522)	LR 1.414e-04
1: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.47e-04 (3.48e-04)	Tok/s 15427 (13010)	Loss/tok 8.2589 (9.0638)	LR 2.657e-04
2: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.44e-04 (1.61e-04)	Tok/s 15159 (13010)	Loss/tok 8.2354 (9.0558)	LR 2.657e-04
0: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.68e-04 (3.68e-04)	Tok/s 15190 (12959)	Loss/tok 8.2178 (9.0530)	LR 2.657e-04
15: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.55e-04 (2.21e-03)	Tok/s 15389 (12982)	Loss/tok 8.2348 (9.0584)	LR 2.657e-04
12: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.25e-04 (1.57e-04)	Tok/s 15299 (13007)	Loss/tok 8.2545 (9.0539)	LR 2.657e-04
4: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.36e-04 (1.45e-04)	Tok/s 15397 (12981)	Loss/tok 8.2465 (9.0570)	LR 2.657e-04
11: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.43e-04 (2.63e-04)	Tok/s 15306 (12965)	Loss/tok 8.3201 (9.0497)	LR 2.657e-04
3: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 2.36e-04 (1.44e-04)	Tok/s 15323 (13003)	Loss/tok 8.2626 (9.0578)	LR 2.657e-04
13: TRAIN [0][40/55]	Time 0.854 (0.617)	Data 1.52e-04 (3.39e-04)	Tok/s 15317 (12981)	Loss/tok 8.2060 (9.0529)	LR 2.657e-04
14: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.44e-04 (4.42e-04)	Tok/s 15315 (13025)	Loss/tok 8.2600 (9.0588)	LR 2.657e-04
10: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 2.36e-04 (1.52e-04)	Tok/s 15420 (12970)	Loss/tok 8.2210 (9.0654)	LR 2.657e-04
5: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.53e-04 (3.95e-04)	Tok/s 15314 (12979)	Loss/tok 8.2648 (9.0542)	LR 2.657e-04
9: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.50e-04 (2.97e-04)	Tok/s 15143 (12953)	Loss/tok 8.2337 (9.0566)	LR 2.657e-04
8: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.49e-04 (1.72e-04)	Tok/s 15289 (12979)	Loss/tok 8.2950 (9.0489)	LR 2.657e-04
7: TRAIN [0][40/55]	Time 0.853 (0.617)	Data 1.48e-04 (2.06e-04)	Tok/s 15475 (12985)	Loss/tok 8.3106 (9.0621)	LR 2.657e-04
6: TRAIN [0][40/55]	Time 0.854 (0.617)	Data 1.40e-04 (2.52e-03)	Tok/s 15238 (12977)	Loss/tok 8.2820 (9.0578)	LR 2.657e-04
4: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 7.80e-03 (2.98e-04)	Tok/s 14494 (12656)	Loss/tok 7.9272 (8.8864)	LR 4.992e-04
5: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 1.45e-04 (3.45e-04)	Tok/s 14417 (12666)	Loss/tok 7.9559 (8.8850)	LR 4.992e-04
6: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 1.31e-04 (2.05e-03)	Tok/s 14596 (12666)	Loss/tok 7.9737 (8.8870)	LR 4.992e-04
8: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 1.39e-04 (1.66e-04)	Tok/s 14574 (12662)	Loss/tok 7.9973 (8.8788)	LR 4.992e-04
11: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 1.28e-04 (3.41e-04)	Tok/s 14341 (12662)	Loss/tok 7.9522 (8.8807)	LR 4.992e-04
9: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 1.47e-04 (2.67e-04)	Tok/s 14576 (12618)	Loss/tok 7.9943 (8.8896)	LR 4.992e-04
10: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 2.19e-04 (1.58e-04)	Tok/s 14656 (12676)	Loss/tok 7.9907 (8.8977)	LR 4.992e-04
3: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 7.94e-03 (3.00e-04)	Tok/s 14374 (12682)	Loss/tok 7.9216 (8.8872)	LR 4.992e-04
7: TRAIN [0][50/55]	Time 0.694 (0.598)	Data 1.40e-04 (1.92e-04)	Tok/s 14447 (12652)	Loss/tok 7.8882 (8.8914)	LR 4.992e-04
12: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 1.38e-04 (1.52e-04)	Tok/s 14376 (12691)	Loss/tok 7.9548 (8.8855)	LR 4.992e-04
2: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 1.62e-04 (1.59e-04)	Tok/s 14511 (12697)	Loss/tok 7.9795 (8.8890)	LR 4.992e-04
0: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 1.29e-04 (6.91e-04)	Tok/s 14324 (12654)	Loss/tok 7.9637 (8.8805)	LR 4.992e-04
1: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 1.42e-04 (3.07e-04)	Tok/s 14389 (12694)	Loss/tok 7.9817 (8.8934)	LR 4.992e-04
15: TRAIN [0][50/55]	Time 0.695 (0.598)	Data 1.56e-04 (1.80e-03)	Tok/s 14465 (12657)	Loss/tok 7.9554 (8.8877)	LR 4.992e-04
13: TRAIN [0][50/55]	Time 0.696 (0.598)	Data 1.41e-04 (3.01e-04)	Tok/s 14290 (12656)	Loss/tok 7.9960 (8.8858)	LR 4.992e-04
14: TRAIN [0][50/55]	Time 0.696 (0.598)	Data 1.23e-04 (3.81e-04)	Tok/s 14530 (12694)	Loss/tok 8.0117 (8.8915)	LR 4.992e-04
11: Running validation on dev set
1: Running validation on dev set
5: Running validation on dev set
14: Running validation on dev set
13: Running validation on dev set
15: Running validation on dev set
8: Running validation on dev set
0: Running validation on dev set
10: Running validation on dev set
9: Running validation on dev set
12: Running validation on dev set
7: Running validation on dev set
11: Executing preallocation
15: Executing preallocation
5: Executing preallocation
1: Executing preallocation
13: Executing preallocation
14: Executing preallocation
8: Executing preallocation
10: Executing preallocation
0: Executing preallocation
3: Running validation on dev set
9: Executing preallocation
12: Executing preallocation
7: Executing preallocation
4: Running validation on dev set
3: Executing preallocation
6: Running validation on dev set
2: Running validation on dev set
4: Executing preallocation
6: Executing preallocation
2: Executing preallocation
13: VALIDATION [0][0/5]	Time 0.097 (0.000)	Data 2.12e-03 (0.00e+00)	Tok/s 55816 (0)	Loss/tok 8.5558 (8.5558)
12: VALIDATION [0][0/5]	Time 0.098 (0.000)	Data 2.25e-03 (0.00e+00)	Tok/s 56785 (0)	Loss/tok 8.5307 (8.5307)
11: VALIDATION [0][0/5]	Time 0.101 (0.000)	Data 2.16e-03 (0.00e+00)	Tok/s 56139 (0)	Loss/tok 8.5877 (8.5877)
14: VALIDATION [0][0/5]	Time 0.101 (0.000)	Data 2.07e-03 (0.00e+00)	Tok/s 52727 (0)	Loss/tok 8.5620 (8.5620)
15: VALIDATION [0][0/5]	Time 0.099 (0.000)	Data 2.10e-03 (0.00e+00)	Tok/s 52203 (0)	Loss/tok 8.5387 (8.5387)
10: VALIDATION [0][0/5]	Time 0.099 (0.000)	Data 2.13e-03 (0.00e+00)	Tok/s 58986 (0)	Loss/tok 8.6666 (8.6666)
8: VALIDATION [0][0/5]	Time 0.113 (0.000)	Data 2.15e-03 (0.00e+00)	Tok/s 53978 (0)	Loss/tok 8.6585 (8.6585)
7: VALIDATION [0][0/5]	Time 0.111 (0.000)	Data 2.16e-03 (0.00e+00)	Tok/s 57253 (0)	Loss/tok 8.7057 (8.7057)
9: VALIDATION [0][0/5]	Time 0.116 (0.000)	Data 2.14e-03 (0.00e+00)	Tok/s 51247 (0)	Loss/tok 8.6092 (8.6092)
4: VALIDATION [0][0/5]	Time 0.121 (0.000)	Data 2.10e-03 (0.00e+00)	Tok/s 58639 (0)	Loss/tok 8.6903 (8.6903)
3: VALIDATION [0][0/5]	Time 0.130 (0.000)	Data 2.37e-03 (0.00e+00)	Tok/s 57064 (0)	Loss/tok 8.7140 (8.7140)
6: VALIDATION [0][0/5]	Time 0.119 (0.000)	Data 2.14e-03 (0.00e+00)	Tok/s 54795 (0)	Loss/tok 8.6511 (8.6511)
5: VALIDATION [0][0/5]	Time 0.123 (0.000)	Data 2.17e-03 (0.00e+00)	Tok/s 55568 (0)	Loss/tok 8.7181 (8.7181)
2: VALIDATION [0][0/5]	Time 0.141 (0.000)	Data 2.17e-03 (0.00e+00)	Tok/s 55488 (0)	Loss/tok 8.7491 (8.7491)
1: VALIDATION [0][0/5]	Time 0.162 (0.000)	Data 2.11e-03 (0.00e+00)	Tok/s 52893 (0)	Loss/tok 8.7116 (8.7116)
0: VALIDATION [0][0/5]	Time 0.254 (0.000)	Data 2.09e-03 (0.00e+00)	Tok/s 41245 (0)	Loss/tok 8.7632 (8.7632)
0: Saving model to gnmt/model_best.pth
12: Running evaluation on test set
10: Running evaluation on test set
6: Running evaluation on test set
0: Running evaluation on test set
13: Running evaluation on test set
14: Running evaluation on test set
11: Running evaluation on test set
4: Running evaluation on test set
1: Running evaluation on test set
9: Running evaluation on test set
8: Running evaluation on test set
3: Running evaluation on test set
5: Running evaluation on test set
2: Running evaluation on test set
7: Running evaluation on test set
15: Running evaluation on test set
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
15: Finished evaluation on test set
12: Finished evaluation on test set
6: Finished evaluation on test set
13: Finished evaluation on test set
1: Finished evaluation on test set
14: Finished evaluation on test set
8: Finished evaluation on test set
5: Finished evaluation on test set
11: Finished evaluation on test set
4: Finished evaluation on test set
9: Finished evaluation on test set
10: Finished evaluation on test set
2: Finished evaluation on test set
7: Finished evaluation on test set
3: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
0: Summary: Epoch: 0	Training Loss: 8.8326	Validation Loss: 8.4128	Test BLEU: 0.00
0: Performance: Epoch: 0	Training: 201588 Tok/s	Validation: 895162 Tok/s
0: Finished epoch 0
0: Starting epoch 1
1: Executing preallocation
2: Finished epoch 0
2: Starting epoch 1
0: Executing preallocation
13: Finished epoch 0
15: Finished epoch 0
14: Finished epoch 0
8: Finished epoch 0
5: Finished epoch 0
9: Finished epoch 0
6: Finished epoch 0
11: Finished epoch 0
12: Finished epoch 0
10: Finished epoch 0
4: Finished epoch 0
7: Finished epoch 0
13: Starting epoch 1
15: Starting epoch 1
14: Starting epoch 1
5: Starting epoch 1
8: Starting epoch 1
6: Starting epoch 1
11: Starting epoch 1
12: Starting epoch 1
9: Starting epoch 1
10: Starting epoch 1
4: Starting epoch 1
7: Starting epoch 1
13: Executing preallocation
3: Finished epoch 0
8: Executing preallocation
2: Executing preallocation
15: Executing preallocation
5: Executing preallocation
6: Executing preallocation
3: Starting epoch 1
12: Executing preallocation
11: Executing preallocation
4: Executing preallocation
14: Executing preallocation
9: Executing preallocation
10: Executing preallocation
7: Executing preallocation
3: Executing preallocation
12: Sampler for epoch 1 uses seed 3276568223
10: Sampler for epoch 1 uses seed 3276568223
9: Sampler for epoch 1 uses seed 3276568223
8: Sampler for epoch 1 uses seed 3276568223
13: Sampler for epoch 1 uses seed 3276568223
14: Sampler for epoch 1 uses seed 3276568223
11: Sampler for epoch 1 uses seed 3276568223
4: Sampler for epoch 1 uses seed 3276568223
5: Sampler for epoch 1 uses seed 3276568223
2: Sampler for epoch 1 uses seed 3276568223
0: Sampler for epoch 1 uses seed 3276568223
15: Sampler for epoch 1 uses seed 3276568223
3: Sampler for epoch 1 uses seed 3276568223
1: Sampler for epoch 1 uses seed 3276568223
6: Sampler for epoch 1 uses seed 3276568223
7: Sampler for epoch 1 uses seed 3276568223
1: TRAIN [1][0/55]	Time 0.788 (0.000)	Data 1.34e-01 (0.00e+00)	Tok/s 12939 (0)	Loss/tok 8.0545 (8.0545)	LR 6.843e-04
13: TRAIN [1][0/55]	Time 0.788 (0.000)	Data 1.00e-01 (0.00e+00)	Tok/s 12984 (0)	Loss/tok 8.0822 (8.0822)	LR 6.843e-04
2: TRAIN [1][0/55]	Time 0.792 (0.000)	Data 1.32e-01 (0.00e+00)	Tok/s 12617 (0)	Loss/tok 8.0240 (8.0240)	LR 6.843e-04
14: TRAIN [1][0/55]	Time 0.784 (0.000)	Data 1.02e-01 (0.00e+00)	Tok/s 12705 (0)	Loss/tok 7.9853 (7.9853)	LR 6.843e-04
0: TRAIN [1][0/55]	Time 0.792 (0.000)	Data 1.24e-01 (0.00e+00)	Tok/s 12838 (0)	Loss/tok 8.0290 (8.0290)	LR 6.843e-04
15: TRAIN [1][0/55]	Time 0.758 (0.000)	Data 9.23e-02 (0.00e+00)	Tok/s 13451 (0)	Loss/tok 8.0638 (8.0638)	LR 6.843e-04
4: TRAIN [1][0/55]	Time 0.793 (0.000)	Data 1.16e-01 (0.00e+00)	Tok/s 12781 (0)	Loss/tok 8.0287 (8.0287)	LR 6.843e-04
11: TRAIN [1][0/55]	Time 0.784 (0.000)	Data 1.04e-01 (0.00e+00)	Tok/s 12865 (0)	Loss/tok 8.0766 (8.0766)	LR 6.843e-04
3: TRAIN [1][0/55]	Time 0.784 (0.000)	Data 1.28e-01 (0.00e+00)	Tok/s 12890 (0)	Loss/tok 8.0662 (8.0662)	LR 6.843e-04
10: TRAIN [1][0/55]	Time 0.791 (0.000)	Data 9.52e-02 (0.00e+00)	Tok/s 12772 (0)	Loss/tok 8.0801 (8.0801)	LR 6.843e-04
5: TRAIN [1][0/55]	Time 0.793 (0.000)	Data 1.14e-01 (0.00e+00)	Tok/s 12696 (0)	Loss/tok 8.0773 (8.0773)	LR 6.843e-04
9: TRAIN [1][0/55]	Time 0.793 (0.000)	Data 9.45e-02 (0.00e+00)	Tok/s 12743 (0)	Loss/tok 8.0673 (8.0673)	LR 6.843e-04
8: TRAIN [1][0/55]	Time 0.793 (0.000)	Data 1.10e-01 (0.00e+00)	Tok/s 12875 (0)	Loss/tok 8.0073 (8.0073)	LR 6.843e-04
6: TRAIN [1][0/55]	Time 0.793 (0.000)	Data 1.35e-01 (0.00e+00)	Tok/s 12721 (0)	Loss/tok 8.0476 (8.0476)	LR 6.843e-04
12: TRAIN [1][0/55]	Time 0.790 (0.000)	Data 9.53e-02 (0.00e+00)	Tok/s 12783 (0)	Loss/tok 8.0538 (8.0538)	LR 6.843e-04
7: TRAIN [1][0/55]	Time 0.777 (0.000)	Data 1.21e-01 (0.00e+00)	Tok/s 13072 (0)	Loss/tok 8.0742 (8.0742)	LR 6.843e-04
3: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 2.39e-04 (2.12e-04)	Tok/s 12377 (12548)	Loss/tok 7.7689 (7.8716)	LR 1.286e-03
13: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 2.20e-04 (1.94e-04)	Tok/s 12261 (12618)	Loss/tok 7.6628 (7.8349)	LR 1.286e-03
12: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.02e-02 (3.01e-03)	Tok/s 12462 (12562)	Loss/tok 7.7454 (7.8779)	LR 1.286e-03
2: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.03e-02 (5.09e-03)	Tok/s 12581 (12515)	Loss/tok 7.6994 (7.8214)	LR 1.286e-03
11: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 2.28e-04 (4.85e-03)	Tok/s 12448 (12536)	Loss/tok 7.7661 (7.8484)	LR 1.286e-03
4: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 7.43e-04 (4.93e-03)	Tok/s 12149 (12563)	Loss/tok 7.6888 (7.8512)	LR 1.286e-03
1: TRAIN [1][10/55]	Time 0.584 (0.608)	Data 1.42e-04 (1.57e-04)	Tok/s 12389 (12518)	Loss/tok 7.7811 (7.8511)	LR 1.286e-03
10: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.28e-04 (1.29e-04)	Tok/s 12486 (12518)	Loss/tok 7.7210 (7.8581)	LR 1.286e-03
9: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.38e-04 (1.84e-04)	Tok/s 12547 (12580)	Loss/tok 7.7489 (7.8434)	LR 1.286e-03
5: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.04e-02 (3.97e-03)	Tok/s 12439 (12494)	Loss/tok 7.7362 (7.8592)	LR 1.286e-03
7: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 2.30e-04 (1.92e-04)	Tok/s 12115 (12437)	Loss/tok 7.6629 (7.8420)	LR 1.286e-03
8: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 2.37e-04 (3.71e-04)	Tok/s 12295 (12513)	Loss/tok 7.7521 (7.8345)	LR 1.286e-03
6: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.40e-04 (1.35e-04)	Tok/s 12465 (12560)	Loss/tok 7.7936 (7.8597)	LR 1.286e-03
14: TRAIN [1][10/55]	Time 0.583 (0.608)	Data 1.22e-04 (1.76e-04)	Tok/s 12276 (12572)	Loss/tok 7.8169 (7.8609)	LR 1.286e-03
15: TRAIN [1][10/55]	Time 0.584 (0.608)	Data 1.51e-04 (2.51e-03)	Tok/s 12324 (12515)	Loss/tok 7.7327 (7.8416)	LR 1.286e-03
0: TRAIN [1][10/55]	Time 0.584 (0.608)	Data 1.23e-04 (2.19e-04)	Tok/s 12299 (12504)	Loss/tok 7.7205 (7.8356)	LR 1.286e-03
8: TRAIN [1][20/55]	Time 0.430 (0.621)	Data 1.50e-04 (2.67e-04)	Tok/s 10125 (12753)	Loss/tok 7.1737 (7.7754)	LR 1.000e-03
7: TRAIN [1][20/55]	Time 0.430 (0.621)	Data 2.38e-04 (1.90e-04)	Tok/s 10036 (12711)	Loss/tok 7.2061 (7.7918)	LR 1.000e-03
6: TRAIN [1][20/55]	Time 0.430 (0.621)	Data 1.33e-04 (1.36e-04)	Tok/s 10000 (12779)	Loss/tok 7.1404 (7.7901)	LR 1.000e-03
9: TRAIN [1][20/55]	Time 0.430 (0.621)	Data 1.41e-04 (1.82e-04)	Tok/s 10150 (12800)	Loss/tok 7.2191 (7.7935)	LR 1.000e-03
5: TRAIN [1][20/55]	Time 0.430 (0.621)	Data 1.56e-04 (4.08e-03)	Tok/s 10271 (12759)	Loss/tok 7.2433 (7.8070)	LR 1.000e-03
4: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.25e-04 (3.84e-03)	Tok/s 10101 (12791)	Loss/tok 7.2383 (7.7990)	LR 1.000e-03
10: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.16e-04 (1.30e-04)	Tok/s 10443 (12775)	Loss/tok 7.2918 (7.7994)	LR 1.000e-03
11: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.74e-04 (4.05e-03)	Tok/s 10147 (12814)	Loss/tok 7.2849 (7.8063)	LR 1.000e-03
13: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 2.09e-04 (1.86e-04)	Tok/s 9927 (12791)	Loss/tok 7.2289 (7.7968)	LR 1.000e-03
2: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 9.17e-03 (5.14e-03)	Tok/s 10324 (12773)	Loss/tok 7.2490 (7.7796)	LR 1.000e-03
0: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.43e-04 (1.85e-04)	Tok/s 9851 (12775)	Loss/tok 7.2248 (7.7900)	LR 1.000e-03
3: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 2.39e-04 (2.04e-04)	Tok/s 10026 (12775)	Loss/tok 7.1520 (7.8093)	LR 1.000e-03
1: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.40e-04 (3.97e-04)	Tok/s 9950 (12750)	Loss/tok 7.1504 (7.7918)	LR 1.000e-03
12: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 9.22e-03 (5.79e-03)	Tok/s 10640 (12820)	Loss/tok 7.2511 (7.8034)	LR 1.000e-03
15: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.34e-04 (3.38e-03)	Tok/s 10321 (12770)	Loss/tok 7.3094 (7.7948)	LR 1.000e-03
14: TRAIN [1][20/55]	Time 0.431 (0.621)	Data 1.27e-04 (1.79e-04)	Tok/s 10358 (12807)	Loss/tok 7.3462 (7.8020)	LR 1.000e-03
4: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.19e-04 (2.62e-03)	Tok/s 12067 (12867)	Loss/tok 7.4650 (7.7456)	LR 5.000e-04
11: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.45e-04 (2.75e-03)	Tok/s 12349 (12906)	Loss/tok 7.4774 (7.7451)	LR 5.000e-04
8: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.35e-04 (2.25e-04)	Tok/s 12114 (12868)	Loss/tok 7.4644 (7.7266)	LR 5.000e-04
5: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.45e-04 (2.77e-03)	Tok/s 12326 (12865)	Loss/tok 7.4525 (7.7548)	LR 5.000e-04
10: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.47e-04 (3.56e-04)	Tok/s 12237 (12882)	Loss/tok 7.4437 (7.7489)	LR 5.000e-04
9: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.46e-04 (1.87e-04)	Tok/s 12278 (12884)	Loss/tok 7.4819 (7.7366)	LR 5.000e-04
6: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 1.36e-04 (1.36e-04)	Tok/s 12202 (12856)	Loss/tok 7.4310 (7.7321)	LR 5.000e-04
13: TRAIN [1][30/55]	Time 0.590 (0.630)	Data 2.22e-04 (1.85e-04)	Tok/s 12053 (12884)	Loss/tok 7.3010 (7.7393)	LR 5.000e-04
3: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 2.29e-04 (2.02e-04)	Tok/s 12368 (12868)	Loss/tok 7.4731 (7.7568)	LR 5.000e-04
12: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 2.21e-04 (6.52e-03)	Tok/s 12376 (12896)	Loss/tok 7.4375 (7.7466)	LR 5.000e-04
7: TRAIN [1][30/55]	Time 0.589 (0.630)	Data 2.40e-04 (1.93e-04)	Tok/s 12021 (12826)	Loss/tok 7.4088 (7.7453)	LR 5.000e-04
2: TRAIN [1][30/55]	Time 0.590 (0.630)	Data 1.01e-02 (5.07e-03)	Tok/s 12634 (12905)	Loss/tok 7.4296 (7.7373)	LR 5.000e-04
0: TRAIN [1][30/55]	Time 0.590 (0.630)	Data 1.40e-04 (1.74e-04)	Tok/s 12089 (12839)	Loss/tok 7.4235 (7.7352)	LR 5.000e-04
15: TRAIN [1][30/55]	Time 0.590 (0.630)	Data 1.39e-04 (3.75e-03)	Tok/s 12376 (12859)	Loss/tok 7.4756 (7.7411)	LR 5.000e-04
1: TRAIN [1][30/55]	Time 0.590 (0.630)	Data 1.41e-04 (3.12e-04)	Tok/s 12033 (12843)	Loss/tok 7.4514 (7.7340)	LR 5.000e-04
14: TRAIN [1][30/55]	Time 0.590 (0.630)	Data 1.36e-04 (1.78e-04)	Tok/s 12247 (12926)	Loss/tok 7.5164 (7.7561)	LR 5.000e-04
4: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.14e-02 (2.28e-03)	Tok/s 14399 (12470)	Loss/tok 7.5800 (7.6970)	LR 2.500e-04
11: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.48e-04 (2.10e-03)	Tok/s 14510 (12494)	Loss/tok 7.6143 (7.7000)	LR 2.500e-04
3: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 2.43e-04 (2.00e-04)	Tok/s 14361 (12460)	Loss/tok 7.6207 (7.7118)	LR 2.500e-04
2: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.53e-04 (4.89e-03)	Tok/s 14502 (12512)	Loss/tok 7.6081 (7.6906)	LR 2.500e-04
13: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 2.47e-04 (1.85e-04)	Tok/s 14529 (12487)	Loss/tok 7.5610 (7.6884)	LR 2.500e-04
10: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.32e-04 (5.16e-04)	Tok/s 14434 (12500)	Loss/tok 7.5531 (7.7004)	LR 2.500e-04
9: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.31e-04 (1.86e-04)	Tok/s 14519 (12493)	Loss/tok 7.6539 (7.6943)	LR 2.500e-04
5: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.35e-04 (2.11e-03)	Tok/s 14698 (12469)	Loss/tok 7.6613 (7.7074)	LR 2.500e-04
1: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.63e-04 (2.71e-04)	Tok/s 14495 (12456)	Loss/tok 7.6653 (7.6911)	LR 2.500e-04
6: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.42e-04 (1.36e-04)	Tok/s 14353 (12464)	Loss/tok 7.6071 (7.6865)	LR 2.500e-04
14: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.25e-04 (1.82e-04)	Tok/s 14546 (12529)	Loss/tok 7.6110 (7.7005)	LR 2.500e-04
12: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 2.35e-04 (5.95e-03)	Tok/s 14616 (12490)	Loss/tok 7.5649 (7.6970)	LR 2.500e-04
7: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.33e-04 (4.00e-04)	Tok/s 14607 (12446)	Loss/tok 7.6392 (7.7003)	LR 2.500e-04
0: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.35e-04 (1.63e-04)	Tok/s 14341 (12459)	Loss/tok 7.6410 (7.6927)	LR 2.500e-04
8: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.44e-04 (2.03e-04)	Tok/s 14448 (12476)	Loss/tok 7.6313 (7.6879)	LR 2.500e-04
15: TRAIN [1][40/55]	Time 0.694 (0.601)	Data 1.84e-04 (2.86e-03)	Tok/s 14675 (12475)	Loss/tok 7.6166 (7.6960)	LR 2.500e-04
1: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.20e-04 (3.23e-04)	Tok/s 14450 (12437)	Loss/tok 7.5413 (7.6417)	LR 1.250e-04
0: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.36e-04 (1.60e-04)	Tok/s 14535 (12430)	Loss/tok 7.5815 (7.6434)	LR 1.250e-04
2: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.48e-04 (3.95e-03)	Tok/s 14566 (12483)	Loss/tok 7.4478 (7.6449)	LR 1.250e-04
15: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.36e-04 (2.33e-03)	Tok/s 14485 (12455)	Loss/tok 7.5884 (7.6520)	LR 1.250e-04
13: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.47e-04 (1.82e-04)	Tok/s 14554 (12460)	Loss/tok 7.5207 (7.6469)	LR 1.250e-04
14: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.34e-04 (3.12e-04)	Tok/s 14380 (12487)	Loss/tok 7.4225 (7.6515)	LR 1.250e-04
4: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.32e-04 (2.25e-03)	Tok/s 14689 (12440)	Loss/tok 7.5181 (7.6479)	LR 1.250e-04
3: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 2.35e-04 (1.97e-04)	Tok/s 14583 (12442)	Loss/tok 7.4812 (7.6550)	LR 1.250e-04
12: TRAIN [1][50/55]	Time 0.691 (0.599)	Data 1.52e-04 (5.41e-03)	Tok/s 14358 (12447)	Loss/tok 7.5207 (7.6489)	LR 1.250e-04
11: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.43e-04 (1.70e-03)	Tok/s 14745 (12475)	Loss/tok 7.5582 (7.6542)	LR 1.250e-04
10: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.44e-04 (1.18e-03)	Tok/s 14683 (12471)	Loss/tok 7.4477 (7.6468)	LR 1.250e-04
9: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.45e-04 (1.86e-04)	Tok/s 14495 (12457)	Loss/tok 7.4051 (7.6463)	LR 1.250e-04
5: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.43e-04 (1.72e-03)	Tok/s 14427 (12442)	Loss/tok 7.5145 (7.6569)	LR 1.250e-04
6: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.36e-04 (1.38e-04)	Tok/s 14638 (12444)	Loss/tok 7.4964 (7.6391)	LR 1.250e-04
8: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 1.36e-04 (1.92e-04)	Tok/s 14353 (12453)	Loss/tok 7.5738 (7.6408)	LR 1.250e-04
7: TRAIN [1][50/55]	Time 0.692 (0.599)	Data 9.21e-03 (1.05e-03)	Tok/s 14588 (12422)	Loss/tok 7.5050 (7.6529)	LR 1.250e-04
1: Running validation on dev set
9: Running validation on dev set
11: Running validation on dev set
15: Running validation on dev set
10: Running validation on dev set
0: Running validation on dev set
2: Running validation on dev set
7: Running validation on dev set
14: Running validation on dev set
13: Running validation on dev set
6: Running validation on dev set
3: Running validation on dev set
4: Running validation on dev set
12: Running validation on dev set
8: Running validation on dev set
1: Executing preallocation
5: Running validation on dev set
10: Executing preallocation
15: Executing preallocation
0: Executing preallocation
7: Executing preallocation
11: Executing preallocation
9: Executing preallocation
2: Executing preallocation
6: Executing preallocation
14: Executing preallocation
4: Executing preallocation
13: Executing preallocation
3: Executing preallocation
8: Executing preallocation
12: Executing preallocation
5: Executing preallocation
12: VALIDATION [1][0/5]	Time 0.099 (0.000)	Data 2.18e-03 (0.00e+00)	Tok/s 56086 (0)	Loss/tok 8.0502 (8.0502)
15: VALIDATION [1][0/5]	Time 0.100 (0.000)	Data 2.18e-03 (0.00e+00)	Tok/s 51992 (0)	Loss/tok 8.0608 (8.0608)
13: VALIDATION [1][0/5]	Time 0.099 (0.000)	Data 2.11e-03 (0.00e+00)	Tok/s 54933 (0)	Loss/tok 8.1098 (8.1098)
10: VALIDATION [1][0/5]	Time 0.100 (0.000)	Data 2.23e-03 (0.00e+00)	Tok/s 58593 (0)	Loss/tok 8.2020 (8.2020)
11: VALIDATION [1][0/5]	Time 0.101 (0.000)	Data 2.22e-03 (0.00e+00)	Tok/s 56269 (0)	Loss/tok 8.1473 (8.1473)
14: VALIDATION [1][0/5]	Time 0.103 (0.000)	Data 2.17e-03 (0.00e+00)	Tok/s 51368 (0)	Loss/tok 8.0663 (8.0663)
7: VALIDATION [1][0/5]	Time 0.108 (0.000)	Data 2.24e-03 (0.00e+00)	Tok/s 58642 (0)	Loss/tok 8.2407 (8.2407)
8: VALIDATION [1][0/5]	Time 0.113 (0.000)	Data 2.28e-03 (0.00e+00)	Tok/s 53962 (0)	Loss/tok 8.1945 (8.1945)
9: VALIDATION [1][0/5]	Time 0.116 (0.000)	Data 2.26e-03 (0.00e+00)	Tok/s 51448 (0)	Loss/tok 8.1189 (8.1189)
4: VALIDATION [1][0/5]	Time 0.120 (0.000)	Data 2.21e-03 (0.00e+00)	Tok/s 58808 (0)	Loss/tok 8.2490 (8.2490)
6: VALIDATION [1][0/5]	Time 0.120 (0.000)	Data 2.14e-03 (0.00e+00)	Tok/s 54727 (0)	Loss/tok 8.1863 (8.1863)
3: VALIDATION [1][0/5]	Time 0.130 (0.000)	Data 2.24e-03 (0.00e+00)	Tok/s 57184 (0)	Loss/tok 8.2293 (8.2293)
5: VALIDATION [1][0/5]	Time 0.124 (0.000)	Data 2.21e-03 (0.00e+00)	Tok/s 55127 (0)	Loss/tok 8.2069 (8.2069)
2: VALIDATION [1][0/5]	Time 0.143 (0.000)	Data 2.21e-03 (0.00e+00)	Tok/s 54789 (0)	Loss/tok 8.2849 (8.2849)
1: VALIDATION [1][0/5]	Time 0.163 (0.000)	Data 2.12e-03 (0.00e+00)	Tok/s 52630 (0)	Loss/tok 8.2905 (8.2905)
0: VALIDATION [1][0/5]	Time 0.255 (0.000)	Data 2.16e-03 (0.00e+00)	Tok/s 41086 (0)	Loss/tok 8.3129 (8.3129)
0: Saving model to gnmt/model_best.pth
12: Running evaluation on test set
15: Running evaluation on test set
13: Running evaluation on test set
10: Running evaluation on test set
9: Running evaluation on test set
8: Running evaluation on test set
1: Running evaluation on test set
7: Running evaluation on test set
14: Running evaluation on test set
2: Running evaluation on test set
3: Running evaluation on test set
11: Running evaluation on test set
5: Running evaluation on test set
4: Running evaluation on test set
6: Running evaluation on test set
0: Running evaluation on test set
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
6: Finished evaluation on test set
13: Finished evaluation on test set
15: Finished evaluation on test set
4: Finished evaluation on test set
12: Finished evaluation on test set
11: Finished evaluation on test set
9: Finished evaluation on test set
2: Finished evaluation on test set
0: Finished evaluation on test set
10: Finished evaluation on test set
5: Finished evaluation on test set
3: Finished evaluation on test set
14: Finished evaluation on test set
7: Finished evaluation on test set
8: Finished evaluation on test set
1: Finished epoch 1
6: Finished epoch 1
2: Finished epoch 1
4: Finished epoch 1
5: Finished epoch 1
3: Finished epoch 1
13: Finished epoch 1
15: Finished epoch 1
12: Finished epoch 1
9: Finished epoch 1
11: Finished epoch 1
14: Finished epoch 1
10: Finished epoch 1
7: Finished epoch 1
8: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 7.6296	Validation Loss: 7.9760	Test BLEU: 0.11
0: Performance: Epoch: 1	Training: 198983 Tok/s	Validation: 892018 Tok/s
0: Finished epoch 1
5: Total training time 192 s
7: Total training time 192 s
6: Total training time 192 s
10: Total training time 192 s
15: Total training time 192 s
11: Total training time 192 s
8: Total training time 192 s
4: Total training time 192 s
2: Total training time 192 s
1: Total training time 192 s
3: Total training time 192 s
14: Total training time 192 s
9: Total training time 192 s
12: Total training time 192 s
13: Total training time 192 s
0: Total training time 192 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|      16|                 160|                      0.11|                     200285.9|                         3.198|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
