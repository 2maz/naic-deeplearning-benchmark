DLL 2021-02-05 05:25:51.222364 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.234169 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.364899 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.394293 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.400910 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.401904 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.409689 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-02-05 05:25:51.416155 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 140  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 4345
loading annotations into memory...
Using seed = 8798
loading annotations into memory...
Using seed = 29
Using seed = 1460
loading annotations into memory...
Using seed = 646
Using seed = 9901
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.65s)
creating index...
Done (t=0.65s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Using seed = 6025
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Using seed = 3548
loading annotations into memory...
Done (t=0.65s)
creating index...
Done (t=0.65s)
creating index...
Done (t=0.65s)
creating index...
loading annotations into memory...
Done (t=0.65s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.67s)
creating index...
Done (t=0.68s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
DLL 2021-02-05 05:27:03.840932 - () avg_img/sec : 194.30688921966288  med_img/sec : 194.19845397215695  min_img/sec : 191.96162876378165  max_img/sec : 196.76569804438134 
Done benchmarking. Total images: 2800	total time: 14.410	Average images/sec: 194.307	Median images/sec: 194.198
DLL 2021-02-05 05:27:03.841096 - () avg_img/sec : 194.35993558810065  med_img/sec : 194.23661841282626  min_img/sec : 191.8697382682067  max_img/sec : 196.83336981483598 
Done benchmarking. Total images: 2800	total time: 14.406	Average images/sec: 194.360	Median images/sec: 194.237
DLL 2021-02-05 05:27:03.841330 - () avg_img/sec : 194.41653186224582  med_img/sec : 194.29405623881826  min_img/sec : 191.93979288113056  max_img/sec : 196.89719275185428 
DLL 2021-02-05 05:27:03.841357 - () avg_img/sec : 194.39597128023922  med_img/sec : 194.28636317053247  min_img/sec : 191.9138221798073  max_img/sec : 196.9052478104782 
Done benchmarking. Total images: 2800	total time: 14.402	Average images/sec: 194.417	Median images/sec: 194.294
Done benchmarking. Total images: 2800	total time: 14.404	Average images/sec: 194.396	Median images/sec: 194.286
DLL 2021-02-05 05:27:03.841509 - () avg_img/sec : 194.34198867946304  med_img/sec : 194.23290963842084  min_img/sec : 191.8967631287925  max_img/sec : 196.89065675422205 
Done benchmarking. Total images: 2800	total time: 14.408	Average images/sec: 194.342	Median images/sec: 194.233
DLL 2021-02-05 05:27:03.841751 - () avg_img/sec : 194.60358846856144  med_img/sec : 194.51210811181812  min_img/sec : 192.8660752614536  max_img/sec : 196.86610112610808 
Done benchmarking. Total images: 2800	total time: 14.388	Average images/sec: 194.604	Median images/sec: 194.512
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.841971 - () total time : 61.28021430969238 
DLL 2021-02-05 05:27:03.841999 - () 
DLL 2021-02-05 05:27:03.841968 - () avg_img/sec : 194.56356901189739  med_img/sec : 194.38252044621962  min_img/sec : 192.11650480354604  max_img/sec : 197.1641525436818 
Done benchmarking. Total images: 2800	total time: 14.391	Average images/sec: 194.564	Median images/sec: 194.383
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.842182 - () total time : 61.281118631362915 
DLL 2021-02-05 05:27:03.842193 - () total time : 61.281424045562744 
DLL 2021-02-05 05:27:03.842205 - () 
DLL 2021-02-05 05:27:03.842216 - () 
DLL 2021-02-05 05:27:03.842182 - () avg_img/sec : 194.57345225147873  med_img/sec : 194.2789772736943  min_img/sec : 192.09142884060964  max_img/sec : 197.0678102277479 
Done benchmarking. Total images: 2800	total time: 14.390	Average images/sec: 194.573	Median images/sec: 194.279
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.842479 - () total time : 61.2808096408844 
DLL 2021-02-05 05:27:03.842504 - () 
Training performance = 1554.4219970703125 FPS
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.842785 - () total time : 61.28081464767456 
DLL 2021-02-05 05:27:03.842809 - () 
DLL 2021-02-05 05:27:03.842883 - (0,) time : 61.28922772407532 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.843106 - () total time : 61.28145241737366 
DLL 2021-02-05 05:27:03.843131 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.843191 - () total time : 61.28922772407532 
DLL 2021-02-05 05:27:03.843206 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-02-05 05:27:03.843293 - () total time : 61.28227376937866 
DLL 2021-02-05 05:27:03.843315 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
