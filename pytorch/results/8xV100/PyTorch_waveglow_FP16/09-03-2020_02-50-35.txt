:::NVLOGv0.2.2 Tacotron2_PyT 1583722237.780150890 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1583722237.803517103 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1583722237.824519396 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1583722253.750807524 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.33.01", "num": 8, "name": ["Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB"], "mem": ["32510 MiB", "32510 MiB", "32510 MiB", "32510 MiB", "32510 MiB", "32510 MiB", "32510 MiB", "32510 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1583722253.755793810 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 30, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1583722257.249813080 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1583722359.283520699 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1583722359.285656691 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/5 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722359.662665844 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722364.986523151 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020993538200855255
:::NVLOGv0.2.2 Tacotron2_PyT 1583722368.537472963 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722368.538402319 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 216306.86441035115
:::NVLOGv0.2.2 Tacotron2_PyT 1583722368.539075851 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.876278638839722
Batch: 1/5 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722368.543692350 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722369.899891138 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023136886302381754
:::NVLOGv0.2.2 Tacotron2_PyT 1583722371.732809544 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722371.733570337 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 601788.4835946867
:::NVLOGv0.2.2 Tacotron2_PyT 1583722371.734061003 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.1904897689819336
Batch: 2/5 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722371.741164207 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583722372.739502192 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021105327177792788
:::NVLOGv0.2.2 Tacotron2_PyT 1583722374.567681551 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583722374.568499565 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 678736.9877464205
:::NVLOGv0.2.2 Tacotron2_PyT 1583722374.569068909 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8287835121154785
Batch: 3/5 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722374.572129011 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583722375.464302301 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002311608986929059
:::NVLOGv0.2.2 Tacotron2_PyT 1583722377.251862049 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583722377.252522945 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 716310.0070554154
:::NVLOGv0.2.2 Tacotron2_PyT 1583722377.253007889 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.680403709411621
Batch: 4/5 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722377.258640528 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583722378.291574001 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019059008918702602
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.189666271 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.190336943 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 654659.6292735653
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.190825224 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.932821750640869
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.514618874 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.515219688 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 452188.4377234814
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.515686989 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 573560.3944160879
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.516149044 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.00214821700938046
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.516622066 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 21.230087280273438
:::NVLOGv0.2.2 Tacotron2_PyT 1583722380.517078161 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1583722381.972897530 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0021024695597589016
:::NVLOGv0.2.2 Tacotron2_PyT 1583722381.974069595 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722385.098376513 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/5 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722385.401067495 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722386.086212397 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022960291244089603
:::NVLOGv0.2.2 Tacotron2_PyT 1583722387.954380751 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583722387.954994440 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 751044.8076336065
:::NVLOGv0.2.2 Tacotron2_PyT 1583722387.955524445 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.556438684463501
Batch: 1/5 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722387.958735943 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722388.969278097 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002222512848675251
:::NVLOGv0.2.2 Tacotron2_PyT 1583722390.805744648 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722390.806372166 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 674208.0320121121
:::NVLOGv0.2.2 Tacotron2_PyT 1583722390.806890011 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.847785711288452
Batch: 2/5 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722390.809827089 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583722391.801759243 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021379946265369654
:::NVLOGv0.2.2 Tacotron2_PyT 1583722393.625840187 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583722393.626379967 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 681602.7024872166
:::NVLOGv0.2.2 Tacotron2_PyT 1583722393.626846790 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.816890239715576
Batch: 3/5 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722393.631486893 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583722394.560199499 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023024443071335554
:::NVLOGv0.2.2 Tacotron2_PyT 1583722396.352507353 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583722396.353197575 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 705438.5597919493
:::NVLOGv0.2.2 Tacotron2_PyT 1583722396.353718042 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7217111587524414
Batch: 4/5 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722396.357049942 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583722397.248774290 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.001932154526002705
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.078888655 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.079457998 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 705248.5896234433
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.079940557 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7224442958831787
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.136579037 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.138129711 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 683794.0113873855
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.139566183 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 703508.5383096656
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.141012192 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0021782270865514874
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.142448425 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 14.039315700531006
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.143863440 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.776132584 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.002288270276039839
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.777396202 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.779308319 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 142.52818751335144
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.780051708 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 142.52818751335144
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.780858517 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 162.13915991783142
:::NVLOGv0.2.2 Tacotron2_PyT 1583722399.781570911 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
