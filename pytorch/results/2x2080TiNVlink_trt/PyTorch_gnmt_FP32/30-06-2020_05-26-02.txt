1: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Filtering data, min len: 0, max len: 50
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 909
0: Scheduler decay interval: 114
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 909
1: Scheduler decay interval: 114
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
1: TRAIN [0][0/683]	Time 0.620 (0.620)	Data 1.26e-01 (1.26e-01)	Tok/s 10928 (10928)	Loss/tok 10.6986 (10.6986)	LR 2.047e-05
0: TRAIN [0][0/683]	Time 0.616 (0.616)	Data 1.47e-01 (1.47e-01)	Tok/s 11044 (11044)	Loss/tok 10.7036 (10.7036)	LR 2.047e-05
1: TRAIN [0][10/683]	Time 0.393 (0.394)	Data 1.07e-04 (1.15e-02)	Tok/s 12187 (11155)	Loss/tok 9.6614 (10.1808)	LR 2.576e-05
0: TRAIN [0][10/683]	Time 0.394 (0.394)	Data 1.68e-04 (1.35e-02)	Tok/s 12526 (11213)	Loss/tok 9.7140 (10.1935)	LR 2.576e-05
0: TRAIN [0][20/683]	Time 0.314 (0.394)	Data 1.03e-04 (7.16e-03)	Tok/s 9766 (11248)	Loss/tok 9.1079 (9.8237)	LR 3.244e-05
1: TRAIN [0][20/683]	Time 0.314 (0.395)	Data 1.33e-04 (6.09e-03)	Tok/s 9479 (11209)	Loss/tok 9.1135 (9.8124)	LR 3.244e-05
0: TRAIN [0][30/683]	Time 0.314 (0.413)	Data 1.08e-04 (4.89e-03)	Tok/s 9723 (11799)	Loss/tok 8.8299 (9.5483)	LR 4.083e-05
1: TRAIN [0][30/683]	Time 0.314 (0.414)	Data 1.27e-04 (4.17e-03)	Tok/s 9703 (11778)	Loss/tok 8.7528 (9.5369)	LR 4.083e-05
0: TRAIN [0][40/683]	Time 0.605 (0.417)	Data 1.76e-04 (3.73e-03)	Tok/s 14586 (11759)	Loss/tok 8.7691 (9.3681)	LR 5.141e-05
1: TRAIN [0][40/683]	Time 0.605 (0.417)	Data 1.60e-04 (3.19e-03)	Tok/s 14466 (11755)	Loss/tok 8.8338 (9.3544)	LR 5.141e-05
0: TRAIN [0][50/683]	Time 0.616 (0.420)	Data 1.17e-04 (3.02e-03)	Tok/s 14369 (11747)	Loss/tok 8.6714 (9.2263)	LR 6.472e-05
1: TRAIN [0][50/683]	Time 0.616 (0.420)	Data 1.21e-04 (2.58e-03)	Tok/s 14511 (11756)	Loss/tok 8.6443 (9.2155)	LR 6.472e-05
0: TRAIN [0][60/683]	Time 0.500 (0.421)	Data 1.76e-04 (2.55e-03)	Tok/s 13661 (11752)	Loss/tok 8.3371 (9.0865)	LR 8.148e-05
1: TRAIN [0][60/683]	Time 0.500 (0.422)	Data 1.46e-04 (2.18e-03)	Tok/s 13593 (11749)	Loss/tok 8.3308 (9.0796)	LR 8.148e-05
0: TRAIN [0][70/683]	Time 0.401 (0.421)	Data 9.85e-05 (2.21e-03)	Tok/s 12042 (11739)	Loss/tok 8.1363 (8.9897)	LR 1.026e-04
1: TRAIN [0][70/683]	Time 0.401 (0.421)	Data 1.13e-04 (1.89e-03)	Tok/s 11989 (11733)	Loss/tok 8.0898 (8.9825)	LR 1.026e-04
0: TRAIN [0][80/683]	Time 0.408 (0.427)	Data 1.18e-04 (1.95e-03)	Tok/s 11867 (11844)	Loss/tok 7.9632 (8.8651)	LR 1.291e-04
1: TRAIN [0][80/683]	Time 0.408 (0.428)	Data 1.04e-04 (1.67e-03)	Tok/s 12125 (11840)	Loss/tok 7.9535 (8.8594)	LR 1.291e-04
0: TRAIN [0][90/683]	Time 0.323 (0.432)	Data 1.13e-04 (1.75e-03)	Tok/s 8972 (11861)	Loss/tok 7.5812 (8.7541)	LR 1.626e-04
1: TRAIN [0][90/683]	Time 0.324 (0.432)	Data 1.05e-04 (1.50e-03)	Tok/s 8753 (11853)	Loss/tok 7.6269 (8.7506)	LR 1.626e-04
0: TRAIN [0][100/683]	Time 0.526 (0.428)	Data 1.09e-04 (1.58e-03)	Tok/s 12993 (11774)	Loss/tok 7.9487 (8.6730)	LR 2.047e-04
1: TRAIN [0][100/683]	Time 0.526 (0.428)	Data 1.29e-04 (1.36e-03)	Tok/s 12807 (11764)	Loss/tok 7.9330 (8.6726)	LR 2.047e-04
0: TRAIN [0][110/683]	Time 0.521 (0.427)	Data 1.23e-04 (1.45e-03)	Tok/s 12884 (11678)	Loss/tok 7.8416 (8.6038)	LR 2.576e-04
1: TRAIN [0][110/683]	Time 0.520 (0.427)	Data 1.08e-04 (1.25e-03)	Tok/s 13082 (11672)	Loss/tok 7.8757 (8.6022)	LR 2.576e-04
0: TRAIN [0][120/683]	Time 0.321 (0.428)	Data 1.51e-04 (1.34e-03)	Tok/s 9181 (11671)	Loss/tok 7.5888 (8.5328)	LR 3.244e-04
1: TRAIN [0][120/683]	Time 0.321 (0.428)	Data 1.15e-04 (1.16e-03)	Tok/s 9054 (11656)	Loss/tok 7.5120 (8.5332)	LR 3.244e-04
0: TRAIN [0][130/683]	Time 0.414 (0.428)	Data 1.69e-04 (1.25e-03)	Tok/s 11584 (11653)	Loss/tok 7.6546 (8.4735)	LR 4.083e-04
1: TRAIN [0][130/683]	Time 0.414 (0.429)	Data 1.05e-04 (1.08e-03)	Tok/s 11903 (11637)	Loss/tok 7.8406 (8.4756)	LR 4.083e-04
0: TRAIN [0][140/683]	Time 0.329 (0.434)	Data 1.15e-04 (1.17e-03)	Tok/s 8614 (11725)	Loss/tok 7.4502 (8.4118)	LR 5.141e-04
1: TRAIN [0][140/683]	Time 0.329 (0.435)	Data 1.11e-04 (1.01e-03)	Tok/s 8811 (11705)	Loss/tok 7.3877 (8.4149)	LR 5.141e-04
0: TRAIN [0][150/683]	Time 0.528 (0.431)	Data 1.02e-04 (1.10e-03)	Tok/s 12582 (11613)	Loss/tok 7.8227 (8.3721)	LR 6.472e-04
1: TRAIN [0][150/683]	Time 0.528 (0.432)	Data 1.10e-04 (9.50e-04)	Tok/s 12782 (11593)	Loss/tok 7.7639 (8.3745)	LR 6.472e-04
0: TRAIN [0][160/683]	Time 0.320 (0.431)	Data 1.02e-04 (1.03e-03)	Tok/s 9305 (11589)	Loss/tok 7.2648 (8.3277)	LR 8.148e-04
1: TRAIN [0][160/683]	Time 0.320 (0.431)	Data 1.12e-04 (8.98e-04)	Tok/s 9079 (11571)	Loss/tok 7.2690 (8.3303)	LR 8.148e-04
0: TRAIN [0][170/683]	Time 0.323 (0.431)	Data 9.70e-05 (9.80e-04)	Tok/s 9208 (11559)	Loss/tok 7.3006 (8.2895)	LR 1.026e-03
1: TRAIN [0][170/683]	Time 0.323 (0.431)	Data 1.11e-04 (8.52e-04)	Tok/s 9142 (11540)	Loss/tok 7.3802 (8.2932)	LR 1.026e-03
0: TRAIN [0][180/683]	Time 0.630 (0.434)	Data 9.82e-05 (9.32e-04)	Tok/s 13925 (11570)	Loss/tok 7.7385 (8.2485)	LR 1.291e-03
1: TRAIN [0][180/683]	Time 0.630 (0.434)	Data 1.43e-04 (8.12e-04)	Tok/s 13825 (11548)	Loss/tok 7.7692 (8.2549)	LR 1.291e-03
0: TRAIN [0][190/683]	Time 0.523 (0.441)	Data 1.62e-04 (8.89e-04)	Tok/s 12961 (11653)	Loss/tok 7.5808 (8.2048)	LR 1.626e-03
1: TRAIN [0][190/683]	Time 0.528 (0.441)	Data 1.04e-04 (7.76e-04)	Tok/s 12830 (11630)	Loss/tok 7.4855 (8.2098)	LR 1.626e-03
0: TRAIN [0][200/683]	Time 0.620 (0.442)	Data 1.18e-04 (8.51e-04)	Tok/s 14227 (11648)	Loss/tok 7.5637 (8.1658)	LR 2.000e-03
1: TRAIN [0][200/683]	Time 0.641 (0.442)	Data 1.28e-04 (7.44e-04)	Tok/s 13795 (11631)	Loss/tok 7.5815 (8.1704)	LR 2.000e-03
0: TRAIN [0][210/683]	Time 0.322 (0.445)	Data 1.09e-04 (8.16e-04)	Tok/s 8989 (11685)	Loss/tok 6.9892 (8.1268)	LR 2.000e-03
1: TRAIN [0][210/683]	Time 0.322 (0.445)	Data 1.87e-04 (7.15e-04)	Tok/s 9237 (11673)	Loss/tok 6.9593 (8.1288)	LR 2.000e-03
0: TRAIN [0][220/683]	Time 0.328 (0.444)	Data 1.03e-04 (7.83e-04)	Tok/s 8743 (11628)	Loss/tok 6.7994 (8.0928)	LR 2.000e-03
1: TRAIN [0][220/683]	Time 0.327 (0.444)	Data 1.06e-04 (6.88e-04)	Tok/s 9171 (11616)	Loss/tok 7.1160 (8.0941)	LR 2.000e-03
0: TRAIN [0][230/683]	Time 0.529 (0.446)	Data 1.06e-04 (7.54e-04)	Tok/s 12829 (11654)	Loss/tok 7.4991 (8.0505)	LR 2.000e-03
1: TRAIN [0][230/683]	Time 0.530 (0.446)	Data 1.11e-04 (6.63e-04)	Tok/s 13102 (11643)	Loss/tok 7.4153 (8.0517)	LR 2.000e-03
0: TRAIN [0][240/683]	Time 0.327 (0.445)	Data 1.08e-04 (7.27e-04)	Tok/s 9066 (11620)	Loss/tok 6.6616 (8.0164)	LR 2.000e-03
1: TRAIN [0][240/683]	Time 0.327 (0.445)	Data 1.73e-04 (6.41e-04)	Tok/s 8826 (11610)	Loss/tok 6.7199 (8.0167)	LR 2.000e-03
0: TRAIN [0][250/683]	Time 0.648 (0.447)	Data 1.04e-04 (7.02e-04)	Tok/s 13539 (11623)	Loss/tok 7.1546 (7.9748)	LR 2.000e-03
1: TRAIN [0][250/683]	Time 0.648 (0.447)	Data 1.31e-04 (6.20e-04)	Tok/s 13638 (11614)	Loss/tok 7.1725 (7.9751)	LR 2.000e-03
0: TRAIN [0][260/683]	Time 0.412 (0.447)	Data 1.08e-04 (6.80e-04)	Tok/s 11922 (11608)	Loss/tok 6.7640 (7.9369)	LR 2.000e-03
1: TRAIN [0][260/683]	Time 0.413 (0.447)	Data 1.29e-04 (6.02e-04)	Tok/s 11876 (11604)	Loss/tok 6.8834 (7.9368)	LR 2.000e-03
0: TRAIN [0][270/683]	Time 0.647 (0.446)	Data 1.09e-04 (6.59e-04)	Tok/s 13647 (11594)	Loss/tok 7.1341 (7.9002)	LR 2.000e-03
1: TRAIN [0][270/683]	Time 0.647 (0.446)	Data 1.10e-04 (5.85e-04)	Tok/s 13721 (11591)	Loss/tok 7.1860 (7.8996)	LR 2.000e-03
0: TRAIN [0][280/683]	Time 0.330 (0.447)	Data 1.03e-04 (6.39e-04)	Tok/s 8859 (11595)	Loss/tok 6.4204 (7.8601)	LR 2.000e-03
1: TRAIN [0][280/683]	Time 0.330 (0.447)	Data 1.09e-04 (5.68e-04)	Tok/s 9000 (11590)	Loss/tok 6.4129 (7.8587)	LR 2.000e-03
0: TRAIN [0][290/683]	Time 0.331 (0.447)	Data 1.18e-04 (6.21e-04)	Tok/s 9110 (11592)	Loss/tok 6.3007 (7.8200)	LR 2.000e-03
1: TRAIN [0][290/683]	Time 0.331 (0.447)	Data 1.08e-04 (5.53e-04)	Tok/s 9096 (11588)	Loss/tok 6.1659 (7.8182)	LR 2.000e-03
0: TRAIN [0][300/683]	Time 0.332 (0.448)	Data 1.05e-04 (6.04e-04)	Tok/s 8761 (11599)	Loss/tok 6.1020 (7.7758)	LR 2.000e-03
1: TRAIN [0][300/683]	Time 0.332 (0.448)	Data 1.10e-04 (5.40e-04)	Tok/s 8828 (11594)	Loss/tok 6.2220 (7.7752)	LR 2.000e-03
0: TRAIN [0][310/683]	Time 0.331 (0.449)	Data 1.07e-04 (5.88e-04)	Tok/s 8844 (11604)	Loss/tok 6.1458 (7.7343)	LR 2.000e-03
1: TRAIN [0][310/683]	Time 0.331 (0.449)	Data 1.07e-04 (5.26e-04)	Tok/s 8773 (11602)	Loss/tok 6.2456 (7.7329)	LR 2.000e-03
0: TRAIN [0][320/683]	Time 0.424 (0.448)	Data 1.16e-04 (5.73e-04)	Tok/s 11594 (11588)	Loss/tok 6.3953 (7.6976)	LR 2.000e-03
1: TRAIN [0][320/683]	Time 0.424 (0.448)	Data 1.19e-04 (5.14e-04)	Tok/s 11605 (11586)	Loss/tok 6.3162 (7.6958)	LR 2.000e-03
0: TRAIN [0][330/683]	Time 0.532 (0.448)	Data 1.07e-04 (5.59e-04)	Tok/s 12972 (11578)	Loss/tok 6.4173 (7.6594)	LR 2.000e-03
1: TRAIN [0][330/683]	Time 0.532 (0.448)	Data 1.10e-04 (5.02e-04)	Tok/s 12896 (11575)	Loss/tok 6.5471 (7.6575)	LR 2.000e-03
0: TRAIN [0][340/683]	Time 0.530 (0.448)	Data 1.03e-04 (5.46e-04)	Tok/s 12694 (11570)	Loss/tok 6.4302 (7.6225)	LR 2.000e-03
1: TRAIN [0][340/683]	Time 0.531 (0.448)	Data 1.22e-04 (4.91e-04)	Tok/s 12785 (11569)	Loss/tok 6.4124 (7.6196)	LR 2.000e-03
0: TRAIN [0][350/683]	Time 0.413 (0.446)	Data 1.03e-04 (5.34e-04)	Tok/s 11656 (11530)	Loss/tok 6.1376 (7.5903)	LR 2.000e-03
1: TRAIN [0][350/683]	Time 0.414 (0.446)	Data 1.05e-04 (4.80e-04)	Tok/s 11860 (11528)	Loss/tok 6.0763 (7.5860)	LR 2.000e-03
0: TRAIN [0][360/683]	Time 0.528 (0.446)	Data 9.78e-05 (5.22e-04)	Tok/s 12789 (11499)	Loss/tok 6.3784 (7.5561)	LR 2.000e-03
1: TRAIN [0][360/683]	Time 0.528 (0.446)	Data 1.10e-04 (4.70e-04)	Tok/s 12754 (11503)	Loss/tok 6.3100 (7.5521)	LR 2.000e-03
0: TRAIN [0][370/683]	Time 0.412 (0.446)	Data 9.89e-05 (5.11e-04)	Tok/s 11952 (11515)	Loss/tok 5.9885 (7.5171)	LR 2.000e-03
1: TRAIN [0][370/683]	Time 0.412 (0.446)	Data 1.22e-04 (4.60e-04)	Tok/s 11866 (11519)	Loss/tok 5.9449 (7.5107)	LR 2.000e-03
0: TRAIN [0][380/683]	Time 0.528 (0.445)	Data 9.49e-05 (5.00e-04)	Tok/s 12845 (11498)	Loss/tok 6.3670 (7.4835)	LR 2.000e-03
1: TRAIN [0][380/683]	Time 0.524 (0.445)	Data 1.46e-04 (4.51e-04)	Tok/s 12995 (11502)	Loss/tok 6.3782 (7.4770)	LR 2.000e-03
0: TRAIN [0][390/683]	Time 0.412 (0.446)	Data 1.07e-04 (4.90e-04)	Tok/s 11916 (11515)	Loss/tok 5.8611 (7.4445)	LR 2.000e-03
1: TRAIN [0][390/683]	Time 0.412 (0.446)	Data 1.06e-04 (4.43e-04)	Tok/s 11913 (11518)	Loss/tok 5.9435 (7.4374)	LR 2.000e-03
0: TRAIN [0][400/683]	Time 0.523 (0.447)	Data 1.05e-04 (4.80e-04)	Tok/s 13068 (11525)	Loss/tok 6.0875 (7.4050)	LR 2.000e-03
1: TRAIN [0][400/683]	Time 0.523 (0.447)	Data 1.18e-04 (4.34e-04)	Tok/s 12975 (11528)	Loss/tok 6.0200 (7.3995)	LR 2.000e-03
0: TRAIN [0][410/683]	Time 0.516 (0.448)	Data 9.35e-05 (4.71e-04)	Tok/s 13181 (11532)	Loss/tok 6.0513 (7.3670)	LR 2.000e-03
1: TRAIN [0][410/683]	Time 0.516 (0.448)	Data 1.19e-04 (4.26e-04)	Tok/s 13055 (11534)	Loss/tok 6.0091 (7.3610)	LR 2.000e-03
0: TRAIN [0][420/683]	Time 0.329 (0.447)	Data 9.44e-05 (4.62e-04)	Tok/s 8937 (11520)	Loss/tok 5.4173 (7.3330)	LR 2.000e-03
1: TRAIN [0][420/683]	Time 0.329 (0.447)	Data 1.13e-04 (4.19e-04)	Tok/s 9276 (11521)	Loss/tok 5.3913 (7.3286)	LR 2.000e-03
0: TRAIN [0][430/683]	Time 0.414 (0.448)	Data 9.42e-05 (4.54e-04)	Tok/s 11765 (11516)	Loss/tok 5.5851 (7.2978)	LR 2.000e-03
1: TRAIN [0][430/683]	Time 0.413 (0.448)	Data 1.02e-04 (4.12e-04)	Tok/s 11498 (11516)	Loss/tok 5.6503 (7.2924)	LR 2.000e-03
0: TRAIN [0][440/683]	Time 0.415 (0.446)	Data 9.66e-05 (4.46e-04)	Tok/s 11880 (11487)	Loss/tok 5.6531 (7.2683)	LR 2.000e-03
1: TRAIN [0][440/683]	Time 0.415 (0.446)	Data 1.04e-04 (4.05e-04)	Tok/s 11597 (11487)	Loss/tok 5.6564 (7.2634)	LR 2.000e-03
0: TRAIN [0][450/683]	Time 0.519 (0.447)	Data 1.02e-04 (4.38e-04)	Tok/s 12914 (11498)	Loss/tok 5.8367 (7.2326)	LR 2.000e-03
1: TRAIN [0][450/683]	Time 0.519 (0.447)	Data 1.14e-04 (3.99e-04)	Tok/s 13265 (11501)	Loss/tok 5.8090 (7.2263)	LR 2.000e-03
0: TRAIN [0][460/683]	Time 0.416 (0.448)	Data 1.02e-04 (4.31e-04)	Tok/s 11792 (11509)	Loss/tok 5.4864 (7.1954)	LR 2.000e-03
1: TRAIN [0][460/683]	Time 0.416 (0.448)	Data 1.06e-04 (3.92e-04)	Tok/s 11701 (11510)	Loss/tok 5.6038 (7.1898)	LR 2.000e-03
0: TRAIN [0][470/683]	Time 0.644 (0.447)	Data 1.01e-04 (4.24e-04)	Tok/s 13852 (11500)	Loss/tok 5.8380 (7.1635)	LR 2.000e-03
1: TRAIN [0][470/683]	Time 0.644 (0.447)	Data 9.87e-05 (3.86e-04)	Tok/s 13753 (11503)	Loss/tok 5.9139 (7.1576)	LR 2.000e-03
0: TRAIN [0][480/683]	Time 0.415 (0.447)	Data 9.51e-05 (4.18e-04)	Tok/s 11868 (11491)	Loss/tok 5.5030 (7.1318)	LR 2.000e-03
1: TRAIN [0][480/683]	Time 0.415 (0.447)	Data 1.12e-04 (3.81e-04)	Tok/s 11825 (11494)	Loss/tok 5.4003 (7.1255)	LR 2.000e-03
0: TRAIN [0][490/683]	Time 0.328 (0.446)	Data 9.70e-05 (4.11e-04)	Tok/s 9147 (11475)	Loss/tok 5.0626 (7.1029)	LR 2.000e-03
1: TRAIN [0][490/683]	Time 0.328 (0.446)	Data 1.01e-04 (3.75e-04)	Tok/s 8710 (11477)	Loss/tok 5.0182 (7.0966)	LR 2.000e-03
0: TRAIN [0][500/683]	Time 0.409 (0.446)	Data 1.02e-04 (4.05e-04)	Tok/s 11703 (11475)	Loss/tok 5.2706 (7.0707)	LR 2.000e-03
1: TRAIN [0][500/683]	Time 0.409 (0.446)	Data 1.07e-04 (3.70e-04)	Tok/s 11786 (11478)	Loss/tok 5.2909 (7.0633)	LR 2.000e-03
0: TRAIN [0][510/683]	Time 0.323 (0.446)	Data 1.01e-04 (3.99e-04)	Tok/s 9447 (11476)	Loss/tok 4.9474 (7.0382)	LR 2.000e-03
1: TRAIN [0][510/683]	Time 0.323 (0.446)	Data 1.09e-04 (3.65e-04)	Tok/s 9270 (11480)	Loss/tok 4.9895 (7.0314)	LR 2.000e-03
0: TRAIN [0][520/683]	Time 0.405 (0.444)	Data 9.56e-05 (3.93e-04)	Tok/s 12144 (11455)	Loss/tok 5.2239 (7.0108)	LR 2.000e-03
1: TRAIN [0][520/683]	Time 0.405 (0.444)	Data 9.97e-05 (3.60e-04)	Tok/s 12276 (11459)	Loss/tok 5.2025 (7.0037)	LR 2.000e-03
0: TRAIN [0][530/683]	Time 0.330 (0.445)	Data 9.63e-05 (3.88e-04)	Tok/s 8924 (11462)	Loss/tok 4.6820 (6.9771)	LR 2.000e-03
1: TRAIN [0][530/683]	Time 0.330 (0.445)	Data 1.04e-04 (3.55e-04)	Tok/s 8754 (11467)	Loss/tok 4.8609 (6.9712)	LR 2.000e-03
0: TRAIN [0][540/683]	Time 0.407 (0.443)	Data 1.14e-04 (3.83e-04)	Tok/s 11956 (11442)	Loss/tok 4.9973 (6.9492)	LR 2.000e-03
1: TRAIN [0][540/683]	Time 0.407 (0.443)	Data 1.09e-04 (3.51e-04)	Tok/s 11816 (11447)	Loss/tok 4.9554 (6.9441)	LR 2.000e-03
0: TRAIN [0][550/683]	Time 0.328 (0.443)	Data 9.25e-05 (3.78e-04)	Tok/s 8958 (11418)	Loss/tok 4.6324 (6.9227)	LR 2.000e-03
1: TRAIN [0][550/683]	Time 0.328 (0.443)	Data 1.05e-04 (3.47e-04)	Tok/s 9117 (11425)	Loss/tok 4.8549 (6.9176)	LR 2.000e-03
0: TRAIN [0][560/683]	Time 0.408 (0.443)	Data 9.18e-05 (3.73e-04)	Tok/s 12002 (11422)	Loss/tok 4.9190 (6.8924)	LR 2.000e-03
1: TRAIN [0][560/683]	Time 0.408 (0.443)	Data 1.05e-04 (3.42e-04)	Tok/s 12054 (11429)	Loss/tok 5.0581 (6.8869)	LR 2.000e-03
0: TRAIN [0][570/683]	Time 0.325 (0.442)	Data 1.01e-04 (3.68e-04)	Tok/s 8529 (11419)	Loss/tok 4.4695 (6.8634)	LR 2.000e-03
1: TRAIN [0][570/683]	Time 0.325 (0.442)	Data 1.09e-04 (3.38e-04)	Tok/s 9124 (11427)	Loss/tok 4.7214 (6.8575)	LR 2.000e-03
0: TRAIN [0][580/683]	Time 0.330 (0.442)	Data 1.04e-04 (3.63e-04)	Tok/s 8973 (11414)	Loss/tok 4.6679 (6.8345)	LR 2.000e-03
1: TRAIN [0][580/683]	Time 0.330 (0.442)	Data 1.05e-04 (3.35e-04)	Tok/s 8857 (11421)	Loss/tok 4.7897 (6.8297)	LR 2.000e-03
0: TRAIN [0][590/683]	Time 0.417 (0.442)	Data 9.56e-05 (3.59e-04)	Tok/s 11754 (11415)	Loss/tok 4.9423 (6.8051)	LR 2.000e-03
1: TRAIN [0][590/683]	Time 0.418 (0.442)	Data 1.05e-04 (3.31e-04)	Tok/s 11729 (11421)	Loss/tok 5.0124 (6.7992)	LR 2.000e-03
0: TRAIN [0][600/683]	Time 0.326 (0.442)	Data 9.75e-05 (3.55e-04)	Tok/s 8936 (11416)	Loss/tok 4.5748 (6.7752)	LR 2.000e-03
1: TRAIN [0][600/683]	Time 0.325 (0.442)	Data 1.05e-04 (3.27e-04)	Tok/s 8834 (11421)	Loss/tok 4.7265 (6.7700)	LR 2.000e-03
0: TRAIN [0][610/683]	Time 0.416 (0.442)	Data 9.13e-05 (3.50e-04)	Tok/s 11768 (11408)	Loss/tok 4.8308 (6.7476)	LR 2.000e-03
1: TRAIN [0][610/683]	Time 0.416 (0.442)	Data 1.05e-04 (3.23e-04)	Tok/s 11880 (11412)	Loss/tok 4.8880 (6.7427)	LR 2.000e-03
0: TRAIN [0][620/683]	Time 0.259 (0.442)	Data 1.02e-04 (3.47e-04)	Tok/s 5535 (11400)	Loss/tok 4.1147 (6.7206)	LR 2.000e-03
1: TRAIN [0][620/683]	Time 0.258 (0.442)	Data 1.07e-04 (3.20e-04)	Tok/s 5649 (11405)	Loss/tok 4.2986 (6.7153)	LR 2.000e-03
0: TRAIN [0][630/683]	Time 0.410 (0.442)	Data 9.85e-05 (3.43e-04)	Tok/s 11816 (11403)	Loss/tok 4.8236 (6.6929)	LR 2.000e-03
1: TRAIN [0][630/683]	Time 0.410 (0.442)	Data 1.02e-04 (3.17e-04)	Tok/s 11826 (11407)	Loss/tok 4.9728 (6.6877)	LR 2.000e-03
0: TRAIN [0][640/683]	Time 0.419 (0.441)	Data 9.80e-05 (3.39e-04)	Tok/s 11631 (11398)	Loss/tok 4.7612 (6.6666)	LR 2.000e-03
1: TRAIN [0][640/683]	Time 0.419 (0.441)	Data 1.04e-04 (3.13e-04)	Tok/s 11484 (11402)	Loss/tok 4.8366 (6.6614)	LR 2.000e-03
0: TRAIN [0][650/683]	Time 0.327 (0.441)	Data 1.63e-04 (3.35e-04)	Tok/s 9390 (11398)	Loss/tok 4.4197 (6.6396)	LR 2.000e-03
1: TRAIN [0][650/683]	Time 0.326 (0.441)	Data 1.15e-04 (3.10e-04)	Tok/s 9035 (11402)	Loss/tok 4.4062 (6.6343)	LR 2.000e-03
0: TRAIN [0][660/683]	Time 0.418 (0.441)	Data 9.66e-05 (3.32e-04)	Tok/s 12024 (11399)	Loss/tok 4.6608 (6.6119)	LR 2.000e-03
1: TRAIN [0][660/683]	Time 0.418 (0.441)	Data 1.04e-04 (3.07e-04)	Tok/s 11756 (11403)	Loss/tok 4.5847 (6.6076)	LR 2.000e-03
0: TRAIN [0][670/683]	Time 0.258 (0.441)	Data 1.03e-04 (3.28e-04)	Tok/s 5619 (11392)	Loss/tok 4.0732 (6.5875)	LR 2.000e-03
1: TRAIN [0][670/683]	Time 0.257 (0.441)	Data 1.05e-04 (3.04e-04)	Tok/s 5870 (11395)	Loss/tok 4.0649 (6.5833)	LR 2.000e-03
0: TRAIN [0][680/683]	Time 0.639 (0.441)	Data 4.79e-05 (3.27e-04)	Tok/s 13968 (11392)	Loss/tok 5.1020 (6.5614)	LR 2.000e-03
1: TRAIN [0][680/683]	Time 0.639 (0.441)	Data 4.86e-05 (3.03e-04)	Tok/s 13667 (11394)	Loss/tok 5.1174 (6.5572)	LR 2.000e-03
0: Running validation on dev set
1: Running validation on dev set
0: Executing preallocation
1: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.116 (0.116)	Data 1.62e-03 (1.62e-03)	Tok/s 41005 (41005)	Loss/tok 6.2691 (6.2691)
0: VALIDATION [0][0/80]	Time 0.153 (0.153)	Data 1.62e-03 (1.62e-03)	Tok/s 37429 (37429)	Loss/tok 6.3774 (6.3774)
1: VALIDATION [0][10/80]	Time 0.066 (0.082)	Data 1.30e-03 (1.42e-03)	Tok/s 44115 (43120)	Loss/tok 5.9295 (6.1259)
0: VALIDATION [0][10/80]	Time 0.063 (0.083)	Data 1.29e-03 (1.39e-03)	Tok/s 46392 (44775)	Loss/tok 6.0056 (6.1155)
0: VALIDATION [0][20/80]	Time 0.052 (0.071)	Data 1.30e-03 (1.36e-03)	Tok/s 45255 (45056)	Loss/tok 5.6154 (6.0522)
1: VALIDATION [0][20/80]	Time 0.054 (0.071)	Data 1.34e-03 (1.38e-03)	Tok/s 42509 (43229)	Loss/tok 5.6931 (6.0376)
0: VALIDATION [0][30/80]	Time 0.043 (0.063)	Data 1.22e-03 (1.34e-03)	Tok/s 44908 (45274)	Loss/tok 5.6152 (5.9817)
1: VALIDATION [0][30/80]	Time 0.046 (0.064)	Data 1.26e-03 (1.37e-03)	Tok/s 41678 (43045)	Loss/tok 5.7647 (5.9608)
0: VALIDATION [0][40/80]	Time 0.037 (0.057)	Data 1.34e-03 (1.32e-03)	Tok/s 44125 (45087)	Loss/tok 5.6168 (5.9413)
1: VALIDATION [0][40/80]	Time 0.039 (0.059)	Data 1.35e-03 (1.36e-03)	Tok/s 40584 (42534)	Loss/tok 5.6734 (5.9036)
0: VALIDATION [0][50/80]	Time 0.030 (0.053)	Data 1.27e-03 (1.31e-03)	Tok/s 44304 (44819)	Loss/tok 5.7199 (5.8957)
1: VALIDATION [0][50/80]	Time 0.032 (0.054)	Data 1.25e-03 (1.34e-03)	Tok/s 41393 (42314)	Loss/tok 5.5498 (5.8690)
0: VALIDATION [0][60/80]	Time 0.026 (0.049)	Data 1.23e-03 (1.30e-03)	Tok/s 41210 (44443)	Loss/tok 5.6378 (5.8569)
1: VALIDATION [0][60/80]	Time 0.027 (0.050)	Data 1.23e-03 (1.33e-03)	Tok/s 39643 (41931)	Loss/tok 5.4214 (5.8332)
0: VALIDATION [0][70/80]	Time 0.021 (0.045)	Data 1.24e-03 (1.30e-03)	Tok/s 38058 (43871)	Loss/tok 5.3459 (5.8250)
1: VALIDATION [0][70/80]	Time 0.022 (0.046)	Data 1.30e-03 (1.33e-03)	Tok/s 36428 (41399)	Loss/tok 5.3557 (5.8000)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
0: TEST [0][9/47]	Time 0.8354 (1.0443)	Decoder iters 149.0 (149.0)	Tok/s 3955 (4471)
1: TEST [0][9/47]	Time 0.8376 (1.0445)	Decoder iters 149.0 (149.0)	Tok/s 4180 (4402)
0: TEST [0][19/47]	Time 0.5265 (0.8679)	Decoder iters 123.0 (147.7)	Tok/s 4247 (4351)
1: TEST [0][19/47]	Time 0.5264 (0.8679)	Decoder iters 103.0 (144.6)	Tok/s 4104 (4195)
0: TEST [0][29/47]	Time 0.5339 (0.7678)	Decoder iters 149.0 (145.4)	Tok/s 3161 (4068)
1: TEST [0][29/47]	Time 0.5337 (0.7678)	Decoder iters 149.0 (146.0)	Tok/s 3399 (3970)
0: TEST [0][39/47]	Time 0.4545 (0.6878)	Decoder iters 55.0 (138.8)	Tok/s 2271 (3813)
1: TEST [0][39/47]	Time 0.4545 (0.6877)	Decoder iters 149.0 (130.9)	Tok/s 2271 (3733)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.5530	Validation Loss: 5.7797	Test BLEU: 2.64
0: Performance: Epoch: 0	Training: 22795 Tok/s	Validation: 83496 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
0: TRAIN [1][0/683]	Time 0.628 (0.628)	Data 1.42e-01 (1.42e-01)	Tok/s 10727 (10727)	Loss/tok 4.7758 (4.7758)	LR 2.000e-03
1: TRAIN [1][0/683]	Time 0.621 (0.621)	Data 1.38e-01 (1.38e-01)	Tok/s 10827 (10827)	Loss/tok 4.7531 (4.7531)	LR 2.000e-03
0: TRAIN [1][10/683]	Time 0.319 (0.488)	Data 9.70e-05 (1.30e-02)	Tok/s 9170 (12592)	Loss/tok 3.9892 (4.6713)	LR 2.000e-03
1: TRAIN [1][10/683]	Time 0.319 (0.487)	Data 9.94e-05 (1.26e-02)	Tok/s 9426 (12673)	Loss/tok 4.0844 (4.6919)	LR 2.000e-03
0: TRAIN [1][20/683]	Time 0.405 (0.483)	Data 9.68e-05 (6.85e-03)	Tok/s 11935 (12727)	Loss/tok 4.4584 (4.6658)	LR 2.000e-03
1: TRAIN [1][20/683]	Time 0.405 (0.483)	Data 9.75e-05 (6.66e-03)	Tok/s 12043 (12788)	Loss/tok 4.4287 (4.6609)	LR 2.000e-03
0: TRAIN [1][30/683]	Time 0.521 (0.483)	Data 1.03e-04 (4.67e-03)	Tok/s 13098 (12484)	Loss/tok 4.5272 (4.6662)	LR 2.000e-03
1: TRAIN [1][30/683]	Time 0.521 (0.483)	Data 1.13e-04 (4.54e-03)	Tok/s 13217 (12523)	Loss/tok 4.7774 (4.6753)	LR 2.000e-03
0: TRAIN [1][40/683]	Time 0.321 (0.468)	Data 9.37e-05 (3.56e-03)	Tok/s 9519 (12146)	Loss/tok 3.9209 (4.6317)	LR 2.000e-03
1: TRAIN [1][40/683]	Time 0.321 (0.467)	Data 9.94e-05 (3.46e-03)	Tok/s 8978 (12165)	Loss/tok 4.0355 (4.6265)	LR 2.000e-03
0: TRAIN [1][50/683]	Time 0.513 (0.466)	Data 1.26e-04 (2.88e-03)	Tok/s 13295 (12103)	Loss/tok 4.5840 (4.6211)	LR 2.000e-03
1: TRAIN [1][50/683]	Time 0.513 (0.466)	Data 8.75e-05 (2.80e-03)	Tok/s 13103 (12104)	Loss/tok 4.4167 (4.6042)	LR 2.000e-03
0: TRAIN [1][60/683]	Time 0.258 (0.462)	Data 9.92e-05 (2.43e-03)	Tok/s 5784 (11982)	Loss/tok 3.6078 (4.6074)	LR 2.000e-03
1: TRAIN [1][60/683]	Time 0.258 (0.462)	Data 1.03e-04 (2.36e-03)	Tok/s 5599 (11987)	Loss/tok 3.8160 (4.5932)	LR 2.000e-03
0: TRAIN [1][70/683]	Time 0.522 (0.464)	Data 1.64e-04 (2.10e-03)	Tok/s 12943 (12008)	Loss/tok 4.6955 (4.5934)	LR 2.000e-03
1: TRAIN [1][70/683]	Time 0.523 (0.464)	Data 1.20e-04 (2.05e-03)	Tok/s 12914 (12015)	Loss/tok 4.5191 (4.5728)	LR 2.000e-03
0: TRAIN [1][80/683]	Time 0.414 (0.462)	Data 1.03e-04 (1.85e-03)	Tok/s 11719 (11961)	Loss/tok 4.3779 (4.5826)	LR 2.000e-03
1: TRAIN [1][80/683]	Time 0.414 (0.462)	Data 1.13e-04 (1.81e-03)	Tok/s 12077 (11980)	Loss/tok 4.3173 (4.5665)	LR 2.000e-03
0: TRAIN [1][90/683]	Time 0.415 (0.458)	Data 1.03e-04 (1.66e-03)	Tok/s 11788 (11865)	Loss/tok 4.0749 (4.5599)	LR 2.000e-03
1: TRAIN [1][90/683]	Time 0.415 (0.458)	Data 1.13e-04 (1.62e-03)	Tok/s 11607 (11887)	Loss/tok 4.3124 (4.5504)	LR 2.000e-03
0: TRAIN [1][100/683]	Time 0.415 (0.454)	Data 1.10e-04 (1.51e-03)	Tok/s 11642 (11825)	Loss/tok 4.2434 (4.5367)	LR 2.000e-03
1: TRAIN [1][100/683]	Time 0.415 (0.454)	Data 1.07e-04 (1.47e-03)	Tok/s 11565 (11843)	Loss/tok 4.4219 (4.5313)	LR 2.000e-03
0: TRAIN [1][110/683]	Time 0.324 (0.446)	Data 9.47e-05 (1.38e-03)	Tok/s 9222 (11691)	Loss/tok 3.9889 (4.5105)	LR 2.000e-03
1: TRAIN [1][110/683]	Time 0.324 (0.446)	Data 1.08e-04 (1.35e-03)	Tok/s 8962 (11706)	Loss/tok 3.9481 (4.5089)	LR 2.000e-03
0: TRAIN [1][120/683]	Time 0.529 (0.447)	Data 9.78e-05 (1.27e-03)	Tok/s 12967 (11704)	Loss/tok 4.6004 (4.5042)	LR 2.000e-03
1: TRAIN [1][120/683]	Time 0.529 (0.447)	Data 1.14e-04 (1.25e-03)	Tok/s 12850 (11708)	Loss/tok 4.5577 (4.5027)	LR 2.000e-03
0: TRAIN [1][130/683]	Time 0.508 (0.445)	Data 1.08e-04 (1.18e-03)	Tok/s 13208 (11681)	Loss/tok 4.5862 (4.4927)	LR 2.000e-03
1: TRAIN [1][130/683]	Time 0.508 (0.445)	Data 1.13e-04 (1.16e-03)	Tok/s 13320 (11684)	Loss/tok 4.5407 (4.4914)	LR 2.000e-03
0: TRAIN [1][140/683]	Time 0.327 (0.447)	Data 1.03e-04 (1.11e-03)	Tok/s 9282 (11697)	Loss/tok 3.9943 (4.4853)	LR 2.000e-03
1: TRAIN [1][140/683]	Time 0.327 (0.447)	Data 1.16e-04 (1.09e-03)	Tok/s 8921 (11695)	Loss/tok 3.6166 (4.4852)	LR 2.000e-03
0: TRAIN [1][150/683]	Time 0.403 (0.448)	Data 1.02e-04 (1.04e-03)	Tok/s 12078 (11701)	Loss/tok 4.2298 (4.4811)	LR 2.000e-03
1: TRAIN [1][150/683]	Time 0.403 (0.448)	Data 1.16e-04 (1.02e-03)	Tok/s 12029 (11699)	Loss/tok 4.1606 (4.4799)	LR 2.000e-03
0: TRAIN [1][160/683]	Time 0.531 (0.448)	Data 1.00e-04 (9.83e-04)	Tok/s 12758 (11700)	Loss/tok 4.5020 (4.4770)	LR 2.000e-03
1: TRAIN [1][160/683]	Time 0.531 (0.448)	Data 1.08e-04 (9.64e-04)	Tok/s 12624 (11696)	Loss/tok 4.3171 (4.4698)	LR 2.000e-03
0: TRAIN [1][170/683]	Time 0.520 (0.447)	Data 1.06e-04 (9.31e-04)	Tok/s 13128 (11679)	Loss/tok 4.6016 (4.4693)	LR 2.000e-03
1: TRAIN [1][170/683]	Time 0.520 (0.447)	Data 1.15e-04 (9.15e-04)	Tok/s 13018 (11671)	Loss/tok 4.5185 (4.4600)	LR 2.000e-03
0: TRAIN [1][180/683]	Time 0.526 (0.451)	Data 9.75e-05 (8.85e-04)	Tok/s 12992 (11714)	Loss/tok 4.4550 (4.4711)	LR 2.000e-03
1: TRAIN [1][180/683]	Time 0.526 (0.451)	Data 1.00e-04 (8.70e-04)	Tok/s 12878 (11706)	Loss/tok 4.4613 (4.4631)	LR 2.000e-03
0: TRAIN [1][190/683]	Time 0.323 (0.452)	Data 1.16e-04 (8.45e-04)	Tok/s 8930 (11737)	Loss/tok 3.8668 (4.4653)	LR 2.000e-03
1: TRAIN [1][190/683]	Time 0.323 (0.452)	Data 1.05e-04 (8.30e-04)	Tok/s 9037 (11732)	Loss/tok 3.7988 (4.4553)	LR 2.000e-03
0: TRAIN [1][200/683]	Time 0.534 (0.452)	Data 9.27e-05 (8.08e-04)	Tok/s 12654 (11727)	Loss/tok 4.4234 (4.4574)	LR 2.000e-03
1: TRAIN [1][200/683]	Time 0.534 (0.452)	Data 1.58e-04 (7.95e-04)	Tok/s 12561 (11723)	Loss/tok 4.3009 (4.4489)	LR 2.000e-03
0: TRAIN [1][210/683]	Time 0.257 (0.448)	Data 1.14e-04 (7.75e-04)	Tok/s 5672 (11630)	Loss/tok 3.8013 (4.4442)	LR 2.000e-03
1: TRAIN [1][210/683]	Time 0.256 (0.448)	Data 1.09e-04 (7.64e-04)	Tok/s 5833 (11635)	Loss/tok 3.7433 (4.4361)	LR 2.000e-03
0: TRAIN [1][220/683]	Time 0.415 (0.446)	Data 1.12e-04 (7.45e-04)	Tok/s 11627 (11588)	Loss/tok 4.1967 (4.4343)	LR 2.000e-03
1: TRAIN [1][220/683]	Time 0.415 (0.446)	Data 1.00e-04 (7.34e-04)	Tok/s 11694 (11594)	Loss/tok 4.1360 (4.4269)	LR 2.000e-03
0: TRAIN [1][230/683]	Time 0.317 (0.445)	Data 1.11e-04 (7.17e-04)	Tok/s 8942 (11582)	Loss/tok 3.8366 (4.4259)	LR 1.000e-03
1: TRAIN [1][230/683]	Time 0.317 (0.445)	Data 1.39e-04 (7.08e-04)	Tok/s 9153 (11589)	Loss/tok 3.7760 (4.4197)	LR 1.000e-03
0: TRAIN [1][240/683]	Time 0.627 (0.447)	Data 1.23e-04 (6.92e-04)	Tok/s 14016 (11579)	Loss/tok 4.6437 (4.4228)	LR 1.000e-03
1: TRAIN [1][240/683]	Time 0.627 (0.447)	Data 1.29e-04 (6.83e-04)	Tok/s 14042 (11588)	Loss/tok 4.5388 (4.4171)	LR 1.000e-03
1: TRAIN [1][250/683]	Time 0.416 (0.448)	Data 1.39e-04 (6.61e-04)	Tok/s 11818 (11616)	Loss/tok 3.9056 (4.4075)	LR 1.000e-03
0: TRAIN [1][250/683]	Time 0.419 (0.448)	Data 1.10e-04 (6.68e-04)	Tok/s 11877 (11609)	Loss/tok 4.1152 (4.4163)	LR 1.000e-03
0: TRAIN [1][260/683]	Time 0.524 (0.448)	Data 1.11e-04 (6.47e-04)	Tok/s 12962 (11606)	Loss/tok 4.3372 (4.4064)	LR 1.000e-03
1: TRAIN [1][260/683]	Time 0.524 (0.448)	Data 1.05e-04 (6.40e-04)	Tok/s 12895 (11613)	Loss/tok 4.2074 (4.3962)	LR 1.000e-03
0: TRAIN [1][270/683]	Time 0.415 (0.448)	Data 1.11e-04 (6.27e-04)	Tok/s 11637 (11597)	Loss/tok 4.0218 (4.3961)	LR 1.000e-03
1: TRAIN [1][270/683]	Time 0.415 (0.448)	Data 1.16e-04 (6.21e-04)	Tok/s 11680 (11606)	Loss/tok 3.8965 (4.3872)	LR 1.000e-03
0: TRAIN [1][280/683]	Time 0.320 (0.447)	Data 1.25e-04 (6.08e-04)	Tok/s 8856 (11575)	Loss/tok 3.7516 (4.3871)	LR 1.000e-03
1: TRAIN [1][280/683]	Time 0.320 (0.447)	Data 1.07e-04 (6.02e-04)	Tok/s 9346 (11586)	Loss/tok 3.7857 (4.3793)	LR 1.000e-03
0: TRAIN [1][290/683]	Time 0.518 (0.446)	Data 1.02e-04 (5.91e-04)	Tok/s 13020 (11564)	Loss/tok 4.2471 (4.3770)	LR 1.000e-03
1: TRAIN [1][290/683]	Time 0.518 (0.446)	Data 1.13e-04 (5.86e-04)	Tok/s 13034 (11577)	Loss/tok 4.2174 (4.3684)	LR 1.000e-03
0: TRAIN [1][300/683]	Time 0.632 (0.445)	Data 1.03e-04 (5.75e-04)	Tok/s 13855 (11530)	Loss/tok 4.3297 (4.3705)	LR 1.000e-03
1: TRAIN [1][300/683]	Time 0.633 (0.445)	Data 1.11e-04 (5.70e-04)	Tok/s 13971 (11544)	Loss/tok 4.5129 (4.3620)	LR 1.000e-03
0: TRAIN [1][310/683]	Time 0.331 (0.444)	Data 1.12e-04 (5.60e-04)	Tok/s 9115 (11504)	Loss/tok 3.4722 (4.3615)	LR 1.000e-03
1: TRAIN [1][310/683]	Time 0.331 (0.444)	Data 1.16e-04 (5.55e-04)	Tok/s 8903 (11516)	Loss/tok 3.7049 (4.3537)	LR 1.000e-03
0: TRAIN [1][320/683]	Time 0.418 (0.443)	Data 1.01e-04 (5.46e-04)	Tok/s 11743 (11485)	Loss/tok 3.8308 (4.3513)	LR 1.000e-03
1: TRAIN [1][320/683]	Time 0.418 (0.443)	Data 1.23e-04 (5.42e-04)	Tok/s 11793 (11501)	Loss/tok 4.0060 (4.3432)	LR 1.000e-03
0: TRAIN [1][330/683]	Time 0.418 (0.441)	Data 9.94e-05 (5.32e-04)	Tok/s 11621 (11435)	Loss/tok 3.9283 (4.3408)	LR 1.000e-03
1: TRAIN [1][330/683]	Time 0.418 (0.441)	Data 1.52e-04 (5.29e-04)	Tok/s 11754 (11451)	Loss/tok 4.0022 (4.3313)	LR 1.000e-03
0: TRAIN [1][340/683]	Time 0.419 (0.440)	Data 1.06e-04 (5.19e-04)	Tok/s 11622 (11419)	Loss/tok 3.9351 (4.3328)	LR 5.000e-04
1: TRAIN [1][340/683]	Time 0.419 (0.440)	Data 1.16e-04 (5.17e-04)	Tok/s 11675 (11439)	Loss/tok 3.8696 (4.3226)	LR 5.000e-04
0: TRAIN [1][350/683]	Time 0.619 (0.441)	Data 1.01e-04 (5.07e-04)	Tok/s 14373 (11443)	Loss/tok 4.2780 (4.3253)	LR 5.000e-04
1: TRAIN [1][350/683]	Time 0.619 (0.441)	Data 1.81e-04 (5.05e-04)	Tok/s 14328 (11462)	Loss/tok 4.3900 (4.3194)	LR 5.000e-04
0: TRAIN [1][360/683]	Time 0.323 (0.441)	Data 9.87e-05 (4.96e-04)	Tok/s 9428 (11436)	Loss/tok 3.5288 (4.3167)	LR 5.000e-04
1: TRAIN [1][360/683]	Time 0.323 (0.441)	Data 1.27e-04 (4.95e-04)	Tok/s 8832 (11453)	Loss/tok 3.6680 (4.3096)	LR 5.000e-04
0: TRAIN [1][370/683]	Time 0.417 (0.440)	Data 1.06e-04 (4.86e-04)	Tok/s 11581 (11429)	Loss/tok 3.8581 (4.3074)	LR 5.000e-04
1: TRAIN [1][370/683]	Time 0.417 (0.440)	Data 1.06e-04 (4.85e-04)	Tok/s 11714 (11445)	Loss/tok 3.8213 (4.2996)	LR 5.000e-04
0: TRAIN [1][380/683]	Time 0.518 (0.441)	Data 9.80e-05 (4.76e-04)	Tok/s 13017 (11453)	Loss/tok 4.0763 (4.3002)	LR 5.000e-04
1: TRAIN [1][380/683]	Time 0.518 (0.441)	Data 1.10e-04 (4.75e-04)	Tok/s 13125 (11467)	Loss/tok 4.0950 (4.2942)	LR 5.000e-04
0: TRAIN [1][390/683]	Time 0.520 (0.441)	Data 1.07e-04 (4.66e-04)	Tok/s 13389 (11454)	Loss/tok 3.9970 (4.2915)	LR 5.000e-04
1: TRAIN [1][390/683]	Time 0.520 (0.441)	Data 1.16e-04 (4.66e-04)	Tok/s 13094 (11471)	Loss/tok 4.0126 (4.2860)	LR 5.000e-04
0: TRAIN [1][400/683]	Time 0.613 (0.440)	Data 1.09e-04 (4.57e-04)	Tok/s 14201 (11447)	Loss/tok 4.2375 (4.2830)	LR 5.000e-04
1: TRAIN [1][400/683]	Time 0.613 (0.440)	Data 1.29e-04 (4.57e-04)	Tok/s 14243 (11464)	Loss/tok 4.2761 (4.2786)	LR 5.000e-04
0: TRAIN [1][410/683]	Time 0.508 (0.440)	Data 1.11e-04 (4.49e-04)	Tok/s 13410 (11448)	Loss/tok 4.0444 (4.2758)	LR 5.000e-04
1: TRAIN [1][410/683]	Time 0.508 (0.440)	Data 1.22e-04 (4.49e-04)	Tok/s 13433 (11466)	Loss/tok 3.9169 (4.2710)	LR 5.000e-04
0: TRAIN [1][420/683]	Time 0.328 (0.440)	Data 1.16e-04 (4.41e-04)	Tok/s 8859 (11429)	Loss/tok 3.5900 (4.2685)	LR 5.000e-04
1: TRAIN [1][420/683]	Time 0.328 (0.440)	Data 1.14e-04 (4.41e-04)	Tok/s 9026 (11447)	Loss/tok 3.5518 (4.2651)	LR 5.000e-04
0: TRAIN [1][430/683]	Time 0.414 (0.440)	Data 1.09e-04 (4.33e-04)	Tok/s 11734 (11420)	Loss/tok 3.7219 (4.2605)	LR 5.000e-04
1: TRAIN [1][430/683]	Time 0.414 (0.440)	Data 1.12e-04 (4.33e-04)	Tok/s 11935 (11435)	Loss/tok 3.7584 (4.2570)	LR 5.000e-04
0: TRAIN [1][440/683]	Time 0.328 (0.439)	Data 1.07e-04 (4.25e-04)	Tok/s 9194 (11405)	Loss/tok 3.7864 (4.2558)	LR 5.000e-04
1: TRAIN [1][440/683]	Time 0.328 (0.439)	Data 9.94e-05 (4.26e-04)	Tok/s 8847 (11420)	Loss/tok 3.6191 (4.2513)	LR 5.000e-04
0: TRAIN [1][450/683]	Time 0.405 (0.439)	Data 1.11e-04 (4.18e-04)	Tok/s 12072 (11395)	Loss/tok 3.9366 (4.2490)	LR 5.000e-04
1: TRAIN [1][450/683]	Time 0.405 (0.439)	Data 1.19e-04 (4.19e-04)	Tok/s 12058 (11407)	Loss/tok 3.7475 (4.2451)	LR 5.000e-04
0: TRAIN [1][460/683]	Time 0.254 (0.439)	Data 1.06e-04 (4.12e-04)	Tok/s 5652 (11396)	Loss/tok 3.2487 (4.2441)	LR 2.500e-04
1: TRAIN [1][460/683]	Time 0.254 (0.439)	Data 1.46e-04 (4.13e-04)	Tok/s 5604 (11409)	Loss/tok 3.3898 (4.2390)	LR 2.500e-04
0: TRAIN [1][470/683]	Time 0.323 (0.440)	Data 1.20e-04 (4.05e-04)	Tok/s 9129 (11408)	Loss/tok 3.4425 (4.2372)	LR 2.500e-04
1: TRAIN [1][470/683]	Time 0.322 (0.440)	Data 1.60e-04 (4.07e-04)	Tok/s 9387 (11420)	Loss/tok 3.5526 (4.2321)	LR 2.500e-04
0: TRAIN [1][480/683]	Time 0.413 (0.439)	Data 1.02e-04 (3.99e-04)	Tok/s 11731 (11405)	Loss/tok 3.7734 (4.2300)	LR 2.500e-04
1: TRAIN [1][480/683]	Time 0.413 (0.439)	Data 1.50e-04 (4.02e-04)	Tok/s 12002 (11415)	Loss/tok 3.7228 (4.2248)	LR 2.500e-04
0: TRAIN [1][490/683]	Time 0.321 (0.439)	Data 1.58e-04 (3.93e-04)	Tok/s 9149 (11399)	Loss/tok 3.5779 (4.2244)	LR 2.500e-04
1: TRAIN [1][490/683]	Time 0.321 (0.439)	Data 1.59e-04 (3.96e-04)	Tok/s 9258 (11407)	Loss/tok 3.4783 (4.2194)	LR 2.500e-04
0: TRAIN [1][500/683]	Time 0.327 (0.439)	Data 1.08e-04 (3.87e-04)	Tok/s 9253 (11400)	Loss/tok 3.6126 (4.2197)	LR 2.500e-04
1: TRAIN [1][500/683]	Time 0.326 (0.439)	Data 1.76e-04 (3.91e-04)	Tok/s 9218 (11410)	Loss/tok 3.5498 (4.2136)	LR 2.500e-04
0: TRAIN [1][510/683]	Time 0.410 (0.439)	Data 1.13e-04 (3.82e-04)	Tok/s 12150 (11395)	Loss/tok 3.8011 (4.2140)	LR 2.500e-04
1: TRAIN [1][510/683]	Time 0.410 (0.439)	Data 1.69e-04 (3.86e-04)	Tok/s 12024 (11402)	Loss/tok 3.9084 (4.2082)	LR 2.500e-04
0: TRAIN [1][520/683]	Time 0.520 (0.440)	Data 1.03e-04 (3.76e-04)	Tok/s 13146 (11405)	Loss/tok 4.0493 (4.2088)	LR 2.500e-04
1: TRAIN [1][520/683]	Time 0.519 (0.440)	Data 1.63e-04 (3.82e-04)	Tok/s 12920 (11413)	Loss/tok 3.9457 (4.2025)	LR 2.500e-04
0: TRAIN [1][530/683]	Time 0.636 (0.439)	Data 1.00e-04 (3.71e-04)	Tok/s 13760 (11405)	Loss/tok 4.2345 (4.2021)	LR 2.500e-04
1: TRAIN [1][530/683]	Time 0.636 (0.439)	Data 1.65e-04 (3.77e-04)	Tok/s 13990 (11413)	Loss/tok 4.1696 (4.1972)	LR 2.500e-04
0: TRAIN [1][540/683]	Time 0.412 (0.438)	Data 9.97e-05 (3.66e-04)	Tok/s 12069 (11391)	Loss/tok 3.7826 (4.1962)	LR 2.500e-04
1: TRAIN [1][540/683]	Time 0.412 (0.438)	Data 1.54e-04 (3.73e-04)	Tok/s 11856 (11400)	Loss/tok 3.8617 (4.1910)	LR 2.500e-04
0: TRAIN [1][550/683]	Time 0.526 (0.440)	Data 1.03e-04 (3.61e-04)	Tok/s 12865 (11418)	Loss/tok 3.9843 (4.1926)	LR 2.500e-04
1: TRAIN [1][550/683]	Time 0.526 (0.440)	Data 1.56e-04 (3.68e-04)	Tok/s 12916 (11426)	Loss/tok 4.1254 (4.1876)	LR 2.500e-04
0: TRAIN [1][560/683]	Time 0.418 (0.440)	Data 9.94e-05 (3.57e-04)	Tok/s 11467 (11428)	Loss/tok 3.7764 (4.1876)	LR 2.500e-04
1: TRAIN [1][560/683]	Time 0.418 (0.440)	Data 1.17e-04 (3.64e-04)	Tok/s 11554 (11438)	Loss/tok 3.6986 (4.1818)	LR 2.500e-04
0: TRAIN [1][570/683]	Time 0.328 (0.440)	Data 9.80e-05 (3.52e-04)	Tok/s 9158 (11432)	Loss/tok 3.6964 (4.1817)	LR 1.250e-04
1: TRAIN [1][570/683]	Time 0.328 (0.440)	Data 1.09e-04 (3.60e-04)	Tok/s 8749 (11439)	Loss/tok 3.5286 (4.1755)	LR 1.250e-04
0: TRAIN [1][580/683]	Time 0.252 (0.440)	Data 9.97e-05 (3.48e-04)	Tok/s 5860 (11426)	Loss/tok 3.1425 (4.1769)	LR 1.250e-04
1: TRAIN [1][580/683]	Time 0.252 (0.440)	Data 1.18e-04 (3.55e-04)	Tok/s 5764 (11433)	Loss/tok 3.2466 (4.1708)	LR 1.250e-04
0: TRAIN [1][590/683]	Time 0.520 (0.439)	Data 9.51e-05 (3.44e-04)	Tok/s 12915 (11407)	Loss/tok 4.0880 (4.1709)	LR 1.250e-04
1: TRAIN [1][590/683]	Time 0.520 (0.439)	Data 1.15e-04 (3.51e-04)	Tok/s 13240 (11414)	Loss/tok 3.9027 (4.1653)	LR 1.250e-04
0: TRAIN [1][600/683]	Time 0.640 (0.440)	Data 1.02e-04 (3.40e-04)	Tok/s 13764 (11417)	Loss/tok 4.1800 (4.1679)	LR 1.250e-04
1: TRAIN [1][600/683]	Time 0.640 (0.440)	Data 1.17e-04 (3.47e-04)	Tok/s 13843 (11425)	Loss/tok 4.0189 (4.1614)	LR 1.250e-04
0: TRAIN [1][610/683]	Time 0.643 (0.439)	Data 1.02e-04 (3.36e-04)	Tok/s 13640 (11410)	Loss/tok 4.1647 (4.1630)	LR 1.250e-04
1: TRAIN [1][610/683]	Time 0.644 (0.439)	Data 1.24e-04 (3.44e-04)	Tok/s 13777 (11418)	Loss/tok 4.1323 (4.1565)	LR 1.250e-04
0: TRAIN [1][620/683]	Time 0.254 (0.440)	Data 1.01e-04 (3.32e-04)	Tok/s 5526 (11411)	Loss/tok 2.9586 (4.1601)	LR 1.250e-04
1: TRAIN [1][620/683]	Time 0.254 (0.440)	Data 1.06e-04 (3.40e-04)	Tok/s 5817 (11420)	Loss/tok 3.1334 (4.1530)	LR 1.250e-04
0: TRAIN [1][630/683]	Time 0.407 (0.440)	Data 1.00e-04 (3.28e-04)	Tok/s 12151 (11421)	Loss/tok 3.7304 (4.1555)	LR 1.250e-04
1: TRAIN [1][630/683]	Time 0.407 (0.440)	Data 9.61e-05 (3.37e-04)	Tok/s 11938 (11428)	Loss/tok 3.8312 (4.1489)	LR 1.250e-04
0: TRAIN [1][640/683]	Time 0.405 (0.440)	Data 1.01e-04 (3.25e-04)	Tok/s 11996 (11430)	Loss/tok 3.7878 (4.1504)	LR 1.250e-04
1: TRAIN [1][640/683]	Time 0.405 (0.440)	Data 1.14e-04 (3.33e-04)	Tok/s 11994 (11436)	Loss/tok 3.8150 (4.1442)	LR 1.250e-04
0: TRAIN [1][650/683]	Time 0.411 (0.441)	Data 9.58e-05 (3.22e-04)	Tok/s 12207 (11447)	Loss/tok 3.9138 (4.1463)	LR 1.250e-04
1: TRAIN [1][650/683]	Time 0.407 (0.441)	Data 1.73e-04 (3.30e-04)	Tok/s 11682 (11453)	Loss/tok 3.6901 (4.1402)	LR 1.250e-04
0: TRAIN [1][660/683]	Time 0.406 (0.440)	Data 1.06e-04 (3.18e-04)	Tok/s 11934 (11430)	Loss/tok 3.7389 (4.1425)	LR 1.250e-04
1: TRAIN [1][660/683]	Time 0.406 (0.440)	Data 1.05e-04 (3.27e-04)	Tok/s 11677 (11436)	Loss/tok 3.8064 (4.1371)	LR 1.250e-04
0: TRAIN [1][670/683]	Time 0.632 (0.440)	Data 9.92e-05 (3.15e-04)	Tok/s 14266 (11428)	Loss/tok 4.1610 (4.1388)	LR 1.250e-04
1: TRAIN [1][670/683]	Time 0.633 (0.440)	Data 1.13e-04 (3.24e-04)	Tok/s 13814 (11433)	Loss/tok 4.2212 (4.1333)	LR 1.250e-04
0: TRAIN [1][680/683]	Time 0.413 (0.440)	Data 4.20e-05 (3.13e-04)	Tok/s 11749 (11422)	Loss/tok 3.6592 (4.1353)	LR 1.250e-04
1: TRAIN [1][680/683]	Time 0.413 (0.440)	Data 4.77e-05 (3.22e-04)	Tok/s 11770 (11428)	Loss/tok 3.6477 (4.1297)	LR 1.250e-04
0: Running validation on dev set
1: Running validation on dev set
0: Executing preallocation
1: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.117 (0.117)	Data 1.73e-03 (1.73e-03)	Tok/s 40749 (40749)	Loss/tok 5.4260 (5.4260)
0: VALIDATION [1][0/80]	Time 0.152 (0.152)	Data 1.57e-03 (1.57e-03)	Tok/s 37664 (37664)	Loss/tok 5.5675 (5.5675)
1: VALIDATION [1][10/80]	Time 0.067 (0.081)	Data 1.31e-03 (1.45e-03)	Tok/s 43602 (43187)	Loss/tok 4.9464 (5.2188)
0: VALIDATION [1][10/80]	Time 0.062 (0.083)	Data 1.28e-03 (1.38e-03)	Tok/s 47035 (44751)	Loss/tok 5.0116 (5.2026)
0: VALIDATION [1][20/80]	Time 0.051 (0.071)	Data 1.37e-03 (1.38e-03)	Tok/s 45500 (45018)	Loss/tok 4.6974 (5.1439)
1: VALIDATION [1][20/80]	Time 0.055 (0.071)	Data 1.36e-03 (1.42e-03)	Tok/s 41965 (43149)	Loss/tok 4.7286 (5.1203)
0: VALIDATION [1][30/80]	Time 0.044 (0.063)	Data 1.30e-03 (1.35e-03)	Tok/s 44655 (45171)	Loss/tok 4.7448 (5.0776)
1: VALIDATION [1][30/80]	Time 0.045 (0.064)	Data 1.34e-03 (1.40e-03)	Tok/s 42841 (43088)	Loss/tok 4.8619 (5.0467)
0: VALIDATION [1][40/80]	Time 0.037 (0.057)	Data 1.29e-03 (1.34e-03)	Tok/s 44388 (44976)	Loss/tok 4.6359 (5.0419)
1: VALIDATION [1][40/80]	Time 0.039 (0.059)	Data 1.28e-03 (1.38e-03)	Tok/s 41538 (42577)	Loss/tok 4.7665 (4.9903)
0: VALIDATION [1][50/80]	Time 0.031 (0.053)	Data 1.28e-03 (1.33e-03)	Tok/s 43561 (44683)	Loss/tok 4.9167 (5.0006)
1: VALIDATION [1][50/80]	Time 0.032 (0.054)	Data 1.32e-03 (1.37e-03)	Tok/s 41191 (42371)	Loss/tok 4.8141 (4.9645)
0: VALIDATION [1][60/80]	Time 0.026 (0.049)	Data 1.27e-03 (1.33e-03)	Tok/s 41186 (44315)	Loss/tok 4.6296 (4.9651)
1: VALIDATION [1][60/80]	Time 0.027 (0.050)	Data 1.31e-03 (1.37e-03)	Tok/s 39224 (42021)	Loss/tok 4.6072 (4.9373)
0: VALIDATION [1][70/80]	Time 0.021 (0.045)	Data 1.25e-03 (1.32e-03)	Tok/s 37747 (43733)	Loss/tok 4.5515 (4.9414)
1: VALIDATION [1][70/80]	Time 0.022 (0.046)	Data 1.29e-03 (1.36e-03)	Tok/s 36596 (41515)	Loss/tok 4.5498 (4.9094)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
0: TEST [1][9/47]	Time 0.6662 (0.7591)	Decoder iters 149.0 (149.0)	Tok/s 4302 (4628)
1: TEST [1][9/47]	Time 0.6660 (0.7590)	Decoder iters 149.0 (146.2)	Tok/s 4079 (4514)
0: TEST [1][19/47]	Time 0.5435 (0.6549)	Decoder iters 149.0 (141.9)	Tok/s 3586 (4452)
1: TEST [1][19/47]	Time 0.5425 (0.6549)	Decoder iters 149.0 (123.0)	Tok/s 3957 (4311)
0: TEST [1][29/47]	Time 0.4921 (0.5902)	Decoder iters 149.0 (125.1)	Tok/s 3229 (4331)
1: TEST [1][29/47]	Time 0.4924 (0.5901)	Decoder iters 149.0 (117.9)	Tok/s 2949 (4218)
0: TEST [1][39/47]	Time 0.1743 (0.5310)	Decoder iters 38.0 (114.7)	Tok/s 6008 (4297)
1: TEST [1][39/47]	Time 0.1741 (0.5310)	Decoder iters 30.0 (105.2)	Tok/s 5883 (4180)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.1313	Validation Loss: 4.8987	Test BLEU: 7.60
0: Performance: Epoch: 1	Training: 22844 Tok/s	Validation: 83425 Tok/s
0: Finished epoch 1
1: Total training time 693 s
0: Total training time 693 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 108|                       7.6|                      22819.4|                         11.55|
DONE!
