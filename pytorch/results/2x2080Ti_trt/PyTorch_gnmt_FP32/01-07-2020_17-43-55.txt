1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
1: Worker 1 is using worker seed: 364522461
0: Worker 0 is using worker seed: 242886303
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
0: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 909
0: Scheduler decay interval: 114
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 909
1: Scheduler decay interval: 114
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
1: TRAIN [0][0/683]	Time 0.605 (0.605)	Data 1.44e-01 (1.44e-01)	Tok/s 11194 (11194)	Loss/tok 10.6986 (10.6986)	LR 2.047e-05
0: TRAIN [0][0/683]	Time 0.634 (0.634)	Data 1.43e-01 (1.43e-01)	Tok/s 10724 (10724)	Loss/tok 10.7036 (10.7036)	LR 2.047e-05
1: TRAIN [0][10/683]	Time 0.384 (0.390)	Data 7.87e-05 (1.31e-02)	Tok/s 12463 (11253)	Loss/tok 9.6614 (10.1808)	LR 2.576e-05
0: TRAIN [0][10/683]	Time 0.384 (0.393)	Data 9.61e-05 (1.31e-02)	Tok/s 12836 (11253)	Loss/tok 9.7140 (10.1935)	LR 2.576e-05
1: TRAIN [0][20/683]	Time 0.312 (0.392)	Data 8.82e-05 (6.92e-03)	Tok/s 9549 (11279)	Loss/tok 9.1135 (9.8124)	LR 3.244e-05
0: TRAIN [0][20/683]	Time 0.312 (0.394)	Data 1.06e-04 (6.92e-03)	Tok/s 9836 (11278)	Loss/tok 9.1079 (9.8237)	LR 3.244e-05
0: TRAIN [0][30/683]	Time 0.312 (0.412)	Data 1.05e-04 (4.72e-03)	Tok/s 9767 (11831)	Loss/tok 8.8299 (9.5483)	LR 4.083e-05
1: TRAIN [0][30/683]	Time 0.313 (0.411)	Data 8.58e-05 (4.72e-03)	Tok/s 9740 (11837)	Loss/tok 8.7528 (9.5369)	LR 4.083e-05
1: TRAIN [0][40/683]	Time 0.596 (0.414)	Data 9.06e-05 (3.59e-03)	Tok/s 14687 (11839)	Loss/tok 8.8338 (9.3544)	LR 5.141e-05
0: TRAIN [0][40/683]	Time 0.597 (0.415)	Data 1.06e-04 (3.60e-03)	Tok/s 14804 (11823)	Loss/tok 8.7691 (9.3681)	LR 5.141e-05
1: TRAIN [0][50/683]	Time 0.614 (0.417)	Data 8.77e-05 (2.90e-03)	Tok/s 14547 (11832)	Loss/tok 8.6443 (9.2155)	LR 6.472e-05
0: TRAIN [0][50/683]	Time 0.614 (0.418)	Data 9.30e-05 (2.91e-03)	Tok/s 14403 (11807)	Loss/tok 8.6714 (9.2263)	LR 6.472e-05
1: TRAIN [0][60/683]	Time 0.494 (0.419)	Data 8.56e-05 (2.44e-03)	Tok/s 13763 (11827)	Loss/tok 8.3308 (9.0796)	LR 8.148e-05
0: TRAIN [0][60/683]	Time 0.494 (0.419)	Data 1.17e-04 (2.45e-03)	Tok/s 13836 (11817)	Loss/tok 8.3371 (9.0865)	LR 8.148e-05
1: TRAIN [0][70/683]	Time 0.394 (0.419)	Data 8.39e-05 (2.11e-03)	Tok/s 12198 (11796)	Loss/tok 8.0898 (8.9825)	LR 1.026e-04
0: TRAIN [0][70/683]	Time 0.394 (0.420)	Data 9.94e-05 (2.12e-03)	Tok/s 12253 (11790)	Loss/tok 8.1363 (8.9897)	LR 1.026e-04
0: TRAIN [0][80/683]	Time 0.405 (0.426)	Data 9.35e-05 (1.87e-03)	Tok/s 11970 (11885)	Loss/tok 7.9632 (8.8651)	LR 1.291e-04
1: TRAIN [0][80/683]	Time 0.405 (0.426)	Data 1.08e-04 (1.86e-03)	Tok/s 12233 (11892)	Loss/tok 7.9535 (8.8594)	LR 1.291e-04
1: TRAIN [0][90/683]	Time 0.320 (0.430)	Data 8.63e-05 (1.67e-03)	Tok/s 8846 (11909)	Loss/tok 7.6269 (8.7506)	LR 1.626e-04
0: TRAIN [0][90/683]	Time 0.320 (0.430)	Data 9.92e-05 (1.68e-03)	Tok/s 9057 (11907)	Loss/tok 7.5812 (8.7541)	LR 1.626e-04
0: TRAIN [0][100/683]	Time 0.520 (0.427)	Data 1.18e-04 (1.53e-03)	Tok/s 13127 (11819)	Loss/tok 7.9487 (8.6730)	LR 2.047e-04
1: TRAIN [0][100/683]	Time 0.524 (0.427)	Data 9.39e-05 (1.51e-03)	Tok/s 12838 (11816)	Loss/tok 7.9330 (8.6726)	LR 2.047e-04
0: TRAIN [0][110/683]	Time 0.520 (0.426)	Data 9.44e-05 (1.40e-03)	Tok/s 12902 (11715)	Loss/tok 7.8416 (8.6038)	LR 2.576e-04
1: TRAIN [0][110/683]	Time 0.520 (0.426)	Data 8.20e-05 (1.38e-03)	Tok/s 13079 (11716)	Loss/tok 7.8757 (8.6022)	LR 2.576e-04
0: TRAIN [0][120/683]	Time 0.325 (0.427)	Data 9.18e-05 (1.29e-03)	Tok/s 9057 (11703)	Loss/tok 7.5888 (8.5328)	LR 3.244e-04
1: TRAIN [0][120/683]	Time 0.326 (0.427)	Data 7.96e-05 (1.28e-03)	Tok/s 8933 (11695)	Loss/tok 7.5120 (8.5332)	LR 3.244e-04
0: TRAIN [0][130/683]	Time 0.406 (0.427)	Data 9.08e-05 (1.20e-03)	Tok/s 11821 (11682)	Loss/tok 7.6550 (8.4735)	LR 4.083e-04
1: TRAIN [0][130/683]	Time 0.406 (0.427)	Data 9.82e-05 (1.19e-03)	Tok/s 12149 (11674)	Loss/tok 7.8410 (8.4756)	LR 4.083e-04
1: TRAIN [0][140/683]	Time 0.330 (0.434)	Data 8.08e-05 (1.11e-03)	Tok/s 8782 (11732)	Loss/tok 7.3878 (8.4149)	LR 5.141e-04
0: TRAIN [0][140/683]	Time 0.330 (0.434)	Data 1.16e-04 (1.12e-03)	Tok/s 8571 (11746)	Loss/tok 7.4503 (8.4119)	LR 5.141e-04
0: TRAIN [0][150/683]	Time 0.527 (0.431)	Data 1.07e-04 (1.05e-03)	Tok/s 12607 (11632)	Loss/tok 7.8111 (8.3719)	LR 6.472e-04
1: TRAIN [0][150/683]	Time 0.527 (0.431)	Data 7.99e-05 (1.04e-03)	Tok/s 12801 (11618)	Loss/tok 7.7529 (8.3743)	LR 6.472e-04
1: TRAIN [0][160/683]	Time 0.325 (0.430)	Data 8.68e-05 (9.86e-04)	Tok/s 8939 (11593)	Loss/tok 7.2606 (8.3297)	LR 8.148e-04
0: TRAIN [0][160/683]	Time 0.325 (0.430)	Data 1.09e-04 (9.97e-04)	Tok/s 9166 (11605)	Loss/tok 7.2535 (8.3271)	LR 8.148e-04
0: TRAIN [0][170/683]	Time 0.328 (0.431)	Data 9.58e-05 (9.45e-04)	Tok/s 9070 (11576)	Loss/tok 7.2658 (8.2894)	LR 1.026e-03
1: TRAIN [0][170/683]	Time 0.328 (0.430)	Data 9.80e-05 (9.34e-04)	Tok/s 8999 (11563)	Loss/tok 7.3426 (8.2930)	LR 1.026e-03
1: TRAIN [0][180/683]	Time 0.642 (0.433)	Data 9.20e-05 (8.88e-04)	Tok/s 13561 (11570)	Loss/tok 7.7545 (8.2541)	LR 1.291e-03
0: TRAIN [0][180/683]	Time 0.642 (0.433)	Data 1.17e-04 (8.99e-04)	Tok/s 13657 (11586)	Loss/tok 7.7147 (8.2475)	LR 1.291e-03
0: TRAIN [0][190/683]	Time 0.520 (0.440)	Data 1.04e-04 (8.57e-04)	Tok/s 13028 (11678)	Loss/tok 7.6498 (8.2034)	LR 1.626e-03
1: TRAIN [0][190/683]	Time 0.520 (0.440)	Data 1.19e-04 (8.47e-04)	Tok/s 13013 (11659)	Loss/tok 7.5619 (8.2087)	LR 1.626e-03
0: TRAIN [0][200/683]	Time 0.646 (0.441)	Data 1.02e-04 (8.20e-04)	Tok/s 13671 (11678)	Loss/tok 7.7512 (8.1676)	LR 2.000e-03
1: TRAIN [0][200/683]	Time 0.675 (0.441)	Data 1.20e-04 (8.10e-04)	Tok/s 13110 (11658)	Loss/tok 7.7633 (8.1728)	LR 2.000e-03
0: TRAIN [0][210/683]	Time 0.332 (0.445)	Data 9.51e-05 (7.87e-04)	Tok/s 8741 (11705)	Loss/tok 7.0236 (8.1305)	LR 2.000e-03
1: TRAIN [0][210/683]	Time 0.331 (0.444)	Data 9.35e-05 (7.77e-04)	Tok/s 8984 (11694)	Loss/tok 6.9426 (8.1323)	LR 2.000e-03
1: TRAIN [0][220/683]	Time 0.328 (0.444)	Data 8.37e-05 (7.46e-04)	Tok/s 9160 (11636)	Loss/tok 7.0981 (8.0982)	LR 2.000e-03
0: TRAIN [0][220/683]	Time 0.319 (0.444)	Data 1.30e-04 (7.56e-04)	Tok/s 8974 (11648)	Loss/tok 6.7768 (8.0972)	LR 2.000e-03
1: TRAIN [0][230/683]	Time 0.530 (0.445)	Data 8.82e-05 (7.18e-04)	Tok/s 13102 (11661)	Loss/tok 7.2948 (8.0560)	LR 2.000e-03
0: TRAIN [0][230/683]	Time 0.530 (0.446)	Data 8.75e-05 (7.28e-04)	Tok/s 12813 (11672)	Loss/tok 7.3539 (8.0546)	LR 2.000e-03
1: TRAIN [0][240/683]	Time 0.329 (0.445)	Data 1.00e-04 (6.92e-04)	Tok/s 8771 (11627)	Loss/tok 6.6990 (8.0191)	LR 2.000e-03
0: TRAIN [0][240/683]	Time 0.329 (0.445)	Data 9.58e-05 (7.02e-04)	Tok/s 9008 (11637)	Loss/tok 6.6261 (8.0185)	LR 2.000e-03
0: TRAIN [0][250/683]	Time 0.654 (0.446)	Data 9.66e-05 (6.79e-04)	Tok/s 13427 (11638)	Loss/tok 7.1360 (7.9758)	LR 2.000e-03
1: TRAIN [0][250/683]	Time 0.654 (0.446)	Data 8.82e-05 (6.68e-04)	Tok/s 13525 (11630)	Loss/tok 7.1604 (7.9761)	LR 2.000e-03
1: TRAIN [0][260/683]	Time 0.424 (0.446)	Data 1.10e-04 (6.46e-04)	Tok/s 11565 (11622)	Loss/tok 6.8553 (7.9369)	LR 2.000e-03
0: TRAIN [0][260/683]	Time 0.424 (0.446)	Data 1.07e-04 (6.57e-04)	Tok/s 11590 (11626)	Loss/tok 6.7324 (7.9373)	LR 2.000e-03
0: TRAIN [0][270/683]	Time 0.648 (0.446)	Data 9.18e-05 (6.36e-04)	Tok/s 13626 (11612)	Loss/tok 7.0129 (7.8989)	LR 2.000e-03
1: TRAIN [0][270/683]	Time 0.648 (0.446)	Data 8.01e-05 (6.26e-04)	Tok/s 13691 (11609)	Loss/tok 7.0721 (7.8981)	LR 2.000e-03
0: TRAIN [0][280/683]	Time 0.331 (0.446)	Data 1.15e-04 (6.18e-04)	Tok/s 8839 (11614)	Loss/tok 6.3856 (7.8574)	LR 2.000e-03
1: TRAIN [0][280/683]	Time 0.330 (0.446)	Data 1.03e-04 (6.07e-04)	Tok/s 8982 (11610)	Loss/tok 6.3953 (7.8556)	LR 2.000e-03
1: TRAIN [0][290/683]	Time 0.324 (0.446)	Data 1.00e-04 (5.89e-04)	Tok/s 9284 (11610)	Loss/tok 6.1241 (7.8140)	LR 2.000e-03
0: TRAIN [0][290/683]	Time 0.324 (0.447)	Data 1.07e-04 (6.00e-04)	Tok/s 9300 (11614)	Loss/tok 6.2624 (7.8164)	LR 2.000e-03
1: TRAIN [0][300/683]	Time 0.325 (0.447)	Data 9.70e-05 (5.73e-04)	Tok/s 9009 (11614)	Loss/tok 6.2056 (7.7706)	LR 2.000e-03
0: TRAIN [0][300/683]	Time 0.326 (0.447)	Data 1.05e-04 (5.84e-04)	Tok/s 8921 (11619)	Loss/tok 6.0955 (7.7721)	LR 2.000e-03
0: TRAIN [0][310/683]	Time 0.327 (0.449)	Data 1.22e-04 (5.69e-04)	Tok/s 8945 (11625)	Loss/tok 6.2012 (7.7296)	LR 2.000e-03
1: TRAIN [0][310/683]	Time 0.328 (0.448)	Data 9.20e-05 (5.58e-04)	Tok/s 8852 (11622)	Loss/tok 6.3192 (7.7276)	LR 2.000e-03
1: TRAIN [0][320/683]	Time 0.427 (0.447)	Data 9.73e-05 (5.43e-04)	Tok/s 11525 (11606)	Loss/tok 6.2882 (7.6906)	LR 2.000e-03
0: TRAIN [0][320/683]	Time 0.428 (0.447)	Data 9.25e-05 (5.55e-04)	Tok/s 11511 (11609)	Loss/tok 6.3550 (7.6930)	LR 2.000e-03
1: TRAIN [0][330/683]	Time 0.529 (0.447)	Data 8.51e-05 (5.30e-04)	Tok/s 12962 (11596)	Loss/tok 6.5147 (7.6514)	LR 2.000e-03
0: TRAIN [0][330/683]	Time 0.529 (0.447)	Data 9.32e-05 (5.42e-04)	Tok/s 13041 (11599)	Loss/tok 6.3865 (7.6540)	LR 2.000e-03
1: TRAIN [0][340/683]	Time 0.520 (0.447)	Data 1.35e-04 (5.17e-04)	Tok/s 13051 (11589)	Loss/tok 6.4440 (7.6132)	LR 2.000e-03
0: TRAIN [0][340/683]	Time 0.531 (0.447)	Data 1.37e-04 (5.29e-04)	Tok/s 12692 (11590)	Loss/tok 6.4449 (7.6167)	LR 2.000e-03
0: TRAIN [0][350/683]	Time 0.420 (0.446)	Data 1.03e-04 (5.17e-04)	Tok/s 11483 (11549)	Loss/tok 6.1166 (7.5849)	LR 2.000e-03
1: TRAIN [0][350/683]	Time 0.420 (0.446)	Data 8.49e-05 (5.05e-04)	Tok/s 11690 (11547)	Loss/tok 6.0566 (7.5799)	LR 2.000e-03
0: TRAIN [0][360/683]	Time 0.528 (0.445)	Data 1.13e-04 (5.06e-04)	Tok/s 12777 (11519)	Loss/tok 6.2968 (7.5496)	LR 2.000e-03
1: TRAIN [0][360/683]	Time 0.528 (0.445)	Data 8.15e-05 (4.94e-04)	Tok/s 12742 (11522)	Loss/tok 6.2202 (7.5450)	LR 2.000e-03
1: TRAIN [0][370/683]	Time 0.417 (0.445)	Data 8.49e-05 (4.83e-04)	Tok/s 11726 (11537)	Loss/tok 5.9289 (7.5028)	LR 2.000e-03
0: TRAIN [0][370/683]	Time 0.418 (0.446)	Data 1.12e-04 (4.95e-04)	Tok/s 11796 (11533)	Loss/tok 5.9772 (7.5099)	LR 2.000e-03
1: TRAIN [0][380/683]	Time 0.518 (0.445)	Data 8.87e-05 (4.73e-04)	Tok/s 13144 (11519)	Loss/tok 6.3915 (7.4687)	LR 2.000e-03
0: TRAIN [0][380/683]	Time 0.519 (0.445)	Data 9.87e-05 (4.85e-04)	Tok/s 13083 (11515)	Loss/tok 6.4007 (7.4761)	LR 2.000e-03
0: TRAIN [0][390/683]	Time 0.414 (0.445)	Data 9.85e-05 (4.76e-04)	Tok/s 11859 (11532)	Loss/tok 5.8733 (7.4374)	LR 2.000e-03
1: TRAIN [0][390/683]	Time 0.414 (0.445)	Data 1.03e-04 (4.63e-04)	Tok/s 11851 (11536)	Loss/tok 5.9753 (7.4292)	LR 2.000e-03
0: TRAIN [0][400/683]	Time 0.519 (0.446)	Data 9.51e-05 (4.66e-04)	Tok/s 13166 (11543)	Loss/tok 6.0823 (7.3981)	LR 2.000e-03
1: TRAIN [0][400/683]	Time 0.519 (0.446)	Data 1.06e-04 (4.54e-04)	Tok/s 13073 (11545)	Loss/tok 6.0214 (7.3915)	LR 2.000e-03
0: TRAIN [0][410/683]	Time 0.530 (0.447)	Data 1.17e-04 (4.57e-04)	Tok/s 12824 (11550)	Loss/tok 6.0475 (7.3597)	LR 2.000e-03
1: TRAIN [0][410/683]	Time 0.531 (0.447)	Data 8.68e-05 (4.45e-04)	Tok/s 12702 (11551)	Loss/tok 5.9713 (7.3528)	LR 2.000e-03
1: TRAIN [0][420/683]	Time 0.324 (0.447)	Data 8.37e-05 (4.37e-04)	Tok/s 9409 (11540)	Loss/tok 5.4019 (7.3202)	LR 2.000e-03
0: TRAIN [0][420/683]	Time 0.324 (0.447)	Data 9.82e-05 (4.49e-04)	Tok/s 9068 (11539)	Loss/tok 5.4041 (7.3256)	LR 2.000e-03
0: TRAIN [0][430/683]	Time 0.423 (0.447)	Data 9.23e-05 (4.41e-04)	Tok/s 11508 (11536)	Loss/tok 5.5815 (7.2904)	LR 2.000e-03
1: TRAIN [0][430/683]	Time 0.423 (0.447)	Data 9.37e-05 (4.29e-04)	Tok/s 11239 (11536)	Loss/tok 5.6360 (7.2841)	LR 2.000e-03
0: TRAIN [0][440/683]	Time 0.414 (0.446)	Data 1.11e-04 (4.33e-04)	Tok/s 11922 (11506)	Loss/tok 5.6642 (7.2611)	LR 2.000e-03
1: TRAIN [0][440/683]	Time 0.414 (0.446)	Data 9.16e-05 (4.21e-04)	Tok/s 11637 (11506)	Loss/tok 5.6670 (7.2554)	LR 2.000e-03
1: TRAIN [0][450/683]	Time 0.526 (0.446)	Data 8.92e-05 (4.14e-04)	Tok/s 13095 (11519)	Loss/tok 5.7965 (7.2182)	LR 2.000e-03
0: TRAIN [0][450/683]	Time 0.526 (0.446)	Data 9.49e-05 (4.26e-04)	Tok/s 12739 (11517)	Loss/tok 5.8084 (7.2252)	LR 2.000e-03
1: TRAIN [0][460/683]	Time 0.417 (0.447)	Data 9.37e-05 (4.07e-04)	Tok/s 11684 (11532)	Loss/tok 5.5662 (7.1815)	LR 2.000e-03
0: TRAIN [0][460/683]	Time 0.417 (0.447)	Data 1.05e-04 (4.19e-04)	Tok/s 11755 (11530)	Loss/tok 5.4745 (7.1878)	LR 2.000e-03
0: TRAIN [0][470/683]	Time 0.644 (0.446)	Data 1.15e-04 (4.12e-04)	Tok/s 13842 (11524)	Loss/tok 5.8378 (7.1557)	LR 2.000e-03
1: TRAIN [0][470/683]	Time 0.644 (0.446)	Data 9.70e-05 (4.01e-04)	Tok/s 13740 (11526)	Loss/tok 5.8972 (7.1490)	LR 2.000e-03
1: TRAIN [0][480/683]	Time 0.413 (0.446)	Data 8.70e-05 (3.95e-04)	Tok/s 11873 (11517)	Loss/tok 5.3513 (7.1170)	LR 2.000e-03
0: TRAIN [0][480/683]	Time 0.414 (0.446)	Data 9.37e-05 (4.06e-04)	Tok/s 11900 (11515)	Loss/tok 5.4660 (7.1241)	LR 2.000e-03
0: TRAIN [0][490/683]	Time 0.328 (0.445)	Data 9.70e-05 (4.00e-04)	Tok/s 9147 (11498)	Loss/tok 5.0550 (7.0947)	LR 2.000e-03
1: TRAIN [0][490/683]	Time 0.328 (0.445)	Data 8.80e-05 (3.88e-04)	Tok/s 8722 (11500)	Loss/tok 5.0158 (7.0877)	LR 2.000e-03
1: TRAIN [0][500/683]	Time 0.420 (0.445)	Data 8.51e-05 (3.83e-04)	Tok/s 11480 (11498)	Loss/tok 5.3306 (7.0548)	LR 2.000e-03
0: TRAIN [0][500/683]	Time 0.420 (0.445)	Data 1.13e-04 (3.94e-04)	Tok/s 11395 (11496)	Loss/tok 5.2900 (7.0628)	LR 2.000e-03
0: TRAIN [0][510/683]	Time 0.330 (0.445)	Data 9.04e-05 (3.88e-04)	Tok/s 9251 (11493)	Loss/tok 4.9173 (7.0306)	LR 2.000e-03
1: TRAIN [0][510/683]	Time 0.330 (0.445)	Data 8.56e-05 (3.77e-04)	Tok/s 9089 (11497)	Loss/tok 4.9986 (7.0232)	LR 2.000e-03
1: TRAIN [0][520/683]	Time 0.417 (0.444)	Data 9.89e-05 (3.72e-04)	Tok/s 11904 (11476)	Loss/tok 5.1763 (6.9955)	LR 2.000e-03
0: TRAIN [0][520/683]	Time 0.417 (0.444)	Data 9.06e-05 (3.83e-04)	Tok/s 11779 (11472)	Loss/tok 5.2082 (7.0032)	LR 2.000e-03
1: TRAIN [0][530/683]	Time 0.330 (0.444)	Data 8.82e-05 (3.66e-04)	Tok/s 8749 (11484)	Loss/tok 4.8673 (6.9625)	LR 2.000e-03
0: TRAIN [0][530/683]	Time 0.330 (0.444)	Data 9.01e-05 (3.78e-04)	Tok/s 8908 (11479)	Loss/tok 4.7036 (6.9692)	LR 2.000e-03
1: TRAIN [0][540/683]	Time 0.406 (0.443)	Data 9.42e-05 (3.61e-04)	Tok/s 11830 (11465)	Loss/tok 4.9278 (6.9350)	LR 2.000e-03
0: TRAIN [0][540/683]	Time 0.407 (0.443)	Data 9.44e-05 (3.73e-04)	Tok/s 11957 (11461)	Loss/tok 5.0064 (6.9410)	LR 2.000e-03
0: TRAIN [0][550/683]	Time 0.324 (0.442)	Data 1.14e-04 (3.68e-04)	Tok/s 9061 (11437)	Loss/tok 4.6608 (6.9142)	LR 2.000e-03
1: TRAIN [0][550/683]	Time 0.324 (0.442)	Data 8.30e-05 (3.57e-04)	Tok/s 9232 (11444)	Loss/tok 4.8522 (6.9084)	LR 2.000e-03
0: TRAIN [0][560/683]	Time 0.424 (0.442)	Data 1.08e-04 (3.63e-04)	Tok/s 11550 (11438)	Loss/tok 4.9224 (6.8845)	LR 2.000e-03
1: TRAIN [0][560/683]	Time 0.424 (0.442)	Data 1.36e-04 (3.52e-04)	Tok/s 11610 (11445)	Loss/tok 5.0708 (6.8782)	LR 2.000e-03
1: TRAIN [0][570/683]	Time 0.323 (0.442)	Data 8.58e-05 (3.48e-04)	Tok/s 9178 (11445)	Loss/tok 4.7233 (6.8484)	LR 2.000e-03
0: TRAIN [0][570/683]	Time 0.323 (0.442)	Data 1.13e-04 (3.59e-04)	Tok/s 8576 (11437)	Loss/tok 4.4962 (6.8552)	LR 2.000e-03
0: TRAIN [0][580/683]	Time 0.321 (0.442)	Data 9.39e-05 (3.54e-04)	Tok/s 9234 (11435)	Loss/tok 4.6645 (6.8263)	LR 2.000e-03
1: TRAIN [0][580/683]	Time 0.321 (0.442)	Data 7.70e-05 (3.43e-04)	Tok/s 9115 (11441)	Loss/tok 4.7575 (6.8205)	LR 2.000e-03
0: TRAIN [0][590/683]	Time 0.410 (0.441)	Data 1.03e-04 (3.50e-04)	Tok/s 11966 (11436)	Loss/tok 4.9588 (6.7967)	LR 2.000e-03
1: TRAIN [0][590/683]	Time 0.410 (0.441)	Data 8.46e-05 (3.39e-04)	Tok/s 11953 (11443)	Loss/tok 4.9923 (6.7898)	LR 2.000e-03
1: TRAIN [0][600/683]	Time 0.317 (0.442)	Data 8.87e-05 (3.35e-04)	Tok/s 9061 (11443)	Loss/tok 4.6929 (6.7604)	LR 2.000e-03
0: TRAIN [0][600/683]	Time 0.317 (0.442)	Data 1.15e-04 (3.46e-04)	Tok/s 9191 (11438)	Loss/tok 4.5927 (6.7669)	LR 2.000e-03
0: TRAIN [0][610/683]	Time 0.409 (0.441)	Data 1.44e-04 (3.42e-04)	Tok/s 11972 (11429)	Loss/tok 4.8270 (6.7392)	LR 2.000e-03
1: TRAIN [0][610/683]	Time 0.413 (0.441)	Data 8.58e-05 (3.31e-04)	Tok/s 11965 (11433)	Loss/tok 4.8738 (6.7330)	LR 2.000e-03
0: TRAIN [0][620/683]	Time 0.256 (0.441)	Data 1.25e-04 (3.38e-04)	Tok/s 5595 (11419)	Loss/tok 4.0735 (6.7123)	LR 2.000e-03
1: TRAIN [0][620/683]	Time 0.256 (0.441)	Data 8.77e-05 (3.27e-04)	Tok/s 5701 (11424)	Loss/tok 4.2817 (6.7058)	LR 2.000e-03
1: TRAIN [0][630/683]	Time 0.420 (0.441)	Data 1.01e-04 (3.24e-04)	Tok/s 11529 (11424)	Loss/tok 4.9935 (6.6784)	LR 2.000e-03
0: TRAIN [0][630/683]	Time 0.425 (0.441)	Data 9.70e-05 (3.35e-04)	Tok/s 11404 (11420)	Loss/tok 4.8684 (6.6848)	LR 2.000e-03
0: TRAIN [0][640/683]	Time 0.413 (0.441)	Data 9.35e-05 (3.31e-04)	Tok/s 11783 (11414)	Loss/tok 4.7051 (6.6583)	LR 2.000e-03
1: TRAIN [0][640/683]	Time 0.413 (0.441)	Data 8.63e-05 (3.20e-04)	Tok/s 11639 (11419)	Loss/tok 4.7939 (6.6520)	LR 2.000e-03
1: TRAIN [0][650/683]	Time 0.325 (0.440)	Data 7.63e-05 (3.16e-04)	Tok/s 9065 (11418)	Loss/tok 4.3816 (6.6244)	LR 2.000e-03
0: TRAIN [0][650/683]	Time 0.325 (0.441)	Data 9.44e-05 (3.28e-04)	Tok/s 9429 (11414)	Loss/tok 4.4081 (6.6308)	LR 2.000e-03
1: TRAIN [0][660/683]	Time 0.402 (0.440)	Data 8.51e-05 (3.13e-04)	Tok/s 12225 (11421)	Loss/tok 4.5964 (6.5976)	LR 2.000e-03
0: TRAIN [0][660/683]	Time 0.402 (0.441)	Data 9.42e-05 (3.24e-04)	Tok/s 12508 (11418)	Loss/tok 4.6481 (6.6031)	LR 2.000e-03
1: TRAIN [0][670/683]	Time 0.258 (0.440)	Data 9.78e-05 (3.10e-04)	Tok/s 5852 (11414)	Loss/tok 4.0842 (6.5734)	LR 2.000e-03
0: TRAIN [0][670/683]	Time 0.258 (0.440)	Data 9.42e-05 (3.21e-04)	Tok/s 5610 (11410)	Loss/tok 4.0865 (6.5786)	LR 2.000e-03
1: TRAIN [0][680/683]	Time 0.633 (0.440)	Data 3.91e-05 (3.08e-04)	Tok/s 13807 (11413)	Loss/tok 5.0947 (6.5472)	LR 2.000e-03
0: TRAIN [0][680/683]	Time 0.633 (0.440)	Data 4.58e-05 (3.20e-04)	Tok/s 14100 (11412)	Loss/tok 5.0948 (6.5525)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.112 (0.112)	Data 1.62e-03 (1.62e-03)	Tok/s 42332 (42332)	Loss/tok 6.2559 (6.2559)
0: VALIDATION [0][0/80]	Time 0.153 (0.153)	Data 1.67e-03 (1.67e-03)	Tok/s 37304 (37304)	Loss/tok 6.3678 (6.3678)
1: VALIDATION [0][10/80]	Time 0.065 (0.081)	Data 1.30e-03 (1.39e-03)	Tok/s 44527 (43260)	Loss/tok 5.9033 (6.1143)
0: VALIDATION [0][10/80]	Time 0.062 (0.084)	Data 1.38e-03 (1.43e-03)	Tok/s 46917 (44705)	Loss/tok 5.9642 (6.0914)
1: VALIDATION [0][20/80]	Time 0.054 (0.071)	Data 1.31e-03 (1.36e-03)	Tok/s 42773 (43271)	Loss/tok 5.6233 (6.0216)
0: VALIDATION [0][20/80]	Time 0.052 (0.071)	Data 1.29e-03 (1.39e-03)	Tok/s 45027 (44951)	Loss/tok 5.5910 (6.0308)
0: VALIDATION [0][30/80]	Time 0.043 (0.063)	Data 1.27e-03 (1.36e-03)	Tok/s 45221 (45140)	Loss/tok 5.6476 (5.9620)
1: VALIDATION [0][30/80]	Time 0.046 (0.064)	Data 1.26e-03 (1.34e-03)	Tok/s 41583 (43201)	Loss/tok 5.7876 (5.9469)
0: VALIDATION [0][40/80]	Time 0.037 (0.057)	Data 1.28e-03 (1.35e-03)	Tok/s 44024 (44984)	Loss/tok 5.6477 (5.9249)
1: VALIDATION [0][40/80]	Time 0.039 (0.058)	Data 1.25e-03 (1.33e-03)	Tok/s 40940 (42699)	Loss/tok 5.6621 (5.8892)
0: VALIDATION [0][50/80]	Time 0.030 (0.053)	Data 1.28e-03 (1.34e-03)	Tok/s 44106 (44726)	Loss/tok 5.7527 (5.8794)
1: VALIDATION [0][50/80]	Time 0.032 (0.054)	Data 1.26e-03 (1.32e-03)	Tok/s 40582 (42445)	Loss/tok 5.5918 (5.8582)
0: VALIDATION [0][60/80]	Time 0.026 (0.049)	Data 1.30e-03 (1.33e-03)	Tok/s 40975 (44324)	Loss/tok 5.5819 (5.8416)
1: VALIDATION [0][60/80]	Time 0.027 (0.050)	Data 1.29e-03 (1.31e-03)	Tok/s 39419 (41997)	Loss/tok 5.4846 (5.8243)
0: VALIDATION [0][70/80]	Time 0.021 (0.045)	Data 1.23e-03 (1.32e-03)	Tok/s 38409 (43746)	Loss/tok 5.3316 (5.8114)
1: VALIDATION [0][70/80]	Time 0.022 (0.046)	Data 1.23e-03 (1.30e-03)	Tok/s 36601 (41517)	Loss/tok 5.3622 (5.7930)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
1: TEST [0][9/47]	Time 0.6921 (0.9422)	Decoder iters 149.0 (149.0)	Tok/s 4154 (4093)
0: TEST [0][9/47]	Time 0.6918 (0.9417)	Decoder iters 149.0 (149.0)	Tok/s 4167 (4264)
0: TEST [0][19/47]	Time 0.5503 (0.7921)	Decoder iters 149.0 (146.4)	Tok/s 3636 (4071)
1: TEST [0][19/47]	Time 0.5501 (0.7925)	Decoder iters 66.0 (144.8)	Tok/s 3658 (3930)
0: TEST [0][29/47]	Time 0.5057 (0.7048)	Decoder iters 149.0 (142.0)	Tok/s 3285 (3848)
1: TEST [0][29/47]	Time 0.5058 (0.7051)	Decoder iters 149.0 (144.5)	Tok/s 3039 (3736)
0: TEST [0][39/47]	Time 0.4318 (0.6198)	Decoder iters 30.0 (123.1)	Tok/s 2291 (3828)
1: TEST [0][39/47]	Time 0.4320 (0.6200)	Decoder iters 149.0 (130.9)	Tok/s 2303 (3744)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.5436	Validation Loss: 5.7698	Test BLEU: 3.19
0: Performance: Epoch: 0	Training: 22832 Tok/s	Validation: 83379 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
0: TRAIN [1][0/683]	Time 0.619 (0.619)	Data 1.47e-01 (1.47e-01)	Tok/s 10876 (10876)	Loss/tok 4.7666 (4.7666)	LR 2.000e-03
1: TRAIN [1][0/683]	Time 0.609 (0.609)	Data 1.32e-01 (1.32e-01)	Tok/s 11033 (11033)	Loss/tok 4.7305 (4.7305)	LR 2.000e-03
0: TRAIN [1][10/683]	Time 0.316 (0.490)	Data 1.85e-04 (1.34e-02)	Tok/s 9257 (12547)	Loss/tok 3.9793 (4.6609)	LR 2.000e-03
1: TRAIN [1][10/683]	Time 0.316 (0.489)	Data 1.65e-04 (1.21e-02)	Tok/s 9512 (12633)	Loss/tok 4.0977 (4.6798)	LR 2.000e-03
1: TRAIN [1][20/683]	Time 0.396 (0.483)	Data 1.04e-04 (6.41e-03)	Tok/s 12317 (12794)	Loss/tok 4.4248 (4.6552)	LR 2.000e-03
0: TRAIN [1][20/683]	Time 0.396 (0.483)	Data 9.89e-05 (7.09e-03)	Tok/s 12202 (12727)	Loss/tok 4.4358 (4.6666)	LR 2.000e-03
1: TRAIN [1][30/683]	Time 0.520 (0.482)	Data 9.18e-05 (4.38e-03)	Tok/s 13235 (12543)	Loss/tok 4.7600 (4.6712)	LR 2.000e-03
0: TRAIN [1][30/683]	Time 0.520 (0.482)	Data 1.10e-04 (4.84e-03)	Tok/s 13114 (12501)	Loss/tok 4.4932 (4.6689)	LR 2.000e-03
1: TRAIN [1][40/683]	Time 0.322 (0.467)	Data 1.00e-04 (3.33e-03)	Tok/s 8944 (12182)	Loss/tok 4.0615 (4.6204)	LR 2.000e-03
0: TRAIN [1][40/683]	Time 0.322 (0.467)	Data 9.92e-05 (3.69e-03)	Tok/s 9474 (12161)	Loss/tok 3.8724 (4.6326)	LR 2.000e-03
0: TRAIN [1][50/683]	Time 0.519 (0.466)	Data 1.09e-04 (2.99e-03)	Tok/s 13142 (12102)	Loss/tok 4.5616 (4.6196)	LR 2.000e-03
1: TRAIN [1][50/683]	Time 0.519 (0.466)	Data 9.39e-05 (2.70e-03)	Tok/s 12968 (12105)	Loss/tok 4.4114 (4.5976)	LR 2.000e-03
0: TRAIN [1][60/683]	Time 0.252 (0.462)	Data 1.01e-04 (2.52e-03)	Tok/s 5921 (11983)	Loss/tok 3.5899 (4.6041)	LR 2.000e-03
1: TRAIN [1][60/683]	Time 0.252 (0.462)	Data 1.16e-04 (2.28e-03)	Tok/s 5728 (11990)	Loss/tok 3.7864 (4.5841)	LR 2.000e-03
1: TRAIN [1][70/683]	Time 0.518 (0.463)	Data 9.39e-05 (1.97e-03)	Tok/s 13023 (12032)	Loss/tok 4.5251 (4.5631)	LR 2.000e-03
0: TRAIN [1][70/683]	Time 0.518 (0.463)	Data 1.59e-04 (2.18e-03)	Tok/s 13045 (12022)	Loss/tok 4.7397 (4.5897)	LR 2.000e-03
0: TRAIN [1][80/683]	Time 0.410 (0.462)	Data 9.85e-05 (1.92e-03)	Tok/s 11812 (11976)	Loss/tok 4.3773 (4.5785)	LR 2.000e-03
1: TRAIN [1][80/683]	Time 0.410 (0.461)	Data 8.70e-05 (1.74e-03)	Tok/s 12183 (11996)	Loss/tok 4.2990 (4.5573)	LR 2.000e-03
0: TRAIN [1][90/683]	Time 0.407 (0.457)	Data 1.13e-04 (1.72e-03)	Tok/s 12006 (11889)	Loss/tok 4.0387 (4.5547)	LR 2.000e-03
1: TRAIN [1][90/683]	Time 0.407 (0.457)	Data 8.20e-05 (1.56e-03)	Tok/s 11820 (11914)	Loss/tok 4.2931 (4.5396)	LR 2.000e-03
0: TRAIN [1][100/683]	Time 0.412 (0.453)	Data 1.24e-04 (1.56e-03)	Tok/s 11743 (11859)	Loss/tok 4.2622 (4.5311)	LR 2.000e-03
1: TRAIN [1][100/683]	Time 0.412 (0.453)	Data 1.16e-04 (1.41e-03)	Tok/s 11659 (11878)	Loss/tok 4.3876 (4.5205)	LR 2.000e-03
0: TRAIN [1][110/683]	Time 0.326 (0.445)	Data 1.10e-04 (1.43e-03)	Tok/s 9178 (11717)	Loss/tok 3.9359 (4.5051)	LR 2.000e-03
1: TRAIN [1][110/683]	Time 0.326 (0.445)	Data 8.89e-05 (1.30e-03)	Tok/s 8907 (11734)	Loss/tok 3.9742 (4.4992)	LR 2.000e-03
1: TRAIN [1][120/683]	Time 0.524 (0.445)	Data 8.25e-05 (1.20e-03)	Tok/s 12968 (11739)	Loss/tok 4.5346 (4.4941)	LR 2.000e-03
0: TRAIN [1][120/683]	Time 0.524 (0.446)	Data 9.18e-05 (1.32e-03)	Tok/s 13080 (11733)	Loss/tok 4.6044 (4.4993)	LR 2.000e-03
1: TRAIN [1][130/683]	Time 0.504 (0.444)	Data 1.02e-04 (1.11e-03)	Tok/s 13431 (11720)	Loss/tok 4.5163 (4.4829)	LR 2.000e-03
0: TRAIN [1][130/683]	Time 0.504 (0.444)	Data 9.35e-05 (1.23e-03)	Tok/s 13315 (11714)	Loss/tok 4.5924 (4.4881)	LR 2.000e-03
1: TRAIN [1][140/683]	Time 0.326 (0.446)	Data 9.13e-05 (1.04e-03)	Tok/s 8941 (11723)	Loss/tok 3.6210 (4.4756)	LR 2.000e-03
0: TRAIN [1][140/683]	Time 0.326 (0.446)	Data 1.13e-04 (1.15e-03)	Tok/s 9296 (11725)	Loss/tok 3.9781 (4.4801)	LR 2.000e-03
1: TRAIN [1][150/683]	Time 0.421 (0.447)	Data 8.85e-05 (9.78e-04)	Tok/s 11524 (11722)	Loss/tok 4.1863 (4.4703)	LR 2.000e-03
0: TRAIN [1][150/683]	Time 0.421 (0.447)	Data 1.07e-04 (1.08e-03)	Tok/s 11567 (11723)	Loss/tok 4.2393 (4.4764)	LR 2.000e-03
0: TRAIN [1][160/683]	Time 0.524 (0.447)	Data 9.82e-05 (1.02e-03)	Tok/s 12913 (11723)	Loss/tok 4.4942 (4.4740)	LR 2.000e-03
1: TRAIN [1][160/683]	Time 0.524 (0.447)	Data 1.01e-04 (9.24e-04)	Tok/s 12786 (11720)	Loss/tok 4.3086 (4.4607)	LR 2.000e-03
1: TRAIN [1][170/683]	Time 0.523 (0.446)	Data 8.32e-05 (8.75e-04)	Tok/s 12964 (11695)	Loss/tok 4.5396 (4.4519)	LR 2.000e-03
0: TRAIN [1][170/683]	Time 0.523 (0.447)	Data 1.32e-04 (9.67e-04)	Tok/s 13064 (11702)	Loss/tok 4.5812 (4.4663)	LR 2.000e-03
1: TRAIN [1][180/683]	Time 0.529 (0.450)	Data 9.87e-05 (8.32e-04)	Tok/s 12805 (11730)	Loss/tok 4.4951 (4.4556)	LR 2.000e-03
0: TRAIN [1][180/683]	Time 0.529 (0.450)	Data 1.83e-04 (9.21e-04)	Tok/s 12923 (11736)	Loss/tok 4.4612 (4.4688)	LR 2.000e-03
1: TRAIN [1][190/683]	Time 0.323 (0.451)	Data 9.16e-05 (7.93e-04)	Tok/s 9040 (11749)	Loss/tok 3.7671 (4.4475)	LR 2.000e-03
0: TRAIN [1][190/683]	Time 0.324 (0.451)	Data 1.24e-04 (8.80e-04)	Tok/s 8926 (11754)	Loss/tok 3.8629 (4.4626)	LR 2.000e-03
0: TRAIN [1][200/683]	Time 0.526 (0.451)	Data 1.27e-04 (8.42e-04)	Tok/s 12830 (11751)	Loss/tok 4.3928 (4.4543)	LR 2.000e-03
1: TRAIN [1][200/683]	Time 0.526 (0.451)	Data 8.46e-05 (7.59e-04)	Tok/s 12745 (11746)	Loss/tok 4.2787 (4.4403)	LR 2.000e-03
1: TRAIN [1][210/683]	Time 0.256 (0.447)	Data 1.01e-04 (7.27e-04)	Tok/s 5833 (11660)	Loss/tok 3.6936 (4.4277)	LR 2.000e-03
0: TRAIN [1][210/683]	Time 0.256 (0.447)	Data 1.07e-04 (8.07e-04)	Tok/s 5686 (11654)	Loss/tok 3.7954 (4.4409)	LR 2.000e-03
1: TRAIN [1][220/683]	Time 0.419 (0.445)	Data 8.42e-05 (6.98e-04)	Tok/s 11601 (11616)	Loss/tok 4.1447 (4.4183)	LR 2.000e-03
0: TRAIN [1][220/683]	Time 0.419 (0.445)	Data 1.37e-04 (7.75e-04)	Tok/s 11533 (11610)	Loss/tok 4.1560 (4.4305)	LR 2.000e-03
0: TRAIN [1][230/683]	Time 0.322 (0.445)	Data 1.09e-04 (7.47e-04)	Tok/s 8810 (11599)	Loss/tok 3.8602 (4.4223)	LR 1.000e-03
1: TRAIN [1][230/683]	Time 0.322 (0.445)	Data 8.82e-05 (6.72e-04)	Tok/s 9014 (11607)	Loss/tok 3.7787 (4.4115)	LR 1.000e-03
0: TRAIN [1][240/683]	Time 0.641 (0.446)	Data 1.19e-04 (7.20e-04)	Tok/s 13694 (11594)	Loss/tok 4.6208 (4.4192)	LR 1.000e-03
1: TRAIN [1][240/683]	Time 0.642 (0.446)	Data 8.61e-05 (6.48e-04)	Tok/s 13715 (11604)	Loss/tok 4.4870 (4.4089)	LR 1.000e-03
0: TRAIN [1][250/683]	Time 0.419 (0.448)	Data 1.03e-04 (6.96e-04)	Tok/s 11882 (11622)	Loss/tok 4.1091 (4.4127)	LR 1.000e-03
1: TRAIN [1][250/683]	Time 0.419 (0.448)	Data 1.55e-04 (6.26e-04)	Tok/s 11725 (11629)	Loss/tok 3.9109 (4.3993)	LR 1.000e-03
1: TRAIN [1][260/683]	Time 0.525 (0.447)	Data 9.42e-05 (6.06e-04)	Tok/s 12877 (11626)	Loss/tok 4.1895 (4.3879)	LR 1.000e-03
0: TRAIN [1][260/683]	Time 0.525 (0.447)	Data 1.19e-04 (6.74e-04)	Tok/s 12938 (11619)	Loss/tok 4.3527 (4.4028)	LR 1.000e-03
0: TRAIN [1][270/683]	Time 0.418 (0.447)	Data 1.09e-04 (6.53e-04)	Tok/s 11554 (11610)	Loss/tok 4.0332 (4.3921)	LR 1.000e-03
1: TRAIN [1][270/683]	Time 0.418 (0.447)	Data 8.92e-05 (5.86e-04)	Tok/s 11598 (11620)	Loss/tok 3.8744 (4.3788)	LR 1.000e-03
1: TRAIN [1][280/683]	Time 0.329 (0.446)	Data 8.61e-05 (5.69e-04)	Tok/s 9075 (11599)	Loss/tok 3.7404 (4.3708)	LR 1.000e-03
0: TRAIN [1][280/683]	Time 0.329 (0.446)	Data 9.99e-05 (6.34e-04)	Tok/s 8598 (11589)	Loss/tok 3.7316 (4.3826)	LR 1.000e-03
1: TRAIN [1][290/683]	Time 0.523 (0.445)	Data 8.80e-05 (5.52e-04)	Tok/s 12904 (11590)	Loss/tok 4.2035 (4.3602)	LR 1.000e-03
0: TRAIN [1][290/683]	Time 0.523 (0.445)	Data 1.05e-04 (6.16e-04)	Tok/s 12881 (11577)	Loss/tok 4.2352 (4.3725)	LR 1.000e-03
1: TRAIN [1][300/683]	Time 0.623 (0.445)	Data 8.42e-05 (5.37e-04)	Tok/s 14189 (11556)	Loss/tok 4.4818 (4.3536)	LR 1.000e-03
0: TRAIN [1][300/683]	Time 0.623 (0.445)	Data 1.24e-04 (5.99e-04)	Tok/s 14067 (11541)	Loss/tok 4.3237 (4.3661)	LR 1.000e-03
1: TRAIN [1][310/683]	Time 0.323 (0.444)	Data 1.13e-04 (5.23e-04)	Tok/s 9127 (11529)	Loss/tok 3.7032 (4.3450)	LR 1.000e-03
0: TRAIN [1][310/683]	Time 0.325 (0.444)	Data 1.08e-04 (5.83e-04)	Tok/s 9285 (11516)	Loss/tok 3.4868 (4.3574)	LR 1.000e-03
0: TRAIN [1][320/683]	Time 0.420 (0.443)	Data 1.01e-04 (5.68e-04)	Tok/s 11677 (11495)	Loss/tok 3.8176 (4.3471)	LR 1.000e-03
1: TRAIN [1][320/683]	Time 0.420 (0.443)	Data 9.85e-05 (5.09e-04)	Tok/s 11726 (11511)	Loss/tok 3.9801 (4.3341)	LR 1.000e-03
0: TRAIN [1][330/683]	Time 0.413 (0.440)	Data 1.02e-04 (5.54e-04)	Tok/s 11763 (11444)	Loss/tok 3.9024 (4.3364)	LR 1.000e-03
1: TRAIN [1][330/683]	Time 0.413 (0.440)	Data 8.46e-05 (4.96e-04)	Tok/s 11897 (11460)	Loss/tok 3.9973 (4.3223)	LR 1.000e-03
0: TRAIN [1][340/683]	Time 0.419 (0.440)	Data 1.02e-04 (5.41e-04)	Tok/s 11621 (11426)	Loss/tok 3.9521 (4.3287)	LR 5.000e-04
1: TRAIN [1][340/683]	Time 0.419 (0.440)	Data 8.01e-05 (4.84e-04)	Tok/s 11672 (11445)	Loss/tok 3.8682 (4.3140)	LR 5.000e-04
0: TRAIN [1][350/683]	Time 0.636 (0.441)	Data 1.11e-04 (5.29e-04)	Tok/s 13975 (11451)	Loss/tok 4.2682 (4.3213)	LR 5.000e-04
1: TRAIN [1][350/683]	Time 0.636 (0.441)	Data 8.73e-05 (4.73e-04)	Tok/s 13932 (11469)	Loss/tok 4.3944 (4.3107)	LR 5.000e-04
0: TRAIN [1][360/683]	Time 0.324 (0.440)	Data 1.16e-04 (5.18e-04)	Tok/s 9401 (11446)	Loss/tok 3.4783 (4.3124)	LR 5.000e-04
1: TRAIN [1][360/683]	Time 0.324 (0.440)	Data 8.06e-05 (4.63e-04)	Tok/s 8804 (11463)	Loss/tok 3.6708 (4.3009)	LR 5.000e-04
1: TRAIN [1][370/683]	Time 0.415 (0.440)	Data 8.13e-05 (4.53e-04)	Tok/s 11767 (11455)	Loss/tok 3.8185 (4.2909)	LR 5.000e-04
0: TRAIN [1][370/683]	Time 0.415 (0.440)	Data 1.18e-04 (5.07e-04)	Tok/s 11626 (11438)	Loss/tok 3.8367 (4.3032)	LR 5.000e-04
1: TRAIN [1][380/683]	Time 0.530 (0.441)	Data 8.39e-05 (4.43e-04)	Tok/s 12836 (11474)	Loss/tok 4.0957 (4.2857)	LR 5.000e-04
0: TRAIN [1][380/683]	Time 0.530 (0.441)	Data 1.16e-04 (4.97e-04)	Tok/s 12728 (11460)	Loss/tok 4.0543 (4.2959)	LR 5.000e-04
1: TRAIN [1][390/683]	Time 0.519 (0.441)	Data 1.08e-04 (4.34e-04)	Tok/s 13117 (11479)	Loss/tok 4.0153 (4.2776)	LR 5.000e-04
0: TRAIN [1][390/683]	Time 0.519 (0.441)	Data 1.05e-04 (4.86e-04)	Tok/s 13402 (11462)	Loss/tok 3.9727 (4.2871)	LR 5.000e-04
0: TRAIN [1][400/683]	Time 0.624 (0.440)	Data 1.09e-04 (4.77e-04)	Tok/s 13955 (11455)	Loss/tok 4.2353 (4.2783)	LR 5.000e-04
1: TRAIN [1][400/683]	Time 0.623 (0.440)	Data 8.34e-05 (4.25e-04)	Tok/s 13998 (11472)	Loss/tok 4.2632 (4.2702)	LR 5.000e-04
1: TRAIN [1][410/683]	Time 0.526 (0.440)	Data 9.97e-05 (4.17e-04)	Tok/s 12966 (11473)	Loss/tok 3.9181 (4.2626)	LR 5.000e-04
0: TRAIN [1][410/683]	Time 0.526 (0.440)	Data 1.66e-04 (4.68e-04)	Tok/s 12943 (11455)	Loss/tok 4.0475 (4.2711)	LR 5.000e-04
1: TRAIN [1][420/683]	Time 0.322 (0.440)	Data 8.54e-05 (4.10e-04)	Tok/s 9193 (11454)	Loss/tok 3.5668 (4.2568)	LR 5.000e-04
0: TRAIN [1][420/683]	Time 0.322 (0.440)	Data 1.44e-04 (4.60e-04)	Tok/s 9023 (11436)	Loss/tok 3.6144 (4.2637)	LR 5.000e-04
0: TRAIN [1][430/683]	Time 0.413 (0.439)	Data 9.94e-05 (4.52e-04)	Tok/s 11775 (11426)	Loss/tok 3.7247 (4.2557)	LR 5.000e-04
1: TRAIN [1][430/683]	Time 0.413 (0.439)	Data 1.54e-04 (4.02e-04)	Tok/s 11973 (11441)	Loss/tok 3.7560 (4.2489)	LR 5.000e-04
1: TRAIN [1][440/683]	Time 0.318 (0.439)	Data 8.94e-05 (3.95e-04)	Tok/s 9143 (11427)	Loss/tok 3.6115 (4.2429)	LR 5.000e-04
0: TRAIN [1][440/683]	Time 0.318 (0.439)	Data 1.07e-04 (4.44e-04)	Tok/s 9511 (11413)	Loss/tok 3.7832 (4.2508)	LR 5.000e-04
1: TRAIN [1][450/683]	Time 0.409 (0.439)	Data 8.61e-05 (3.88e-04)	Tok/s 11946 (11415)	Loss/tok 3.7441 (4.2369)	LR 5.000e-04
0: TRAIN [1][450/683]	Time 0.413 (0.439)	Data 1.01e-04 (4.37e-04)	Tok/s 11840 (11402)	Loss/tok 3.9112 (4.2438)	LR 5.000e-04
0: TRAIN [1][460/683]	Time 0.251 (0.439)	Data 1.21e-04 (4.30e-04)	Tok/s 5718 (11408)	Loss/tok 3.2349 (4.2386)	LR 2.500e-04
1: TRAIN [1][460/683]	Time 0.252 (0.439)	Data 8.18e-05 (3.82e-04)	Tok/s 5649 (11421)	Loss/tok 3.3630 (4.2306)	LR 2.500e-04
1: TRAIN [1][470/683]	Time 0.324 (0.439)	Data 8.49e-05 (3.76e-04)	Tok/s 9345 (11429)	Loss/tok 3.5485 (4.2238)	LR 2.500e-04
0: TRAIN [1][470/683]	Time 0.324 (0.439)	Data 9.51e-05 (4.24e-04)	Tok/s 9112 (11417)	Loss/tok 3.4319 (4.2314)	LR 2.500e-04
1: TRAIN [1][480/683]	Time 0.413 (0.439)	Data 8.99e-05 (3.70e-04)	Tok/s 11995 (11423)	Loss/tok 3.7235 (4.2166)	LR 2.500e-04
0: TRAIN [1][480/683]	Time 0.413 (0.439)	Data 1.11e-04 (4.18e-04)	Tok/s 11731 (11413)	Loss/tok 3.7684 (4.2241)	LR 2.500e-04
1: TRAIN [1][490/683]	Time 0.328 (0.439)	Data 7.89e-05 (3.64e-04)	Tok/s 9050 (11415)	Loss/tok 3.4637 (4.2112)	LR 2.500e-04
0: TRAIN [1][490/683]	Time 0.328 (0.439)	Data 2.08e-04 (4.12e-04)	Tok/s 8943 (11406)	Loss/tok 3.6098 (4.2187)	LR 2.500e-04
1: TRAIN [1][500/683]	Time 0.324 (0.439)	Data 1.05e-04 (3.59e-04)	Tok/s 9271 (11415)	Loss/tok 3.5507 (4.2055)	LR 2.500e-04
0: TRAIN [1][500/683]	Time 0.324 (0.439)	Data 1.11e-04 (4.06e-04)	Tok/s 9318 (11406)	Loss/tok 3.5858 (4.2138)	LR 2.500e-04
1: TRAIN [1][510/683]	Time 0.403 (0.439)	Data 1.02e-04 (3.53e-04)	Tok/s 12243 (11409)	Loss/tok 3.8594 (4.2000)	LR 2.500e-04
0: TRAIN [1][510/683]	Time 0.404 (0.439)	Data 1.09e-04 (4.00e-04)	Tok/s 12357 (11401)	Loss/tok 3.8000 (4.2082)	LR 2.500e-04
1: TRAIN [1][520/683]	Time 0.511 (0.439)	Data 9.08e-05 (3.48e-04)	Tok/s 13108 (11419)	Loss/tok 3.9497 (4.1946)	LR 2.500e-04
0: TRAIN [1][520/683]	Time 0.512 (0.439)	Data 1.30e-04 (3.94e-04)	Tok/s 13360 (11411)	Loss/tok 4.0073 (4.2027)	LR 2.500e-04
1: TRAIN [1][530/683]	Time 0.609 (0.439)	Data 7.99e-05 (3.44e-04)	Tok/s 14611 (11421)	Loss/tok 4.1827 (4.1893)	LR 2.500e-04
0: TRAIN [1][530/683]	Time 0.609 (0.439)	Data 1.02e-04 (3.89e-04)	Tok/s 14368 (11413)	Loss/tok 4.2040 (4.1959)	LR 2.500e-04
0: TRAIN [1][540/683]	Time 0.413 (0.438)	Data 1.03e-04 (3.84e-04)	Tok/s 12048 (11398)	Loss/tok 3.7499 (4.1901)	LR 2.500e-04
1: TRAIN [1][540/683]	Time 0.413 (0.438)	Data 8.61e-05 (3.39e-04)	Tok/s 11832 (11406)	Loss/tok 3.8332 (4.1832)	LR 2.500e-04
0: TRAIN [1][550/683]	Time 0.501 (0.440)	Data 1.05e-04 (3.79e-04)	Tok/s 13514 (11428)	Loss/tok 3.9474 (4.1863)	LR 2.500e-04
1: TRAIN [1][550/683]	Time 0.502 (0.440)	Data 9.13e-05 (3.35e-04)	Tok/s 13534 (11436)	Loss/tok 4.1211 (4.1796)	LR 2.500e-04
1: TRAIN [1][560/683]	Time 0.417 (0.440)	Data 9.44e-05 (3.30e-04)	Tok/s 11577 (11446)	Loss/tok 3.6805 (4.1737)	LR 2.500e-04
0: TRAIN [1][560/683]	Time 0.422 (0.440)	Data 1.14e-04 (3.74e-04)	Tok/s 11373 (11436)	Loss/tok 3.7644 (4.1812)	LR 2.500e-04
1: TRAIN [1][570/683]	Time 0.324 (0.440)	Data 9.27e-05 (3.26e-04)	Tok/s 8867 (11446)	Loss/tok 3.4890 (4.1673)	LR 1.250e-04
0: TRAIN [1][570/683]	Time 0.324 (0.440)	Data 1.10e-04 (3.70e-04)	Tok/s 9276 (11439)	Loss/tok 3.6962 (4.1753)	LR 1.250e-04
0: TRAIN [1][580/683]	Time 0.252 (0.440)	Data 1.22e-04 (3.66e-04)	Tok/s 5850 (11433)	Loss/tok 3.1181 (4.1704)	LR 1.250e-04
1: TRAIN [1][580/683]	Time 0.252 (0.440)	Data 8.03e-05 (3.22e-04)	Tok/s 5756 (11439)	Loss/tok 3.2010 (4.1624)	LR 1.250e-04
1: TRAIN [1][590/683]	Time 0.520 (0.438)	Data 8.34e-05 (3.18e-04)	Tok/s 13256 (11420)	Loss/tok 3.9121 (4.1568)	LR 1.250e-04
0: TRAIN [1][590/683]	Time 0.520 (0.438)	Data 1.22e-04 (3.61e-04)	Tok/s 12927 (11413)	Loss/tok 4.0527 (4.1645)	LR 1.250e-04
0: TRAIN [1][600/683]	Time 0.631 (0.439)	Data 1.14e-04 (3.57e-04)	Tok/s 13959 (11423)	Loss/tok 4.1932 (4.1615)	LR 1.250e-04
1: TRAIN [1][600/683]	Time 0.632 (0.439)	Data 9.18e-05 (3.15e-04)	Tok/s 14030 (11431)	Loss/tok 4.0271 (4.1529)	LR 1.250e-04
0: TRAIN [1][610/683]	Time 0.642 (0.439)	Data 1.12e-04 (3.53e-04)	Tok/s 13663 (11417)	Loss/tok 4.1610 (4.1564)	LR 1.250e-04
1: TRAIN [1][610/683]	Time 0.642 (0.439)	Data 8.89e-05 (3.11e-04)	Tok/s 13815 (11424)	Loss/tok 4.1212 (4.1480)	LR 1.250e-04
0: TRAIN [1][620/683]	Time 0.257 (0.439)	Data 1.21e-04 (3.49e-04)	Tok/s 5463 (11418)	Loss/tok 3.0047 (4.1536)	LR 1.250e-04
1: TRAIN [1][620/683]	Time 0.257 (0.439)	Data 9.49e-05 (3.07e-04)	Tok/s 5752 (11426)	Loss/tok 3.1606 (4.1447)	LR 1.250e-04
1: TRAIN [1][630/683]	Time 0.404 (0.440)	Data 8.65e-05 (3.04e-04)	Tok/s 12027 (11434)	Loss/tok 3.8145 (4.1406)	LR 1.250e-04
0: TRAIN [1][630/683]	Time 0.404 (0.440)	Data 1.20e-04 (3.46e-04)	Tok/s 12238 (11428)	Loss/tok 3.7177 (4.1491)	LR 1.250e-04
0: TRAIN [1][640/683]	Time 0.410 (0.440)	Data 1.16e-04 (3.42e-04)	Tok/s 11841 (11436)	Loss/tok 3.7734 (4.1439)	LR 1.250e-04
1: TRAIN [1][640/683]	Time 0.410 (0.440)	Data 1.44e-04 (3.01e-04)	Tok/s 11849 (11442)	Loss/tok 3.7889 (4.1358)	LR 1.250e-04
1: TRAIN [1][650/683]	Time 0.401 (0.441)	Data 7.75e-05 (2.97e-04)	Tok/s 11864 (11462)	Loss/tok 3.6907 (4.1318)	LR 1.250e-04
0: TRAIN [1][650/683]	Time 0.401 (0.441)	Data 1.13e-04 (3.39e-04)	Tok/s 12502 (11457)	Loss/tok 3.9068 (4.1396)	LR 1.250e-04
0: TRAIN [1][660/683]	Time 0.410 (0.440)	Data 8.56e-05 (3.35e-04)	Tok/s 11805 (11440)	Loss/tok 3.7420 (4.1357)	LR 1.250e-04
1: TRAIN [1][660/683]	Time 0.412 (0.440)	Data 8.32e-05 (2.94e-04)	Tok/s 11502 (11445)	Loss/tok 3.7857 (4.1286)	LR 1.250e-04
1: TRAIN [1][670/683]	Time 0.614 (0.440)	Data 8.32e-05 (2.91e-04)	Tok/s 14245 (11443)	Loss/tok 4.1871 (4.1248)	LR 1.250e-04
0: TRAIN [1][670/683]	Time 0.614 (0.440)	Data 1.02e-04 (3.32e-04)	Tok/s 14696 (11438)	Loss/tok 4.1651 (4.1319)	LR 1.250e-04
1: TRAIN [1][680/683]	Time 0.414 (0.440)	Data 3.62e-05 (2.90e-04)	Tok/s 11750 (11438)	Loss/tok 3.6603 (4.1211)	LR 1.250e-04
0: TRAIN [1][680/683]	Time 0.414 (0.440)	Data 4.39e-05 (3.30e-04)	Tok/s 11739 (11432)	Loss/tok 3.6602 (4.1284)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.112 (0.112)	Data 1.67e-03 (1.67e-03)	Tok/s 42444 (42444)	Loss/tok 5.4089 (5.4089)
0: VALIDATION [1][0/80]	Time 0.152 (0.152)	Data 1.75e-03 (1.75e-03)	Tok/s 37763 (37763)	Loss/tok 5.5641 (5.5641)
1: VALIDATION [1][10/80]	Time 0.065 (0.081)	Data 1.39e-03 (1.45e-03)	Tok/s 44494 (43266)	Loss/tok 4.9561 (5.2077)
0: VALIDATION [1][10/80]	Time 0.062 (0.083)	Data 1.38e-03 (1.47e-03)	Tok/s 47028 (44967)	Loss/tok 4.9948 (5.1919)
0: VALIDATION [1][20/80]	Time 0.051 (0.071)	Data 1.34e-03 (1.41e-03)	Tok/s 45374 (45014)	Loss/tok 4.7159 (5.1357)
1: VALIDATION [1][20/80]	Time 0.054 (0.071)	Data 1.33e-03 (1.39e-03)	Tok/s 42794 (43296)	Loss/tok 4.6613 (5.1120)
0: VALIDATION [1][30/80]	Time 0.043 (0.063)	Data 1.27e-03 (1.39e-03)	Tok/s 44965 (45229)	Loss/tok 4.7908 (5.0721)
1: VALIDATION [1][30/80]	Time 0.045 (0.064)	Data 1.27e-03 (1.37e-03)	Tok/s 42554 (43236)	Loss/tok 4.8475 (5.0393)
0: VALIDATION [1][40/80]	Time 0.037 (0.057)	Data 1.29e-03 (1.37e-03)	Tok/s 44000 (45016)	Loss/tok 4.6620 (5.0377)
1: VALIDATION [1][40/80]	Time 0.039 (0.058)	Data 1.25e-03 (1.35e-03)	Tok/s 40785 (42745)	Loss/tok 4.7651 (4.9832)
0: VALIDATION [1][50/80]	Time 0.030 (0.053)	Data 1.32e-03 (1.36e-03)	Tok/s 43904 (44708)	Loss/tok 4.8791 (4.9964)
1: VALIDATION [1][50/80]	Time 0.032 (0.054)	Data 1.25e-03 (1.34e-03)	Tok/s 40720 (42419)	Loss/tok 4.8776 (4.9606)
0: VALIDATION [1][60/80]	Time 0.026 (0.049)	Data 1.25e-03 (1.35e-03)	Tok/s 41069 (44335)	Loss/tok 4.6697 (4.9610)
1: VALIDATION [1][60/80]	Time 0.028 (0.050)	Data 1.25e-03 (1.33e-03)	Tok/s 38226 (41992)	Loss/tok 4.5688 (4.9337)
0: VALIDATION [1][70/80]	Time 0.021 (0.045)	Data 1.26e-03 (1.34e-03)	Tok/s 37654 (43758)	Loss/tok 4.6137 (4.9374)
1: VALIDATION [1][70/80]	Time 0.022 (0.046)	Data 1.31e-03 (1.32e-03)	Tok/s 36450 (41451)	Loss/tok 4.5433 (4.9064)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
0: TEST [1][9/47]	Time 0.4166 (0.7017)	Decoder iters 57.0 (139.8)	Tok/s 6219 (4895)
1: TEST [1][9/47]	Time 0.4167 (0.7017)	Decoder iters 67.0 (132.1)	Tok/s 6362 (4738)
0: TEST [1][19/47]	Time 0.5368 (0.6180)	Decoder iters 149.0 (130.6)	Tok/s 3668 (4614)
1: TEST [1][19/47]	Time 0.5368 (0.6180)	Decoder iters 149.0 (122.7)	Tok/s 3999 (4528)
0: TEST [1][29/47]	Time 0.4842 (0.5413)	Decoder iters 149.0 (105.5)	Tok/s 3046 (4676)
1: TEST [1][29/47]	Time 0.4843 (0.5413)	Decoder iters 149.0 (114.8)	Tok/s 3040 (4617)
0: TEST [1][39/47]	Time 0.4345 (0.4942)	Decoder iters 149.0 (103.0)	Tok/s 2456 (4533)
1: TEST [1][39/47]	Time 0.4344 (0.4941)	Decoder iters 27.0 (97.2)	Tok/s 2337 (4463)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.1235	Validation Loss: 4.8956	Test BLEU: 8.05
0: Performance: Epoch: 1	Training: 22864 Tok/s	Validation: 83416 Tok/s
0: Finished epoch 1
1: Total training time 686 s
0: Total training time 686 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 108|                      8.05|                      22847.7|                         11.43|
DONE!
