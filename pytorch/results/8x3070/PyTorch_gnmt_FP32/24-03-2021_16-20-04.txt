1: Collecting environment information...
6: Collecting environment information...
7: Collecting environment information...
5: Collecting environment information...
3: Collecting environment information...
2: Collecting environment information...
4: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
7: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
7: Saving results to: gnmt
7: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=7, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=7, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
7: Using master seed from command line: 2
6: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
6: Saving results to: gnmt
6: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=6, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=6, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
6: Using master seed from command line: 2
5: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
5: Saving results to: gnmt
5: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=5, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=5, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
5: Using master seed from command line: 2
3: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
3: Saving results to: gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
3: Using master seed from command line: 2
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
2: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
2: Saving results to: gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
2: Using master seed from command line: 2
4: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
4: Saving results to: gnmt
4: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=4, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=4, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=40, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=8, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
4: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
2: Worker 2 is using worker seed: 3588440356
4: Worker 4 is using worker seed: 2602510382
3: Worker 3 is using worker seed: 1323436024
7: Worker 7 is using worker seed: 117874757
5: Worker 5 is using worker seed: 2606193617
6: Worker 6 is using worker seed: 4077622522
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
7: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
6: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
4: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Size of vocabulary: 31794
2: Size of vocabulary: 31794
1: Size of vocabulary: 31794
7: Size of vocabulary: 31794
3: Size of vocabulary: 31794
6: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Size of vocabulary: 31794
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
5: Filtering data, min len: 0, max len: 50
7: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
4: Filtering data, min len: 0, max len: 50
6: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
5: Pairs before: 160078, after: 148120
7: Pairs before: 160078, after: 148120
4: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
6: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Pairs before: 160078, after: 148120
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
5: Filtering data, min len: 0, max len: 125
7: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
3: Filtering data, min len: 0, max len: 125
5: Pairs before: 5100, after: 5100
7: Pairs before: 5100, after: 5100
3: Pairs before: 5100, after: 5100
6: Filtering data, min len: 0, max len: 125
4: Filtering data, min len: 0, max len: 125
2: Filtering data, min len: 0, max len: 125
6: Pairs before: 5100, after: 5100
4: Pairs before: 5100, after: 5100
2: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 125
7: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Pairs before: 5100, after: 5100
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
5: Filtering data, min len: 0, max len: 150
6: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Pairs before: 3003, after: 3003
4: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Filtering data, min len: 0, max len: 150
3: Filtering data, min len: 0, max len: 150
7: Pairs before: 3003, after: 3003
3: Pairs before: 3003, after: 3003
6: Filtering data, min len: 0, max len: 150
4: Filtering data, min len: 0, max len: 150
6: Pairs before: 3003, after: 3003
2: Filtering data, min len: 0, max len: 150
4: Pairs before: 3003, after: 3003
2: Pairs before: 3003, after: 3003
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
7: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: Building LabelSmoothingLoss (smoothing: 0.1)
7: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
7: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
7: Number of parameters: 159593523
7: Saving state of the tokenizer
7: Initializing fp32 optimizer
7: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
7: Scheduler warmup steps: 200
7: Scheduler remain steps: 614
7: Scheduler decay interval: 77
7: Scheduler decay factor: 0.5
7: Scheduler max decay steps: 4
7: Starting epoch 0
7: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159593523
4: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
4: Building LabelSmoothingLoss (smoothing: 0.1)
4: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
4: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
4: Number of parameters: 159593523
2: Saving state of the tokenizer
2: Initializing fp32 optimizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 614
2: Scheduler decay interval: 77
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
4: Saving state of the tokenizer
4: Initializing fp32 optimizer
4: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
4: Scheduler warmup steps: 200
4: Scheduler remain steps: 614
4: Scheduler decay interval: 77
4: Scheduler decay factor: 0.5
4: Scheduler max decay steps: 4
2: Starting epoch 0
2: Executing preallocation
4: Starting epoch 0
4: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 614
1: Scheduler decay interval: 77
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
6: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
6: Building LabelSmoothingLoss (smoothing: 0.1)
6: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
6: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
6: Number of parameters: 159593523
6: Saving state of the tokenizer
6: Initializing fp32 optimizer
6: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Scheduler warmup steps: 200
6: Scheduler remain steps: 614
6: Scheduler decay interval: 77
6: Scheduler decay factor: 0.5
6: Scheduler max decay steps: 4
6: Starting epoch 0
6: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
3: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Initializing fp32 optimizer
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 614
0: Scheduler decay interval: 77
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 614
3: Scheduler decay interval: 77
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
3: Starting epoch 0
3: Executing preallocation
5: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
5: Building LabelSmoothingLoss (smoothing: 0.1)
5: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
5: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
5: Number of parameters: 159593523
5: Saving state of the tokenizer
5: Initializing fp32 optimizer
5: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
5: Scheduler warmup steps: 200
5: Scheduler remain steps: 614
5: Scheduler decay interval: 77
5: Scheduler decay factor: 0.5
5: Scheduler max decay steps: 4
5: Starting epoch 0
5: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 1632151663
1: Sampler for epoch 0 uses seed 1632151663
4: Sampler for epoch 0 uses seed 1632151663
3: Sampler for epoch 0 uses seed 1632151663
2: Sampler for epoch 0 uses seed 1632151663
7: Sampler for epoch 0 uses seed 1632151663
5: Sampler for epoch 0 uses seed 1632151663
6: Sampler for epoch 0 uses seed 1632151663
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
7: TRAIN [0][0/461]	Time 1.274 (0.000)	Data 1.13e-01 (0.00e+00)	Tok/s 1395 (0)	Loss/tok 10.6674 (10.6674)	LR 2.047e-05
0: TRAIN [0][0/461]	Time 1.311 (0.000)	Data 1.02e-01 (0.00e+00)	Tok/s 1313 (0)	Loss/tok 10.7028 (10.7028)	LR 2.047e-05
1: TRAIN [0][0/461]	Time 1.314 (0.000)	Data 1.15e-01 (0.00e+00)	Tok/s 1387 (0)	Loss/tok 10.6537 (10.6537)	LR 2.047e-05
6: TRAIN [0][0/461]	Time 1.277 (0.000)	Data 1.30e-01 (0.00e+00)	Tok/s 1398 (0)	Loss/tok 10.6932 (10.6932)	LR 2.047e-05
5: TRAIN [0][0/461]	Time 1.279 (0.000)	Data 1.11e-01 (0.00e+00)	Tok/s 1432 (0)	Loss/tok 10.6926 (10.6926)	LR 2.047e-05
2: TRAIN [0][0/461]	Time 1.286 (0.000)	Data 1.05e-01 (0.00e+00)	Tok/s 1439 (0)	Loss/tok 10.7107 (10.7107)	LR 2.047e-05
3: TRAIN [0][0/461]	Time 1.289 (0.000)	Data 1.03e-01 (0.00e+00)	Tok/s 1404 (0)	Loss/tok 10.7003 (10.7003)	LR 2.047e-05
4: TRAIN [0][0/461]	Time 1.289 (0.000)	Data 9.68e-02 (0.00e+00)	Tok/s 1396 (0)	Loss/tok 10.6847 (10.6847)	LR 2.047e-05
7: TRAIN [0][10/461]	Time 0.729 (0.718)	Data 6.51e-05 (8.47e-05)	Tok/s 3480 (2911)	Loss/tok 9.6655 (10.0629)	LR 2.576e-05
0: TRAIN [0][10/461]	Time 0.729 (0.718)	Data 8.77e-05 (1.12e-04)	Tok/s 3413 (2905)	Loss/tok 9.6429 (10.0725)	LR 2.576e-05
1: TRAIN [0][10/461]	Time 0.728 (0.718)	Data 1.29e-04 (1.13e-04)	Tok/s 3391 (2898)	Loss/tok 9.5829 (10.0658)	LR 2.576e-05
6: TRAIN [0][10/461]	Time 0.728 (0.718)	Data 1.77e-04 (1.08e-04)	Tok/s 3430 (2892)	Loss/tok 9.5571 (10.0584)	LR 2.576e-05
5: TRAIN [0][10/461]	Time 0.729 (0.718)	Data 8.99e-05 (8.84e-05)	Tok/s 3419 (2884)	Loss/tok 9.6016 (10.0791)	LR 2.576e-05
2: TRAIN [0][10/461]	Time 0.729 (0.718)	Data 1.08e-04 (1.06e-04)	Tok/s 3433 (2921)	Loss/tok 9.7145 (10.1030)	LR 2.576e-05
3: TRAIN [0][10/461]	Time 0.729 (0.718)	Data 9.39e-05 (8.64e-05)	Tok/s 3399 (2848)	Loss/tok 9.6133 (10.0620)	LR 2.576e-05
4: TRAIN [0][10/461]	Time 0.729 (0.718)	Data 6.91e-05 (7.58e-05)	Tok/s 3496 (2877)	Loss/tok 9.5740 (10.0608)	LR 2.576e-05
7: TRAIN [0][20/461]	Time 0.680 (0.725)	Data 6.72e-05 (8.22e-05)	Tok/s 1642 (3067)	Loss/tok 8.9158 (9.7118)	LR 3.244e-05
0: TRAIN [0][20/461]	Time 0.681 (0.725)	Data 1.31e-04 (1.10e-04)	Tok/s 1623 (3068)	Loss/tok 9.0669 (9.7290)	LR 3.244e-05
1: TRAIN [0][20/461]	Time 0.681 (0.725)	Data 8.68e-05 (1.10e-04)	Tok/s 1578 (3055)	Loss/tok 9.0269 (9.7259)	LR 3.244e-05
6: TRAIN [0][20/461]	Time 0.680 (0.725)	Data 7.01e-05 (1.17e-04)	Tok/s 1625 (3041)	Loss/tok 9.0496 (9.7201)	LR 3.244e-05
2: TRAIN [0][20/461]	Time 0.680 (0.725)	Data 1.10e-04 (1.02e-04)	Tok/s 1617 (3077)	Loss/tok 9.0459 (9.7440)	LR 3.244e-05
5: TRAIN [0][20/461]	Time 0.681 (0.725)	Data 7.41e-05 (8.72e-05)	Tok/s 1641 (3058)	Loss/tok 8.9975 (9.7334)	LR 3.244e-05
3: TRAIN [0][20/461]	Time 0.680 (0.725)	Data 1.18e-04 (8.72e-05)	Tok/s 1535 (3031)	Loss/tok 8.9939 (9.7042)	LR 3.244e-05
4: TRAIN [0][20/461]	Time 0.680 (0.725)	Data 7.10e-05 (8.01e-05)	Tok/s 1695 (3061)	Loss/tok 9.0557 (9.7236)	LR 3.244e-05
7: TRAIN [0][30/461]	Time 0.707 (0.720)	Data 7.99e-05 (8.41e-05)	Tok/s 2588 (2896)	Loss/tok 8.8481 (9.5163)	LR 4.083e-05
0: TRAIN [0][30/461]	Time 0.707 (0.720)	Data 1.13e-04 (1.19e-04)	Tok/s 2452 (2883)	Loss/tok 8.9057 (9.5248)	LR 4.083e-05
1: TRAIN [0][30/461]	Time 0.707 (0.720)	Data 9.04e-05 (1.08e-04)	Tok/s 2496 (2872)	Loss/tok 8.8704 (9.5249)	LR 4.083e-05
6: TRAIN [0][30/461]	Time 0.707 (0.720)	Data 7.99e-05 (1.13e-04)	Tok/s 2423 (2870)	Loss/tok 8.8488 (9.5232)	LR 4.083e-05
5: TRAIN [0][30/461]	Time 0.708 (0.720)	Data 7.56e-05 (8.73e-05)	Tok/s 2547 (2873)	Loss/tok 8.8470 (9.5271)	LR 4.083e-05
2: TRAIN [0][30/461]	Time 0.708 (0.720)	Data 9.54e-05 (1.00e-04)	Tok/s 2552 (2893)	Loss/tok 8.9198 (9.5317)	LR 4.083e-05
4: TRAIN [0][30/461]	Time 0.709 (0.720)	Data 1.00e-04 (8.23e-05)	Tok/s 2455 (2881)	Loss/tok 8.8831 (9.5241)	LR 4.083e-05
3: TRAIN [0][30/461]	Time 0.710 (0.720)	Data 8.96e-05 (8.70e-05)	Tok/s 2504 (2859)	Loss/tok 8.8674 (9.5143)	LR 4.083e-05
7: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 8.73e-05 (8.38e-05)	Tok/s 1571 (2793)	Loss/tok 8.7816 (9.3616)	LR 5.141e-05
0: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 1.10e-04 (1.17e-04)	Tok/s 1575 (2781)	Loss/tok 8.5310 (9.3620)	LR 5.141e-05
6: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 7.22e-05 (1.10e-04)	Tok/s 1642 (2775)	Loss/tok 8.6203 (9.3654)	LR 5.141e-05
1: TRAIN [0][40/461]	Time 0.684 (0.719)	Data 1.19e-04 (1.07e-04)	Tok/s 1646 (2776)	Loss/tok 8.5756 (9.3605)	LR 5.141e-05
5: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 9.32e-05 (8.78e-05)	Tok/s 1529 (2766)	Loss/tok 8.6019 (9.3749)	LR 5.141e-05
2: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 9.11e-05 (9.94e-05)	Tok/s 1567 (2789)	Loss/tok 8.7108 (9.3784)	LR 5.141e-05
3: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 8.85e-05 (8.81e-05)	Tok/s 1638 (2765)	Loss/tok 8.7891 (9.3674)	LR 5.141e-05
4: TRAIN [0][40/461]	Time 0.683 (0.719)	Data 7.37e-05 (8.25e-05)	Tok/s 1528 (2778)	Loss/tok 8.6742 (9.3756)	LR 5.141e-05
0: TRAIN [0][50/461]	Time 0.697 (0.720)	Data 1.52e-04 (1.15e-04)	Tok/s 2638 (2817)	Loss/tok 8.4333 (9.2029)	LR 6.472e-05
7: TRAIN [0][50/461]	Time 0.698 (0.720)	Data 1.01e-04 (8.65e-05)	Tok/s 2584 (2820)	Loss/tok 8.5430 (9.2041)	LR 6.472e-05
1: TRAIN [0][50/461]	Time 0.697 (0.720)	Data 1.02e-04 (1.06e-04)	Tok/s 2642 (2817)	Loss/tok 8.5537 (9.2037)	LR 6.472e-05
6: TRAIN [0][50/461]	Time 0.697 (0.720)	Data 1.06e-04 (1.07e-04)	Tok/s 2573 (2811)	Loss/tok 8.4121 (9.2109)	LR 6.472e-05
5: TRAIN [0][50/461]	Time 0.697 (0.720)	Data 1.12e-04 (8.76e-05)	Tok/s 2506 (2803)	Loss/tok 8.5541 (9.2112)	LR 6.472e-05
2: TRAIN [0][50/461]	Time 0.698 (0.720)	Data 9.11e-05 (9.74e-05)	Tok/s 2554 (2822)	Loss/tok 8.4949 (9.2150)	LR 6.472e-05
4: TRAIN [0][50/461]	Time 0.698 (0.720)	Data 7.89e-05 (8.27e-05)	Tok/s 2614 (2818)	Loss/tok 8.4987 (9.2189)	LR 6.472e-05
3: TRAIN [0][50/461]	Time 0.698 (0.720)	Data 8.65e-05 (8.75e-05)	Tok/s 2606 (2808)	Loss/tok 8.3816 (9.2016)	LR 6.472e-05
7: TRAIN [0][60/461]	Time 0.688 (0.720)	Data 7.84e-05 (8.99e-05)	Tok/s 1518 (2791)	Loss/tok 7.9702 (9.0835)	LR 8.148e-05
0: TRAIN [0][60/461]	Time 0.688 (0.720)	Data 1.06e-04 (1.15e-04)	Tok/s 1519 (2788)	Loss/tok 8.0390 (9.0866)	LR 8.148e-05
6: TRAIN [0][60/461]	Time 0.687 (0.720)	Data 6.91e-05 (1.05e-04)	Tok/s 1612 (2781)	Loss/tok 8.0918 (9.0891)	LR 8.148e-05
1: TRAIN [0][60/461]	Time 0.687 (0.720)	Data 9.94e-05 (1.04e-04)	Tok/s 1609 (2790)	Loss/tok 8.0039 (9.0783)	LR 8.148e-05
5: TRAIN [0][60/461]	Time 0.686 (0.720)	Data 1.03e-04 (8.77e-05)	Tok/s 1562 (2779)	Loss/tok 8.2226 (9.0887)	LR 8.148e-05
2: TRAIN [0][60/461]	Time 0.686 (0.720)	Data 1.16e-04 (9.53e-05)	Tok/s 1540 (2792)	Loss/tok 8.0949 (9.0925)	LR 8.148e-05
4: TRAIN [0][60/461]	Time 0.686 (0.720)	Data 7.89e-05 (8.28e-05)	Tok/s 1562 (2791)	Loss/tok 8.0138 (9.0988)	LR 8.148e-05
3: TRAIN [0][60/461]	Time 0.686 (0.720)	Data 8.15e-05 (8.77e-05)	Tok/s 1696 (2783)	Loss/tok 8.1334 (9.0792)	LR 8.148e-05
0: TRAIN [0][70/461]	Time 0.699 (0.720)	Data 1.15e-04 (1.13e-04)	Tok/s 2679 (2792)	Loss/tok 8.1952 (8.9610)	LR 1.026e-04
7: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 2.17e-04 (9.57e-05)	Tok/s 2543 (2790)	Loss/tok 7.9206 (8.9518)	LR 1.026e-04
6: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 7.87e-05 (1.01e-04)	Tok/s 2529 (2780)	Loss/tok 8.0488 (8.9587)	LR 1.026e-04
1: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 1.02e-04 (1.03e-04)	Tok/s 2584 (2794)	Loss/tok 8.0700 (8.9502)	LR 1.026e-04
5: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 8.18e-05 (8.80e-05)	Tok/s 2643 (2783)	Loss/tok 8.1013 (8.9528)	LR 1.026e-04
2: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 1.12e-04 (9.34e-05)	Tok/s 2527 (2793)	Loss/tok 8.1369 (8.9573)	LR 1.026e-04
3: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 8.01e-05 (8.79e-05)	Tok/s 2578 (2788)	Loss/tok 8.1131 (8.9477)	LR 1.026e-04
4: TRAIN [0][70/461]	Time 0.698 (0.720)	Data 6.84e-05 (8.25e-05)	Tok/s 2541 (2788)	Loss/tok 8.1810 (8.9731)	LR 1.026e-04
7: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 1.17e-04 (1.07e-04)	Tok/s 1500 (2712)	Loss/tok 7.7311 (8.8527)	LR 1.291e-04
0: TRAIN [0][80/461]	Time 0.716 (0.718)	Data 1.24e-04 (1.12e-04)	Tok/s 1627 (2715)	Loss/tok 7.8545 (8.8705)	LR 1.291e-04
6: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 7.03e-05 (1.00e-04)	Tok/s 1533 (2702)	Loss/tok 7.7577 (8.8630)	LR 1.291e-04
1: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 9.20e-05 (1.02e-04)	Tok/s 1507 (2713)	Loss/tok 7.7407 (8.8520)	LR 1.291e-04
2: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 7.63e-05 (9.16e-05)	Tok/s 1487 (2712)	Loss/tok 7.6488 (8.8626)	LR 1.291e-04
5: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 7.70e-05 (8.81e-05)	Tok/s 1625 (2706)	Loss/tok 7.8140 (8.8575)	LR 1.291e-04
4: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 1.03e-04 (8.26e-05)	Tok/s 1552 (2711)	Loss/tok 7.6259 (8.8707)	LR 1.291e-04
3: TRAIN [0][80/461]	Time 0.715 (0.718)	Data 8.89e-05 (8.81e-05)	Tok/s 1504 (2710)	Loss/tok 7.4633 (8.8501)	LR 1.291e-04
0: TRAIN [0][90/461]	Time 0.696 (0.717)	Data 8.03e-05 (1.10e-04)	Tok/s 2559 (2686)	Loss/tok 7.7799 (8.7734)	LR 1.626e-04
7: TRAIN [0][90/461]	Time 0.695 (0.717)	Data 1.38e-04 (1.13e-04)	Tok/s 2623 (2686)	Loss/tok 7.6820 (8.7558)	LR 1.626e-04
1: TRAIN [0][90/461]	Time 0.694 (0.717)	Data 1.09e-04 (1.02e-04)	Tok/s 2678 (2684)	Loss/tok 7.8214 (8.7564)	LR 1.626e-04
6: TRAIN [0][90/461]	Time 0.694 (0.717)	Data 7.22e-05 (9.82e-05)	Tok/s 2576 (2672)	Loss/tok 7.8303 (8.7649)	LR 1.626e-04
5: TRAIN [0][90/461]	Time 0.695 (0.717)	Data 9.37e-05 (8.79e-05)	Tok/s 2599 (2677)	Loss/tok 7.7971 (8.7567)	LR 1.626e-04
2: TRAIN [0][90/461]	Time 0.695 (0.717)	Data 7.65e-05 (9.03e-05)	Tok/s 2626 (2684)	Loss/tok 7.8632 (8.7645)	LR 1.626e-04
3: TRAIN [0][90/461]	Time 0.695 (0.717)	Data 1.01e-04 (8.84e-05)	Tok/s 2568 (2686)	Loss/tok 7.8321 (8.7556)	LR 1.626e-04
4: TRAIN [0][90/461]	Time 0.695 (0.717)	Data 6.82e-05 (8.21e-05)	Tok/s 2536 (2679)	Loss/tok 7.7385 (8.7744)	LR 1.626e-04
0: TRAIN [0][100/461]	Time 0.744 (0.716)	Data 7.01e-05 (1.09e-04)	Tok/s 3366 (2707)	Loss/tok 7.8931 (8.6730)	LR 2.047e-04
7: TRAIN [0][100/461]	Time 0.744 (0.716)	Data 9.87e-05 (1.12e-04)	Tok/s 3346 (2711)	Loss/tok 7.8535 (8.6595)	LR 2.047e-04
1: TRAIN [0][100/461]	Time 0.744 (0.716)	Data 9.39e-05 (1.01e-04)	Tok/s 3415 (2708)	Loss/tok 7.8768 (8.6562)	LR 2.047e-04
6: TRAIN [0][100/461]	Time 0.744 (0.716)	Data 1.08e-04 (9.68e-05)	Tok/s 3354 (2694)	Loss/tok 7.8746 (8.6673)	LR 2.047e-04
5: TRAIN [0][100/461]	Time 0.743 (0.716)	Data 7.61e-05 (8.73e-05)	Tok/s 3377 (2700)	Loss/tok 7.7273 (8.6540)	LR 2.047e-04
2: TRAIN [0][100/461]	Time 0.743 (0.716)	Data 1.09e-04 (8.97e-05)	Tok/s 3330 (2708)	Loss/tok 7.8066 (8.6648)	LR 2.047e-04
3: TRAIN [0][100/461]	Time 0.743 (0.716)	Data 8.27e-05 (8.82e-05)	Tok/s 3371 (2708)	Loss/tok 7.8551 (8.6595)	LR 2.047e-04
4: TRAIN [0][100/461]	Time 0.743 (0.716)	Data 7.25e-05 (8.17e-05)	Tok/s 3430 (2704)	Loss/tok 7.8644 (8.6744)	LR 2.047e-04
0: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 7.41e-05 (1.06e-04)	Tok/s 1652 (2671)	Loss/tok 7.4603 (8.6009)	LR 2.576e-04
7: TRAIN [0][110/461]	Time 0.677 (0.715)	Data 1.56e-04 (1.14e-04)	Tok/s 1665 (2681)	Loss/tok 7.5292 (8.5840)	LR 2.576e-04
6: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 6.39e-05 (9.54e-05)	Tok/s 1584 (2661)	Loss/tok 7.7922 (8.5955)	LR 2.576e-04
1: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 8.89e-05 (1.00e-04)	Tok/s 1633 (2674)	Loss/tok 7.4725 (8.5834)	LR 2.576e-04
5: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 8.65e-05 (8.73e-05)	Tok/s 1523 (2667)	Loss/tok 7.4704 (8.5831)	LR 2.576e-04
2: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 7.99e-05 (8.90e-05)	Tok/s 1669 (2674)	Loss/tok 7.5629 (8.5935)	LR 2.576e-04
3: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 8.37e-05 (8.84e-05)	Tok/s 1622 (2675)	Loss/tok 7.6833 (8.5878)	LR 2.576e-04
4: TRAIN [0][110/461]	Time 0.678 (0.715)	Data 7.30e-05 (8.19e-05)	Tok/s 1555 (2671)	Loss/tok 7.4103 (8.6002)	LR 2.576e-04
7: TRAIN [0][120/461]	Time 0.764 (0.715)	Data 1.37e-04 (1.14e-04)	Tok/s 4308 (2686)	Loss/tok 8.1224 (8.5358)	LR 3.244e-04
0: TRAIN [0][120/461]	Time 0.765 (0.715)	Data 7.25e-05 (1.05e-04)	Tok/s 4238 (2674)	Loss/tok 8.0365 (8.5543)	LR 3.244e-04
6: TRAIN [0][120/461]	Time 0.766 (0.715)	Data 6.96e-05 (9.40e-05)	Tok/s 4231 (2667)	Loss/tok 7.9468 (8.5462)	LR 3.244e-04
1: TRAIN [0][120/461]	Time 0.766 (0.715)	Data 7.10e-05 (9.83e-05)	Tok/s 4224 (2678)	Loss/tok 8.0429 (8.5331)	LR 3.244e-04
5: TRAIN [0][120/461]	Time 0.765 (0.715)	Data 8.30e-05 (8.75e-05)	Tok/s 4256 (2674)	Loss/tok 8.0077 (8.5342)	LR 3.244e-04
2: TRAIN [0][120/461]	Time 0.765 (0.715)	Data 7.20e-05 (8.81e-05)	Tok/s 4350 (2680)	Loss/tok 7.8901 (8.5430)	LR 3.244e-04
4: TRAIN [0][120/461]	Time 0.765 (0.715)	Data 7.22e-05 (8.16e-05)	Tok/s 4184 (2677)	Loss/tok 8.1095 (8.5529)	LR 3.244e-04
3: TRAIN [0][120/461]	Time 0.765 (0.715)	Data 9.82e-05 (8.86e-05)	Tok/s 4315 (2683)	Loss/tok 7.9371 (8.5367)	LR 3.244e-04
0: TRAIN [0][130/461]	Time 0.730 (0.714)	Data 6.94e-05 (1.03e-04)	Tok/s 3535 (2676)	Loss/tok 7.8215 (8.4902)	LR 4.083e-04
7: TRAIN [0][130/461]	Time 0.729 (0.714)	Data 1.21e-04 (1.14e-04)	Tok/s 3447 (2689)	Loss/tok 7.8744 (8.4701)	LR 4.083e-04
1: TRAIN [0][130/461]	Time 0.729 (0.714)	Data 6.44e-05 (9.61e-05)	Tok/s 3400 (2682)	Loss/tok 7.6551 (8.4678)	LR 4.083e-04
6: TRAIN [0][130/461]	Time 0.729 (0.714)	Data 7.94e-05 (9.28e-05)	Tok/s 3452 (2670)	Loss/tok 7.7937 (8.4798)	LR 4.083e-04
2: TRAIN [0][130/461]	Time 0.729 (0.714)	Data 7.34e-05 (8.76e-05)	Tok/s 3498 (2683)	Loss/tok 7.8418 (8.4795)	LR 4.083e-04
5: TRAIN [0][130/461]	Time 0.729 (0.714)	Data 8.34e-05 (8.75e-05)	Tok/s 3395 (2678)	Loss/tok 7.7295 (8.4707)	LR 4.083e-04
3: TRAIN [0][130/461]	Time 0.729 (0.714)	Data 1.02e-04 (8.88e-05)	Tok/s 3529 (2683)	Loss/tok 7.6998 (8.4723)	LR 4.083e-04
4: TRAIN [0][130/461]	Time 0.730 (0.714)	Data 6.96e-05 (8.12e-05)	Tok/s 3495 (2681)	Loss/tok 7.6669 (8.4865)	LR 4.083e-04
0: TRAIN [0][140/461]	Time 0.682 (0.714)	Data 9.35e-05 (1.02e-04)	Tok/s 1656 (2700)	Loss/tok 7.4424 (8.4309)	LR 5.141e-04
7: TRAIN [0][140/461]	Time 0.683 (0.714)	Data 1.16e-04 (1.15e-04)	Tok/s 1597 (2710)	Loss/tok 7.2976 (8.4129)	LR 5.141e-04
1: TRAIN [0][140/461]	Time 0.681 (0.714)	Data 6.20e-05 (9.45e-05)	Tok/s 1591 (2706)	Loss/tok 7.3746 (8.4105)	LR 5.141e-04
6: TRAIN [0][140/461]	Time 0.682 (0.714)	Data 7.06e-05 (9.18e-05)	Tok/s 1569 (2697)	Loss/tok 7.3331 (8.4200)	LR 5.141e-04
5: TRAIN [0][140/461]	Time 0.682 (0.714)	Data 7.58e-05 (8.71e-05)	Tok/s 1521 (2701)	Loss/tok 7.3274 (8.4106)	LR 5.141e-04
2: TRAIN [0][140/461]	Time 0.682 (0.714)	Data 9.87e-05 (8.70e-05)	Tok/s 1527 (2705)	Loss/tok 7.2386 (8.4248)	LR 5.141e-04
3: TRAIN [0][140/461]	Time 0.682 (0.714)	Data 8.58e-05 (8.88e-05)	Tok/s 1669 (2709)	Loss/tok 7.4650 (8.4143)	LR 5.141e-04
4: TRAIN [0][140/461]	Time 0.683 (0.714)	Data 6.99e-05 (8.07e-05)	Tok/s 1553 (2703)	Loss/tok 7.6163 (8.4249)	LR 5.141e-04
0: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 7.34e-05 (1.01e-04)	Tok/s 2620 (2703)	Loss/tok 7.3994 (8.3767)	LR 6.472e-04
7: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 1.07e-04 (1.15e-04)	Tok/s 2604 (2716)	Loss/tok 7.6210 (8.3648)	LR 6.472e-04
1: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 6.15e-05 (9.29e-05)	Tok/s 2671 (2711)	Loss/tok 7.5688 (8.3598)	LR 6.472e-04
6: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 7.39e-05 (9.16e-05)	Tok/s 2585 (2699)	Loss/tok 7.6288 (8.3729)	LR 6.472e-04
5: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 8.49e-05 (8.69e-05)	Tok/s 2663 (2704)	Loss/tok 7.5193 (8.3565)	LR 6.472e-04
2: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 7.65e-05 (8.63e-05)	Tok/s 2648 (2707)	Loss/tok 7.5338 (8.3719)	LR 6.472e-04
3: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 8.70e-05 (8.87e-05)	Tok/s 2576 (2709)	Loss/tok 7.3461 (8.3601)	LR 6.472e-04
4: TRAIN [0][150/461]	Time 0.689 (0.714)	Data 1.03e-04 (8.09e-05)	Tok/s 2671 (2707)	Loss/tok 7.6157 (8.3739)	LR 6.472e-04
7: TRAIN [0][160/461]	Time 0.714 (0.713)	Data 1.13e-04 (1.14e-04)	Tok/s 2527 (2698)	Loss/tok 7.5944 (8.3212)	LR 8.148e-04
0: TRAIN [0][160/461]	Time 0.715 (0.713)	Data 8.03e-05 (9.99e-05)	Tok/s 2536 (2687)	Loss/tok 7.6148 (8.3330)	LR 8.148e-04
1: TRAIN [0][160/461]	Time 0.714 (0.713)	Data 6.48e-05 (9.14e-05)	Tok/s 2581 (2693)	Loss/tok 7.5226 (8.3138)	LR 8.148e-04
6: TRAIN [0][160/461]	Time 0.714 (0.713)	Data 8.37e-05 (9.12e-05)	Tok/s 2579 (2682)	Loss/tok 7.6306 (8.3299)	LR 8.148e-04
5: TRAIN [0][160/461]	Time 0.714 (0.713)	Data 9.25e-05 (8.71e-05)	Tok/s 2405 (2686)	Loss/tok 7.6548 (8.3138)	LR 8.148e-04
2: TRAIN [0][160/461]	Time 0.714 (0.713)	Data 7.32e-05 (8.60e-05)	Tok/s 2514 (2690)	Loss/tok 7.5680 (8.3274)	LR 8.148e-04
3: TRAIN [0][160/461]	Time 0.715 (0.713)	Data 9.42e-05 (8.88e-05)	Tok/s 2501 (2693)	Loss/tok 7.5054 (8.3149)	LR 8.148e-04
4: TRAIN [0][160/461]	Time 0.715 (0.713)	Data 8.13e-05 (8.10e-05)	Tok/s 2493 (2690)	Loss/tok 7.6107 (8.3288)	LR 8.148e-04
0: TRAIN [0][170/461]	Time 0.718 (0.713)	Data 1.06e-04 (9.90e-05)	Tok/s 3485 (2682)	Loss/tok 7.5866 (8.2869)	LR 1.026e-03
7: TRAIN [0][170/461]	Time 0.719 (0.713)	Data 1.10e-04 (1.14e-04)	Tok/s 3453 (2694)	Loss/tok 7.6555 (8.2756)	LR 1.026e-03
1: TRAIN [0][170/461]	Time 0.718 (0.713)	Data 7.49e-05 (9.03e-05)	Tok/s 3484 (2689)	Loss/tok 7.5696 (8.2678)	LR 1.026e-03
6: TRAIN [0][170/461]	Time 0.718 (0.713)	Data 7.77e-05 (9.05e-05)	Tok/s 3468 (2680)	Loss/tok 7.6372 (8.2820)	LR 1.026e-03
5: TRAIN [0][170/461]	Time 0.718 (0.713)	Data 9.44e-05 (8.69e-05)	Tok/s 3529 (2683)	Loss/tok 7.5567 (8.2703)	LR 1.026e-03
2: TRAIN [0][170/461]	Time 0.718 (0.713)	Data 8.63e-05 (8.56e-05)	Tok/s 3488 (2689)	Loss/tok 7.6234 (8.2839)	LR 1.026e-03
3: TRAIN [0][170/461]	Time 0.718 (0.713)	Data 9.66e-05 (8.88e-05)	Tok/s 3425 (2690)	Loss/tok 7.4725 (8.2716)	LR 1.026e-03
4: TRAIN [0][170/461]	Time 0.719 (0.713)	Data 8.39e-05 (8.10e-05)	Tok/s 3595 (2688)	Loss/tok 7.6230 (8.2834)	LR 1.026e-03
0: TRAIN [0][180/461]	Time 0.729 (0.713)	Data 8.11e-05 (9.84e-05)	Tok/s 3516 (2671)	Loss/tok 7.6756 (8.2543)	LR 1.291e-03
7: TRAIN [0][180/461]	Time 0.729 (0.713)	Data 1.44e-04 (1.14e-04)	Tok/s 3394 (2682)	Loss/tok 7.6494 (8.2392)	LR 1.291e-03
1: TRAIN [0][180/461]	Time 0.728 (0.713)	Data 7.08e-05 (8.93e-05)	Tok/s 3407 (2676)	Loss/tok 7.6831 (8.2342)	LR 1.291e-03
6: TRAIN [0][180/461]	Time 0.728 (0.713)	Data 7.53e-05 (9.01e-05)	Tok/s 3421 (2667)	Loss/tok 7.6237 (8.2484)	LR 1.291e-03
5: TRAIN [0][180/461]	Time 0.729 (0.713)	Data 8.32e-05 (8.67e-05)	Tok/s 3479 (2671)	Loss/tok 7.6713 (8.2355)	LR 1.291e-03
2: TRAIN [0][180/461]	Time 0.729 (0.713)	Data 7.72e-05 (8.52e-05)	Tok/s 3463 (2678)	Loss/tok 7.6630 (8.2503)	LR 1.291e-03
3: TRAIN [0][180/461]	Time 0.728 (0.713)	Data 1.21e-04 (8.88e-05)	Tok/s 3431 (2678)	Loss/tok 7.8349 (8.2394)	LR 1.291e-03
4: TRAIN [0][180/461]	Time 0.729 (0.713)	Data 8.13e-05 (8.08e-05)	Tok/s 3403 (2674)	Loss/tok 7.8008 (8.2507)	LR 1.291e-03
0: TRAIN [0][190/461]	Time 0.680 (0.713)	Data 1.04e-04 (9.79e-05)	Tok/s 1646 (2671)	Loss/tok 7.1101 (8.2188)	LR 1.626e-03
7: TRAIN [0][190/461]	Time 0.680 (0.713)	Data 1.26e-04 (1.14e-04)	Tok/s 1607 (2683)	Loss/tok 7.1823 (8.2037)	LR 1.626e-03
1: TRAIN [0][190/461]	Time 0.679 (0.713)	Data 6.01e-05 (8.84e-05)	Tok/s 1634 (2676)	Loss/tok 7.1398 (8.1991)	LR 1.626e-03
6: TRAIN [0][190/461]	Time 0.679 (0.713)	Data 6.96e-05 (8.97e-05)	Tok/s 1592 (2666)	Loss/tok 7.0575 (8.2129)	LR 1.626e-03
5: TRAIN [0][190/461]	Time 0.679 (0.713)	Data 7.10e-05 (8.66e-05)	Tok/s 1620 (2673)	Loss/tok 7.1341 (8.1973)	LR 1.626e-03
2: TRAIN [0][190/461]	Time 0.679 (0.713)	Data 9.54e-05 (8.50e-05)	Tok/s 1610 (2677)	Loss/tok 7.1656 (8.2115)	LR 1.626e-03
3: TRAIN [0][190/461]	Time 0.679 (0.713)	Data 8.13e-05 (8.88e-05)	Tok/s 1545 (2677)	Loss/tok 7.2097 (8.2029)	LR 1.626e-03
4: TRAIN [0][190/461]	Time 0.679 (0.713)	Data 6.91e-05 (8.08e-05)	Tok/s 1656 (2676)	Loss/tok 7.1297 (8.2194)	LR 1.626e-03
0: TRAIN [0][200/461]	Time 0.706 (0.712)	Data 7.61e-05 (9.74e-05)	Tok/s 2597 (2644)	Loss/tok 7.3588 (8.1843)	LR 2.000e-03
7: TRAIN [0][200/461]	Time 0.706 (0.712)	Data 1.17e-04 (1.14e-04)	Tok/s 2504 (2656)	Loss/tok 7.4216 (8.1715)	LR 2.000e-03
1: TRAIN [0][200/461]	Time 0.706 (0.712)	Data 6.68e-05 (8.76e-05)	Tok/s 2491 (2651)	Loss/tok 7.2684 (8.1650)	LR 2.000e-03
6: TRAIN [0][200/461]	Time 0.706 (0.712)	Data 7.77e-05 (8.93e-05)	Tok/s 2554 (2641)	Loss/tok 7.4153 (8.1782)	LR 2.000e-03
5: TRAIN [0][200/461]	Time 0.706 (0.712)	Data 1.06e-04 (8.66e-05)	Tok/s 2548 (2647)	Loss/tok 7.3060 (8.1649)	LR 2.000e-03
2: TRAIN [0][200/461]	Time 0.707 (0.712)	Data 1.32e-04 (8.49e-05)	Tok/s 2569 (2650)	Loss/tok 7.3669 (8.1778)	LR 2.000e-03
3: TRAIN [0][200/461]	Time 0.707 (0.712)	Data 1.18e-04 (8.91e-05)	Tok/s 2575 (2652)	Loss/tok 7.3240 (8.1678)	LR 2.000e-03
4: TRAIN [0][200/461]	Time 0.707 (0.712)	Data 8.11e-05 (8.11e-05)	Tok/s 2454 (2649)	Loss/tok 7.3602 (8.1882)	LR 2.000e-03
0: TRAIN [0][210/461]	Time 0.720 (0.713)	Data 8.25e-05 (9.63e-05)	Tok/s 3558 (2673)	Loss/tok 7.5514 (8.1419)	LR 2.000e-03
7: TRAIN [0][210/461]	Time 0.720 (0.713)	Data 1.14e-04 (1.14e-04)	Tok/s 3599 (2684)	Loss/tok 7.5785 (8.1355)	LR 2.000e-03
1: TRAIN [0][210/461]	Time 0.720 (0.713)	Data 1.04e-04 (8.70e-05)	Tok/s 3481 (2679)	Loss/tok 7.7057 (8.1252)	LR 2.000e-03
6: TRAIN [0][210/461]	Time 0.721 (0.713)	Data 8.44e-05 (8.88e-05)	Tok/s 3520 (2669)	Loss/tok 7.5841 (8.1392)	LR 2.000e-03
2: TRAIN [0][210/461]	Time 0.720 (0.713)	Data 8.13e-05 (8.47e-05)	Tok/s 3375 (2678)	Loss/tok 7.5484 (8.1382)	LR 2.000e-03
5: TRAIN [0][210/461]	Time 0.720 (0.713)	Data 9.70e-05 (8.64e-05)	Tok/s 3472 (2675)	Loss/tok 7.5873 (8.1270)	LR 2.000e-03
3: TRAIN [0][210/461]	Time 0.719 (0.713)	Data 1.46e-04 (8.99e-05)	Tok/s 3436 (2680)	Loss/tok 7.4945 (8.1274)	LR 2.000e-03
4: TRAIN [0][210/461]	Time 0.720 (0.713)	Data 8.94e-05 (8.09e-05)	Tok/s 3572 (2677)	Loss/tok 7.3972 (8.1467)	LR 2.000e-03
0: TRAIN [0][220/461]	Time 0.692 (0.713)	Data 7.56e-05 (9.58e-05)	Tok/s 2560 (2676)	Loss/tok 7.0841 (8.1070)	LR 2.000e-03
7: TRAIN [0][220/461]	Time 0.692 (0.713)	Data 1.03e-04 (1.13e-04)	Tok/s 2704 (2688)	Loss/tok 7.0813 (8.1024)	LR 2.000e-03
1: TRAIN [0][220/461]	Time 0.691 (0.713)	Data 9.44e-05 (8.62e-05)	Tok/s 2598 (2681)	Loss/tok 7.3241 (8.0893)	LR 2.000e-03
6: TRAIN [0][220/461]	Time 0.691 (0.713)	Data 8.13e-05 (8.85e-05)	Tok/s 2613 (2672)	Loss/tok 7.2240 (8.1051)	LR 2.000e-03
2: TRAIN [0][220/461]	Time 0.691 (0.713)	Data 9.06e-05 (8.45e-05)	Tok/s 2665 (2680)	Loss/tok 7.0995 (8.1015)	LR 2.000e-03
5: TRAIN [0][220/461]	Time 0.691 (0.713)	Data 8.80e-05 (8.66e-05)	Tok/s 2674 (2679)	Loss/tok 7.1925 (8.0927)	LR 2.000e-03
3: TRAIN [0][220/461]	Time 0.692 (0.713)	Data 8.44e-05 (8.98e-05)	Tok/s 2559 (2683)	Loss/tok 7.2154 (8.0905)	LR 2.000e-03
4: TRAIN [0][220/461]	Time 0.692 (0.713)	Data 7.06e-05 (8.07e-05)	Tok/s 2571 (2679)	Loss/tok 7.0425 (8.1097)	LR 2.000e-03
0: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 6.58e-05 (9.56e-05)	Tok/s 2488 (2695)	Loss/tok 6.9015 (8.0590)	LR 2.000e-03
7: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 1.10e-04 (1.13e-04)	Tok/s 2493 (2706)	Loss/tok 6.8039 (8.0568)	LR 2.000e-03
1: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 7.18e-05 (8.55e-05)	Tok/s 2626 (2701)	Loss/tok 6.8835 (8.0423)	LR 2.000e-03
6: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 8.01e-05 (8.81e-05)	Tok/s 2532 (2692)	Loss/tok 6.9319 (8.0581)	LR 2.000e-03
5: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 9.27e-05 (8.63e-05)	Tok/s 2524 (2699)	Loss/tok 6.7787 (8.0473)	LR 2.000e-03
2: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 7.63e-05 (8.41e-05)	Tok/s 2518 (2699)	Loss/tok 6.9228 (8.0531)	LR 2.000e-03
4: TRAIN [0][230/461]	Time 0.712 (0.714)	Data 8.32e-05 (8.06e-05)	Tok/s 2542 (2699)	Loss/tok 6.9799 (8.0642)	LR 2.000e-03
3: TRAIN [0][230/461]	Time 0.711 (0.714)	Data 1.15e-04 (8.98e-05)	Tok/s 2584 (2702)	Loss/tok 6.9805 (8.0444)	LR 2.000e-03
0: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 1.04e-04 (9.52e-05)	Tok/s 3580 (2690)	Loss/tok 7.1442 (8.0177)	LR 2.000e-03
7: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 1.41e-04 (1.13e-04)	Tok/s 3568 (2699)	Loss/tok 7.2359 (8.0168)	LR 2.000e-03
1: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 7.27e-05 (8.50e-05)	Tok/s 3616 (2695)	Loss/tok 7.0789 (8.0022)	LR 2.000e-03
6: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 7.49e-05 (8.79e-05)	Tok/s 3504 (2686)	Loss/tok 7.1526 (8.0162)	LR 2.000e-03
2: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 8.03e-05 (8.40e-05)	Tok/s 3453 (2691)	Loss/tok 7.1649 (8.0139)	LR 2.000e-03
5: TRAIN [0][240/461]	Time 0.715 (0.714)	Data 9.20e-05 (8.64e-05)	Tok/s 3543 (2694)	Loss/tok 7.1478 (8.0050)	LR 2.000e-03
3: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 9.08e-05 (8.96e-05)	Tok/s 3465 (2695)	Loss/tok 6.9954 (8.0054)	LR 2.000e-03
4: TRAIN [0][240/461]	Time 0.714 (0.714)	Data 8.87e-05 (8.07e-05)	Tok/s 3549 (2693)	Loss/tok 7.2097 (8.0236)	LR 2.000e-03
1: TRAIN [0][250/461]	Time 0.686 (0.714)	Data 6.20e-05 (8.44e-05)	Tok/s 2674 (2706)	Loss/tok 6.8101 (7.9555)	LR 2.000e-03
6: TRAIN [0][250/461]	Time 0.686 (0.714)	Data 6.46e-05 (8.76e-05)	Tok/s 2598 (2698)	Loss/tok 6.6300 (7.9673)	LR 2.000e-03
5: TRAIN [0][250/461]	Time 0.685 (0.714)	Data 7.77e-05 (8.61e-05)	Tok/s 2700 (2705)	Loss/tok 6.8969 (7.9599)	LR 2.000e-03
0: TRAIN [0][250/461]	Time 0.688 (0.714)	Data 6.72e-05 (9.47e-05)	Tok/s 2624 (2702)	Loss/tok 6.8027 (7.9719)	LR 2.000e-03
7: TRAIN [0][250/461]	Time 0.688 (0.714)	Data 1.03e-04 (1.13e-04)	Tok/s 2564 (2710)	Loss/tok 6.7192 (7.9693)	LR 2.000e-03
2: TRAIN [0][250/461]	Time 0.685 (0.714)	Data 8.20e-05 (8.38e-05)	Tok/s 2649 (2702)	Loss/tok 6.7979 (7.9671)	LR 2.000e-03
3: TRAIN [0][250/461]	Time 0.685 (0.714)	Data 8.32e-05 (8.94e-05)	Tok/s 2694 (2707)	Loss/tok 6.7520 (7.9602)	LR 2.000e-03
4: TRAIN [0][250/461]	Time 0.685 (0.714)	Data 7.37e-05 (8.08e-05)	Tok/s 2695 (2705)	Loss/tok 6.8339 (7.9768)	LR 2.000e-03
0: TRAIN [0][260/461]	Time 0.703 (0.714)	Data 1.08e-04 (9.42e-05)	Tok/s 3589 (2713)	Loss/tok 6.6864 (7.9225)	LR 2.000e-03
7: TRAIN [0][260/461]	Time 0.703 (0.714)	Data 1.50e-04 (1.13e-04)	Tok/s 3500 (2721)	Loss/tok 6.6372 (7.9198)	LR 2.000e-03
1: TRAIN [0][260/461]	Time 0.704 (0.714)	Data 7.46e-05 (8.39e-05)	Tok/s 3539 (2717)	Loss/tok 6.9200 (7.9095)	LR 2.000e-03
6: TRAIN [0][260/461]	Time 0.703 (0.714)	Data 7.61e-05 (8.71e-05)	Tok/s 3624 (2709)	Loss/tok 6.8516 (7.9206)	LR 2.000e-03
5: TRAIN [0][260/461]	Time 0.703 (0.714)	Data 1.10e-04 (8.61e-05)	Tok/s 3567 (2717)	Loss/tok 6.9234 (7.9150)	LR 2.000e-03
2: TRAIN [0][260/461]	Time 0.703 (0.714)	Data 8.73e-05 (8.36e-05)	Tok/s 3658 (2713)	Loss/tok 6.8655 (7.9189)	LR 2.000e-03
3: TRAIN [0][260/461]	Time 0.703 (0.714)	Data 1.08e-04 (8.94e-05)	Tok/s 3577 (2718)	Loss/tok 6.8471 (7.9172)	LR 2.000e-03
4: TRAIN [0][260/461]	Time 0.704 (0.714)	Data 8.44e-05 (8.07e-05)	Tok/s 3535 (2716)	Loss/tok 6.6965 (7.9261)	LR 2.000e-03
0: TRAIN [0][270/461]	Time 0.708 (0.714)	Data 1.06e-04 (9.40e-05)	Tok/s 2451 (2705)	Loss/tok 6.5977 (7.8818)	LR 2.000e-03
7: TRAIN [0][270/461]	Time 0.710 (0.714)	Data 1.29e-04 (1.13e-04)	Tok/s 2601 (2713)	Loss/tok 6.5806 (7.8784)	LR 2.000e-03
1: TRAIN [0][270/461]	Time 0.711 (0.714)	Data 6.46e-05 (8.36e-05)	Tok/s 2557 (2709)	Loss/tok 6.7963 (7.8679)	LR 2.000e-03
6: TRAIN [0][270/461]	Time 0.711 (0.714)	Data 7.01e-05 (8.70e-05)	Tok/s 2527 (2702)	Loss/tok 6.5159 (7.8802)	LR 2.000e-03
5: TRAIN [0][270/461]	Time 0.711 (0.714)	Data 8.70e-05 (8.60e-05)	Tok/s 2509 (2709)	Loss/tok 6.6082 (7.8737)	LR 2.000e-03
2: TRAIN [0][270/461]	Time 0.710 (0.714)	Data 9.75e-05 (8.36e-05)	Tok/s 2451 (2705)	Loss/tok 6.4800 (7.8766)	LR 2.000e-03
3: TRAIN [0][270/461]	Time 0.710 (0.714)	Data 8.58e-05 (8.92e-05)	Tok/s 2516 (2709)	Loss/tok 6.6303 (7.8756)	LR 2.000e-03
4: TRAIN [0][270/461]	Time 0.710 (0.714)	Data 7.25e-05 (8.06e-05)	Tok/s 2584 (2709)	Loss/tok 6.7793 (7.8873)	LR 2.000e-03
7: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 1.03e-04 (1.13e-04)	Tok/s 2576 (2725)	Loss/tok 6.5157 (7.8326)	LR 2.000e-03
0: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 6.94e-05 (9.35e-05)	Tok/s 2560 (2718)	Loss/tok 6.5110 (7.8361)	LR 2.000e-03
1: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 6.39e-05 (8.30e-05)	Tok/s 2515 (2721)	Loss/tok 6.7124 (7.8215)	LR 2.000e-03
6: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 7.30e-05 (8.66e-05)	Tok/s 2665 (2715)	Loss/tok 6.6609 (7.8324)	LR 2.000e-03
5: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 7.22e-05 (8.59e-05)	Tok/s 2495 (2721)	Loss/tok 6.4089 (7.8269)	LR 2.000e-03
2: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 7.80e-05 (8.35e-05)	Tok/s 2590 (2717)	Loss/tok 6.4934 (7.8287)	LR 2.000e-03
3: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 9.13e-05 (8.91e-05)	Tok/s 2631 (2722)	Loss/tok 6.7609 (7.8308)	LR 2.000e-03
4: TRAIN [0][280/461]	Time 0.700 (0.714)	Data 7.27e-05 (8.07e-05)	Tok/s 2591 (2720)	Loss/tok 6.5935 (7.8389)	LR 2.000e-03
0: TRAIN [0][290/461]	Time 0.694 (0.714)	Data 7.82e-05 (9.37e-05)	Tok/s 2535 (2709)	Loss/tok 6.6064 (7.7960)	LR 2.000e-03
7: TRAIN [0][290/461]	Time 0.694 (0.714)	Data 1.14e-04 (1.13e-04)	Tok/s 2536 (2717)	Loss/tok 6.2516 (7.7918)	LR 2.000e-03
6: TRAIN [0][290/461]	Time 0.694 (0.714)	Data 7.65e-05 (8.67e-05)	Tok/s 2598 (2706)	Loss/tok 6.4681 (7.7932)	LR 2.000e-03
1: TRAIN [0][290/461]	Time 0.694 (0.714)	Data 7.06e-05 (8.26e-05)	Tok/s 2700 (2714)	Loss/tok 6.4096 (7.7813)	LR 2.000e-03
5: TRAIN [0][290/461]	Time 0.694 (0.714)	Data 8.42e-05 (8.60e-05)	Tok/s 2708 (2713)	Loss/tok 6.3393 (7.7869)	LR 2.000e-03
2: TRAIN [0][290/461]	Time 0.694 (0.714)	Data 8.08e-05 (8.34e-05)	Tok/s 2643 (2709)	Loss/tok 6.6145 (7.7892)	LR 2.000e-03
4: TRAIN [0][290/461]	Time 0.695 (0.714)	Data 8.80e-05 (8.06e-05)	Tok/s 2653 (2713)	Loss/tok 6.3916 (7.7984)	LR 2.000e-03
3: TRAIN [0][290/461]	Time 0.695 (0.714)	Data 9.37e-05 (8.92e-05)	Tok/s 2651 (2715)	Loss/tok 6.5747 (7.7928)	LR 2.000e-03
0: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 8.42e-05 (9.31e-05)	Tok/s 3538 (2707)	Loss/tok 6.5445 (7.7530)	LR 2.000e-03
7: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 1.21e-04 (1.13e-04)	Tok/s 3407 (2714)	Loss/tok 6.6556 (7.7508)	LR 2.000e-03
1: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 7.27e-05 (8.25e-05)	Tok/s 3406 (2710)	Loss/tok 6.5632 (7.7390)	LR 2.000e-03
6: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 1.12e-04 (8.64e-05)	Tok/s 3463 (2704)	Loss/tok 6.6827 (7.7535)	LR 2.000e-03
2: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 8.27e-05 (8.32e-05)	Tok/s 3438 (2707)	Loss/tok 6.5261 (7.7454)	LR 2.000e-03
5: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 8.94e-05 (8.59e-05)	Tok/s 3524 (2712)	Loss/tok 6.5750 (7.7436)	LR 2.000e-03
3: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 1.00e-04 (8.94e-05)	Tok/s 3508 (2712)	Loss/tok 6.6336 (7.7490)	LR 2.000e-03
4: TRAIN [0][300/461]	Time 0.724 (0.714)	Data 9.30e-05 (8.05e-05)	Tok/s 3361 (2709)	Loss/tok 6.6940 (7.7559)	LR 2.000e-03
0: TRAIN [0][310/461]	Time 0.715 (0.714)	Data 7.39e-05 (9.26e-05)	Tok/s 2494 (2702)	Loss/tok 6.2480 (7.7103)	LR 2.000e-03
7: TRAIN [0][310/461]	Time 0.715 (0.714)	Data 9.66e-05 (1.13e-04)	Tok/s 2554 (2708)	Loss/tok 6.2428 (7.7075)	LR 2.000e-03
1: TRAIN [0][310/461]	Time 0.716 (0.714)	Data 6.39e-05 (8.20e-05)	Tok/s 2550 (2705)	Loss/tok 6.3348 (7.6976)	LR 2.000e-03
6: TRAIN [0][310/461]	Time 0.716 (0.714)	Data 6.56e-05 (8.59e-05)	Tok/s 2470 (2698)	Loss/tok 6.0852 (7.7105)	LR 2.000e-03
5: TRAIN [0][310/461]	Time 0.716 (0.714)	Data 7.84e-05 (8.59e-05)	Tok/s 2497 (2706)	Loss/tok 6.0811 (7.7008)	LR 2.000e-03
2: TRAIN [0][310/461]	Time 0.716 (0.714)	Data 8.03e-05 (8.32e-05)	Tok/s 2488 (2702)	Loss/tok 6.1995 (7.7026)	LR 2.000e-03
3: TRAIN [0][310/461]	Time 0.716 (0.714)	Data 1.15e-04 (8.93e-05)	Tok/s 2453 (2706)	Loss/tok 6.1788 (7.7079)	LR 2.000e-03
4: TRAIN [0][310/461]	Time 0.716 (0.714)	Data 6.89e-05 (8.04e-05)	Tok/s 2517 (2704)	Loss/tok 6.2540 (7.7139)	LR 2.000e-03
0: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 8.70e-05 (9.22e-05)	Tok/s 2505 (2688)	Loss/tok 5.8795 (7.6732)	LR 2.000e-03
7: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 1.22e-04 (1.13e-04)	Tok/s 2483 (2693)	Loss/tok 6.1246 (7.6713)	LR 2.000e-03
1: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 6.70e-05 (8.17e-05)	Tok/s 2520 (2691)	Loss/tok 6.2221 (7.6586)	LR 2.000e-03
6: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 7.82e-05 (8.56e-05)	Tok/s 2567 (2685)	Loss/tok 6.3556 (7.6747)	LR 2.000e-03
5: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 1.15e-04 (8.59e-05)	Tok/s 2471 (2693)	Loss/tok 6.3063 (7.6627)	LR 2.000e-03
2: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 8.44e-05 (8.30e-05)	Tok/s 2480 (2689)	Loss/tok 6.0654 (7.6649)	LR 2.000e-03
4: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 7.82e-05 (8.03e-05)	Tok/s 2473 (2690)	Loss/tok 6.3083 (7.6767)	LR 2.000e-03
3: TRAIN [0][320/461]	Time 0.715 (0.714)	Data 9.54e-05 (8.91e-05)	Tok/s 2454 (2693)	Loss/tok 5.9077 (7.6697)	LR 2.000e-03
0: TRAIN [0][330/461]	Time 0.678 (0.713)	Data 6.65e-05 (9.18e-05)	Tok/s 2585 (2676)	Loss/tok 5.9541 (7.6368)	LR 2.000e-03
7: TRAIN [0][330/461]	Time 0.678 (0.713)	Data 1.02e-04 (1.13e-04)	Tok/s 2542 (2681)	Loss/tok 6.1253 (7.6354)	LR 2.000e-03
1: TRAIN [0][330/461]	Time 0.677 (0.713)	Data 6.53e-05 (8.13e-05)	Tok/s 2656 (2679)	Loss/tok 6.0767 (7.6220)	LR 2.000e-03
6: TRAIN [0][330/461]	Time 0.677 (0.713)	Data 7.18e-05 (8.52e-05)	Tok/s 2665 (2673)	Loss/tok 6.1633 (7.6377)	LR 2.000e-03
2: TRAIN [0][330/461]	Time 0.677 (0.713)	Data 7.41e-05 (8.30e-05)	Tok/s 2644 (2676)	Loss/tok 6.1232 (7.6283)	LR 2.000e-03
5: TRAIN [0][330/461]	Time 0.678 (0.713)	Data 7.75e-05 (8.60e-05)	Tok/s 2679 (2682)	Loss/tok 5.9942 (7.6261)	LR 2.000e-03
3: TRAIN [0][330/461]	Time 0.677 (0.713)	Data 9.92e-05 (8.92e-05)	Tok/s 2596 (2681)	Loss/tok 6.0321 (7.6337)	LR 2.000e-03
4: TRAIN [0][330/461]	Time 0.677 (0.713)	Data 7.13e-05 (8.04e-05)	Tok/s 2628 (2678)	Loss/tok 6.3142 (7.6407)	LR 2.000e-03
0: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 6.70e-05 (9.14e-05)	Tok/s 4200 (2671)	Loss/tok 6.4382 (7.5968)	LR 2.000e-03
7: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 1.10e-04 (1.13e-04)	Tok/s 4320 (2678)	Loss/tok 6.5139 (7.5935)	LR 2.000e-03
1: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 6.10e-05 (8.11e-05)	Tok/s 4195 (2676)	Loss/tok 6.3866 (7.5818)	LR 2.000e-03
6: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 9.32e-05 (8.51e-05)	Tok/s 4226 (2669)	Loss/tok 6.3876 (7.5992)	LR 2.000e-03
2: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 7.10e-05 (8.29e-05)	Tok/s 4170 (2673)	Loss/tok 6.4696 (7.5891)	LR 2.000e-03
5: TRAIN [0][340/461]	Time 0.761 (0.713)	Data 7.46e-05 (8.59e-05)	Tok/s 4336 (2678)	Loss/tok 6.4239 (7.5859)	LR 2.000e-03
3: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 8.58e-05 (8.92e-05)	Tok/s 4271 (2677)	Loss/tok 6.3949 (7.5943)	LR 2.000e-03
4: TRAIN [0][340/461]	Time 0.760 (0.713)	Data 7.10e-05 (8.04e-05)	Tok/s 4241 (2674)	Loss/tok 6.4755 (7.5997)	LR 2.000e-03
0: TRAIN [0][350/461]	Time 0.685 (0.714)	Data 6.58e-05 (9.09e-05)	Tok/s 2676 (2688)	Loss/tok 6.0227 (7.5486)	LR 2.000e-03
7: TRAIN [0][350/461]	Time 0.686 (0.714)	Data 1.09e-04 (1.13e-04)	Tok/s 2516 (2694)	Loss/tok 5.8994 (7.5460)	LR 2.000e-03
1: TRAIN [0][350/461]	Time 0.685 (0.714)	Data 6.63e-05 (8.13e-05)	Tok/s 2584 (2692)	Loss/tok 5.8722 (7.5340)	LR 2.000e-03
6: TRAIN [0][350/461]	Time 0.685 (0.714)	Data 6.77e-05 (8.47e-05)	Tok/s 2690 (2686)	Loss/tok 5.8401 (7.5498)	LR 2.000e-03
5: TRAIN [0][350/461]	Time 0.684 (0.714)	Data 7.41e-05 (8.57e-05)	Tok/s 2671 (2694)	Loss/tok 6.0423 (7.5402)	LR 2.000e-03
2: TRAIN [0][350/461]	Time 0.684 (0.714)	Data 8.99e-05 (8.27e-05)	Tok/s 2605 (2689)	Loss/tok 6.2266 (7.5405)	LR 2.000e-03
3: TRAIN [0][350/461]	Time 0.684 (0.714)	Data 8.08e-05 (8.93e-05)	Tok/s 2638 (2693)	Loss/tok 5.9213 (7.5458)	LR 2.000e-03
4: TRAIN [0][350/461]	Time 0.684 (0.714)	Data 1.08e-04 (8.07e-05)	Tok/s 2690 (2691)	Loss/tok 5.9920 (7.5522)	LR 2.000e-03
7: TRAIN [0][360/461]	Time 0.702 (0.713)	Data 1.04e-04 (1.13e-04)	Tok/s 1523 (2681)	Loss/tok 5.5700 (7.5105)	LR 2.000e-03
0: TRAIN [0][360/461]	Time 0.702 (0.713)	Data 6.99e-05 (9.06e-05)	Tok/s 1594 (2675)	Loss/tok 5.7142 (7.5128)	LR 2.000e-03
6: TRAIN [0][360/461]	Time 0.702 (0.713)	Data 6.63e-05 (8.44e-05)	Tok/s 1558 (2672)	Loss/tok 5.6786 (7.5141)	LR 2.000e-03
1: TRAIN [0][360/461]	Time 0.702 (0.713)	Data 6.72e-05 (8.10e-05)	Tok/s 1564 (2678)	Loss/tok 5.6466 (7.4995)	LR 2.000e-03
5: TRAIN [0][360/461]	Time 0.701 (0.713)	Data 7.92e-05 (8.59e-05)	Tok/s 1571 (2682)	Loss/tok 5.4337 (7.5059)	LR 2.000e-03
2: TRAIN [0][360/461]	Time 0.702 (0.713)	Data 7.39e-05 (8.27e-05)	Tok/s 1551 (2676)	Loss/tok 5.4468 (7.5053)	LR 2.000e-03
3: TRAIN [0][360/461]	Time 0.701 (0.713)	Data 8.15e-05 (8.93e-05)	Tok/s 1613 (2680)	Loss/tok 5.6713 (7.5102)	LR 2.000e-03
4: TRAIN [0][360/461]	Time 0.701 (0.713)	Data 8.75e-05 (8.06e-05)	Tok/s 1594 (2678)	Loss/tok 5.6574 (7.5151)	LR 2.000e-03
0: TRAIN [0][370/461]	Time 0.697 (0.713)	Data 7.18e-05 (9.03e-05)	Tok/s 1494 (2675)	Loss/tok 5.7404 (7.4725)	LR 2.000e-03
7: TRAIN [0][370/461]	Time 0.697 (0.713)	Data 1.08e-04 (1.13e-04)	Tok/s 1570 (2682)	Loss/tok 5.3657 (7.4688)	LR 2.000e-03
1: TRAIN [0][370/461]	Time 0.698 (0.713)	Data 6.27e-05 (8.08e-05)	Tok/s 1484 (2679)	Loss/tok 5.6053 (7.4596)	LR 2.000e-03
6: TRAIN [0][370/461]	Time 0.698 (0.713)	Data 6.75e-05 (8.42e-05)	Tok/s 1542 (2673)	Loss/tok 5.6933 (7.4728)	LR 2.000e-03
5: TRAIN [0][370/461]	Time 0.698 (0.713)	Data 8.15e-05 (8.58e-05)	Tok/s 1627 (2682)	Loss/tok 5.3358 (7.4661)	LR 2.000e-03
2: TRAIN [0][370/461]	Time 0.699 (0.713)	Data 9.68e-05 (8.26e-05)	Tok/s 1510 (2676)	Loss/tok 5.3767 (7.4652)	LR 2.000e-03
3: TRAIN [0][370/461]	Time 0.698 (0.713)	Data 7.96e-05 (8.92e-05)	Tok/s 1560 (2681)	Loss/tok 5.4556 (7.4701)	LR 2.000e-03
4: TRAIN [0][370/461]	Time 0.698 (0.713)	Data 7.44e-05 (8.06e-05)	Tok/s 1548 (2680)	Loss/tok 5.5795 (7.4735)	LR 2.000e-03
0: TRAIN [0][380/461]	Time 0.773 (0.713)	Data 1.04e-04 (9.04e-05)	Tok/s 4263 (2659)	Loss/tok 6.2312 (7.4405)	LR 2.000e-03
7: TRAIN [0][380/461]	Time 0.773 (0.713)	Data 1.10e-04 (1.13e-04)	Tok/s 4232 (2666)	Loss/tok 6.3443 (7.4365)	LR 2.000e-03
1: TRAIN [0][380/461]	Time 0.775 (0.713)	Data 9.01e-05 (8.06e-05)	Tok/s 4290 (2663)	Loss/tok 6.2370 (7.4277)	LR 2.000e-03
6: TRAIN [0][380/461]	Time 0.775 (0.713)	Data 6.72e-05 (8.41e-05)	Tok/s 4238 (2658)	Loss/tok 6.2459 (7.4393)	LR 2.000e-03
5: TRAIN [0][380/461]	Time 0.775 (0.713)	Data 9.06e-05 (8.63e-05)	Tok/s 4150 (2666)	Loss/tok 6.2761 (7.4335)	LR 2.000e-03
2: TRAIN [0][380/461]	Time 0.775 (0.713)	Data 7.82e-05 (8.26e-05)	Tok/s 4120 (2659)	Loss/tok 6.2219 (7.4321)	LR 2.000e-03
3: TRAIN [0][380/461]	Time 0.775 (0.713)	Data 8.65e-05 (8.93e-05)	Tok/s 4184 (2666)	Loss/tok 6.1406 (7.4365)	LR 2.000e-03
4: TRAIN [0][380/461]	Time 0.775 (0.713)	Data 7.30e-05 (8.07e-05)	Tok/s 4183 (2664)	Loss/tok 6.2507 (7.4401)	LR 2.000e-03
0: TRAIN [0][390/461]	Time 0.703 (0.713)	Data 6.48e-05 (9.00e-05)	Tok/s 2585 (2660)	Loss/tok 5.8471 (7.3981)	LR 2.000e-03
7: TRAIN [0][390/461]	Time 0.703 (0.713)	Data 9.85e-05 (1.13e-04)	Tok/s 2496 (2667)	Loss/tok 5.6578 (7.3960)	LR 2.000e-03
6: TRAIN [0][390/461]	Time 0.702 (0.713)	Data 7.20e-05 (8.39e-05)	Tok/s 2523 (2660)	Loss/tok 5.6891 (7.3978)	LR 2.000e-03
1: TRAIN [0][390/461]	Time 0.702 (0.713)	Data 6.29e-05 (8.02e-05)	Tok/s 2594 (2665)	Loss/tok 5.5283 (7.3868)	LR 2.000e-03
5: TRAIN [0][390/461]	Time 0.703 (0.713)	Data 7.75e-05 (8.61e-05)	Tok/s 2549 (2668)	Loss/tok 5.8204 (7.3940)	LR 2.000e-03
2: TRAIN [0][390/461]	Time 0.703 (0.713)	Data 7.37e-05 (8.25e-05)	Tok/s 2533 (2661)	Loss/tok 5.9244 (7.3901)	LR 2.000e-03
3: TRAIN [0][390/461]	Time 0.703 (0.713)	Data 9.08e-05 (8.91e-05)	Tok/s 2550 (2667)	Loss/tok 5.5426 (7.3944)	LR 2.000e-03
4: TRAIN [0][390/461]	Time 0.703 (0.713)	Data 7.01e-05 (8.05e-05)	Tok/s 2471 (2665)	Loss/tok 5.4819 (7.3966)	LR 2.000e-03
7: TRAIN [0][400/461]	Time 0.691 (0.713)	Data 1.04e-04 (1.13e-04)	Tok/s 1511 (2665)	Loss/tok 5.4176 (7.3583)	LR 2.000e-03
0: TRAIN [0][400/461]	Time 0.692 (0.713)	Data 7.06e-05 (8.96e-05)	Tok/s 1561 (2659)	Loss/tok 5.2123 (7.3590)	LR 2.000e-03
6: TRAIN [0][400/461]	Time 0.692 (0.713)	Data 7.25e-05 (8.36e-05)	Tok/s 1569 (2658)	Loss/tok 5.5295 (7.3617)	LR 2.000e-03
1: TRAIN [0][400/461]	Time 0.693 (0.713)	Data 6.08e-05 (7.99e-05)	Tok/s 1598 (2663)	Loss/tok 5.1013 (7.3486)	LR 2.000e-03
5: TRAIN [0][400/461]	Time 0.693 (0.713)	Data 7.41e-05 (8.61e-05)	Tok/s 1558 (2665)	Loss/tok 5.3052 (7.3562)	LR 2.000e-03
2: TRAIN [0][400/461]	Time 0.693 (0.713)	Data 9.99e-05 (8.24e-05)	Tok/s 1578 (2659)	Loss/tok 5.2383 (7.3529)	LR 2.000e-03
3: TRAIN [0][400/461]	Time 0.693 (0.713)	Data 8.70e-05 (8.91e-05)	Tok/s 1495 (2665)	Loss/tok 5.3387 (7.3553)	LR 2.000e-03
4: TRAIN [0][400/461]	Time 0.693 (0.713)	Data 8.46e-05 (8.06e-05)	Tok/s 1506 (2664)	Loss/tok 5.4355 (7.3573)	LR 2.000e-03
0: TRAIN [0][410/461]	Time 0.734 (0.714)	Data 8.44e-05 (8.98e-05)	Tok/s 2490 (2668)	Loss/tok 5.6423 (7.3158)	LR 2.000e-03
7: TRAIN [0][410/461]	Time 0.734 (0.714)	Data 1.18e-04 (1.13e-04)	Tok/s 2425 (2675)	Loss/tok 5.8390 (7.3165)	LR 2.000e-03
1: TRAIN [0][410/461]	Time 0.735 (0.714)	Data 7.44e-05 (7.97e-05)	Tok/s 2445 (2674)	Loss/tok 5.6999 (7.3057)	LR 2.000e-03
6: TRAIN [0][410/461]	Time 0.735 (0.714)	Data 7.63e-05 (8.34e-05)	Tok/s 2519 (2668)	Loss/tok 5.5759 (7.3196)	LR 2.000e-03
5: TRAIN [0][410/461]	Time 0.735 (0.714)	Data 8.58e-05 (8.60e-05)	Tok/s 2447 (2676)	Loss/tok 5.6434 (7.3133)	LR 2.000e-03
2: TRAIN [0][410/461]	Time 0.735 (0.714)	Data 7.72e-05 (8.24e-05)	Tok/s 2448 (2668)	Loss/tok 5.5179 (7.3119)	LR 2.000e-03
3: TRAIN [0][410/461]	Time 0.735 (0.714)	Data 9.20e-05 (8.91e-05)	Tok/s 2478 (2674)	Loss/tok 5.8240 (7.3121)	LR 2.000e-03
4: TRAIN [0][410/461]	Time 0.735 (0.714)	Data 9.18e-05 (8.08e-05)	Tok/s 2454 (2673)	Loss/tok 5.5735 (7.3143)	LR 2.000e-03
0: TRAIN [0][420/461]	Time 0.698 (0.714)	Data 7.65e-05 (8.96e-05)	Tok/s 2579 (2667)	Loss/tok 5.4678 (7.2787)	LR 2.000e-03
7: TRAIN [0][420/461]	Time 0.698 (0.714)	Data 1.03e-04 (1.13e-04)	Tok/s 2603 (2674)	Loss/tok 5.5119 (7.2782)	LR 2.000e-03
1: TRAIN [0][420/461]	Time 0.698 (0.714)	Data 6.63e-05 (7.95e-05)	Tok/s 2563 (2672)	Loss/tok 5.6640 (7.2681)	LR 2.000e-03
6: TRAIN [0][420/461]	Time 0.698 (0.714)	Data 6.68e-05 (8.34e-05)	Tok/s 2623 (2666)	Loss/tok 5.5598 (7.2831)	LR 2.000e-03
5: TRAIN [0][420/461]	Time 0.699 (0.714)	Data 7.77e-05 (8.60e-05)	Tok/s 2567 (2674)	Loss/tok 5.5629 (7.2737)	LR 2.000e-03
2: TRAIN [0][420/461]	Time 0.699 (0.714)	Data 7.30e-05 (8.23e-05)	Tok/s 2644 (2667)	Loss/tok 5.5847 (7.2733)	LR 2.000e-03
3: TRAIN [0][420/461]	Time 0.699 (0.714)	Data 8.08e-05 (8.90e-05)	Tok/s 2567 (2673)	Loss/tok 5.3132 (7.2733)	LR 2.000e-03
4: TRAIN [0][420/461]	Time 0.699 (0.714)	Data 1.09e-04 (8.11e-05)	Tok/s 2463 (2672)	Loss/tok 5.5587 (7.2766)	LR 2.000e-03
0: TRAIN [0][430/461]	Time 0.739 (0.713)	Data 6.96e-05 (8.93e-05)	Tok/s 3485 (2668)	Loss/tok 5.6363 (7.2371)	LR 2.000e-03
7: TRAIN [0][430/461]	Time 0.739 (0.713)	Data 1.08e-04 (1.13e-04)	Tok/s 3356 (2675)	Loss/tok 5.8015 (7.2378)	LR 2.000e-03
1: TRAIN [0][430/461]	Time 0.740 (0.713)	Data 6.44e-05 (7.93e-05)	Tok/s 3407 (2673)	Loss/tok 5.6619 (7.2293)	LR 2.000e-03
6: TRAIN [0][430/461]	Time 0.740 (0.713)	Data 6.56e-05 (8.32e-05)	Tok/s 3411 (2667)	Loss/tok 5.5189 (7.2421)	LR 2.000e-03
5: TRAIN [0][430/461]	Time 0.740 (0.713)	Data 7.96e-05 (8.59e-05)	Tok/s 3410 (2675)	Loss/tok 5.6904 (7.2350)	LR 2.000e-03
2: TRAIN [0][430/461]	Time 0.740 (0.713)	Data 7.63e-05 (8.23e-05)	Tok/s 3384 (2668)	Loss/tok 5.6985 (7.2337)	LR 2.000e-03
3: TRAIN [0][430/461]	Time 0.741 (0.713)	Data 8.18e-05 (8.94e-05)	Tok/s 3398 (2673)	Loss/tok 5.6051 (7.2342)	LR 2.000e-03
4: TRAIN [0][430/461]	Time 0.740 (0.713)	Data 7.03e-05 (8.12e-05)	Tok/s 3305 (2673)	Loss/tok 5.6385 (7.2362)	LR 2.000e-03
0: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 6.53e-05 (8.90e-05)	Tok/s 3296 (2681)	Loss/tok 5.6756 (7.1936)	LR 2.000e-03
7: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 1.14e-04 (1.13e-04)	Tok/s 3376 (2686)	Loss/tok 5.5560 (7.1929)	LR 2.000e-03
1: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 6.99e-05 (7.91e-05)	Tok/s 3370 (2686)	Loss/tok 5.5414 (7.1843)	LR 2.000e-03
6: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 6.96e-05 (8.30e-05)	Tok/s 3352 (2680)	Loss/tok 5.4926 (7.1970)	LR 2.000e-03
5: TRAIN [0][440/461]	Time 0.742 (0.714)	Data 8.13e-05 (8.58e-05)	Tok/s 3456 (2687)	Loss/tok 5.6107 (7.1900)	LR 2.000e-03
2: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 9.08e-05 (8.21e-05)	Tok/s 3487 (2681)	Loss/tok 5.6461 (7.1883)	LR 2.000e-03
3: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 8.87e-05 (8.94e-05)	Tok/s 3355 (2685)	Loss/tok 5.6418 (7.1898)	LR 2.000e-03
4: TRAIN [0][440/461]	Time 0.743 (0.714)	Data 1.09e-04 (8.12e-05)	Tok/s 3524 (2686)	Loss/tok 5.7024 (7.1921)	LR 2.000e-03
0: TRAIN [0][450/461]	Time 0.706 (0.714)	Data 6.48e-05 (8.87e-05)	Tok/s 1645 (2687)	Loss/tok 5.0003 (7.1534)	LR 2.000e-03
7: TRAIN [0][450/461]	Time 0.706 (0.714)	Data 1.03e-04 (1.13e-04)	Tok/s 1638 (2693)	Loss/tok 4.9455 (7.1526)	LR 2.000e-03
1: TRAIN [0][450/461]	Time 0.705 (0.714)	Data 9.73e-05 (7.90e-05)	Tok/s 1478 (2691)	Loss/tok 5.1850 (7.1445)	LR 2.000e-03
6: TRAIN [0][450/461]	Time 0.705 (0.714)	Data 7.58e-05 (8.28e-05)	Tok/s 1526 (2686)	Loss/tok 4.8700 (7.1565)	LR 2.000e-03
5: TRAIN [0][450/461]	Time 0.705 (0.714)	Data 8.32e-05 (8.57e-05)	Tok/s 1521 (2692)	Loss/tok 4.9744 (7.1509)	LR 2.000e-03
2: TRAIN [0][450/461]	Time 0.705 (0.714)	Data 9.63e-05 (8.21e-05)	Tok/s 1452 (2687)	Loss/tok 4.7385 (7.1482)	LR 2.000e-03
3: TRAIN [0][450/461]	Time 0.705 (0.714)	Data 8.89e-05 (8.93e-05)	Tok/s 1572 (2690)	Loss/tok 5.2332 (7.1502)	LR 2.000e-03
4: TRAIN [0][450/461]	Time 0.705 (0.714)	Data 6.96e-05 (8.11e-05)	Tok/s 1611 (2692)	Loss/tok 5.0260 (7.1529)	LR 2.000e-03
0: TRAIN [0][460/461]	Time 0.729 (0.714)	Data 5.79e-05 (8.88e-05)	Tok/s 3449 (2683)	Loss/tok 5.5624 (7.1170)	LR 2.000e-03
7: TRAIN [0][460/461]	Time 0.729 (0.714)	Data 6.51e-05 (1.13e-04)	Tok/s 3417 (2689)	Loss/tok 5.4345 (7.1167)	LR 2.000e-03
1: TRAIN [0][460/461]	Time 0.730 (0.714)	Data 3.98e-05 (7.91e-05)	Tok/s 3498 (2687)	Loss/tok 5.3354 (7.1072)	LR 2.000e-03
6: TRAIN [0][460/461]	Time 0.730 (0.714)	Data 4.58e-05 (8.30e-05)	Tok/s 3364 (2682)	Loss/tok 5.3992 (7.1194)	LR 2.000e-03
5: TRAIN [0][460/461]	Time 0.729 (0.714)	Data 5.03e-05 (8.61e-05)	Tok/s 3426 (2689)	Loss/tok 5.3612 (7.1139)	LR 2.000e-03
2: TRAIN [0][460/461]	Time 0.729 (0.714)	Data 3.91e-05 (8.23e-05)	Tok/s 3519 (2685)	Loss/tok 5.6577 (7.1115)	LR 2.000e-03
3: TRAIN [0][460/461]	Time 0.729 (0.714)	Data 4.12e-05 (8.94e-05)	Tok/s 3460 (2687)	Loss/tok 5.5301 (7.1136)	LR 2.000e-03
4: TRAIN [0][460/461]	Time 0.730 (0.714)	Data 4.51e-05 (8.16e-05)	Tok/s 3332 (2689)	Loss/tok 5.3507 (7.1151)	LR 2.000e-03
1: Running validation on dev set
5: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
2: Running validation on dev set
7: Running validation on dev set
4: Running validation on dev set
6: Running validation on dev set
3: Running validation on dev set
5: Executing preallocation
7: Executing preallocation
2: Executing preallocation
0: Executing preallocation
6: Executing preallocation
4: Executing preallocation
3: Executing preallocation
7: VALIDATION [0][0/79]	Time 0.050 (0.000)	Data 6.78e-04 (0.00e+00)	Tok/s 22827 (0)	Loss/tok 6.5045 (6.5045)
5: VALIDATION [0][0/80]	Time 0.054 (0.000)	Data 6.90e-04 (0.00e+00)	Tok/s 22224 (0)	Loss/tok 6.9182 (6.9182)
6: VALIDATION [0][0/79]	Time 0.052 (0.000)	Data 7.15e-04 (0.00e+00)	Tok/s 22596 (0)	Loss/tok 6.6739 (6.6739)
4: VALIDATION [0][0/80]	Time 0.055 (0.000)	Data 7.78e-04 (0.00e+00)	Tok/s 22214 (0)	Loss/tok 6.9394 (6.9394)
3: VALIDATION [0][0/80]	Time 0.055 (0.000)	Data 7.08e-04 (0.00e+00)	Tok/s 23023 (0)	Loss/tok 6.7433 (6.7433)
2: VALIDATION [0][0/80]	Time 0.058 (0.000)	Data 7.52e-04 (0.00e+00)	Tok/s 22990 (0)	Loss/tok 6.5168 (6.5168)
1: VALIDATION [0][0/80]	Time 0.065 (0.000)	Data 6.90e-04 (0.00e+00)	Tok/s 22753 (0)	Loss/tok 7.1284 (7.1284)
0: VALIDATION [0][0/80]	Time 0.077 (0.000)	Data 6.82e-04 (0.00e+00)	Tok/s 21334 (0)	Loss/tok 6.8883 (6.8883)
5: VALIDATION [0][10/80]	Time 0.034 (0.038)	Data 5.14e-04 (5.24e-04)	Tok/s 21372 (22440)	Loss/tok 6.4321 (6.6878)
7: VALIDATION [0][10/79]	Time 0.033 (0.038)	Data 5.25e-04 (5.29e-04)	Tok/s 21672 (22051)	Loss/tok 6.4893 (6.5939)
6: VALIDATION [0][10/79]	Time 0.033 (0.038)	Data 5.21e-04 (5.35e-04)	Tok/s 22144 (22040)	Loss/tok 6.6474 (6.6241)
4: VALIDATION [0][10/80]	Time 0.034 (0.039)	Data 5.20e-04 (5.24e-04)	Tok/s 21402 (22036)	Loss/tok 6.5500 (6.6144)
2: VALIDATION [0][10/80]	Time 0.033 (0.039)	Data 5.25e-04 (5.30e-04)	Tok/s 22166 (22332)	Loss/tok 6.6945 (6.6478)
3: VALIDATION [0][10/80]	Time 0.032 (0.039)	Data 5.13e-04 (5.19e-04)	Tok/s 22517 (22128)	Loss/tok 6.3795 (6.5418)
1: VALIDATION [0][10/80]	Time 0.033 (0.039)	Data 5.38e-04 (5.38e-04)	Tok/s 22259 (22387)	Loss/tok 6.6746 (6.5852)
0: VALIDATION [0][10/80]	Time 0.033 (0.040)	Data 5.05e-04 (5.14e-04)	Tok/s 22140 (22076)	Loss/tok 6.2778 (6.6199)
7: VALIDATION [0][20/79]	Time 0.027 (0.034)	Data 5.23e-04 (5.24e-04)	Tok/s 21494 (21948)	Loss/tok 6.0821 (6.4828)
5: VALIDATION [0][20/80]	Time 0.026 (0.034)	Data 5.09e-04 (5.20e-04)	Tok/s 21816 (22098)	Loss/tok 6.3179 (6.6049)
6: VALIDATION [0][20/79]	Time 0.028 (0.034)	Data 5.22e-04 (5.30e-04)	Tok/s 20702 (21734)	Loss/tok 6.4671 (6.5372)
4: VALIDATION [0][20/80]	Time 0.028 (0.034)	Data 5.14e-04 (5.19e-04)	Tok/s 20905 (21944)	Loss/tok 5.9575 (6.4908)
2: VALIDATION [0][20/80]	Time 0.027 (0.034)	Data 5.08e-04 (5.26e-04)	Tok/s 21952 (22014)	Loss/tok 6.0215 (6.5225)
3: VALIDATION [0][20/80]	Time 0.027 (0.034)	Data 4.99e-04 (5.18e-04)	Tok/s 21791 (21895)	Loss/tok 6.0875 (6.5067)
1: VALIDATION [0][20/80]	Time 0.026 (0.034)	Data 5.20e-04 (5.37e-04)	Tok/s 22099 (22073)	Loss/tok 6.4909 (6.5092)
0: VALIDATION [0][20/80]	Time 0.028 (0.035)	Data 5.03e-04 (5.08e-04)	Tok/s 21048 (21954)	Loss/tok 5.9006 (6.5227)
7: VALIDATION [0][30/79]	Time 0.023 (0.031)	Data 5.13e-04 (5.22e-04)	Tok/s 20616 (21664)	Loss/tok 6.2475 (6.4192)
5: VALIDATION [0][30/80]	Time 0.023 (0.031)	Data 5.11e-04 (5.17e-04)	Tok/s 20794 (21715)	Loss/tok 6.3774 (6.5018)
6: VALIDATION [0][30/79]	Time 0.024 (0.031)	Data 5.08e-04 (5.32e-04)	Tok/s 20151 (21423)	Loss/tok 6.3392 (6.4738)
4: VALIDATION [0][30/80]	Time 0.023 (0.031)	Data 5.21e-04 (5.16e-04)	Tok/s 21227 (21611)	Loss/tok 5.8611 (6.4107)
2: VALIDATION [0][30/80]	Time 0.024 (0.031)	Data 5.12e-04 (5.24e-04)	Tok/s 20716 (21682)	Loss/tok 6.4629 (6.4549)
3: VALIDATION [0][30/80]	Time 0.023 (0.031)	Data 5.01e-04 (5.14e-04)	Tok/s 21058 (21542)	Loss/tok 5.7786 (6.4401)
1: VALIDATION [0][30/80]	Time 0.024 (0.031)	Data 5.17e-04 (5.32e-04)	Tok/s 20215 (21787)	Loss/tok 6.1901 (6.4237)
0: VALIDATION [0][30/80]	Time 0.025 (0.032)	Data 4.96e-04 (5.06e-04)	Tok/s 19892 (21499)	Loss/tok 6.1542 (6.4464)
7: VALIDATION [0][40/79]	Time 0.020 (0.028)	Data 5.13e-04 (5.19e-04)	Tok/s 19607 (21294)	Loss/tok 6.3806 (6.3737)
5: VALIDATION [0][40/80]	Time 0.019 (0.028)	Data 5.18e-04 (5.15e-04)	Tok/s 20862 (21389)	Loss/tok 6.4379 (6.4403)
6: VALIDATION [0][40/79]	Time 0.020 (0.029)	Data 5.15e-04 (5.28e-04)	Tok/s 19585 (21180)	Loss/tok 5.8019 (6.4110)
4: VALIDATION [0][40/80]	Time 0.020 (0.029)	Data 5.08e-04 (5.14e-04)	Tok/s 19888 (21386)	Loss/tok 6.0466 (6.3621)
2: VALIDATION [0][40/80]	Time 0.020 (0.029)	Data 5.13e-04 (5.23e-04)	Tok/s 20640 (21394)	Loss/tok 6.0852 (6.4065)
1: VALIDATION [0][40/80]	Time 0.020 (0.029)	Data 5.07e-04 (5.28e-04)	Tok/s 20496 (21524)	Loss/tok 5.9793 (6.4018)
3: VALIDATION [0][40/80]	Time 0.019 (0.029)	Data 5.04e-04 (5.10e-04)	Tok/s 20577 (21258)	Loss/tok 6.3135 (6.3996)
0: VALIDATION [0][40/80]	Time 0.020 (0.029)	Data 4.91e-04 (5.03e-04)	Tok/s 20531 (21309)	Loss/tok 6.0736 (6.3912)
5: VALIDATION [0][50/80]	Time 0.016 (0.026)	Data 4.87e-04 (5.12e-04)	Tok/s 20219 (21061)	Loss/tok 6.1669 (6.4024)
7: VALIDATION [0][50/79]	Time 0.018 (0.026)	Data 5.09e-04 (5.17e-04)	Tok/s 18535 (20928)	Loss/tok 6.0995 (6.3276)
6: VALIDATION [0][50/79]	Time 0.017 (0.027)	Data 5.05e-04 (5.24e-04)	Tok/s 19169 (20856)	Loss/tok 5.8399 (6.3672)
4: VALIDATION [0][50/80]	Time 0.017 (0.027)	Data 4.97e-04 (5.15e-04)	Tok/s 18965 (21044)	Loss/tok 5.7454 (6.3355)
2: VALIDATION [0][50/80]	Time 0.017 (0.027)	Data 5.09e-04 (5.20e-04)	Tok/s 19534 (21081)	Loss/tok 5.5852 (6.3502)
3: VALIDATION [0][50/80]	Time 0.017 (0.027)	Data 4.96e-04 (5.09e-04)	Tok/s 19133 (20951)	Loss/tok 6.4216 (6.3661)
1: VALIDATION [0][50/80]	Time 0.018 (0.027)	Data 5.04e-04 (5.25e-04)	Tok/s 19133 (21163)	Loss/tok 6.2112 (6.3561)
0: VALIDATION [0][50/80]	Time 0.018 (0.027)	Data 4.90e-04 (5.02e-04)	Tok/s 18852 (20966)	Loss/tok 6.5510 (6.3459)
7: VALIDATION [0][60/79]	Time 0.015 (0.025)	Data 5.06e-04 (5.16e-04)	Tok/s 17986 (20497)	Loss/tok 5.7460 (6.2931)
5: VALIDATION [0][60/80]	Time 0.015 (0.025)	Data 5.12e-04 (5.12e-04)	Tok/s 17274 (20541)	Loss/tok 5.7280 (6.3586)
4: VALIDATION [0][60/80]	Time 0.015 (0.025)	Data 4.97e-04 (5.13e-04)	Tok/s 17809 (20579)	Loss/tok 5.6404 (6.3003)
6: VALIDATION [0][60/79]	Time 0.015 (0.025)	Data 5.02e-04 (5.22e-04)	Tok/s 17669 (20362)	Loss/tok 6.1248 (6.3370)
2: VALIDATION [0][60/80]	Time 0.015 (0.025)	Data 5.61e-04 (5.20e-04)	Tok/s 17507 (20628)	Loss/tok 5.8517 (6.3341)
3: VALIDATION [0][60/80]	Time 0.015 (0.025)	Data 4.96e-04 (5.08e-04)	Tok/s 17726 (20492)	Loss/tok 6.5281 (6.3184)
1: VALIDATION [0][60/80]	Time 0.016 (0.025)	Data 5.06e-04 (5.22e-04)	Tok/s 17233 (20705)	Loss/tok 6.0527 (6.3200)
0: VALIDATION [0][60/80]	Time 0.015 (0.025)	Data 4.88e-04 (5.01e-04)	Tok/s 17590 (20510)	Loss/tok 6.0339 (6.3099)
7: VALIDATION [0][70/79]	Time 0.012 (0.023)	Data 5.00e-04 (5.14e-04)	Tok/s 16659 (20014)	Loss/tok 6.5584 (6.2678)
5: VALIDATION [0][70/80]	Time 0.011 (0.023)	Data 5.09e-04 (5.10e-04)	Tok/s 17983 (20131)	Loss/tok 5.6884 (6.3211)
6: VALIDATION [0][70/79]	Time 0.012 (0.023)	Data 4.97e-04 (5.20e-04)	Tok/s 17208 (19927)	Loss/tok 5.5082 (6.2866)
4: VALIDATION [0][70/80]	Time 0.013 (0.023)	Data 5.26e-04 (5.12e-04)	Tok/s 15444 (20100)	Loss/tok 5.5874 (6.2714)
2: VALIDATION [0][70/80]	Time 0.011 (0.023)	Data 5.07e-04 (5.18e-04)	Tok/s 17484 (20218)	Loss/tok 6.0213 (6.3123)
1: VALIDATION [0][70/80]	Time 0.012 (0.023)	Data 5.07e-04 (5.20e-04)	Tok/s 16497 (20283)	Loss/tok 5.8880 (6.2851)
3: VALIDATION [0][70/80]	Time 0.012 (0.023)	Data 4.89e-04 (5.06e-04)	Tok/s 16668 (20066)	Loss/tok 6.3562 (6.2906)
0: VALIDATION [0][70/80]	Time 0.012 (0.024)	Data 4.99e-04 (5.00e-04)	Tok/s 16979 (20062)	Loss/tok 5.3776 (6.2764)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
2: Running evaluation on test set
5: Running evaluation on test set
3: Running evaluation on test set
4: Running evaluation on test set
6: Running evaluation on test set
0: Running evaluation on test set
7: Running evaluation on test set
7: TEST [0][9/12]	Time 0.4720 (0.6034)	Decoder iters 149.0 (139.3)	Tok/s 2305 (3120)
6: TEST [0][9/12]	Time 0.4724 (0.6035)	Decoder iters 149.0 (149.0)	Tok/s 2253 (3280)
3: TEST [0][9/12]	Time 0.4724 (0.6035)	Decoder iters 149.0 (138.4)	Tok/s 2267 (3210)
0: TEST [0][9/12]	Time 0.4724 (0.6033)	Decoder iters 149.0 (149.0)	Tok/s 2375 (3620)
5: TEST [0][9/12]	Time 0.4720 (0.6035)	Decoder iters 30.0 (137.1)	Tok/s 2013 (3158)
1: TEST [0][9/12]	Time 0.4725 (0.6035)	Decoder iters 149.0 (149.0)	Tok/s 2495 (3571)
4: TEST [0][9/12]	Time 0.4723 (0.6035)	Decoder iters 149.0 (149.0)	Tok/s 2249 (3318)
2: TEST [0][9/12]	Time 0.4722 (0.6036)	Decoder iters 149.0 (149.0)	Tok/s 2482 (3425)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
4: Finished evaluation on test set
6: Finished evaluation on test set
3: Finished evaluation on test set
7: Finished evaluation on test set
5: Finished evaluation on test set
0: Finished evaluation on test set
2: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
3: Finished epoch 0
6: Finished epoch 0
4: Finished epoch 0
7: Finished epoch 0
5: Finished epoch 0
4: Starting epoch 1
6: Starting epoch 1
3: Starting epoch 1
7: Starting epoch 1
5: Starting epoch 1
1: Executing preallocation
2: Finished epoch 0
2: Starting epoch 1
6: Executing preallocation
3: Executing preallocation
4: Executing preallocation
7: Executing preallocation
5: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.1143	Validation Loss: 6.2558	Test BLEU: 1.82
0: Performance: Epoch: 0	Training: 21490 Tok/s	Validation: 156315 Tok/s
2: Executing preallocation
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
4: Sampler for epoch 1 uses seed 2258090960
2: Sampler for epoch 1 uses seed 2258090960
0: Sampler for epoch 1 uses seed 2258090960
5: Sampler for epoch 1 uses seed 2258090960
3: Sampler for epoch 1 uses seed 2258090960
1: Sampler for epoch 1 uses seed 2258090960
7: Sampler for epoch 1 uses seed 2258090960
6: Sampler for epoch 1 uses seed 2258090960
4: TRAIN [1][0/461]	Time 0.902 (0.000)	Data 9.72e-02 (0.00e+00)	Tok/s 2755 (0)	Loss/tok 5.3885 (5.3885)	LR 2.000e-03
3: TRAIN [1][0/461]	Time 0.889 (0.000)	Data 1.10e-01 (0.00e+00)	Tok/s 2791 (0)	Loss/tok 5.4009 (5.4009)	LR 2.000e-03
2: TRAIN [1][0/461]	Time 0.902 (0.000)	Data 1.11e-01 (0.00e+00)	Tok/s 2773 (0)	Loss/tok 5.2012 (5.2012)	LR 2.000e-03
1: TRAIN [1][0/461]	Time 0.906 (0.000)	Data 1.26e-01 (0.00e+00)	Tok/s 2795 (0)	Loss/tok 5.4381 (5.4381)	LR 2.000e-03
7: TRAIN [1][0/461]	Time 0.895 (0.000)	Data 1.17e-01 (0.00e+00)	Tok/s 2857 (0)	Loss/tok 5.4628 (5.4628)	LR 2.000e-03
5: TRAIN [1][0/461]	Time 0.907 (0.000)	Data 1.08e-01 (0.00e+00)	Tok/s 2800 (0)	Loss/tok 5.4890 (5.4890)	LR 2.000e-03
6: TRAIN [1][0/461]	Time 0.909 (0.000)	Data 1.28e-01 (0.00e+00)	Tok/s 2735 (0)	Loss/tok 5.4294 (5.4294)	LR 2.000e-03
0: TRAIN [1][0/461]	Time 0.910 (0.000)	Data 1.09e-01 (0.00e+00)	Tok/s 2769 (0)	Loss/tok 5.2992 (5.2992)	LR 2.000e-03
1: TRAIN [1][10/461]	Time 0.762 (0.702)	Data 7.77e-05 (7.96e-05)	Tok/s 4207 (2425)	Loss/tok 5.6435 (5.1547)	LR 2.000e-03
6: TRAIN [1][10/461]	Time 0.763 (0.702)	Data 9.44e-05 (7.92e-05)	Tok/s 4349 (2467)	Loss/tok 5.5354 (5.1380)	LR 2.000e-03
2: TRAIN [1][10/461]	Time 0.764 (0.702)	Data 1.20e-04 (8.15e-05)	Tok/s 4271 (2461)	Loss/tok 5.7067 (5.1277)	LR 2.000e-03
5: TRAIN [1][10/461]	Time 0.764 (0.702)	Data 7.34e-05 (7.85e-05)	Tok/s 4295 (2469)	Loss/tok 5.6408 (5.1087)	LR 2.000e-03
0: TRAIN [1][10/461]	Time 0.762 (0.702)	Data 1.52e-04 (1.42e-04)	Tok/s 4333 (2472)	Loss/tok 5.6634 (5.1516)	LR 2.000e-03
7: TRAIN [1][10/461]	Time 0.762 (0.702)	Data 1.05e-04 (9.67e-05)	Tok/s 4233 (2430)	Loss/tok 5.6399 (5.1229)	LR 2.000e-03
4: TRAIN [1][10/461]	Time 0.764 (0.702)	Data 7.44e-05 (7.92e-05)	Tok/s 4242 (2452)	Loss/tok 5.7568 (5.1235)	LR 2.000e-03
3: TRAIN [1][10/461]	Time 0.765 (0.702)	Data 9.97e-05 (9.88e-05)	Tok/s 4259 (2472)	Loss/tok 5.4340 (5.1254)	LR 2.000e-03
1: TRAIN [1][20/461]	Time 0.766 (0.703)	Data 7.61e-05 (7.85e-05)	Tok/s 4367 (2306)	Loss/tok 5.6164 (5.0911)	LR 2.000e-03
6: TRAIN [1][20/461]	Time 0.765 (0.703)	Data 1.21e-04 (8.32e-05)	Tok/s 4239 (2317)	Loss/tok 5.4492 (5.0847)	LR 2.000e-03
0: TRAIN [1][20/461]	Time 0.768 (0.703)	Data 1.05e-04 (1.32e-04)	Tok/s 4198 (2294)	Loss/tok 5.4850 (5.0865)	LR 2.000e-03
7: TRAIN [1][20/461]	Time 0.768 (0.703)	Data 1.43e-04 (1.03e-04)	Tok/s 4301 (2285)	Loss/tok 5.5518 (5.1009)	LR 2.000e-03
5: TRAIN [1][20/461]	Time 0.766 (0.703)	Data 7.15e-05 (7.80e-05)	Tok/s 4247 (2314)	Loss/tok 5.6540 (5.0796)	LR 2.000e-03
2: TRAIN [1][20/461]	Time 0.766 (0.703)	Data 7.53e-05 (8.03e-05)	Tok/s 4283 (2314)	Loss/tok 5.5530 (5.1035)	LR 2.000e-03
4: TRAIN [1][20/461]	Time 0.766 (0.703)	Data 7.41e-05 (7.78e-05)	Tok/s 4240 (2304)	Loss/tok 5.3957 (5.0662)	LR 2.000e-03
3: TRAIN [1][20/461]	Time 0.766 (0.703)	Data 9.63e-05 (1.06e-04)	Tok/s 4223 (2305)	Loss/tok 5.5987 (5.0405)	LR 2.000e-03
7: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 1.00e-04 (1.02e-04)	Tok/s 1488 (2341)	Loss/tok 4.3570 (5.1054)	LR 2.000e-03
0: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 1.30e-04 (1.30e-04)	Tok/s 1523 (2341)	Loss/tok 4.6177 (5.1012)	LR 2.000e-03
1: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 7.80e-05 (7.73e-05)	Tok/s 1497 (2341)	Loss/tok 4.6719 (5.0978)	LR 2.000e-03
6: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 8.68e-05 (8.52e-05)	Tok/s 1551 (2361)	Loss/tok 4.8206 (5.1061)	LR 2.000e-03
5: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 7.08e-05 (7.66e-05)	Tok/s 1533 (2355)	Loss/tok 4.6798 (5.0839)	LR 2.000e-03
2: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 7.37e-05 (7.92e-05)	Tok/s 1554 (2361)	Loss/tok 4.4594 (5.1003)	LR 2.000e-03
4: TRAIN [1][30/461]	Time 0.701 (0.706)	Data 7.32e-05 (7.88e-05)	Tok/s 1649 (2352)	Loss/tok 4.4918 (5.0976)	LR 2.000e-03
3: TRAIN [1][30/461]	Time 0.702 (0.706)	Data 1.02e-04 (1.06e-04)	Tok/s 1537 (2348)	Loss/tok 4.3741 (5.0586)	LR 2.000e-03
3: TRAIN [1][40/461]	Time 0.699 (0.711)	Data 9.35e-05 (1.06e-04)	Tok/s 2634 (2473)	Loss/tok 4.9555 (5.0671)	LR 2.000e-03
4: TRAIN [1][40/461]	Time 0.699 (0.711)	Data 7.61e-05 (7.84e-05)	Tok/s 2602 (2475)	Loss/tok 4.9960 (5.0933)	LR 2.000e-03
2: TRAIN [1][40/461]	Time 0.702 (0.711)	Data 1.04e-04 (7.98e-05)	Tok/s 2559 (2486)	Loss/tok 4.7034 (5.1044)	LR 2.000e-03
5: TRAIN [1][40/461]	Time 0.703 (0.711)	Data 6.96e-05 (7.61e-05)	Tok/s 2542 (2476)	Loss/tok 4.8674 (5.0805)	LR 2.000e-03
1: TRAIN [1][40/461]	Time 0.706 (0.711)	Data 7.99e-05 (7.73e-05)	Tok/s 2610 (2465)	Loss/tok 4.7990 (5.0973)	LR 2.000e-03
6: TRAIN [1][40/461]	Time 0.706 (0.711)	Data 8.56e-05 (8.39e-05)	Tok/s 2555 (2477)	Loss/tok 4.6488 (5.1023)	LR 2.000e-03
0: TRAIN [1][40/461]	Time 0.710 (0.711)	Data 8.80e-05 (1.25e-04)	Tok/s 2504 (2466)	Loss/tok 4.9653 (5.1186)	LR 2.000e-03
7: TRAIN [1][40/461]	Time 0.710 (0.711)	Data 1.01e-04 (9.97e-05)	Tok/s 2525 (2462)	Loss/tok 4.7777 (5.1200)	LR 2.000e-03
0: TRAIN [1][50/461]	Time 0.716 (0.715)	Data 1.39e-04 (1.22e-04)	Tok/s 2516 (2541)	Loss/tok 4.8373 (5.1115)	LR 2.000e-03
7: TRAIN [1][50/461]	Time 0.717 (0.715)	Data 1.08e-04 (9.90e-05)	Tok/s 2567 (2539)	Loss/tok 4.9834 (5.1484)	LR 2.000e-03
1: TRAIN [1][50/461]	Time 0.717 (0.715)	Data 8.68e-05 (7.79e-05)	Tok/s 2474 (2532)	Loss/tok 4.8708 (5.1045)	LR 2.000e-03
6: TRAIN [1][50/461]	Time 0.717 (0.715)	Data 8.61e-05 (8.30e-05)	Tok/s 2430 (2551)	Loss/tok 5.1778 (5.1156)	LR 2.000e-03
5: TRAIN [1][50/461]	Time 0.717 (0.715)	Data 6.96e-05 (7.57e-05)	Tok/s 2533 (2550)	Loss/tok 4.9784 (5.0968)	LR 2.000e-03
2: TRAIN [1][50/461]	Time 0.718 (0.715)	Data 7.72e-05 (7.93e-05)	Tok/s 2545 (2563)	Loss/tok 5.0304 (5.1164)	LR 2.000e-03
4: TRAIN [1][50/461]	Time 0.717 (0.715)	Data 7.25e-05 (7.89e-05)	Tok/s 2459 (2545)	Loss/tok 4.9302 (5.1240)	LR 2.000e-03
3: TRAIN [1][50/461]	Time 0.718 (0.715)	Data 9.58e-05 (1.06e-04)	Tok/s 2457 (2540)	Loss/tok 4.9091 (5.0938)	LR 2.000e-03
1: TRAIN [1][60/461]	Time 0.730 (0.713)	Data 8.68e-05 (7.78e-05)	Tok/s 3457 (2502)	Loss/tok 5.1900 (5.0871)	LR 2.000e-03
6: TRAIN [1][60/461]	Time 0.730 (0.713)	Data 9.47e-05 (8.26e-05)	Tok/s 3434 (2520)	Loss/tok 5.1185 (5.0883)	LR 2.000e-03
2: TRAIN [1][60/461]	Time 0.732 (0.713)	Data 7.44e-05 (7.91e-05)	Tok/s 3427 (2530)	Loss/tok 5.1148 (5.0961)	LR 2.000e-03
5: TRAIN [1][60/461]	Time 0.732 (0.713)	Data 8.34e-05 (7.55e-05)	Tok/s 3390 (2521)	Loss/tok 5.1696 (5.0789)	LR 2.000e-03
0: TRAIN [1][60/461]	Time 0.729 (0.713)	Data 1.27e-04 (1.19e-04)	Tok/s 3483 (2512)	Loss/tok 5.0398 (5.0878)	LR 2.000e-03
7: TRAIN [1][60/461]	Time 0.729 (0.713)	Data 1.09e-04 (9.93e-05)	Tok/s 3400 (2511)	Loss/tok 5.1751 (5.1169)	LR 2.000e-03
4: TRAIN [1][60/461]	Time 0.734 (0.713)	Data 7.18e-05 (7.91e-05)	Tok/s 3396 (2516)	Loss/tok 5.1345 (5.1011)	LR 2.000e-03
3: TRAIN [1][60/461]	Time 0.734 (0.713)	Data 1.01e-04 (1.06e-04)	Tok/s 3390 (2508)	Loss/tok 4.9123 (5.0598)	LR 2.000e-03
0: TRAIN [1][70/461]	Time 0.683 (0.713)	Data 1.03e-04 (1.17e-04)	Tok/s 1604 (2498)	Loss/tok 4.3290 (5.0511)	LR 2.000e-03
7: TRAIN [1][70/461]	Time 0.684 (0.713)	Data 9.73e-05 (9.86e-05)	Tok/s 1621 (2501)	Loss/tok 4.4675 (5.0856)	LR 2.000e-03
6: TRAIN [1][70/461]	Time 0.686 (0.713)	Data 7.46e-05 (8.17e-05)	Tok/s 1599 (2505)	Loss/tok 4.1093 (5.0653)	LR 2.000e-03
1: TRAIN [1][70/461]	Time 0.687 (0.713)	Data 7.20e-05 (7.75e-05)	Tok/s 1576 (2492)	Loss/tok 4.5257 (5.0584)	LR 2.000e-03
2: TRAIN [1][70/461]	Time 0.689 (0.713)	Data 7.75e-05 (7.91e-05)	Tok/s 1631 (2513)	Loss/tok 4.3893 (5.0571)	LR 2.000e-03
5: TRAIN [1][70/461]	Time 0.689 (0.713)	Data 6.94e-05 (7.58e-05)	Tok/s 1532 (2506)	Loss/tok 4.3929 (5.0614)	LR 2.000e-03
4: TRAIN [1][70/461]	Time 0.689 (0.713)	Data 6.41e-05 (7.86e-05)	Tok/s 1565 (2502)	Loss/tok 4.3972 (5.0699)	LR 2.000e-03
3: TRAIN [1][70/461]	Time 0.690 (0.713)	Data 6.60e-05 (1.06e-04)	Tok/s 1547 (2493)	Loss/tok 4.4878 (5.0388)	LR 2.000e-03
7: TRAIN [1][80/461]	Time 0.657 (0.711)	Data 9.87e-05 (9.90e-05)	Tok/s 827 (2436)	Loss/tok 3.7799 (5.0499)	LR 2.000e-03
0: TRAIN [1][80/461]	Time 0.657 (0.711)	Data 1.24e-04 (1.16e-04)	Tok/s 833 (2432)	Loss/tok 3.7108 (5.0174)	LR 2.000e-03
1: TRAIN [1][80/461]	Time 0.661 (0.711)	Data 7.89e-05 (7.79e-05)	Tok/s 817 (2426)	Loss/tok 4.5367 (5.0249)	LR 2.000e-03
6: TRAIN [1][80/461]	Time 0.660 (0.711)	Data 7.65e-05 (8.17e-05)	Tok/s 798 (2439)	Loss/tok 4.5555 (5.0397)	LR 2.000e-03
5: TRAIN [1][80/461]	Time 0.661 (0.711)	Data 9.25e-05 (7.74e-05)	Tok/s 821 (2440)	Loss/tok 4.3622 (5.0250)	LR 2.000e-03
2: TRAIN [1][80/461]	Time 0.662 (0.711)	Data 7.13e-05 (7.96e-05)	Tok/s 760 (2445)	Loss/tok 4.0510 (5.0380)	LR 2.000e-03
4: TRAIN [1][80/461]	Time 0.661 (0.711)	Data 6.51e-05 (7.85e-05)	Tok/s 820 (2435)	Loss/tok 4.1932 (5.0445)	LR 2.000e-03
3: TRAIN [1][80/461]	Time 0.662 (0.711)	Data 7.08e-05 (1.05e-04)	Tok/s 840 (2429)	Loss/tok 4.0552 (5.0177)	LR 2.000e-03
1: TRAIN [1][90/461]	Time 0.729 (0.712)	Data 1.09e-04 (7.79e-05)	Tok/s 3473 (2475)	Loss/tok 4.9548 (5.0199)	LR 2.000e-03
6: TRAIN [1][90/461]	Time 0.729 (0.712)	Data 1.02e-04 (8.17e-05)	Tok/s 3422 (2487)	Loss/tok 4.7837 (5.0253)	LR 2.000e-03
5: TRAIN [1][90/461]	Time 0.731 (0.712)	Data 7.25e-05 (7.70e-05)	Tok/s 3396 (2488)	Loss/tok 4.8879 (5.0195)	LR 2.000e-03
2: TRAIN [1][90/461]	Time 0.732 (0.712)	Data 7.65e-05 (7.97e-05)	Tok/s 3411 (2493)	Loss/tok 4.8937 (5.0302)	LR 2.000e-03
0: TRAIN [1][90/461]	Time 0.729 (0.712)	Data 9.73e-05 (1.15e-04)	Tok/s 3416 (2480)	Loss/tok 5.1262 (5.0091)	LR 2.000e-03
7: TRAIN [1][90/461]	Time 0.728 (0.712)	Data 1.11e-04 (1.01e-04)	Tok/s 3434 (2484)	Loss/tok 5.0337 (5.0436)	LR 2.000e-03
4: TRAIN [1][90/461]	Time 0.735 (0.713)	Data 7.15e-05 (7.85e-05)	Tok/s 3385 (2481)	Loss/tok 4.7531 (5.0320)	LR 2.000e-03
3: TRAIN [1][90/461]	Time 0.736 (0.713)	Data 9.13e-05 (1.04e-04)	Tok/s 3395 (2480)	Loss/tok 4.9668 (5.0130)	LR 2.000e-03
3: TRAIN [1][100/461]	Time 0.682 (0.714)	Data 1.05e-04 (1.02e-04)	Tok/s 1605 (2518)	Loss/tok 4.3171 (4.9989)	LR 2.000e-03
4: TRAIN [1][100/461]	Time 0.682 (0.714)	Data 6.22e-05 (7.82e-05)	Tok/s 1630 (2517)	Loss/tok 4.2718 (5.0221)	LR 2.000e-03
5: TRAIN [1][100/461]	Time 0.684 (0.714)	Data 6.70e-05 (7.64e-05)	Tok/s 1629 (2524)	Loss/tok 4.3726 (5.0033)	LR 2.000e-03
2: TRAIN [1][100/461]	Time 0.685 (0.714)	Data 7.58e-05 (7.99e-05)	Tok/s 1506 (2528)	Loss/tok 4.3992 (5.0062)	LR 2.000e-03
1: TRAIN [1][100/461]	Time 0.685 (0.714)	Data 7.32e-05 (7.76e-05)	Tok/s 1572 (2513)	Loss/tok 4.4191 (5.0064)	LR 2.000e-03
6: TRAIN [1][100/461]	Time 0.685 (0.714)	Data 8.03e-05 (8.14e-05)	Tok/s 1652 (2523)	Loss/tok 4.3996 (5.0089)	LR 2.000e-03
7: TRAIN [1][100/461]	Time 0.685 (0.714)	Data 9.82e-05 (1.01e-04)	Tok/s 1541 (2520)	Loss/tok 4.5941 (5.0297)	LR 2.000e-03
0: TRAIN [1][100/461]	Time 0.685 (0.714)	Data 1.24e-04 (1.14e-04)	Tok/s 1513 (2517)	Loss/tok 4.2958 (4.9930)	LR 2.000e-03
3: TRAIN [1][110/461]	Time 0.793 (0.714)	Data 6.68e-05 (1.01e-04)	Tok/s 4066 (2539)	Loss/tok 5.2506 (4.9889)	LR 2.000e-03
4: TRAIN [1][110/461]	Time 0.793 (0.714)	Data 8.56e-05 (7.80e-05)	Tok/s 4049 (2539)	Loss/tok 5.2219 (5.0049)	LR 2.000e-03
5: TRAIN [1][110/461]	Time 0.793 (0.714)	Data 6.96e-05 (7.65e-05)	Tok/s 4108 (2548)	Loss/tok 5.2115 (4.9883)	LR 2.000e-03
2: TRAIN [1][110/461]	Time 0.793 (0.714)	Data 8.13e-05 (8.02e-05)	Tok/s 4079 (2548)	Loss/tok 5.2447 (4.9995)	LR 2.000e-03
1: TRAIN [1][110/461]	Time 0.793 (0.714)	Data 1.15e-04 (7.80e-05)	Tok/s 4122 (2537)	Loss/tok 5.3234 (4.9933)	LR 2.000e-03
6: TRAIN [1][110/461]	Time 0.792 (0.714)	Data 7.87e-05 (8.16e-05)	Tok/s 4155 (2548)	Loss/tok 5.1989 (4.9911)	LR 2.000e-03
7: TRAIN [1][110/461]	Time 0.792 (0.714)	Data 9.82e-05 (1.01e-04)	Tok/s 4173 (2543)	Loss/tok 5.3254 (5.0200)	LR 2.000e-03
0: TRAIN [1][110/461]	Time 0.793 (0.714)	Data 8.08e-05 (1.13e-04)	Tok/s 4101 (2540)	Loss/tok 5.2694 (4.9745)	LR 2.000e-03
6: TRAIN [1][120/461]	Time 0.709 (0.714)	Data 9.58e-05 (8.19e-05)	Tok/s 2568 (2528)	Loss/tok 4.5964 (4.9681)	LR 2.000e-03
1: TRAIN [1][120/461]	Time 0.710 (0.714)	Data 7.56e-05 (7.78e-05)	Tok/s 2514 (2520)	Loss/tok 4.8084 (4.9663)	LR 2.000e-03
0: TRAIN [1][120/461]	Time 0.711 (0.714)	Data 1.25e-04 (1.13e-04)	Tok/s 2423 (2520)	Loss/tok 4.6362 (4.9525)	LR 2.000e-03
7: TRAIN [1][120/461]	Time 0.712 (0.714)	Data 1.14e-04 (1.01e-04)	Tok/s 2528 (2527)	Loss/tok 4.7235 (4.9946)	LR 2.000e-03
5: TRAIN [1][120/461]	Time 0.709 (0.714)	Data 7.34e-05 (7.64e-05)	Tok/s 2533 (2527)	Loss/tok 4.5190 (4.9584)	LR 2.000e-03
2: TRAIN [1][120/461]	Time 0.709 (0.714)	Data 7.89e-05 (8.03e-05)	Tok/s 2447 (2530)	Loss/tok 4.7204 (4.9757)	LR 2.000e-03
4: TRAIN [1][120/461]	Time 0.709 (0.714)	Data 9.11e-05 (7.80e-05)	Tok/s 2593 (2520)	Loss/tok 4.8051 (4.9804)	LR 2.000e-03
3: TRAIN [1][120/461]	Time 0.711 (0.714)	Data 2.59e-04 (1.02e-04)	Tok/s 2538 (2522)	Loss/tok 4.9363 (4.9615)	LR 2.000e-03
1: TRAIN [1][130/461]	Time 0.775 (0.715)	Data 7.58e-05 (7.77e-05)	Tok/s 4241 (2554)	Loss/tok 5.1570 (4.9578)	LR 2.000e-03
6: TRAIN [1][130/461]	Time 0.775 (0.715)	Data 7.44e-05 (8.18e-05)	Tok/s 4253 (2560)	Loss/tok 5.1496 (4.9585)	LR 2.000e-03
0: TRAIN [1][130/461]	Time 0.774 (0.715)	Data 1.18e-04 (1.12e-04)	Tok/s 4194 (2552)	Loss/tok 5.0593 (4.9367)	LR 2.000e-03
7: TRAIN [1][130/461]	Time 0.775 (0.715)	Data 9.70e-05 (1.01e-04)	Tok/s 4227 (2557)	Loss/tok 5.2571 (4.9887)	LR 2.000e-03
5: TRAIN [1][130/461]	Time 0.780 (0.715)	Data 6.65e-05 (7.63e-05)	Tok/s 4163 (2557)	Loss/tok 5.0457 (4.9422)	LR 2.000e-03
2: TRAIN [1][130/461]	Time 0.780 (0.715)	Data 7.15e-05 (8.03e-05)	Tok/s 4203 (2563)	Loss/tok 5.0502 (4.9599)	LR 2.000e-03
4: TRAIN [1][130/461]	Time 0.783 (0.715)	Data 7.06e-05 (7.75e-05)	Tok/s 4208 (2553)	Loss/tok 5.1009 (4.9641)	LR 2.000e-03
3: TRAIN [1][130/461]	Time 0.783 (0.715)	Data 6.70e-05 (1.01e-04)	Tok/s 4223 (2555)	Loss/tok 4.9855 (4.9487)	LR 2.000e-03
3: TRAIN [1][140/461]	Time 0.707 (0.716)	Data 7.77e-05 (1.00e-04)	Tok/s 1617 (2575)	Loss/tok 4.0693 (4.9374)	LR 2.000e-03
4: TRAIN [1][140/461]	Time 0.707 (0.716)	Data 6.89e-05 (7.81e-05)	Tok/s 1516 (2574)	Loss/tok 4.0189 (4.9478)	LR 2.000e-03
2: TRAIN [1][140/461]	Time 0.709 (0.716)	Data 8.73e-05 (8.05e-05)	Tok/s 1555 (2581)	Loss/tok 4.3029 (4.9444)	LR 2.000e-03
5: TRAIN [1][140/461]	Time 0.709 (0.716)	Data 6.99e-05 (7.61e-05)	Tok/s 1492 (2578)	Loss/tok 4.0528 (4.9271)	LR 2.000e-03
1: TRAIN [1][140/461]	Time 0.712 (0.716)	Data 7.77e-05 (7.77e-05)	Tok/s 1467 (2574)	Loss/tok 4.3818 (4.9413)	LR 2.000e-03
6: TRAIN [1][140/461]	Time 0.712 (0.716)	Data 8.44e-05 (8.21e-05)	Tok/s 1398 (2577)	Loss/tok 4.0415 (4.9469)	LR 2.000e-03
0: TRAIN [1][140/461]	Time 0.715 (0.716)	Data 8.46e-05 (1.11e-04)	Tok/s 1546 (2571)	Loss/tok 4.0788 (4.9207)	LR 2.000e-03
7: TRAIN [1][140/461]	Time 0.715 (0.716)	Data 1.25e-04 (1.01e-04)	Tok/s 1608 (2577)	Loss/tok 4.1697 (4.9659)	LR 2.000e-03
0: TRAIN [1][150/461]	Time 0.763 (0.716)	Data 7.61e-05 (1.10e-04)	Tok/s 4220 (2585)	Loss/tok 5.3012 (4.9068)	LR 2.000e-03
7: TRAIN [1][150/461]	Time 0.762 (0.716)	Data 1.28e-04 (1.01e-04)	Tok/s 4264 (2591)	Loss/tok 5.0446 (4.9475)	LR 2.000e-03
1: TRAIN [1][150/461]	Time 0.763 (0.716)	Data 7.70e-05 (7.77e-05)	Tok/s 4334 (2588)	Loss/tok 4.8964 (4.9220)	LR 2.000e-03
6: TRAIN [1][150/461]	Time 0.763 (0.716)	Data 7.49e-05 (8.20e-05)	Tok/s 4295 (2590)	Loss/tok 5.1727 (4.9292)	LR 2.000e-03
5: TRAIN [1][150/461]	Time 0.763 (0.716)	Data 7.30e-05 (7.63e-05)	Tok/s 4349 (2590)	Loss/tok 4.8576 (4.9111)	LR 2.000e-03
2: TRAIN [1][150/461]	Time 0.763 (0.716)	Data 7.92e-05 (8.07e-05)	Tok/s 4269 (2594)	Loss/tok 4.9948 (4.9246)	LR 2.000e-03
4: TRAIN [1][150/461]	Time 0.763 (0.716)	Data 6.51e-05 (7.83e-05)	Tok/s 4250 (2588)	Loss/tok 4.9215 (4.9254)	LR 2.000e-03
3: TRAIN [1][150/461]	Time 0.764 (0.716)	Data 7.77e-05 (1.00e-04)	Tok/s 4319 (2589)	Loss/tok 5.0150 (4.9246)	LR 2.000e-03
0: TRAIN [1][160/461]	Time 0.693 (0.716)	Data 1.21e-04 (1.09e-04)	Tok/s 1523 (2593)	Loss/tok 4.2291 (4.8937)	LR 1.000e-03
7: TRAIN [1][160/461]	Time 0.694 (0.716)	Data 1.03e-04 (1.01e-04)	Tok/s 1571 (2599)	Loss/tok 4.1768 (4.9306)	LR 1.000e-03
1: TRAIN [1][160/461]	Time 0.694 (0.716)	Data 7.03e-05 (7.77e-05)	Tok/s 1552 (2596)	Loss/tok 4.3951 (4.9104)	LR 1.000e-03
6: TRAIN [1][160/461]	Time 0.694 (0.716)	Data 7.87e-05 (8.21e-05)	Tok/s 1546 (2598)	Loss/tok 4.2355 (4.9116)	LR 1.000e-03
5: TRAIN [1][160/461]	Time 0.694 (0.716)	Data 6.44e-05 (7.60e-05)	Tok/s 1629 (2599)	Loss/tok 4.1867 (4.9000)	LR 1.000e-03
2: TRAIN [1][160/461]	Time 0.694 (0.716)	Data 9.68e-05 (8.04e-05)	Tok/s 1585 (2602)	Loss/tok 4.4934 (4.9112)	LR 1.000e-03
4: TRAIN [1][160/461]	Time 0.694 (0.716)	Data 7.27e-05 (7.81e-05)	Tok/s 1525 (2596)	Loss/tok 4.0064 (4.9087)	LR 1.000e-03
3: TRAIN [1][160/461]	Time 0.695 (0.716)	Data 8.27e-05 (1.00e-04)	Tok/s 1585 (2598)	Loss/tok 4.0514 (4.9132)	LR 1.000e-03
2: TRAIN [1][170/461]	Time 0.692 (0.716)	Data 7.25e-05 (8.04e-05)	Tok/s 1544 (2618)	Loss/tok 4.1265 (4.8935)	LR 1.000e-03
5: TRAIN [1][170/461]	Time 0.692 (0.716)	Data 6.96e-05 (7.57e-05)	Tok/s 1484 (2616)	Loss/tok 4.0468 (4.8797)	LR 1.000e-03
1: TRAIN [1][170/461]	Time 0.694 (0.716)	Data 7.32e-05 (7.78e-05)	Tok/s 1525 (2612)	Loss/tok 4.0579 (4.8959)	LR 1.000e-03
6: TRAIN [1][170/461]	Time 0.694 (0.716)	Data 9.06e-05 (8.21e-05)	Tok/s 1613 (2615)	Loss/tok 4.4824 (4.8953)	LR 1.000e-03
3: TRAIN [1][170/461]	Time 0.692 (0.716)	Data 6.70e-05 (9.89e-05)	Tok/s 1561 (2614)	Loss/tok 3.9046 (4.8946)	LR 1.000e-03
4: TRAIN [1][170/461]	Time 0.692 (0.716)	Data 7.06e-05 (7.81e-05)	Tok/s 1490 (2611)	Loss/tok 4.2200 (4.8911)	LR 1.000e-03
0: TRAIN [1][170/461]	Time 0.695 (0.716)	Data 7.92e-05 (1.08e-04)	Tok/s 1508 (2609)	Loss/tok 4.4137 (4.8755)	LR 1.000e-03
7: TRAIN [1][170/461]	Time 0.695 (0.716)	Data 1.05e-04 (1.01e-04)	Tok/s 1646 (2616)	Loss/tok 4.3853 (4.9086)	LR 1.000e-03
0: TRAIN [1][180/461]	Time 0.682 (0.715)	Data 8.82e-05 (1.08e-04)	Tok/s 2628 (2600)	Loss/tok 4.4543 (4.8571)	LR 1.000e-03
7: TRAIN [1][180/461]	Time 0.682 (0.715)	Data 1.09e-04 (1.01e-04)	Tok/s 2632 (2608)	Loss/tok 4.4449 (4.8872)	LR 1.000e-03
6: TRAIN [1][180/461]	Time 0.682 (0.715)	Data 1.25e-04 (8.21e-05)	Tok/s 2705 (2607)	Loss/tok 4.2008 (4.8757)	LR 1.000e-03
1: TRAIN [1][180/461]	Time 0.683 (0.715)	Data 7.84e-05 (7.77e-05)	Tok/s 2536 (2603)	Loss/tok 4.5985 (4.8766)	LR 1.000e-03
5: TRAIN [1][180/461]	Time 0.683 (0.715)	Data 7.27e-05 (7.57e-05)	Tok/s 2589 (2607)	Loss/tok 4.2717 (4.8607)	LR 1.000e-03
2: TRAIN [1][180/461]	Time 0.683 (0.715)	Data 7.56e-05 (8.02e-05)	Tok/s 2644 (2610)	Loss/tok 4.4852 (4.8742)	LR 1.000e-03
3: TRAIN [1][180/461]	Time 0.682 (0.715)	Data 8.23e-05 (9.77e-05)	Tok/s 2670 (2606)	Loss/tok 4.4076 (4.8733)	LR 1.000e-03
4: TRAIN [1][180/461]	Time 0.683 (0.715)	Data 7.70e-05 (7.79e-05)	Tok/s 2536 (2602)	Loss/tok 4.4285 (4.8726)	LR 1.000e-03
7: TRAIN [1][190/461]	Time 0.695 (0.716)	Data 1.50e-04 (1.01e-04)	Tok/s 2713 (2620)	Loss/tok 4.5187 (4.8689)	LR 1.000e-03
0: TRAIN [1][190/461]	Time 0.696 (0.716)	Data 1.18e-04 (1.07e-04)	Tok/s 2516 (2609)	Loss/tok 4.4824 (4.8382)	LR 1.000e-03
1: TRAIN [1][190/461]	Time 0.700 (0.716)	Data 8.27e-05 (7.78e-05)	Tok/s 2637 (2612)	Loss/tok 4.4682 (4.8578)	LR 1.000e-03
6: TRAIN [1][190/461]	Time 0.699 (0.716)	Data 9.25e-05 (8.19e-05)	Tok/s 2614 (2617)	Loss/tok 4.4673 (4.8539)	LR 1.000e-03
5: TRAIN [1][190/461]	Time 0.703 (0.716)	Data 6.91e-05 (7.59e-05)	Tok/s 2617 (2617)	Loss/tok 4.2990 (4.8407)	LR 1.000e-03
2: TRAIN [1][190/461]	Time 0.703 (0.716)	Data 7.65e-05 (8.03e-05)	Tok/s 2563 (2618)	Loss/tok 4.4896 (4.8527)	LR 1.000e-03
3: TRAIN [1][190/461]	Time 0.705 (0.716)	Data 8.92e-05 (9.66e-05)	Tok/s 2526 (2616)	Loss/tok 4.3059 (4.8536)	LR 1.000e-03
4: TRAIN [1][190/461]	Time 0.705 (0.716)	Data 7.56e-05 (7.77e-05)	Tok/s 2558 (2611)	Loss/tok 4.1756 (4.8561)	LR 1.000e-03
1: TRAIN [1][200/461]	Time 0.708 (0.716)	Data 8.37e-05 (7.78e-05)	Tok/s 2509 (2628)	Loss/tok 4.3778 (4.8398)	LR 1.000e-03
6: TRAIN [1][200/461]	Time 0.708 (0.716)	Data 7.96e-05 (8.19e-05)	Tok/s 2589 (2633)	Loss/tok 4.1788 (4.8333)	LR 1.000e-03
0: TRAIN [1][200/461]	Time 0.708 (0.716)	Data 8.77e-05 (1.06e-04)	Tok/s 2552 (2626)	Loss/tok 4.3254 (4.8198)	LR 1.000e-03
5: TRAIN [1][200/461]	Time 0.709 (0.716)	Data 6.75e-05 (7.58e-05)	Tok/s 2564 (2633)	Loss/tok 4.3943 (4.8233)	LR 1.000e-03
7: TRAIN [1][200/461]	Time 0.708 (0.716)	Data 9.75e-05 (1.01e-04)	Tok/s 2473 (2635)	Loss/tok 4.3798 (4.8510)	LR 1.000e-03
2: TRAIN [1][200/461]	Time 0.709 (0.716)	Data 8.39e-05 (8.05e-05)	Tok/s 2565 (2632)	Loss/tok 4.3489 (4.8359)	LR 1.000e-03
3: TRAIN [1][200/461]	Time 0.708 (0.716)	Data 6.84e-05 (9.57e-05)	Tok/s 2534 (2633)	Loss/tok 4.2622 (4.8338)	LR 1.000e-03
4: TRAIN [1][200/461]	Time 0.708 (0.716)	Data 6.51e-05 (7.76e-05)	Tok/s 2593 (2627)	Loss/tok 4.0716 (4.8417)	LR 1.000e-03
2: TRAIN [1][210/461]	Time 0.704 (0.716)	Data 8.23e-05 (8.06e-05)	Tok/s 2574 (2627)	Loss/tok 4.0443 (4.8136)	LR 1.000e-03
5: TRAIN [1][210/461]	Time 0.705 (0.716)	Data 7.10e-05 (7.56e-05)	Tok/s 2542 (2627)	Loss/tok 4.2651 (4.8076)	LR 1.000e-03
3: TRAIN [1][210/461]	Time 0.705 (0.716)	Data 8.51e-05 (9.50e-05)	Tok/s 2509 (2627)	Loss/tok 4.0838 (4.8140)	LR 1.000e-03
4: TRAIN [1][210/461]	Time 0.705 (0.716)	Data 7.20e-05 (7.74e-05)	Tok/s 2584 (2621)	Loss/tok 4.3830 (4.8208)	LR 1.000e-03
1: TRAIN [1][210/461]	Time 0.709 (0.716)	Data 7.87e-05 (7.78e-05)	Tok/s 2513 (2623)	Loss/tok 4.1898 (4.8183)	LR 1.000e-03
6: TRAIN [1][210/461]	Time 0.709 (0.716)	Data 8.94e-05 (8.19e-05)	Tok/s 2586 (2628)	Loss/tok 4.0224 (4.8139)	LR 1.000e-03
0: TRAIN [1][210/461]	Time 0.711 (0.716)	Data 1.18e-04 (1.06e-04)	Tok/s 2644 (2619)	Loss/tok 4.2536 (4.8022)	LR 1.000e-03
7: TRAIN [1][210/461]	Time 0.711 (0.716)	Data 1.13e-04 (1.01e-04)	Tok/s 2584 (2630)	Loss/tok 4.1700 (4.8290)	LR 1.000e-03
3: TRAIN [1][220/461]	Time 0.699 (0.716)	Data 7.53e-05 (9.42e-05)	Tok/s 1486 (2627)	Loss/tok 3.8986 (4.7938)	LR 1.000e-03
4: TRAIN [1][220/461]	Time 0.699 (0.716)	Data 6.65e-05 (7.82e-05)	Tok/s 1486 (2621)	Loss/tok 3.9026 (4.8006)	LR 1.000e-03
2: TRAIN [1][220/461]	Time 0.702 (0.716)	Data 9.89e-05 (8.04e-05)	Tok/s 1514 (2626)	Loss/tok 4.3187 (4.7964)	LR 1.000e-03
5: TRAIN [1][220/461]	Time 0.702 (0.716)	Data 6.22e-05 (7.53e-05)	Tok/s 1530 (2628)	Loss/tok 4.1851 (4.7856)	LR 1.000e-03
1: TRAIN [1][220/461]	Time 0.707 (0.716)	Data 6.75e-05 (7.76e-05)	Tok/s 1534 (2623)	Loss/tok 4.0166 (4.7994)	LR 1.000e-03
6: TRAIN [1][220/461]	Time 0.707 (0.716)	Data 7.72e-05 (8.19e-05)	Tok/s 1463 (2627)	Loss/tok 4.0494 (4.7949)	LR 1.000e-03
7: TRAIN [1][220/461]	Time 0.707 (0.716)	Data 1.08e-04 (1.02e-04)	Tok/s 1486 (2629)	Loss/tok 3.8262 (4.8086)	LR 1.000e-03
0: TRAIN [1][220/461]	Time 0.708 (0.716)	Data 8.65e-05 (1.05e-04)	Tok/s 1505 (2619)	Loss/tok 4.3083 (4.7834)	LR 1.000e-03
0: TRAIN [1][230/461]	Time 0.675 (0.716)	Data 1.35e-04 (1.05e-04)	Tok/s 1596 (2616)	Loss/tok 3.9761 (4.7607)	LR 5.000e-04
7: TRAIN [1][230/461]	Time 0.675 (0.716)	Data 1.05e-04 (1.02e-04)	Tok/s 1662 (2623)	Loss/tok 4.1071 (4.7889)	LR 5.000e-04
1: TRAIN [1][230/461]	Time 0.677 (0.716)	Data 7.01e-05 (7.75e-05)	Tok/s 1602 (2619)	Loss/tok 4.3125 (4.7797)	LR 5.000e-04
6: TRAIN [1][230/461]	Time 0.677 (0.716)	Data 9.63e-05 (8.17e-05)	Tok/s 1611 (2622)	Loss/tok 3.9107 (4.7747)	LR 5.000e-04
5: TRAIN [1][230/461]	Time 0.680 (0.716)	Data 1.03e-04 (7.54e-05)	Tok/s 1614 (2623)	Loss/tok 3.8010 (4.7670)	LR 5.000e-04
2: TRAIN [1][230/461]	Time 0.681 (0.716)	Data 7.18e-05 (8.04e-05)	Tok/s 1614 (2621)	Loss/tok 3.9718 (4.7768)	LR 5.000e-04
4: TRAIN [1][230/461]	Time 0.684 (0.716)	Data 5.98e-05 (7.80e-05)	Tok/s 1647 (2617)	Loss/tok 3.9611 (4.7800)	LR 5.000e-04
3: TRAIN [1][230/461]	Time 0.684 (0.716)	Data 6.32e-05 (9.34e-05)	Tok/s 1590 (2622)	Loss/tok 4.0699 (4.7747)	LR 5.000e-04
2: TRAIN [1][240/461]	Time 0.687 (0.716)	Data 7.84e-05 (8.01e-05)	Tok/s 1559 (2616)	Loss/tok 3.8813 (4.7587)	LR 5.000e-04
5: TRAIN [1][240/461]	Time 0.688 (0.716)	Data 6.22e-05 (7.51e-05)	Tok/s 1585 (2618)	Loss/tok 4.1329 (4.7496)	LR 5.000e-04
1: TRAIN [1][240/461]	Time 0.690 (0.716)	Data 7.68e-05 (7.74e-05)	Tok/s 1512 (2615)	Loss/tok 3.8391 (4.7618)	LR 5.000e-04
6: TRAIN [1][240/461]	Time 0.690 (0.716)	Data 8.15e-05 (8.17e-05)	Tok/s 1552 (2617)	Loss/tok 4.1222 (4.7557)	LR 5.000e-04
4: TRAIN [1][240/461]	Time 0.688 (0.716)	Data 7.39e-05 (7.80e-05)	Tok/s 1511 (2612)	Loss/tok 4.2134 (4.7633)	LR 5.000e-04
3: TRAIN [1][240/461]	Time 0.688 (0.716)	Data 6.77e-05 (9.28e-05)	Tok/s 1545 (2618)	Loss/tok 3.8769 (4.7562)	LR 5.000e-04
0: TRAIN [1][240/461]	Time 0.692 (0.716)	Data 1.21e-04 (1.05e-04)	Tok/s 1498 (2611)	Loss/tok 3.8515 (4.7459)	LR 5.000e-04
7: TRAIN [1][240/461]	Time 0.693 (0.716)	Data 1.01e-04 (1.02e-04)	Tok/s 1588 (2619)	Loss/tok 4.0467 (4.7695)	LR 5.000e-04
3: TRAIN [1][250/461]	Time 0.696 (0.716)	Data 6.79e-05 (9.24e-05)	Tok/s 1544 (2623)	Loss/tok 3.7057 (4.7363)	LR 5.000e-04
4: TRAIN [1][250/461]	Time 0.696 (0.716)	Data 6.91e-05 (7.82e-05)	Tok/s 1569 (2618)	Loss/tok 3.8280 (4.7508)	LR 5.000e-04
5: TRAIN [1][250/461]	Time 0.699 (0.716)	Data 6.18e-05 (7.50e-05)	Tok/s 1622 (2624)	Loss/tok 3.6948 (4.7321)	LR 5.000e-04
2: TRAIN [1][250/461]	Time 0.699 (0.716)	Data 7.01e-05 (8.01e-05)	Tok/s 1546 (2622)	Loss/tok 3.7339 (4.7431)	LR 5.000e-04
1: TRAIN [1][250/461]	Time 0.701 (0.716)	Data 6.96e-05 (7.72e-05)	Tok/s 1553 (2620)	Loss/tok 4.1429 (4.7461)	LR 5.000e-04
6: TRAIN [1][250/461]	Time 0.701 (0.716)	Data 7.82e-05 (8.17e-05)	Tok/s 1606 (2623)	Loss/tok 4.1900 (4.7398)	LR 5.000e-04
0: TRAIN [1][250/461]	Time 0.701 (0.716)	Data 9.20e-05 (1.04e-04)	Tok/s 1589 (2616)	Loss/tok 4.1744 (4.7303)	LR 5.000e-04
7: TRAIN [1][250/461]	Time 0.701 (0.716)	Data 9.82e-05 (1.02e-04)	Tok/s 1495 (2624)	Loss/tok 3.7777 (4.7496)	LR 5.000e-04
0: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 8.58e-05 (1.04e-04)	Tok/s 2489 (2630)	Loss/tok 4.1864 (4.7123)	LR 5.000e-04
7: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 1.01e-04 (1.02e-04)	Tok/s 2576 (2638)	Loss/tok 4.1355 (4.7313)	LR 5.000e-04
1: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 7.39e-05 (7.72e-05)	Tok/s 2502 (2634)	Loss/tok 4.0043 (4.7277)	LR 5.000e-04
6: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 8.34e-05 (8.17e-05)	Tok/s 2574 (2638)	Loss/tok 4.4273 (4.7216)	LR 5.000e-04
5: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 6.32e-05 (7.47e-05)	Tok/s 2495 (2638)	Loss/tok 4.1420 (4.7144)	LR 5.000e-04
2: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 6.70e-05 (8.00e-05)	Tok/s 2513 (2636)	Loss/tok 4.1388 (4.7243)	LR 5.000e-04
4: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 7.41e-05 (7.80e-05)	Tok/s 2639 (2634)	Loss/tok 4.2430 (4.7334)	LR 5.000e-04
3: TRAIN [1][260/461]	Time 0.697 (0.716)	Data 6.84e-05 (9.17e-05)	Tok/s 2606 (2638)	Loss/tok 3.9853 (4.7177)	LR 5.000e-04
2: TRAIN [1][270/461]	Time 0.691 (0.717)	Data 6.63e-05 (7.99e-05)	Tok/s 1547 (2651)	Loss/tok 3.8160 (4.7091)	LR 5.000e-04
5: TRAIN [1][270/461]	Time 0.691 (0.717)	Data 6.39e-05 (7.44e-05)	Tok/s 1584 (2652)	Loss/tok 3.5271 (4.6974)	LR 5.000e-04
1: TRAIN [1][270/461]	Time 0.691 (0.717)	Data 7.70e-05 (7.72e-05)	Tok/s 1558 (2648)	Loss/tok 3.9202 (4.7120)	LR 5.000e-04
3: TRAIN [1][270/461]	Time 0.693 (0.717)	Data 7.39e-05 (9.12e-05)	Tok/s 1597 (2652)	Loss/tok 4.2647 (4.7039)	LR 5.000e-04
6: TRAIN [1][270/461]	Time 0.691 (0.717)	Data 8.89e-05 (8.18e-05)	Tok/s 1491 (2651)	Loss/tok 3.9528 (4.7094)	LR 5.000e-04
4: TRAIN [1][270/461]	Time 0.693 (0.717)	Data 5.89e-05 (7.79e-05)	Tok/s 1479 (2647)	Loss/tok 3.8975 (4.7152)	LR 5.000e-04
0: TRAIN [1][270/461]	Time 0.690 (0.717)	Data 8.15e-05 (1.03e-04)	Tok/s 1579 (2644)	Loss/tok 4.0606 (4.7002)	LR 5.000e-04
7: TRAIN [1][270/461]	Time 0.690 (0.717)	Data 9.08e-05 (1.02e-04)	Tok/s 1716 (2652)	Loss/tok 3.8561 (4.7160)	LR 5.000e-04
2: TRAIN [1][280/461]	Time 0.689 (0.717)	Data 7.39e-05 (7.99e-05)	Tok/s 2663 (2656)	Loss/tok 4.2205 (4.6949)	LR 5.000e-04
5: TRAIN [1][280/461]	Time 0.689 (0.717)	Data 6.22e-05 (7.44e-05)	Tok/s 2616 (2656)	Loss/tok 4.0119 (4.6811)	LR 5.000e-04
1: TRAIN [1][280/461]	Time 0.688 (0.717)	Data 7.41e-05 (7.72e-05)	Tok/s 2638 (2652)	Loss/tok 4.1458 (4.6953)	LR 5.000e-04
3: TRAIN [1][280/461]	Time 0.691 (0.717)	Data 7.49e-05 (9.08e-05)	Tok/s 2607 (2657)	Loss/tok 4.1110 (4.6883)	LR 5.000e-04
6: TRAIN [1][280/461]	Time 0.689 (0.717)	Data 8.89e-05 (8.19e-05)	Tok/s 2611 (2655)	Loss/tok 3.9276 (4.6939)	LR 5.000e-04
4: TRAIN [1][280/461]	Time 0.691 (0.717)	Data 6.41e-05 (7.80e-05)	Tok/s 2600 (2653)	Loss/tok 4.3194 (4.7000)	LR 5.000e-04
7: TRAIN [1][280/461]	Time 0.689 (0.717)	Data 1.06e-04 (1.02e-04)	Tok/s 2646 (2657)	Loss/tok 4.4705 (4.7013)	LR 5.000e-04
0: TRAIN [1][280/461]	Time 0.689 (0.717)	Data 8.51e-05 (1.03e-04)	Tok/s 2631 (2649)	Loss/tok 4.0927 (4.6824)	LR 5.000e-04
0: TRAIN [1][290/461]	Time 0.687 (0.717)	Data 8.37e-05 (1.03e-04)	Tok/s 1600 (2656)	Loss/tok 3.9700 (4.6692)	LR 5.000e-04
7: TRAIN [1][290/461]	Time 0.687 (0.717)	Data 1.02e-04 (1.02e-04)	Tok/s 1464 (2663)	Loss/tok 3.6280 (4.6899)	LR 5.000e-04
1: TRAIN [1][290/461]	Time 0.690 (0.717)	Data 7.18e-05 (7.73e-05)	Tok/s 1638 (2659)	Loss/tok 3.8327 (4.6838)	LR 5.000e-04
6: TRAIN [1][290/461]	Time 0.690 (0.717)	Data 8.75e-05 (8.21e-05)	Tok/s 1553 (2662)	Loss/tok 3.9125 (4.6826)	LR 5.000e-04
5: TRAIN [1][290/461]	Time 0.693 (0.717)	Data 6.70e-05 (7.44e-05)	Tok/s 1461 (2662)	Loss/tok 3.8677 (4.6650)	LR 5.000e-04
2: TRAIN [1][290/461]	Time 0.694 (0.717)	Data 6.91e-05 (7.98e-05)	Tok/s 1628 (2663)	Loss/tok 3.7883 (4.6784)	LR 5.000e-04
3: TRAIN [1][290/461]	Time 0.695 (0.717)	Data 6.46e-05 (9.04e-05)	Tok/s 1542 (2664)	Loss/tok 3.5637 (4.6736)	LR 5.000e-04
4: TRAIN [1][290/461]	Time 0.695 (0.717)	Data 1.02e-04 (7.83e-05)	Tok/s 1569 (2659)	Loss/tok 3.6991 (4.6837)	LR 5.000e-04
5: TRAIN [1][300/461]	Time 0.737 (0.717)	Data 1.04e-04 (7.46e-05)	Tok/s 3477 (2665)	Loss/tok 4.3913 (4.6525)	LR 5.000e-04
2: TRAIN [1][300/461]	Time 0.737 (0.717)	Data 9.51e-05 (7.98e-05)	Tok/s 3407 (2665)	Loss/tok 4.2811 (4.6638)	LR 5.000e-04
1: TRAIN [1][300/461]	Time 0.737 (0.717)	Data 7.34e-05 (7.73e-05)	Tok/s 3355 (2662)	Loss/tok 4.3208 (4.6689)	LR 5.000e-04
6: TRAIN [1][300/461]	Time 0.737 (0.717)	Data 9.89e-05 (8.21e-05)	Tok/s 3406 (2664)	Loss/tok 4.2613 (4.6685)	LR 5.000e-04
3: TRAIN [1][300/461]	Time 0.740 (0.717)	Data 1.01e-04 (9.01e-05)	Tok/s 3434 (2667)	Loss/tok 4.2323 (4.6565)	LR 5.000e-04
4: TRAIN [1][300/461]	Time 0.740 (0.717)	Data 7.10e-05 (7.83e-05)	Tok/s 3388 (2661)	Loss/tok 4.2998 (4.6699)	LR 5.000e-04
0: TRAIN [1][300/461]	Time 0.736 (0.717)	Data 9.44e-05 (1.02e-04)	Tok/s 3466 (2659)	Loss/tok 3.9388 (4.6541)	LR 5.000e-04
7: TRAIN [1][300/461]	Time 0.736 (0.717)	Data 1.10e-04 (1.02e-04)	Tok/s 3358 (2664)	Loss/tok 4.3431 (4.6755)	LR 5.000e-04
7: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 9.75e-05 (1.02e-04)	Tok/s 2565 (2665)	Loss/tok 3.8151 (4.6602)	LR 2.500e-04
0: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 1.06e-04 (1.02e-04)	Tok/s 2530 (2660)	Loss/tok 4.2299 (4.6414)	LR 2.500e-04
1: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 7.41e-05 (7.74e-05)	Tok/s 2552 (2664)	Loss/tok 4.1916 (4.6545)	LR 2.500e-04
6: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 1.01e-04 (8.21e-05)	Tok/s 2526 (2666)	Loss/tok 3.9847 (4.6517)	LR 2.500e-04
5: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 6.29e-05 (7.45e-05)	Tok/s 2453 (2666)	Loss/tok 4.1702 (4.6388)	LR 2.500e-04
2: TRAIN [1][310/461]	Time 0.714 (0.717)	Data 6.91e-05 (7.96e-05)	Tok/s 2523 (2665)	Loss/tok 3.9886 (4.6473)	LR 2.500e-04
3: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 6.84e-05 (8.96e-05)	Tok/s 2549 (2667)	Loss/tok 4.3363 (4.6421)	LR 2.500e-04
4: TRAIN [1][310/461]	Time 0.713 (0.717)	Data 7.03e-05 (7.83e-05)	Tok/s 2567 (2662)	Loss/tok 3.9278 (4.6533)	LR 2.500e-04
3: TRAIN [1][320/461]	Time 0.669 (0.717)	Data 7.94e-05 (8.92e-05)	Tok/s 826 (2666)	Loss/tok 3.7393 (4.6315)	LR 2.500e-04
4: TRAIN [1][320/461]	Time 0.669 (0.717)	Data 6.77e-05 (7.82e-05)	Tok/s 827 (2661)	Loss/tok 3.2483 (4.6374)	LR 2.500e-04
2: TRAIN [1][320/461]	Time 0.671 (0.717)	Data 6.96e-05 (7.97e-05)	Tok/s 785 (2664)	Loss/tok 3.2115 (4.6331)	LR 2.500e-04
5: TRAIN [1][320/461]	Time 0.671 (0.717)	Data 6.70e-05 (7.45e-05)	Tok/s 811 (2665)	Loss/tok 3.4376 (4.6242)	LR 2.500e-04
6: TRAIN [1][320/461]	Time 0.675 (0.717)	Data 7.99e-05 (8.19e-05)	Tok/s 815 (2664)	Loss/tok 3.8079 (4.6369)	LR 2.500e-04
1: TRAIN [1][320/461]	Time 0.675 (0.717)	Data 7.61e-05 (7.74e-05)	Tok/s 861 (2662)	Loss/tok 3.7760 (4.6381)	LR 2.500e-04
0: TRAIN [1][320/461]	Time 0.678 (0.717)	Data 8.42e-05 (1.02e-04)	Tok/s 772 (2659)	Loss/tok 3.7145 (4.6230)	LR 2.500e-04
7: TRAIN [1][320/461]	Time 0.678 (0.717)	Data 1.34e-04 (1.02e-04)	Tok/s 826 (2665)	Loss/tok 3.9799 (4.6451)	LR 2.500e-04
3: TRAIN [1][330/461]	Time 0.696 (0.717)	Data 7.32e-05 (8.90e-05)	Tok/s 1614 (2670)	Loss/tok 3.8121 (4.6184)	LR 2.500e-04
4: TRAIN [1][330/461]	Time 0.696 (0.717)	Data 6.41e-05 (7.80e-05)	Tok/s 1607 (2665)	Loss/tok 3.8158 (4.6244)	LR 2.500e-04
5: TRAIN [1][330/461]	Time 0.700 (0.717)	Data 6.51e-05 (7.43e-05)	Tok/s 1561 (2670)	Loss/tok 3.8802 (4.6105)	LR 2.500e-04
2: TRAIN [1][330/461]	Time 0.700 (0.717)	Data 7.72e-05 (7.97e-05)	Tok/s 1541 (2669)	Loss/tok 3.7363 (4.6203)	LR 2.500e-04
1: TRAIN [1][330/461]	Time 0.703 (0.717)	Data 7.32e-05 (7.75e-05)	Tok/s 1506 (2667)	Loss/tok 4.0653 (4.6242)	LR 2.500e-04
6: TRAIN [1][330/461]	Time 0.703 (0.717)	Data 8.89e-05 (8.19e-05)	Tok/s 1554 (2669)	Loss/tok 3.9404 (4.6231)	LR 2.500e-04
7: TRAIN [1][330/461]	Time 0.707 (0.717)	Data 9.94e-05 (1.02e-04)	Tok/s 1537 (2669)	Loss/tok 3.7792 (4.6314)	LR 2.500e-04
0: TRAIN [1][330/461]	Time 0.707 (0.717)	Data 7.99e-05 (1.01e-04)	Tok/s 1602 (2663)	Loss/tok 3.7863 (4.6071)	LR 2.500e-04
0: TRAIN [1][340/461]	Time 0.766 (0.717)	Data 8.65e-05 (1.01e-04)	Tok/s 4423 (2667)	Loss/tok 4.4261 (4.5971)	LR 2.500e-04
7: TRAIN [1][340/461]	Time 0.766 (0.717)	Data 9.51e-05 (1.02e-04)	Tok/s 4246 (2672)	Loss/tok 4.5472 (4.6207)	LR 2.500e-04
1: TRAIN [1][340/461]	Time 0.770 (0.717)	Data 6.51e-05 (7.74e-05)	Tok/s 4235 (2671)	Loss/tok 4.4885 (4.6093)	LR 2.500e-04
6: TRAIN [1][340/461]	Time 0.769 (0.717)	Data 7.61e-05 (8.20e-05)	Tok/s 4216 (2673)	Loss/tok 4.4221 (4.6111)	LR 2.500e-04
5: TRAIN [1][340/461]	Time 0.773 (0.717)	Data 6.46e-05 (7.43e-05)	Tok/s 4303 (2673)	Loss/tok 4.5227 (4.5983)	LR 2.500e-04
2: TRAIN [1][340/461]	Time 0.773 (0.717)	Data 7.30e-05 (7.98e-05)	Tok/s 4188 (2672)	Loss/tok 4.5229 (4.6094)	LR 2.500e-04
3: TRAIN [1][340/461]	Time 0.776 (0.717)	Data 7.15e-05 (8.87e-05)	Tok/s 4156 (2674)	Loss/tok 4.6651 (4.6060)	LR 2.500e-04
4: TRAIN [1][340/461]	Time 0.776 (0.717)	Data 1.09e-04 (7.81e-05)	Tok/s 4290 (2669)	Loss/tok 4.4043 (4.6125)	LR 2.500e-04
1: TRAIN [1][350/461]	Time 0.695 (0.717)	Data 7.63e-05 (7.76e-05)	Tok/s 1575 (2667)	Loss/tok 3.5197 (4.5970)	LR 2.500e-04
6: TRAIN [1][350/461]	Time 0.695 (0.717)	Data 8.85e-05 (8.21e-05)	Tok/s 1591 (2668)	Loss/tok 4.0974 (4.5987)	LR 2.500e-04
5: TRAIN [1][350/461]	Time 0.695 (0.717)	Data 6.96e-05 (7.44e-05)	Tok/s 1629 (2669)	Loss/tok 3.6798 (4.5871)	LR 2.500e-04
0: TRAIN [1][350/461]	Time 0.698 (0.717)	Data 7.94e-05 (1.01e-04)	Tok/s 1532 (2663)	Loss/tok 3.7316 (4.5839)	LR 2.500e-04
2: TRAIN [1][350/461]	Time 0.695 (0.717)	Data 7.10e-05 (7.98e-05)	Tok/s 1552 (2667)	Loss/tok 3.6186 (4.5977)	LR 2.500e-04
7: TRAIN [1][350/461]	Time 0.698 (0.717)	Data 9.63e-05 (1.02e-04)	Tok/s 1512 (2667)	Loss/tok 3.7862 (4.6093)	LR 2.500e-04
4: TRAIN [1][350/461]	Time 0.695 (0.717)	Data 6.44e-05 (7.80e-05)	Tok/s 1590 (2665)	Loss/tok 3.8015 (4.5979)	LR 2.500e-04
3: TRAIN [1][350/461]	Time 0.695 (0.717)	Data 7.41e-05 (8.84e-05)	Tok/s 1520 (2669)	Loss/tok 4.0592 (4.5943)	LR 2.500e-04
0: TRAIN [1][360/461]	Time 0.697 (0.716)	Data 8.01e-05 (1.01e-04)	Tok/s 2594 (2661)	Loss/tok 4.1144 (4.5694)	LR 2.500e-04
7: TRAIN [1][360/461]	Time 0.697 (0.716)	Data 9.44e-05 (1.02e-04)	Tok/s 2544 (2664)	Loss/tok 3.9242 (4.5959)	LR 2.500e-04
1: TRAIN [1][360/461]	Time 0.699 (0.716)	Data 7.68e-05 (7.76e-05)	Tok/s 2585 (2665)	Loss/tok 3.9736 (4.5832)	LR 2.500e-04
6: TRAIN [1][360/461]	Time 0.699 (0.716)	Data 8.32e-05 (8.21e-05)	Tok/s 2626 (2665)	Loss/tok 4.1499 (4.5856)	LR 2.500e-04
5: TRAIN [1][360/461]	Time 0.701 (0.716)	Data 6.89e-05 (7.44e-05)	Tok/s 2555 (2666)	Loss/tok 4.0909 (4.5744)	LR 2.500e-04
2: TRAIN [1][360/461]	Time 0.701 (0.716)	Data 7.13e-05 (7.99e-05)	Tok/s 2549 (2664)	Loss/tok 3.8432 (4.5845)	LR 2.500e-04
3: TRAIN [1][360/461]	Time 0.702 (0.716)	Data 6.72e-05 (8.80e-05)	Tok/s 2581 (2665)	Loss/tok 4.0053 (4.5812)	LR 2.500e-04
4: TRAIN [1][360/461]	Time 0.702 (0.716)	Data 6.34e-05 (7.81e-05)	Tok/s 2550 (2662)	Loss/tok 4.2308 (4.5842)	LR 2.500e-04
0: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 8.51e-05 (1.01e-04)	Tok/s 1493 (2667)	Loss/tok 3.8301 (4.5583)	LR 2.500e-04
7: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 9.87e-05 (1.02e-04)	Tok/s 1559 (2670)	Loss/tok 3.8491 (4.5851)	LR 2.500e-04
1: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 7.68e-05 (7.76e-05)	Tok/s 1630 (2670)	Loss/tok 3.8378 (4.5726)	LR 2.500e-04
6: TRAIN [1][370/461]	Time 0.691 (0.716)	Data 7.51e-05 (8.20e-05)	Tok/s 1556 (2671)	Loss/tok 3.7983 (4.5754)	LR 2.500e-04
5: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 6.15e-05 (7.43e-05)	Tok/s 1560 (2671)	Loss/tok 3.7188 (4.5645)	LR 2.500e-04
2: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 1.00e-04 (8.00e-05)	Tok/s 1541 (2670)	Loss/tok 3.4813 (4.5732)	LR 2.500e-04
4: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 6.70e-05 (7.82e-05)	Tok/s 1645 (2668)	Loss/tok 4.1509 (4.5741)	LR 2.500e-04
3: TRAIN [1][370/461]	Time 0.692 (0.716)	Data 6.58e-05 (8.76e-05)	Tok/s 1525 (2671)	Loss/tok 3.9959 (4.5730)	LR 2.500e-04
5: TRAIN [1][380/461]	Time 0.744 (0.716)	Data 7.13e-05 (7.44e-05)	Tok/s 3333 (2669)	Loss/tok 4.2633 (4.5548)	LR 2.500e-04
1: TRAIN [1][380/461]	Time 0.743 (0.716)	Data 8.44e-05 (7.77e-05)	Tok/s 3401 (2669)	Loss/tok 4.3049 (4.5629)	LR 2.500e-04
2: TRAIN [1][380/461]	Time 0.745 (0.716)	Data 7.06e-05 (8.00e-05)	Tok/s 3286 (2669)	Loss/tok 4.2604 (4.5631)	LR 2.500e-04
6: TRAIN [1][380/461]	Time 0.743 (0.716)	Data 8.32e-05 (8.21e-05)	Tok/s 3355 (2669)	Loss/tok 4.3521 (4.5660)	LR 2.500e-04
0: TRAIN [1][380/461]	Time 0.743 (0.716)	Data 8.11e-05 (1.01e-04)	Tok/s 3371 (2666)	Loss/tok 4.3163 (4.5495)	LR 2.500e-04
3: TRAIN [1][380/461]	Time 0.745 (0.716)	Data 7.27e-05 (8.75e-05)	Tok/s 3316 (2670)	Loss/tok 4.2022 (4.5622)	LR 2.500e-04
7: TRAIN [1][380/461]	Time 0.743 (0.716)	Data 1.01e-04 (1.02e-04)	Tok/s 3394 (2668)	Loss/tok 4.3717 (4.5736)	LR 2.500e-04
4: TRAIN [1][380/461]	Time 0.745 (0.716)	Data 6.56e-05 (7.80e-05)	Tok/s 3273 (2667)	Loss/tok 4.3352 (4.5644)	LR 2.500e-04
0: TRAIN [1][390/461]	Time 0.694 (0.716)	Data 9.37e-05 (1.01e-04)	Tok/s 1450 (2673)	Loss/tok 3.8566 (4.5409)	LR 1.250e-04
7: TRAIN [1][390/461]	Time 0.694 (0.716)	Data 1.03e-04 (1.02e-04)	Tok/s 1489 (2676)	Loss/tok 3.8266 (4.5625)	LR 1.250e-04
1: TRAIN [1][390/461]	Time 0.696 (0.716)	Data 7.68e-05 (7.77e-05)	Tok/s 1564 (2677)	Loss/tok 3.8358 (4.5545)	LR 1.250e-04
6: TRAIN [1][390/461]	Time 0.696 (0.716)	Data 8.11e-05 (8.20e-05)	Tok/s 1563 (2678)	Loss/tok 3.9028 (4.5537)	LR 1.250e-04
2: TRAIN [1][390/461]	Time 0.697 (0.716)	Data 7.61e-05 (7.99e-05)	Tok/s 1618 (2677)	Loss/tok 3.6421 (4.5506)	LR 1.250e-04
5: TRAIN [1][390/461]	Time 0.698 (0.716)	Data 7.34e-05 (7.44e-05)	Tok/s 1492 (2677)	Loss/tok 3.8540 (4.5465)	LR 1.250e-04
3: TRAIN [1][390/461]	Time 0.698 (0.716)	Data 7.22e-05 (8.72e-05)	Tok/s 1468 (2677)	Loss/tok 3.7638 (4.5521)	LR 1.250e-04
4: TRAIN [1][390/461]	Time 0.697 (0.716)	Data 1.08e-04 (7.79e-05)	Tok/s 1537 (2675)	Loss/tok 3.9214 (4.5541)	LR 1.250e-04
7: TRAIN [1][400/461]	Time 0.738 (0.717)	Data 9.94e-05 (1.02e-04)	Tok/s 3394 (2690)	Loss/tok 4.3257 (4.5529)	LR 1.250e-04
0: TRAIN [1][400/461]	Time 0.738 (0.717)	Data 8.30e-05 (1.01e-04)	Tok/s 3425 (2688)	Loss/tok 4.2339 (4.5318)	LR 1.250e-04
1: TRAIN [1][400/461]	Time 0.741 (0.717)	Data 7.18e-05 (7.79e-05)	Tok/s 3419 (2691)	Loss/tok 4.0950 (4.5452)	LR 1.250e-04
6: TRAIN [1][400/461]	Time 0.741 (0.717)	Data 7.92e-05 (8.19e-05)	Tok/s 3338 (2692)	Loss/tok 4.3677 (4.5455)	LR 1.250e-04
5: TRAIN [1][400/461]	Time 0.745 (0.717)	Data 6.77e-05 (7.45e-05)	Tok/s 3371 (2692)	Loss/tok 4.1699 (4.5377)	LR 1.250e-04
2: TRAIN [1][400/461]	Time 0.745 (0.717)	Data 7.53e-05 (8.00e-05)	Tok/s 3425 (2691)	Loss/tok 4.1928 (4.5397)	LR 1.250e-04
3: TRAIN [1][400/461]	Time 0.748 (0.717)	Data 7.18e-05 (8.69e-05)	Tok/s 3398 (2692)	Loss/tok 4.0880 (4.5439)	LR 1.250e-04
4: TRAIN [1][400/461]	Time 0.748 (0.717)	Data 7.56e-05 (7.78e-05)	Tok/s 3361 (2689)	Loss/tok 4.3532 (4.5457)	LR 1.250e-04
3: TRAIN [1][410/461]	Time 0.689 (0.716)	Data 6.60e-05 (8.67e-05)	Tok/s 2560 (2691)	Loss/tok 4.1269 (4.5352)	LR 1.250e-04
4: TRAIN [1][410/461]	Time 0.689 (0.716)	Data 6.15e-05 (7.77e-05)	Tok/s 2734 (2688)	Loss/tok 4.0045 (4.5343)	LR 1.250e-04
5: TRAIN [1][410/461]	Time 0.691 (0.716)	Data 6.82e-05 (7.45e-05)	Tok/s 2611 (2691)	Loss/tok 4.0576 (4.5274)	LR 1.250e-04
2: TRAIN [1][410/461]	Time 0.691 (0.716)	Data 6.87e-05 (7.99e-05)	Tok/s 2527 (2688)	Loss/tok 3.9640 (4.5277)	LR 1.250e-04
1: TRAIN [1][410/461]	Time 0.691 (0.716)	Data 7.32e-05 (7.80e-05)	Tok/s 2538 (2689)	Loss/tok 3.9456 (4.5353)	LR 1.250e-04
6: TRAIN [1][410/461]	Time 0.691 (0.716)	Data 8.87e-05 (8.18e-05)	Tok/s 2600 (2691)	Loss/tok 4.1180 (4.5356)	LR 1.250e-04
0: TRAIN [1][410/461]	Time 0.690 (0.716)	Data 8.54e-05 (1.00e-04)	Tok/s 2671 (2687)	Loss/tok 4.0554 (4.5198)	LR 1.250e-04
7: TRAIN [1][410/461]	Time 0.691 (0.716)	Data 1.04e-04 (1.02e-04)	Tok/s 2618 (2689)	Loss/tok 4.0041 (4.5412)	LR 1.250e-04
0: TRAIN [1][420/461]	Time 0.728 (0.716)	Data 1.15e-04 (1.01e-04)	Tok/s 3443 (2679)	Loss/tok 4.4277 (4.5110)	LR 1.250e-04
7: TRAIN [1][420/461]	Time 0.728 (0.716)	Data 1.28e-04 (1.02e-04)	Tok/s 3378 (2680)	Loss/tok 4.1295 (4.5307)	LR 1.250e-04
1: TRAIN [1][420/461]	Time 0.732 (0.716)	Data 8.49e-05 (7.80e-05)	Tok/s 3417 (2682)	Loss/tok 4.1114 (4.5250)	LR 1.250e-04
6: TRAIN [1][420/461]	Time 0.732 (0.716)	Data 9.39e-05 (8.19e-05)	Tok/s 3406 (2683)	Loss/tok 4.4573 (4.5257)	LR 1.250e-04
5: TRAIN [1][420/461]	Time 0.736 (0.716)	Data 6.63e-05 (7.45e-05)	Tok/s 3452 (2683)	Loss/tok 4.2610 (4.5168)	LR 1.250e-04
2: TRAIN [1][420/461]	Time 0.736 (0.716)	Data 6.94e-05 (7.99e-05)	Tok/s 3393 (2680)	Loss/tok 4.2682 (4.5185)	LR 1.250e-04
3: TRAIN [1][420/461]	Time 0.736 (0.716)	Data 6.60e-05 (8.65e-05)	Tok/s 3411 (2683)	Loss/tok 4.2793 (4.5249)	LR 1.250e-04
4: TRAIN [1][420/461]	Time 0.736 (0.716)	Data 6.94e-05 (7.77e-05)	Tok/s 3496 (2680)	Loss/tok 4.2154 (4.5255)	LR 1.250e-04
3: TRAIN [1][430/461]	Time 0.708 (0.716)	Data 6.96e-05 (8.64e-05)	Tok/s 1564 (2682)	Loss/tok 3.6099 (4.5147)	LR 1.250e-04
4: TRAIN [1][430/461]	Time 0.708 (0.716)	Data 6.72e-05 (7.77e-05)	Tok/s 1600 (2679)	Loss/tok 3.7851 (4.5167)	LR 1.250e-04
2: TRAIN [1][430/461]	Time 0.711 (0.716)	Data 8.39e-05 (8.00e-05)	Tok/s 1511 (2679)	Loss/tok 3.7188 (4.5094)	LR 1.250e-04
5: TRAIN [1][430/461]	Time 0.711 (0.716)	Data 7.68e-05 (7.44e-05)	Tok/s 1558 (2682)	Loss/tok 3.6932 (4.5075)	LR 1.250e-04
1: TRAIN [1][430/461]	Time 0.715 (0.716)	Data 7.89e-05 (7.80e-05)	Tok/s 1502 (2680)	Loss/tok 3.6734 (4.5139)	LR 1.250e-04
6: TRAIN [1][430/461]	Time 0.714 (0.716)	Data 8.01e-05 (8.19e-05)	Tok/s 1466 (2681)	Loss/tok 3.7751 (4.5160)	LR 1.250e-04
0: TRAIN [1][430/461]	Time 0.715 (0.716)	Data 8.87e-05 (1.01e-04)	Tok/s 1553 (2678)	Loss/tok 3.8030 (4.5039)	LR 1.250e-04
7: TRAIN [1][430/461]	Time 0.714 (0.716)	Data 1.28e-04 (1.02e-04)	Tok/s 1529 (2680)	Loss/tok 3.6277 (4.5207)	LR 1.250e-04
0: TRAIN [1][440/461]	Time 0.694 (0.716)	Data 1.21e-04 (1.01e-04)	Tok/s 2680 (2679)	Loss/tok 3.8543 (4.4945)	LR 1.250e-04
7: TRAIN [1][440/461]	Time 0.694 (0.716)	Data 1.14e-04 (1.02e-04)	Tok/s 2573 (2680)	Loss/tok 4.3526 (4.5115)	LR 1.250e-04
1: TRAIN [1][440/461]	Time 0.697 (0.716)	Data 8.01e-05 (7.81e-05)	Tok/s 2618 (2681)	Loss/tok 3.8962 (4.5036)	LR 1.250e-04
6: TRAIN [1][440/461]	Time 0.697 (0.716)	Data 8.80e-05 (8.18e-05)	Tok/s 2589 (2682)	Loss/tok 4.1434 (4.5070)	LR 1.250e-04
5: TRAIN [1][440/461]	Time 0.701 (0.716)	Data 7.41e-05 (7.43e-05)	Tok/s 2671 (2683)	Loss/tok 3.9980 (4.4971)	LR 1.250e-04
2: TRAIN [1][440/461]	Time 0.701 (0.716)	Data 7.70e-05 (8.00e-05)	Tok/s 2561 (2679)	Loss/tok 3.9841 (4.4992)	LR 1.250e-04
3: TRAIN [1][440/461]	Time 0.703 (0.716)	Data 8.25e-05 (8.63e-05)	Tok/s 2611 (2683)	Loss/tok 4.0502 (4.5064)	LR 1.250e-04
4: TRAIN [1][440/461]	Time 0.703 (0.716)	Data 1.02e-04 (7.77e-05)	Tok/s 2511 (2679)	Loss/tok 3.9467 (4.5076)	LR 1.250e-04
1: TRAIN [1][450/461]	Time 0.681 (0.716)	Data 7.94e-05 (7.82e-05)	Tok/s 1680 (2683)	Loss/tok 3.8936 (4.4973)	LR 1.250e-04
6: TRAIN [1][450/461]	Time 0.681 (0.716)	Data 9.30e-05 (8.18e-05)	Tok/s 1691 (2684)	Loss/tok 3.5820 (4.4981)	LR 1.250e-04
7: TRAIN [1][450/461]	Time 0.680 (0.716)	Data 1.14e-04 (1.02e-04)	Tok/s 1423 (2682)	Loss/tok 3.8023 (4.5041)	LR 1.250e-04
0: TRAIN [1][450/461]	Time 0.681 (0.716)	Data 9.20e-05 (1.01e-04)	Tok/s 1623 (2682)	Loss/tok 3.8824 (4.4864)	LR 1.250e-04
5: TRAIN [1][450/461]	Time 0.684 (0.716)	Data 7.25e-05 (7.43e-05)	Tok/s 1591 (2684)	Loss/tok 3.5883 (4.4891)	LR 1.250e-04
2: TRAIN [1][450/461]	Time 0.685 (0.716)	Data 8.18e-05 (8.00e-05)	Tok/s 1521 (2681)	Loss/tok 3.9905 (4.4915)	LR 1.250e-04
3: TRAIN [1][450/461]	Time 0.688 (0.716)	Data 1.13e-04 (8.61e-05)	Tok/s 1529 (2685)	Loss/tok 3.6036 (4.4970)	LR 1.250e-04
4: TRAIN [1][450/461]	Time 0.689 (0.716)	Data 7.27e-05 (7.75e-05)	Tok/s 1509 (2681)	Loss/tok 3.4537 (4.5000)	LR 1.250e-04
1: TRAIN [1][460/461]	Time 0.745 (0.716)	Data 4.63e-05 (7.87e-05)	Tok/s 3394 (2676)	Loss/tok 4.2899 (4.4890)	LR 1.250e-04
6: TRAIN [1][460/461]	Time 0.745 (0.716)	Data 4.05e-05 (8.22e-05)	Tok/s 3350 (2677)	Loss/tok 4.3636 (4.4903)	LR 1.250e-04
0: TRAIN [1][460/461]	Time 0.744 (0.716)	Data 7.22e-05 (1.01e-04)	Tok/s 3436 (2675)	Loss/tok 4.1313 (4.4785)	LR 1.250e-04
7: TRAIN [1][460/461]	Time 0.744 (0.716)	Data 6.37e-05 (1.02e-04)	Tok/s 3353 (2675)	Loss/tok 4.4575 (4.4958)	LR 1.250e-04
5: TRAIN [1][460/461]	Time 0.749 (0.716)	Data 4.03e-05 (7.47e-05)	Tok/s 3377 (2678)	Loss/tok 4.2621 (4.4791)	LR 1.250e-04
2: TRAIN [1][460/461]	Time 0.749 (0.716)	Data 3.81e-05 (8.04e-05)	Tok/s 3329 (2674)	Loss/tok 4.2935 (4.4830)	LR 1.250e-04
4: TRAIN [1][460/461]	Time 0.752 (0.716)	Data 4.20e-05 (7.79e-05)	Tok/s 3228 (2674)	Loss/tok 4.2510 (4.4913)	LR 1.250e-04
3: TRAIN [1][460/461]	Time 0.753 (0.716)	Data 4.55e-05 (8.64e-05)	Tok/s 3327 (2678)	Loss/tok 4.1628 (4.4890)	LR 1.250e-04
5: Running validation on dev set
2: Running validation on dev set
4: Running validation on dev set
5: Executing preallocation
1: Running validation on dev set
4: Executing preallocation
2: Executing preallocation
0: Running validation on dev set
3: Running validation on dev set
7: Running validation on dev set
1: Executing preallocation
3: Executing preallocation
0: Executing preallocation
7: Executing preallocation
6: Running validation on dev set
6: Executing preallocation
5: VALIDATION [1][0/80]	Time 0.054 (0.000)	Data 6.74e-04 (0.00e+00)	Tok/s 22167 (0)	Loss/tok 5.8010 (5.8010)
7: VALIDATION [1][0/79]	Time 0.051 (0.000)	Data 7.57e-04 (0.00e+00)	Tok/s 22579 (0)	Loss/tok 5.3203 (5.3203)
4: VALIDATION [1][0/80]	Time 0.055 (0.000)	Data 6.87e-04 (0.00e+00)	Tok/s 22250 (0)	Loss/tok 5.8037 (5.8037)
6: VALIDATION [1][0/79]	Time 0.052 (0.000)	Data 7.05e-04 (0.00e+00)	Tok/s 22351 (0)	Loss/tok 5.6998 (5.6998)
2: VALIDATION [1][0/80]	Time 0.058 (0.000)	Data 6.81e-04 (0.00e+00)	Tok/s 23036 (0)	Loss/tok 5.4451 (5.4451)
3: VALIDATION [1][0/80]	Time 0.055 (0.000)	Data 6.65e-04 (0.00e+00)	Tok/s 22970 (0)	Loss/tok 5.6718 (5.6718)
1: VALIDATION [1][0/80]	Time 0.065 (0.000)	Data 6.85e-04 (0.00e+00)	Tok/s 22665 (0)	Loss/tok 6.2116 (6.2116)
0: VALIDATION [1][0/80]	Time 0.077 (0.000)	Data 6.76e-04 (0.00e+00)	Tok/s 21265 (0)	Loss/tok 5.9062 (5.9062)
5: VALIDATION [1][10/80]	Time 0.034 (0.038)	Data 5.11e-04 (5.23e-04)	Tok/s 21519 (22404)	Loss/tok 5.0658 (5.4841)
7: VALIDATION [1][10/79]	Time 0.033 (0.038)	Data 5.44e-04 (5.84e-04)	Tok/s 21689 (21942)	Loss/tok 5.1064 (5.5268)
6: VALIDATION [1][10/79]	Time 0.032 (0.038)	Data 5.12e-04 (5.35e-04)	Tok/s 22351 (22052)	Loss/tok 5.4304 (5.5002)
4: VALIDATION [1][10/80]	Time 0.034 (0.039)	Data 5.29e-04 (5.64e-04)	Tok/s 21301 (21995)	Loss/tok 5.2018 (5.4339)
2: VALIDATION [1][10/80]	Time 0.033 (0.039)	Data 5.21e-04 (5.46e-04)	Tok/s 22180 (22318)	Loss/tok 5.6129 (5.5618)
3: VALIDATION [1][10/80]	Time 0.032 (0.039)	Data 5.17e-04 (5.25e-04)	Tok/s 22468 (22073)	Loss/tok 5.2704 (5.3928)
1: VALIDATION [1][10/80]	Time 0.033 (0.039)	Data 5.25e-04 (5.41e-04)	Tok/s 22234 (22387)	Loss/tok 5.4736 (5.3850)
0: VALIDATION [1][10/80]	Time 0.033 (0.040)	Data 5.13e-04 (5.21e-04)	Tok/s 22088 (22087)	Loss/tok 4.9558 (5.5273)
5: VALIDATION [1][20/80]	Time 0.026 (0.034)	Data 5.13e-04 (5.17e-04)	Tok/s 22022 (22063)	Loss/tok 5.0419 (5.4308)
7: VALIDATION [1][20/79]	Time 0.027 (0.034)	Data 5.58e-04 (5.62e-04)	Tok/s 21562 (21828)	Loss/tok 5.0509 (5.3927)
6: VALIDATION [1][20/79]	Time 0.027 (0.034)	Data 5.19e-04 (5.26e-04)	Tok/s 20969 (21735)	Loss/tok 5.1979 (5.4023)
4: VALIDATION [1][20/80]	Time 0.027 (0.034)	Data 5.39e-04 (5.55e-04)	Tok/s 21140 (21899)	Loss/tok 4.5770 (5.3198)
2: VALIDATION [1][20/80]	Time 0.027 (0.034)	Data 5.07e-04 (5.36e-04)	Tok/s 21889 (22001)	Loss/tok 4.7636 (5.4318)
3: VALIDATION [1][20/80]	Time 0.027 (0.034)	Data 5.03e-04 (5.18e-04)	Tok/s 21732 (21824)	Loss/tok 4.8983 (5.3615)
1: VALIDATION [1][20/80]	Time 0.026 (0.034)	Data 5.20e-04 (5.34e-04)	Tok/s 22133 (22085)	Loss/tok 5.5322 (5.3407)
0: VALIDATION [1][20/80]	Time 0.028 (0.035)	Data 5.01e-04 (5.24e-04)	Tok/s 20976 (21949)	Loss/tok 4.7274 (5.4397)
5: VALIDATION [1][30/80]	Time 0.023 (0.031)	Data 5.06e-04 (5.15e-04)	Tok/s 20795 (21691)	Loss/tok 5.4620 (5.3431)
7: VALIDATION [1][30/79]	Time 0.023 (0.031)	Data 5.35e-04 (5.57e-04)	Tok/s 20550 (21544)	Loss/tok 5.1506 (5.3177)
4: VALIDATION [1][30/80]	Time 0.023 (0.031)	Data 5.21e-04 (5.46e-04)	Tok/s 21261 (21577)	Loss/tok 4.6570 (5.2484)
6: VALIDATION [1][30/79]	Time 0.024 (0.031)	Data 5.04e-04 (5.23e-04)	Tok/s 20149 (21416)	Loss/tok 5.0545 (5.3198)
2: VALIDATION [1][30/80]	Time 0.024 (0.031)	Data 5.10e-04 (5.31e-04)	Tok/s 20717 (21672)	Loss/tok 5.3214 (5.3578)
3: VALIDATION [1][30/80]	Time 0.023 (0.031)	Data 5.14e-04 (5.16e-04)	Tok/s 20781 (21460)	Loss/tok 4.7412 (5.3081)
1: VALIDATION [1][30/80]	Time 0.024 (0.031)	Data 5.17e-04 (5.29e-04)	Tok/s 19969 (21791)	Loss/tok 4.8781 (5.2613)
0: VALIDATION [1][30/80]	Time 0.024 (0.032)	Data 5.03e-04 (5.20e-04)	Tok/s 20055 (21469)	Loss/tok 5.1338 (5.3731)
5: VALIDATION [1][40/80]	Time 0.019 (0.028)	Data 5.00e-04 (5.12e-04)	Tok/s 20691 (21372)	Loss/tok 5.4120 (5.2916)
7: VALIDATION [1][40/79]	Time 0.021 (0.029)	Data 5.24e-04 (5.49e-04)	Tok/s 19228 (21175)	Loss/tok 5.3759 (5.2640)
4: VALIDATION [1][40/80]	Time 0.020 (0.029)	Data 5.29e-04 (5.40e-04)	Tok/s 19550 (21345)	Loss/tok 4.9978 (5.1920)
6: VALIDATION [1][40/79]	Time 0.021 (0.029)	Data 5.09e-04 (5.20e-04)	Tok/s 19282 (21175)	Loss/tok 4.4735 (5.2577)
2: VALIDATION [1][40/80]	Time 0.020 (0.029)	Data 5.08e-04 (5.27e-04)	Tok/s 20585 (21379)	Loss/tok 4.9175 (5.3176)
1: VALIDATION [1][40/80]	Time 0.020 (0.029)	Data 5.13e-04 (5.25e-04)	Tok/s 20465 (21529)	Loss/tok 4.8811 (5.2639)
3: VALIDATION [1][40/80]	Time 0.020 (0.029)	Data 5.07e-04 (5.14e-04)	Tok/s 20533 (21189)	Loss/tok 5.3115 (5.2708)
0: VALIDATION [1][40/80]	Time 0.020 (0.029)	Data 4.98e-04 (5.14e-04)	Tok/s 20558 (21289)	Loss/tok 4.8449 (5.3100)
5: VALIDATION [1][50/80]	Time 0.016 (0.026)	Data 4.92e-04 (5.14e-04)	Tok/s 20125 (21047)	Loss/tok 5.0532 (5.2589)
7: VALIDATION [1][50/79]	Time 0.018 (0.027)	Data 5.15e-04 (5.44e-04)	Tok/s 18445 (20830)	Loss/tok 5.2953 (5.2246)
4: VALIDATION [1][50/80]	Time 0.017 (0.027)	Data 5.12e-04 (5.36e-04)	Tok/s 19043 (21020)	Loss/tok 4.5967 (5.1925)
6: VALIDATION [1][50/79]	Time 0.017 (0.027)	Data 4.99e-04 (5.18e-04)	Tok/s 19153 (20862)	Loss/tok 5.2194 (5.2196)
2: VALIDATION [1][50/80]	Time 0.017 (0.027)	Data 5.04e-04 (5.24e-04)	Tok/s 19482 (21070)	Loss/tok 4.6544 (5.2636)
3: VALIDATION [1][50/80]	Time 0.018 (0.027)	Data 5.01e-04 (5.12e-04)	Tok/s 18713 (20893)	Loss/tok 5.3525 (5.2456)
1: VALIDATION [1][50/80]	Time 0.018 (0.027)	Data 5.44e-04 (5.97e-04)	Tok/s 18701 (21092)	Loss/tok 5.2419 (5.2264)
0: VALIDATION [1][50/80]	Time 0.018 (0.027)	Data 5.61e-04 (5.13e-04)	Tok/s 19099 (20946)	Loss/tok 5.7035 (5.2558)
5: VALIDATION [1][60/80]	Time 0.015 (0.025)	Data 5.00e-04 (5.14e-04)	Tok/s 17654 (20563)	Loss/tok 4.6814 (5.2168)
7: VALIDATION [1][60/79]	Time 0.015 (0.025)	Data 5.18e-04 (5.39e-04)	Tok/s 18139 (20402)	Loss/tok 4.7251 (5.2016)
4: VALIDATION [1][60/80]	Time 0.015 (0.025)	Data 5.13e-04 (5.33e-04)	Tok/s 17755 (20555)	Loss/tok 4.6761 (5.1591)
6: VALIDATION [1][60/79]	Time 0.015 (0.025)	Data 5.08e-04 (5.17e-04)	Tok/s 17640 (20368)	Loss/tok 5.0425 (5.1958)
2: VALIDATION [1][60/80]	Time 0.015 (0.025)	Data 5.07e-04 (5.24e-04)	Tok/s 17667 (20623)	Loss/tok 4.6479 (5.2516)
3: VALIDATION [1][60/80]	Time 0.015 (0.025)	Data 4.96e-04 (5.10e-04)	Tok/s 17779 (20443)	Loss/tok 5.0572 (5.1970)
1: VALIDATION [1][60/80]	Time 0.016 (0.025)	Data 5.51e-04 (5.91e-04)	Tok/s 17236 (20646)	Loss/tok 4.8397 (5.1815)
0: VALIDATION [1][60/80]	Time 0.016 (0.025)	Data 4.87e-04 (5.10e-04)	Tok/s 17483 (20496)	Loss/tok 5.0673 (5.2179)
5: VALIDATION [1][70/80]	Time 0.011 (0.023)	Data 5.15e-04 (5.13e-04)	Tok/s 17984 (20125)	Loss/tok 4.4763 (5.1815)
7: VALIDATION [1][70/79]	Time 0.012 (0.023)	Data 5.10e-04 (5.36e-04)	Tok/s 16531 (19919)	Loss/tok 5.7729 (5.1820)
4: VALIDATION [1][70/80]	Time 0.013 (0.023)	Data 5.17e-04 (5.30e-04)	Tok/s 15551 (20080)	Loss/tok 4.2888 (5.1316)
6: VALIDATION [1][70/79]	Time 0.012 (0.023)	Data 4.93e-04 (5.15e-04)	Tok/s 17247 (19930)	Loss/tok 4.4037 (5.1540)
2: VALIDATION [1][70/80]	Time 0.011 (0.023)	Data 5.01e-04 (5.22e-04)	Tok/s 17456 (20214)	Loss/tok 4.6444 (5.2332)
3: VALIDATION [1][70/80]	Time 0.012 (0.023)	Data 4.93e-04 (5.08e-04)	Tok/s 16628 (20023)	Loss/tok 5.3772 (5.1739)
1: VALIDATION [1][70/80]	Time 0.012 (0.023)	Data 5.49e-04 (5.85e-04)	Tok/s 16432 (20221)	Loss/tok 4.7982 (5.1549)
0: VALIDATION [1][70/80]	Time 0.012 (0.024)	Data 4.93e-04 (5.07e-04)	Tok/s 17245 (20048)	Loss/tok 4.4869 (5.1869)
0: Saving model to gnmt/model_best.pth
4: Running evaluation on test set
1: Running evaluation on test set
5: Running evaluation on test set
6: Running evaluation on test set
3: Running evaluation on test set
7: Running evaluation on test set
2: Running evaluation on test set
0: Running evaluation on test set
6: TEST [1][9/12]	Time 0.4617 (0.5934)	Decoder iters 34.0 (111.4)	Tok/s 2235 (3362)
7: TEST [1][9/12]	Time 0.4617 (0.5933)	Decoder iters 33.0 (89.1)	Tok/s 2187 (3287)
3: TEST [1][9/12]	Time 0.4620 (0.5934)	Decoder iters 33.0 (106.4)	Tok/s 2329 (3582)
0: TEST [1][9/12]	Time 0.4611 (0.5931)	Decoder iters 31.0 (126.8)	Tok/s 2483 (3767)
1: TEST [1][9/12]	Time 0.4619 (0.5934)	Decoder iters 37.0 (107.3)	Tok/s 2555 (3572)
5: TEST [1][9/12]	Time 0.4620 (0.5934)	Decoder iters 30.0 (95.1)	Tok/s 2234 (3413)
2: TEST [1][9/12]	Time 0.4614 (0.5934)	Decoder iters 39.0 (95.2)	Tok/s 2436 (3573)
4: TEST [1][9/12]	Time 0.4623 (0.5935)	Decoder iters 149.0 (96.3)	Tok/s 2319 (3416)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
2: Finished evaluation on test set
7: Finished evaluation on test set
3: Finished evaluation on test set
5: Finished evaluation on test set
6: Finished evaluation on test set
4: Finished evaluation on test set
0: Finished evaluation on test set
2: Finished epoch 1
3: Finished epoch 1
5: Finished epoch 1
6: Finished epoch 1
4: Finished epoch 1
7: Finished epoch 1
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.4870	Validation Loss: 5.1465	Test BLEU: 6.85
0: Performance: Epoch: 1	Training: 21407 Tok/s	Validation: 156063 Tok/s
0: Finished epoch 1
3: Total training time 716 s
5: Total training time 716 s
6: Total training time 715 s
7: Total training time 715 s
1: Total training time 716 s
4: Total training time 715 s
2: Total training time 715 s
0: Total training time 715 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       8|                  40|                      6.85|                      21448.3|                         11.92|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
