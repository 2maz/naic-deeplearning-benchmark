DLL 2021-03-24 16:36:20.612142 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.687155 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.695777 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.731834 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.752754 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.755638 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.771878 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2021-03-24 16:36:20.799689 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 24  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 3259
Using seed = 3347
Using seed = 1425
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.64s)
creating index...
Done (t=0.64s)
creating index...
Done (t=0.64s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Using seed = 584
loading annotations into memory...
Using seed = 1592
loading annotations into memory...
Using seed = 3624
loading annotations into memory...
Using seed = 8903
Using seed = 2617
loading annotations into memory...
loading annotations into memory...
Done (t=0.66s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.66s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.66s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.66s)
creating index...
Done (t=0.68s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
DLL 2021-03-24 16:37:13.129667 - () avg_img/sec : 98.87603066734263  med_img/sec : 98.87144934503354  min_img/sec : 96.60362525239341  max_img/sec : 100.95637234892894 
Done benchmarking. Total images: 2400	total time: 24.273	Average images/sec: 98.876	Median images/sec: 98.871
DLL 2021-03-24 16:37:13.129784 - () avg_img/sec : 98.90419974983736  med_img/sec : 98.88922342511121  min_img/sec : 96.9459052627017  max_img/sec : 101.11528130744428 
Done benchmarking. Total images: 2400	total time: 24.266	Average images/sec: 98.904	Median images/sec: 98.889
DLL 2021-03-24 16:37:13.129844 - () avg_img/sec : 98.86791206037263  med_img/sec : 98.83354133499004  min_img/sec : 96.88777325191921  max_img/sec : 103.57297148801274 
DLL 2021-03-24 16:37:13.129868 - () avg_img/sec : 98.88503358596915  med_img/sec : 98.89427529685706  min_img/sec : 96.90111038914158  max_img/sec : 101.04575417956967 
Done benchmarking. Total images: 2400	total time: 24.275	Average images/sec: 98.868	Median images/sec: 98.834
Done benchmarking. Total images: 2400	total time: 24.271	Average images/sec: 98.885	Median images/sec: 98.894
DLL 2021-03-24 16:37:13.130027 - () avg_img/sec : 98.90831045887434  med_img/sec : 98.90224269800308  min_img/sec : 96.95262806602888  max_img/sec : 101.04149431321436 
Done benchmarking. Total images: 2400	total time: 24.265	Average images/sec: 98.908	Median images/sec: 98.902
DLL 2021-03-24 16:37:13.130182 - () avg_img/sec : 98.9441775248514  med_img/sec : 98.89704502024031  min_img/sec : 96.95767078044227  max_img/sec : 100.95697985439644 
Done benchmarking. Total images: 2400	total time: 24.256	Average images/sec: 98.944	Median images/sec: 98.897
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.130519 - () total time : 42.660616636276245 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.130540 - () 
DLL 2021-03-24 16:37:13.130565 - () total time : 42.66089153289795 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.130614 - () 
DLL 2021-03-24 16:37:13.130631 - () total time : 42.66468858718872 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.130516 - () avg_img/sec : 98.9703293403695  med_img/sec : 98.94526072710255  min_img/sec : 96.98270534928913  max_img/sec : 101.05255042423416 
DLL 2021-03-24 16:37:13.130652 - () 
DLL 2021-03-24 16:37:13.130671 - () total time : 42.6689658164978 
Done benchmarking. Total images: 2400	total time: 24.250	Average images/sec: 98.970	Median images/sec: 98.945
DLL 2021-03-24 16:37:13.130694 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.130927 - () total time : 42.66147303581238 
DLL 2021-03-24 16:37:13.130955 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.130883 - () avg_img/sec : 98.9363656942613  med_img/sec : 98.90666421358233  min_img/sec : 96.62300504408172  max_img/sec : 101.30414602777168 
DLL 2021-03-24 16:37:13.131031 - () total time : 42.66143083572388 
Done benchmarking. Total images: 2400	total time: 24.258	Average images/sec: 98.936	Median images/sec: 98.907
DLL 2021-03-24 16:37:13.131055 - () 
Training performance = 791.1397094726562 FPS
DLL 2021-03-24 16:37:13.131693 - (0,) time : 42.67381453514099 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.132067 - () total time : 42.662076234817505 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2021-03-24 16:37:13.132092 - () 
DLL 2021-03-24 16:37:13.132111 - () total time : 42.67381453514099 
DLL 2021-03-24 16:37:13.132128 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
