4: Collecting environment information...
3: Collecting environment information...
5: Collecting environment information...
1: Collecting environment information...
2: Collecting environment information...
6: Collecting environment information...
7: Collecting environment information...
0: Collecting environment information...
4: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
4: Saving results to: gnmt
4: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=4, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=4, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
4: Using master seed from command line: 2
1: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
2: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
2: Saving results to: gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
2: Using master seed from command line: 2
6: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
6: Saving results to: gnmt
6: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=6, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=6, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
6: Using master seed from command line: 2
7: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
7: Saving results to: gnmt
7: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=7, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=7, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
7: Using master seed from command line: 2
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
3: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] Could not collect
3: Saving results to: gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
3: Using master seed from command line: 2
5: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3070
GPU 1: GeForce RTX 3070
GPU 2: GeForce RTX 3070
GPU 3: GeForce RTX 3070
GPU 4: GeForce RTX 3070
GPU 5: GeForce RTX 3070
GPU 6: GeForce RTX 3070
GPU 7: GeForce RTX 3070

Nvidia driver version: 460.56
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] Could not collect
5: Saving results to: gnmt
5: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=5, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=5, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=80, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
5: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
2: Worker 2 is using worker seed: 3588440356
6: Worker 6 is using worker seed: 4077622522
3: Worker 3 is using worker seed: 1323436024
5: Worker 5 is using worker seed: 2606193617
1: Worker 1 is using worker seed: 364522461
4: Worker 4 is using worker seed: 2602510382
7: Worker 7 is using worker seed: 117874757
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
4: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
7: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
6: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
4: Size of vocabulary: 31800
3: Size of vocabulary: 31800
1: Size of vocabulary: 31800
7: Size of vocabulary: 31800
5: Size of vocabulary: 31800
6: Size of vocabulary: 31800
2: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
4: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
7: Filtering data, min len: 0, max len: 50
5: Filtering data, min len: 0, max len: 50
6: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
4: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
7: Pairs before: 160078, after: 148120
5: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
6: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Filtering data, min len: 0, max len: 125
6: Filtering data, min len: 0, max len: 125
4: Filtering data, min len: 0, max len: 125
0: Filtering data, min len: 0, max len: 125
3: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
2: Filtering data, min len: 0, max len: 125
5: Filtering data, min len: 0, max len: 125
6: Pairs before: 5100, after: 5100
7: Filtering data, min len: 0, max len: 125
4: Pairs before: 5100, after: 5100
0: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
2: Pairs before: 5100, after: 5100
5: Pairs before: 5100, after: 5100
7: Pairs before: 5100, after: 5100
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
6: Filtering data, min len: 0, max len: 150
0: Filtering data, min len: 0, max len: 150
4: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
6: Pairs before: 3003, after: 3003
4: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
5: Filtering data, min len: 0, max len: 150
2: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
7: Filtering data, min len: 0, max len: 150
5: Pairs before: 3003, after: 3003
2: Pairs before: 3003, after: 3003
7: Pairs before: 3003, after: 3003
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159605817
3: Saving state of the tokenizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 305
3: Scheduler decay interval: 38
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: Initializing amp optimizer
3: Starting epoch 0
3: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159605817
4: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
4: Building LabelSmoothingLoss (smoothing: 0.1)
4: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
4: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
4: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 305
2: Saving state of the tokenizer
0: Scheduler decay interval: 38
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 305
2: Scheduler decay interval: 38
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
4: Saving state of the tokenizer
0: Initializing amp optimizer
4: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
4: Scheduler warmup steps: 200
4: Scheduler remain steps: 305
4: Scheduler decay interval: 38
4: Scheduler decay factor: 0.5
4: Scheduler max decay steps: 4
2: Initializing amp optimizer
6: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
6: Building LabelSmoothingLoss (smoothing: 0.1)
6: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
6: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
6: Number of parameters: 159605817
0: Starting epoch 0
0: Executing preallocation
2: Starting epoch 0
2: Executing preallocation
4: Initializing amp optimizer
6: Saving state of the tokenizer
6: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Scheduler warmup steps: 200
6: Scheduler remain steps: 305
6: Scheduler decay interval: 38
6: Scheduler decay factor: 0.5
6: Scheduler max decay steps: 4
4: Starting epoch 0
4: Executing preallocation
6: Initializing amp optimizer
6: Starting epoch 0
6: Executing preallocation
7: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: Building LabelSmoothingLoss (smoothing: 0.1)
7: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
7: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
7: Number of parameters: 159605817
7: Saving state of the tokenizer
7: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
7: Scheduler warmup steps: 200
7: Scheduler remain steps: 305
7: Scheduler decay interval: 38
7: Scheduler decay factor: 0.5
7: Scheduler max decay steps: 4
7: Initializing amp optimizer
7: Starting epoch 0
7: Executing preallocation
5: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
5: Building LabelSmoothingLoss (smoothing: 0.1)
5: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
5: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
5: Number of parameters: 159605817
5: Saving state of the tokenizer
5: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
5: Scheduler warmup steps: 200
5: Scheduler remain steps: 305
5: Scheduler decay interval: 38
5: Scheduler decay factor: 0.5
5: Scheduler max decay steps: 4
5: Initializing amp optimizer
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
5: Starting epoch 0
5: Executing preallocation
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 305
1: Scheduler decay interval: 38
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
5: Sampler for epoch 0 uses seed 1632151663
0: Sampler for epoch 0 uses seed 1632151663
6: Sampler for epoch 0 uses seed 1632151663
4: Sampler for epoch 0 uses seed 1632151663
7: Sampler for epoch 0 uses seed 1632151663
3: Sampler for epoch 0 uses seed 1632151663
1: Sampler for epoch 0 uses seed 1632151663
2: Sampler for epoch 0 uses seed 1632151663
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
5: TRAIN [0][0/229]	Time 0.465 (0.000)	Data 8.14e-02 (0.00e+00)	Tok/s 7827 (0)	Loss/tok 10.5796 (10.5796)	LR 2.047e-05
2: TRAIN [0][0/229]	Time 0.465 (0.000)	Data 1.34e-01 (0.00e+00)	Tok/s 7725 (0)	Loss/tok 10.5994 (10.5994)	LR 2.047e-05
6: TRAIN [0][0/229]	Time 0.464 (0.000)	Data 1.16e-01 (0.00e+00)	Tok/s 7761 (0)	Loss/tok 10.6253 (10.6253)	LR 2.047e-05
1: TRAIN [0][0/229]	Time 0.464 (0.000)	Data 1.44e-01 (0.00e+00)	Tok/s 7645 (0)	Loss/tok 10.6184 (10.6184)	LR 2.047e-05
4: TRAIN [0][0/229]	Time 0.465 (0.000)	Data 1.20e-01 (0.00e+00)	Tok/s 7656 (0)	Loss/tok 10.6022 (10.6022)	LR 2.047e-05
3: TRAIN [0][0/229]	Time 0.465 (0.000)	Data 1.27e-01 (0.00e+00)	Tok/s 7582 (0)	Loss/tok 10.6100 (10.6100)	LR 2.047e-05
7: TRAIN [0][0/229]	Time 0.464 (0.000)	Data 1.22e-01 (0.00e+00)	Tok/s 7738 (0)	Loss/tok 10.5981 (10.5981)	LR 2.047e-05
0: TRAIN [0][0/229]	Time 0.464 (0.000)	Data 9.95e-02 (0.00e+00)	Tok/s 7653 (0)	Loss/tok 10.5875 (10.5875)	LR 2.047e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
6: TRAIN [0][10/229]	Time 0.384 (0.358)	Data 1.07e-04 (9.55e-05)	Tok/s 13237 (9856)	Loss/tok 9.8552 (10.1555)	LR 2.576e-05
1: TRAIN [0][10/229]	Time 0.384 (0.358)	Data 1.30e-04 (1.05e-04)	Tok/s 13164 (10016)	Loss/tok 9.7860 (10.1447)	LR 2.576e-05
5: TRAIN [0][10/229]	Time 0.386 (0.358)	Data 7.63e-05 (7.64e-05)	Tok/s 13062 (9977)	Loss/tok 9.8228 (10.1489)	LR 2.576e-05
0: TRAIN [0][10/229]	Time 0.384 (0.358)	Data 1.13e-04 (1.01e-04)	Tok/s 13177 (9875)	Loss/tok 9.8093 (10.1371)	LR 2.576e-05
7: TRAIN [0][10/229]	Time 0.383 (0.358)	Data 1.37e-04 (1.02e-04)	Tok/s 13067 (9967)	Loss/tok 9.8036 (10.1551)	LR 2.576e-05
2: TRAIN [0][10/229]	Time 0.386 (0.358)	Data 1.23e-04 (1.01e-04)	Tok/s 13099 (9974)	Loss/tok 9.8013 (10.1497)	LR 2.576e-05
4: TRAIN [0][10/229]	Time 0.386 (0.358)	Data 9.35e-05 (9.24e-05)	Tok/s 13204 (9935)	Loss/tok 9.8118 (10.1513)	LR 2.576e-05
3: TRAIN [0][10/229]	Time 0.386 (0.358)	Data 7.80e-05 (8.59e-05)	Tok/s 13053 (9946)	Loss/tok 9.8432 (10.1483)	LR 2.576e-05
4: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 9.30e-05 (9.73e-05)	Tok/s 13397 (10922)	Loss/tok 9.2498 (9.7772)	LR 3.244e-05
3: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 8.77e-05 (9.02e-05)	Tok/s 13047 (10881)	Loss/tok 9.2412 (9.7659)	LR 3.244e-05
5: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 7.94e-05 (7.65e-05)	Tok/s 13167 (10928)	Loss/tok 9.2558 (9.7814)	LR 3.244e-05
2: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 1.23e-04 (1.05e-04)	Tok/s 13283 (10939)	Loss/tok 9.3326 (9.7838)	LR 3.244e-05
6: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 9.82e-05 (9.59e-05)	Tok/s 13322 (10861)	Loss/tok 9.2507 (9.7772)	LR 3.244e-05
1: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 8.42e-05 (9.51e-05)	Tok/s 13252 (10925)	Loss/tok 9.2984 (9.7727)	LR 3.244e-05
0: TRAIN [0][20/229]	Time 0.382 (0.365)	Data 7.27e-05 (9.41e-05)	Tok/s 13212 (10870)	Loss/tok 9.2496 (9.7642)	LR 3.244e-05
7: TRAIN [0][20/229]	Time 0.381 (0.365)	Data 1.05e-04 (9.94e-05)	Tok/s 13409 (10922)	Loss/tok 9.2837 (9.7734)	LR 3.244e-05
4: TRAIN [0][30/229]	Time 0.365 (0.365)	Data 9.32e-05 (9.67e-05)	Tok/s 9936 (10671)	Loss/tok 8.8793 (9.5480)	LR 4.083e-05
3: TRAIN [0][30/229]	Time 0.365 (0.365)	Data 7.65e-05 (8.95e-05)	Tok/s 10101 (10648)	Loss/tok 8.8555 (9.5340)	LR 4.083e-05
5: TRAIN [0][30/229]	Time 0.365 (0.365)	Data 7.51e-05 (7.75e-05)	Tok/s 9930 (10631)	Loss/tok 8.8258 (9.5367)	LR 4.083e-05
2: TRAIN [0][30/229]	Time 0.366 (0.365)	Data 1.09e-04 (1.05e-04)	Tok/s 9989 (10667)	Loss/tok 8.8778 (9.5596)	LR 4.083e-05
6: TRAIN [0][30/229]	Time 0.365 (0.365)	Data 8.51e-05 (9.34e-05)	Tok/s 10072 (10661)	Loss/tok 8.9270 (9.5459)	LR 4.083e-05
1: TRAIN [0][30/229]	Time 0.365 (0.365)	Data 1.03e-04 (9.38e-05)	Tok/s 9768 (10648)	Loss/tok 8.8886 (9.5450)	LR 4.083e-05
0: TRAIN [0][30/229]	Time 0.366 (0.365)	Data 7.65e-05 (8.98e-05)	Tok/s 9667 (10646)	Loss/tok 8.8669 (9.5334)	LR 4.083e-05
7: TRAIN [0][30/229]	Time 0.365 (0.365)	Data 9.54e-05 (9.75e-05)	Tok/s 9813 (10656)	Loss/tok 8.8723 (9.5413)	LR 4.083e-05
5: TRAIN [0][40/229]	Time 0.336 (0.365)	Data 7.51e-05 (7.80e-05)	Tok/s 6212 (10668)	Loss/tok 8.4781 (9.3476)	LR 5.141e-05
2: TRAIN [0][40/229]	Time 0.336 (0.365)	Data 1.08e-04 (1.05e-04)	Tok/s 6600 (10704)	Loss/tok 8.2757 (9.3627)	LR 5.141e-05
6: TRAIN [0][40/229]	Time 0.337 (0.365)	Data 8.56e-05 (9.16e-05)	Tok/s 6496 (10717)	Loss/tok 8.3316 (9.3570)	LR 5.141e-05
1: TRAIN [0][40/229]	Time 0.337 (0.365)	Data 7.51e-05 (9.28e-05)	Tok/s 6466 (10713)	Loss/tok 8.2813 (9.3421)	LR 5.141e-05
4: TRAIN [0][40/229]	Time 0.336 (0.365)	Data 9.32e-05 (9.65e-05)	Tok/s 6424 (10732)	Loss/tok 8.4194 (9.3512)	LR 5.141e-05
3: TRAIN [0][40/229]	Time 0.336 (0.365)	Data 7.96e-05 (9.00e-05)	Tok/s 6609 (10691)	Loss/tok 8.4857 (9.3543)	LR 5.141e-05
0: TRAIN [0][40/229]	Time 0.338 (0.365)	Data 8.30e-05 (8.74e-05)	Tok/s 6510 (10709)	Loss/tok 8.4350 (9.3460)	LR 5.141e-05
7: TRAIN [0][40/229]	Time 0.338 (0.365)	Data 7.32e-05 (9.67e-05)	Tok/s 6416 (10710)	Loss/tok 8.3547 (9.3478)	LR 5.141e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
5: TRAIN [0][50/229]	Time 0.338 (0.364)	Data 1.00e-04 (7.89e-05)	Tok/s 6767 (10775)	Loss/tok 8.3431 (9.2447)	LR 6.472e-05
3: TRAIN [0][50/229]	Time 0.339 (0.364)	Data 8.70e-05 (9.00e-05)	Tok/s 6650 (10800)	Loss/tok 8.2688 (9.2585)	LR 6.472e-05
2: TRAIN [0][50/229]	Time 0.338 (0.364)	Data 9.42e-05 (1.04e-04)	Tok/s 6575 (10796)	Loss/tok 8.3822 (9.2611)	LR 6.472e-05
4: TRAIN [0][50/229]	Time 0.339 (0.364)	Data 9.42e-05 (9.64e-05)	Tok/s 6138 (10805)	Loss/tok 8.3101 (9.2501)	LR 6.472e-05
6: TRAIN [0][50/229]	Time 0.338 (0.364)	Data 1.02e-04 (9.15e-05)	Tok/s 6288 (10806)	Loss/tok 8.2958 (9.2588)	LR 6.472e-05
1: TRAIN [0][50/229]	Time 0.338 (0.364)	Data 8.70e-05 (9.17e-05)	Tok/s 6581 (10808)	Loss/tok 8.2229 (9.2387)	LR 6.472e-05
0: TRAIN [0][50/229]	Time 0.338 (0.364)	Data 8.20e-05 (8.65e-05)	Tok/s 6647 (10808)	Loss/tok 8.2851 (9.2474)	LR 6.472e-05
7: TRAIN [0][50/229]	Time 0.337 (0.364)	Data 1.10e-04 (9.62e-05)	Tok/s 6283 (10775)	Loss/tok 8.3749 (9.2470)	LR 6.472e-05
4: TRAIN [0][60/229]	Time 0.335 (0.365)	Data 9.27e-05 (9.72e-05)	Tok/s 3117 (10883)	Loss/tok 8.0444 (9.1037)	LR 8.148e-05
3: TRAIN [0][60/229]	Time 0.335 (0.365)	Data 7.27e-05 (8.99e-05)	Tok/s 3287 (10865)	Loss/tok 7.9440 (9.1193)	LR 8.148e-05
5: TRAIN [0][60/229]	Time 0.335 (0.365)	Data 7.46e-05 (7.90e-05)	Tok/s 3229 (10848)	Loss/tok 7.8143 (9.1028)	LR 8.148e-05
2: TRAIN [0][60/229]	Time 0.336 (0.365)	Data 8.80e-05 (1.03e-04)	Tok/s 3217 (10867)	Loss/tok 7.9299 (9.1162)	LR 8.148e-05
1: TRAIN [0][60/229]	Time 0.336 (0.365)	Data 8.32e-05 (9.11e-05)	Tok/s 3206 (10863)	Loss/tok 7.8336 (9.1002)	LR 8.148e-05
6: TRAIN [0][60/229]	Time 0.336 (0.365)	Data 7.51e-05 (9.01e-05)	Tok/s 3255 (10882)	Loss/tok 7.8879 (9.1129)	LR 8.148e-05
0: TRAIN [0][60/229]	Time 0.336 (0.365)	Data 7.13e-05 (8.57e-05)	Tok/s 3048 (10877)	Loss/tok 7.8758 (9.1107)	LR 8.148e-05
7: TRAIN [0][60/229]	Time 0.335 (0.365)	Data 9.32e-05 (9.41e-05)	Tok/s 3236 (10854)	Loss/tok 8.0399 (9.1059)	LR 8.148e-05
0: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 6.89e-05 (8.54e-05)	Tok/s 6302 (10666)	Loss/tok 7.8940 (8.9975)	LR 1.026e-04
7: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 6.99e-05 (9.36e-05)	Tok/s 6144 (10638)	Loss/tok 7.7993 (8.9968)	LR 1.026e-04
6: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 8.77e-05 (8.95e-05)	Tok/s 6262 (10674)	Loss/tok 7.7330 (8.9989)	LR 1.026e-04
1: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 8.34e-05 (9.11e-05)	Tok/s 6315 (10648)	Loss/tok 7.9204 (8.9881)	LR 1.026e-04
2: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 8.80e-05 (1.03e-04)	Tok/s 6205 (10651)	Loss/tok 7.8533 (9.0000)	LR 1.026e-04
5: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 7.68e-05 (7.89e-05)	Tok/s 6423 (10644)	Loss/tok 7.8274 (8.9885)	LR 1.026e-04
3: TRAIN [0][70/229]	Time 0.349 (0.365)	Data 1.12e-04 (9.01e-05)	Tok/s 6264 (10650)	Loss/tok 7.8462 (9.0060)	LR 1.026e-04
4: TRAIN [0][70/229]	Time 0.350 (0.365)	Data 8.75e-05 (9.67e-05)	Tok/s 6201 (10664)	Loss/tok 7.7907 (8.9877)	LR 1.026e-04
0: TRAIN [0][80/229]	Time 0.364 (0.364)	Data 8.44e-05 (8.60e-05)	Tok/s 10004 (10479)	Loss/tok 7.8854 (8.8874)	LR 1.291e-04
7: TRAIN [0][80/229]	Time 0.363 (0.364)	Data 1.03e-04 (9.33e-05)	Tok/s 10210 (10467)	Loss/tok 7.8968 (8.8883)	LR 1.291e-04
6: TRAIN [0][80/229]	Time 0.366 (0.364)	Data 8.08e-05 (8.86e-05)	Tok/s 10096 (10493)	Loss/tok 7.8707 (8.8873)	LR 1.291e-04
1: TRAIN [0][80/229]	Time 0.365 (0.364)	Data 1.13e-04 (9.09e-05)	Tok/s 10025 (10460)	Loss/tok 7.8830 (8.8845)	LR 1.291e-04
5: TRAIN [0][80/229]	Time 0.367 (0.364)	Data 7.87e-05 (7.90e-05)	Tok/s 9900 (10469)	Loss/tok 7.8847 (8.8791)	LR 1.291e-04
2: TRAIN [0][80/229]	Time 0.367 (0.364)	Data 1.17e-04 (1.03e-04)	Tok/s 10083 (10466)	Loss/tok 7.9116 (8.8933)	LR 1.291e-04
4: TRAIN [0][80/229]	Time 0.368 (0.364)	Data 8.73e-05 (9.66e-05)	Tok/s 9446 (10477)	Loss/tok 7.8001 (8.8809)	LR 1.291e-04
3: TRAIN [0][80/229]	Time 0.368 (0.364)	Data 7.63e-05 (8.96e-05)	Tok/s 9638 (10470)	Loss/tok 7.8106 (8.8951)	LR 1.291e-04
6: TRAIN [0][90/229]	Time 0.345 (0.365)	Data 7.70e-05 (8.79e-05)	Tok/s 6418 (10551)	Loss/tok 7.6488 (8.7740)	LR 1.626e-04
1: TRAIN [0][90/229]	Time 0.345 (0.365)	Data 9.13e-05 (9.02e-05)	Tok/s 6241 (10523)	Loss/tok 7.7684 (8.7733)	LR 1.626e-04
0: TRAIN [0][90/229]	Time 0.344 (0.365)	Data 7.94e-05 (8.63e-05)	Tok/s 6301 (10528)	Loss/tok 7.5887 (8.7705)	LR 1.626e-04
7: TRAIN [0][90/229]	Time 0.343 (0.365)	Data 1.09e-04 (9.34e-05)	Tok/s 6142 (10525)	Loss/tok 7.5815 (8.7712)	LR 1.626e-04
2: TRAIN [0][90/229]	Time 0.346 (0.365)	Data 1.17e-04 (1.02e-04)	Tok/s 6411 (10535)	Loss/tok 7.5412 (8.7735)	LR 1.626e-04
5: TRAIN [0][90/229]	Time 0.347 (0.365)	Data 7.25e-05 (7.89e-05)	Tok/s 6367 (10531)	Loss/tok 7.6393 (8.7640)	LR 1.626e-04
4: TRAIN [0][90/229]	Time 0.347 (0.365)	Data 1.00e-04 (9.66e-05)	Tok/s 6167 (10532)	Loss/tok 7.5812 (8.7687)	LR 1.626e-04
3: TRAIN [0][90/229]	Time 0.347 (0.365)	Data 7.89e-05 (8.95e-05)	Tok/s 6302 (10536)	Loss/tok 7.6224 (8.7745)	LR 1.626e-04
0: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 7.49e-05 (8.62e-05)	Tok/s 6396 (10580)	Loss/tok 7.5323 (8.6724)	LR 2.047e-04
1: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 8.89e-05 (8.97e-05)	Tok/s 6571 (10569)	Loss/tok 7.4351 (8.6759)	LR 2.047e-04
6: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 8.20e-05 (8.73e-05)	Tok/s 6375 (10603)	Loss/tok 7.4588 (8.6760)	LR 2.047e-04
7: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 7.63e-05 (9.35e-05)	Tok/s 6408 (10572)	Loss/tok 7.5142 (8.6746)	LR 2.047e-04
5: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 7.65e-05 (7.93e-05)	Tok/s 6387 (10577)	Loss/tok 7.4189 (8.6701)	LR 2.047e-04
2: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 8.80e-05 (1.02e-04)	Tok/s 6180 (10583)	Loss/tok 7.4253 (8.6793)	LR 2.047e-04
3: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 8.25e-05 (8.92e-05)	Tok/s 6453 (10580)	Loss/tok 7.5044 (8.6812)	LR 2.047e-04
4: TRAIN [0][100/229]	Time 0.340 (0.365)	Data 9.63e-05 (9.67e-05)	Tok/s 6315 (10580)	Loss/tok 7.5291 (8.6746)	LR 2.047e-04
5: TRAIN [0][110/229]	Time 0.356 (0.365)	Data 7.94e-05 (7.93e-05)	Tok/s 10247 (10535)	Loss/tok 7.6777 (8.5931)	LR 2.576e-04
2: TRAIN [0][110/229]	Time 0.356 (0.365)	Data 1.15e-04 (1.02e-04)	Tok/s 9968 (10540)	Loss/tok 7.6507 (8.6023)	LR 2.576e-04
4: TRAIN [0][110/229]	Time 0.358 (0.365)	Data 9.54e-05 (9.66e-05)	Tok/s 9837 (10537)	Loss/tok 7.6709 (8.5970)	LR 2.576e-04
3: TRAIN [0][110/229]	Time 0.358 (0.365)	Data 8.08e-05 (8.89e-05)	Tok/s 10017 (10534)	Loss/tok 7.6624 (8.6021)	LR 2.576e-04
6: TRAIN [0][110/229]	Time 0.357 (0.365)	Data 8.46e-05 (8.75e-05)	Tok/s 10304 (10566)	Loss/tok 7.6335 (8.5956)	LR 2.576e-04
1: TRAIN [0][110/229]	Time 0.356 (0.365)	Data 1.12e-04 (8.97e-05)	Tok/s 10222 (10530)	Loss/tok 7.6534 (8.5956)	LR 2.576e-04
0: TRAIN [0][110/229]	Time 0.357 (0.365)	Data 7.53e-05 (8.60e-05)	Tok/s 10338 (10542)	Loss/tok 7.7189 (8.5948)	LR 2.576e-04
7: TRAIN [0][110/229]	Time 0.355 (0.365)	Data 1.15e-04 (9.39e-05)	Tok/s 10301 (10534)	Loss/tok 7.7374 (8.5991)	LR 2.576e-04
5: TRAIN [0][120/229]	Time 0.407 (0.366)	Data 7.22e-05 (7.94e-05)	Tok/s 16292 (10621)	Loss/tok 7.9100 (8.5188)	LR 3.244e-04
2: TRAIN [0][120/229]	Time 0.407 (0.366)	Data 8.77e-05 (1.01e-04)	Tok/s 16035 (10622)	Loss/tok 7.8604 (8.5256)	LR 3.244e-04
4: TRAIN [0][120/229]	Time 0.408 (0.366)	Data 9.27e-05 (9.64e-05)	Tok/s 15910 (10618)	Loss/tok 7.9484 (8.5213)	LR 3.244e-04
3: TRAIN [0][120/229]	Time 0.408 (0.366)	Data 8.18e-05 (8.92e-05)	Tok/s 16177 (10616)	Loss/tok 7.8745 (8.5240)	LR 3.244e-04
6: TRAIN [0][120/229]	Time 0.406 (0.366)	Data 1.03e-04 (8.70e-05)	Tok/s 16026 (10644)	Loss/tok 7.9207 (8.5177)	LR 3.244e-04
1: TRAIN [0][120/229]	Time 0.407 (0.366)	Data 9.08e-05 (8.94e-05)	Tok/s 15647 (10613)	Loss/tok 7.9265 (8.5203)	LR 3.244e-04
0: TRAIN [0][120/229]	Time 0.407 (0.366)	Data 7.61e-05 (8.58e-05)	Tok/s 16093 (10622)	Loss/tok 7.9587 (8.5191)	LR 3.244e-04
7: TRAIN [0][120/229]	Time 0.406 (0.366)	Data 1.10e-04 (9.44e-05)	Tok/s 16280 (10614)	Loss/tok 7.9147 (8.5240)	LR 3.244e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
3: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 8.75e-05 (8.92e-05)	Tok/s 12889 (10556)	Loss/tok 7.8217 (8.4660)	LR 4.083e-04
4: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 9.01e-05 (9.63e-05)	Tok/s 12803 (10565)	Loss/tok 7.7433 (8.4613)	LR 4.083e-04
5: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 8.44e-05 (7.97e-05)	Tok/s 13118 (10569)	Loss/tok 7.8220 (8.4585)	LR 4.083e-04
2: TRAIN [0][130/229]	Time 0.391 (0.366)	Data 9.42e-05 (1.01e-04)	Tok/s 13035 (10572)	Loss/tok 7.7464 (8.4630)	LR 4.083e-04
6: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 8.94e-05 (8.66e-05)	Tok/s 12892 (10587)	Loss/tok 7.7027 (8.4590)	LR 4.083e-04
1: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 8.42e-05 (8.92e-05)	Tok/s 13180 (10558)	Loss/tok 7.7132 (8.4614)	LR 4.083e-04
0: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 8.87e-05 (8.59e-05)	Tok/s 12862 (10566)	Loss/tok 7.7735 (8.4604)	LR 4.083e-04
7: TRAIN [0][130/229]	Time 0.390 (0.366)	Data 9.85e-05 (9.42e-05)	Tok/s 13087 (10555)	Loss/tok 7.8825 (8.4649)	LR 4.083e-04
6: TRAIN [0][140/229]	Time 0.355 (0.365)	Data 8.56e-05 (8.62e-05)	Tok/s 10258 (10544)	Loss/tok 7.6384 (8.4019)	LR 5.141e-04
0: TRAIN [0][140/229]	Time 0.356 (0.365)	Data 8.99e-05 (8.57e-05)	Tok/s 10059 (10518)	Loss/tok 7.6584 (8.4043)	LR 5.141e-04
1: TRAIN [0][140/229]	Time 0.355 (0.365)	Data 9.13e-05 (8.89e-05)	Tok/s 10222 (10516)	Loss/tok 7.5796 (8.4053)	LR 5.141e-04
5: TRAIN [0][140/229]	Time 0.355 (0.365)	Data 7.46e-05 (7.96e-05)	Tok/s 10015 (10519)	Loss/tok 7.7003 (8.4040)	LR 5.141e-04
7: TRAIN [0][140/229]	Time 0.356 (0.365)	Data 7.34e-05 (9.35e-05)	Tok/s 9947 (10507)	Loss/tok 7.6075 (8.4110)	LR 5.141e-04
2: TRAIN [0][140/229]	Time 0.355 (0.365)	Data 1.13e-04 (1.00e-04)	Tok/s 10245 (10526)	Loss/tok 7.6614 (8.4075)	LR 5.141e-04
3: TRAIN [0][140/229]	Time 0.355 (0.365)	Data 7.82e-05 (8.89e-05)	Tok/s 10427 (10509)	Loss/tok 7.5483 (8.4092)	LR 5.141e-04
4: TRAIN [0][140/229]	Time 0.355 (0.365)	Data 8.23e-05 (9.61e-05)	Tok/s 9994 (10511)	Loss/tok 7.6157 (8.4038)	LR 5.141e-04
4: TRAIN [0][150/229]	Time 0.389 (0.366)	Data 1.07e-04 (9.59e-05)	Tok/s 12990 (10573)	Loss/tok 7.6583 (8.3489)	LR 6.472e-04
3: TRAIN [0][150/229]	Time 0.389 (0.366)	Data 9.25e-05 (8.89e-05)	Tok/s 12897 (10572)	Loss/tok 7.6733 (8.3558)	LR 6.472e-04
5: TRAIN [0][150/229]	Time 0.390 (0.366)	Data 1.02e-04 (7.96e-05)	Tok/s 13016 (10582)	Loss/tok 7.6317 (8.3494)	LR 6.472e-04
2: TRAIN [0][150/229]	Time 0.390 (0.366)	Data 9.42e-05 (1.00e-04)	Tok/s 13099 (10585)	Loss/tok 7.6655 (8.3537)	LR 6.472e-04
6: TRAIN [0][150/229]	Time 0.390 (0.366)	Data 8.01e-05 (8.59e-05)	Tok/s 13040 (10606)	Loss/tok 7.6242 (8.3495)	LR 6.472e-04
1: TRAIN [0][150/229]	Time 0.390 (0.366)	Data 8.89e-05 (8.89e-05)	Tok/s 12644 (10574)	Loss/tok 7.7050 (8.3502)	LR 6.472e-04
0: TRAIN [0][150/229]	Time 0.390 (0.366)	Data 7.72e-05 (8.58e-05)	Tok/s 12825 (10578)	Loss/tok 7.6971 (8.3489)	LR 6.472e-04
7: TRAIN [0][150/229]	Time 0.390 (0.366)	Data 8.39e-05 (9.30e-05)	Tok/s 12913 (10573)	Loss/tok 7.7064 (8.3565)	LR 6.472e-04
0: TRAIN [0][160/229]	Time 0.395 (0.365)	Data 9.63e-05 (8.60e-05)	Tok/s 16923 (10511)	Loss/tok 7.8665 (8.3070)	LR 8.148e-04
7: TRAIN [0][160/229]	Time 0.395 (0.365)	Data 6.99e-05 (9.23e-05)	Tok/s 16390 (10502)	Loss/tok 7.9121 (8.3162)	LR 8.148e-04
6: TRAIN [0][160/229]	Time 0.396 (0.365)	Data 8.08e-05 (8.57e-05)	Tok/s 16476 (10530)	Loss/tok 7.8764 (8.3086)	LR 8.148e-04
1: TRAIN [0][160/229]	Time 0.396 (0.365)	Data 8.03e-05 (8.87e-05)	Tok/s 16259 (10504)	Loss/tok 7.7950 (8.3087)	LR 8.148e-04
5: TRAIN [0][160/229]	Time 0.396 (0.365)	Data 7.77e-05 (7.97e-05)	Tok/s 16420 (10506)	Loss/tok 7.8497 (8.3067)	LR 8.148e-04
2: TRAIN [0][160/229]	Time 0.396 (0.365)	Data 8.99e-05 (1.00e-04)	Tok/s 16361 (10510)	Loss/tok 7.7562 (8.3109)	LR 8.148e-04
4: TRAIN [0][160/229]	Time 0.396 (0.365)	Data 9.63e-05 (9.59e-05)	Tok/s 16194 (10499)	Loss/tok 7.7463 (8.3048)	LR 8.148e-04
3: TRAIN [0][160/229]	Time 0.396 (0.365)	Data 8.51e-05 (8.90e-05)	Tok/s 16684 (10502)	Loss/tok 7.8982 (8.3143)	LR 8.148e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
4: TRAIN [0][170/229]	Time 0.335 (0.365)	Data 9.06e-05 (9.58e-05)	Tok/s 10810 (10408)	Loss/tok 7.6836 (8.2640)	LR 1.026e-03
3: TRAIN [0][170/229]	Time 0.335 (0.365)	Data 8.58e-05 (8.89e-05)	Tok/s 11245 (10413)	Loss/tok 7.8032 (8.2751)	LR 1.026e-03
5: TRAIN [0][170/229]	Time 0.336 (0.365)	Data 7.68e-05 (7.96e-05)	Tok/s 10824 (10416)	Loss/tok 7.6510 (8.2657)	LR 1.026e-03
2: TRAIN [0][170/229]	Time 0.336 (0.365)	Data 9.63e-05 (9.98e-05)	Tok/s 11003 (10421)	Loss/tok 7.6437 (8.2702)	LR 1.026e-03
6: TRAIN [0][170/229]	Time 0.336 (0.365)	Data 8.51e-05 (8.54e-05)	Tok/s 10654 (10434)	Loss/tok 7.6055 (8.2690)	LR 1.026e-03
1: TRAIN [0][170/229]	Time 0.336 (0.365)	Data 9.01e-05 (8.85e-05)	Tok/s 10515 (10410)	Loss/tok 7.6548 (8.2685)	LR 1.026e-03
0: TRAIN [0][170/229]	Time 0.336 (0.365)	Data 8.15e-05 (8.60e-05)	Tok/s 10971 (10421)	Loss/tok 7.7257 (8.2706)	LR 1.026e-03
7: TRAIN [0][170/229]	Time 0.336 (0.365)	Data 8.20e-05 (9.15e-05)	Tok/s 10896 (10408)	Loss/tok 7.7233 (8.2768)	LR 1.026e-03
4: TRAIN [0][180/229]	Time 0.362 (0.365)	Data 8.85e-05 (9.59e-05)	Tok/s 9818 (10355)	Loss/tok 7.4140 (8.2304)	LR 1.291e-03
5: TRAIN [0][180/229]	Time 0.363 (0.365)	Data 7.37e-05 (7.96e-05)	Tok/s 10074 (10361)	Loss/tok 7.4826 (8.2337)	LR 1.291e-03
3: TRAIN [0][180/229]	Time 0.362 (0.365)	Data 7.63e-05 (8.88e-05)	Tok/s 9782 (10361)	Loss/tok 7.4308 (8.2414)	LR 1.291e-03
6: TRAIN [0][180/229]	Time 0.365 (0.365)	Data 7.68e-05 (8.53e-05)	Tok/s 9767 (10377)	Loss/tok 7.3341 (8.2381)	LR 1.291e-03
1: TRAIN [0][180/229]	Time 0.364 (0.365)	Data 8.70e-05 (8.83e-05)	Tok/s 9781 (10351)	Loss/tok 7.4913 (8.2354)	LR 1.291e-03
0: TRAIN [0][180/229]	Time 0.364 (0.365)	Data 7.58e-05 (8.59e-05)	Tok/s 10091 (10369)	Loss/tok 7.5126 (8.2374)	LR 1.291e-03
7: TRAIN [0][180/229]	Time 0.364 (0.365)	Data 8.03e-05 (9.13e-05)	Tok/s 10079 (10356)	Loss/tok 7.4546 (8.2417)	LR 1.291e-03
2: TRAIN [0][180/229]	Time 0.365 (0.365)	Data 8.11e-05 (9.95e-05)	Tok/s 9924 (10369)	Loss/tok 7.5216 (8.2386)	LR 1.291e-03
0: TRAIN [0][190/229]	Time 0.330 (0.364)	Data 7.49e-05 (8.55e-05)	Tok/s 3146 (10234)	Loss/tok 6.9632 (8.2062)	LR 1.626e-03
7: TRAIN [0][190/229]	Time 0.329 (0.364)	Data 7.58e-05 (9.08e-05)	Tok/s 3275 (10224)	Loss/tok 6.7068 (8.2094)	LR 1.626e-03
6: TRAIN [0][190/229]	Time 0.331 (0.364)	Data 8.23e-05 (8.52e-05)	Tok/s 3230 (10243)	Loss/tok 6.7540 (8.2076)	LR 1.626e-03
1: TRAIN [0][190/229]	Time 0.331 (0.364)	Data 7.72e-05 (8.79e-05)	Tok/s 3162 (10213)	Loss/tok 6.5929 (8.2049)	LR 1.626e-03
5: TRAIN [0][190/229]	Time 0.332 (0.364)	Data 7.77e-05 (7.94e-05)	Tok/s 3342 (10227)	Loss/tok 6.7445 (8.2021)	LR 1.626e-03
2: TRAIN [0][190/229]	Time 0.332 (0.364)	Data 8.99e-05 (9.92e-05)	Tok/s 3168 (10234)	Loss/tok 6.5063 (8.2075)	LR 1.626e-03
4: TRAIN [0][190/229]	Time 0.333 (0.364)	Data 1.08e-04 (9.56e-05)	Tok/s 3247 (10220)	Loss/tok 6.7276 (8.1997)	LR 1.626e-03
3: TRAIN [0][190/229]	Time 0.333 (0.364)	Data 8.61e-05 (8.86e-05)	Tok/s 3297 (10229)	Loss/tok 6.8725 (8.2105)	LR 1.626e-03
0: TRAIN [0][200/229]	Time 0.357 (0.365)	Data 7.51e-05 (8.55e-05)	Tok/s 10163 (10330)	Loss/tok 7.1158 (8.1629)	LR 2.000e-03
7: TRAIN [0][200/229]	Time 0.357 (0.365)	Data 1.18e-04 (9.06e-05)	Tok/s 10112 (10321)	Loss/tok 7.1860 (8.1682)	LR 2.000e-03
6: TRAIN [0][200/229]	Time 0.358 (0.365)	Data 8.65e-05 (8.52e-05)	Tok/s 10280 (10341)	Loss/tok 7.1256 (8.1672)	LR 2.000e-03
1: TRAIN [0][200/229]	Time 0.358 (0.365)	Data 9.25e-05 (8.79e-05)	Tok/s 10017 (10315)	Loss/tok 7.1222 (8.1629)	LR 2.000e-03
5: TRAIN [0][200/229]	Time 0.360 (0.365)	Data 7.58e-05 (7.95e-05)	Tok/s 10074 (10330)	Loss/tok 7.2061 (8.1597)	LR 2.000e-03
2: TRAIN [0][200/229]	Time 0.360 (0.365)	Data 9.39e-05 (9.91e-05)	Tok/s 10046 (10330)	Loss/tok 7.2730 (8.1646)	LR 2.000e-03
3: TRAIN [0][200/229]	Time 0.359 (0.365)	Data 8.70e-05 (8.87e-05)	Tok/s 9980 (10329)	Loss/tok 7.1647 (8.1664)	LR 2.000e-03
4: TRAIN [0][200/229]	Time 0.359 (0.365)	Data 8.70e-05 (9.53e-05)	Tok/s 10209 (10319)	Loss/tok 7.2487 (8.1551)	LR 2.000e-03
5: TRAIN [0][210/229]	Time 0.383 (0.365)	Data 7.25e-05 (7.94e-05)	Tok/s 13239 (10369)	Loss/tok 7.1298 (8.1087)	LR 2.000e-03
2: TRAIN [0][210/229]	Time 0.383 (0.365)	Data 1.16e-04 (9.88e-05)	Tok/s 13116 (10370)	Loss/tok 7.2634 (8.1140)	LR 2.000e-03
3: TRAIN [0][210/229]	Time 0.384 (0.365)	Data 8.20e-05 (8.87e-05)	Tok/s 13235 (10366)	Loss/tok 7.1422 (8.1153)	LR 2.000e-03
6: TRAIN [0][210/229]	Time 0.383 (0.365)	Data 8.75e-05 (8.51e-05)	Tok/s 13005 (10377)	Loss/tok 7.1203 (8.1170)	LR 2.000e-03
1: TRAIN [0][210/229]	Time 0.383 (0.365)	Data 8.89e-05 (8.77e-05)	Tok/s 12935 (10352)	Loss/tok 7.2488 (8.1104)	LR 2.000e-03
4: TRAIN [0][210/229]	Time 0.384 (0.365)	Data 1.18e-04 (9.51e-05)	Tok/s 13260 (10354)	Loss/tok 7.2625 (8.1057)	LR 2.000e-03
0: TRAIN [0][210/229]	Time 0.384 (0.365)	Data 7.89e-05 (8.53e-05)	Tok/s 13196 (10364)	Loss/tok 7.1004 (8.1120)	LR 2.000e-03
7: TRAIN [0][210/229]	Time 0.383 (0.365)	Data 8.37e-05 (9.02e-05)	Tok/s 12915 (10353)	Loss/tok 7.2124 (8.1185)	LR 2.000e-03
0: TRAIN [0][220/229]	Time 0.363 (0.365)	Data 7.25e-05 (8.53e-05)	Tok/s 9794 (10371)	Loss/tok 6.8981 (8.0613)	LR 2.000e-03
6: TRAIN [0][220/229]	Time 0.364 (0.365)	Data 8.82e-05 (8.50e-05)	Tok/s 9921 (10382)	Loss/tok 7.0224 (8.0655)	LR 2.000e-03
7: TRAIN [0][220/229]	Time 0.363 (0.365)	Data 7.51e-05 (9.00e-05)	Tok/s 9734 (10358)	Loss/tok 6.8628 (8.0666)	LR 2.000e-03
1: TRAIN [0][220/229]	Time 0.364 (0.365)	Data 8.27e-05 (8.76e-05)	Tok/s 9700 (10358)	Loss/tok 6.7864 (8.0588)	LR 2.000e-03
5: TRAIN [0][220/229]	Time 0.364 (0.365)	Data 7.75e-05 (7.94e-05)	Tok/s 10003 (10374)	Loss/tok 6.9026 (8.0557)	LR 2.000e-03
2: TRAIN [0][220/229]	Time 0.365 (0.365)	Data 9.37e-05 (9.85e-05)	Tok/s 9918 (10377)	Loss/tok 6.9185 (8.0639)	LR 2.000e-03
4: TRAIN [0][220/229]	Time 0.364 (0.365)	Data 9.61e-05 (9.48e-05)	Tok/s 9738 (10362)	Loss/tok 6.7565 (8.0520)	LR 2.000e-03
3: TRAIN [0][220/229]	Time 0.364 (0.365)	Data 8.34e-05 (8.89e-05)	Tok/s 9997 (10372)	Loss/tok 6.7187 (8.0634)	LR 2.000e-03
5: Running validation on dev set
0: Running validation on dev set
3: Running validation on dev set
7: Running validation on dev set
4: Running validation on dev set
6: Running validation on dev set
2: Running validation on dev set
5: Executing preallocation
4: Executing preallocation
3: Executing preallocation
7: Executing preallocation
2: Executing preallocation
0: Executing preallocation
6: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
4: VALIDATION [0][0/27]	Time 0.041 (0.000)	Data 1.53e-03 (0.00e+00)	Tok/s 76193 (0)	Loss/tok 7.6761 (7.6761)
7: VALIDATION [0][0/26]	Time 0.038 (0.000)	Data 1.51e-03 (0.00e+00)	Tok/s 76355 (0)	Loss/tok 7.8531 (7.8531)
5: VALIDATION [0][0/26]	Time 0.042 (0.000)	Data 1.45e-03 (0.00e+00)	Tok/s 72900 (0)	Loss/tok 7.6490 (7.6490)
6: VALIDATION [0][0/26]	Time 0.042 (0.000)	Data 1.55e-03 (0.00e+00)	Tok/s 70779 (0)	Loss/tok 7.7535 (7.7535)
3: VALIDATION [0][0/27]	Time 0.045 (0.000)	Data 1.47e-03 (0.00e+00)	Tok/s 73637 (0)	Loss/tok 7.7780 (7.7780)
1: VALIDATION [0][0/27]	Time 0.051 (0.000)	Data 1.71e-03 (0.00e+00)	Tok/s 72414 (0)	Loss/tok 7.8955 (7.8955)
2: VALIDATION [0][0/27]	Time 0.048 (0.000)	Data 1.47e-03 (0.00e+00)	Tok/s 71993 (0)	Loss/tok 7.7191 (7.7191)
0: VALIDATION [0][0/27]	Time 0.070 (0.000)	Data 1.50e-03 (0.00e+00)	Tok/s 63229 (0)	Loss/tok 7.7953 (7.7953)
4: VALIDATION [0][10/27]	Time 0.020 (0.026)	Data 1.24e-03 (1.27e-03)	Tok/s 70537 (72411)	Loss/tok 7.4859 (7.6088)
7: VALIDATION [0][10/26]	Time 0.021 (0.026)	Data 1.22e-03 (1.27e-03)	Tok/s 67560 (71822)	Loss/tok 7.5310 (7.6023)
5: VALIDATION [0][10/26]	Time 0.020 (0.026)	Data 1.23e-03 (1.25e-03)	Tok/s 70101 (72144)	Loss/tok 7.5649 (7.5771)
6: VALIDATION [0][10/26]	Time 0.021 (0.026)	Data 1.32e-03 (1.50e-03)	Tok/s 67504 (71400)	Loss/tok 7.4938 (7.5807)
3: VALIDATION [0][10/27]	Time 0.021 (0.027)	Data 1.26e-03 (1.27e-03)	Tok/s 67376 (71816)	Loss/tok 7.5241 (7.6138)
1: VALIDATION [0][10/27]	Time 0.021 (0.027)	Data 1.29e-03 (1.34e-03)	Tok/s 70092 (72585)	Loss/tok 7.2897 (7.5994)
2: VALIDATION [0][10/27]	Time 0.021 (0.027)	Data 1.24e-03 (1.26e-03)	Tok/s 70099 (72114)	Loss/tok 7.4284 (7.5675)
0: VALIDATION [0][10/27]	Time 0.021 (0.027)	Data 1.22e-03 (1.24e-03)	Tok/s 70243 (73188)	Loss/tok 7.4760 (7.6325)
7: VALIDATION [0][20/26]	Time 0.013 (0.021)	Data 1.21e-03 (1.25e-03)	Tok/s 58498 (68554)	Loss/tok 7.0320 (7.5120)
4: VALIDATION [0][20/27]	Time 0.013 (0.021)	Data 1.23e-03 (1.25e-03)	Tok/s 61162 (68953)	Loss/tok 7.1808 (7.5167)
5: VALIDATION [0][20/26]	Time 0.013 (0.021)	Data 1.21e-03 (1.23e-03)	Tok/s 59791 (69064)	Loss/tok 7.3198 (7.4887)
6: VALIDATION [0][20/26]	Time 0.013 (0.021)	Data 1.25e-03 (1.38e-03)	Tok/s 61231 (68543)	Loss/tok 7.0964 (7.5064)
3: VALIDATION [0][20/27]	Time 0.013 (0.021)	Data 1.23e-03 (1.25e-03)	Tok/s 61504 (68417)	Loss/tok 7.2992 (7.5346)
1: VALIDATION [0][20/27]	Time 0.013 (0.022)	Data 1.23e-03 (1.30e-03)	Tok/s 61868 (69075)	Loss/tok 7.3879 (7.5171)
2: VALIDATION [0][20/27]	Time 0.013 (0.022)	Data 1.22e-03 (1.25e-03)	Tok/s 62175 (68473)	Loss/tok 7.1380 (7.5014)
0: VALIDATION [0][20/27]	Time 0.013 (0.022)	Data 1.19e-03 (1.22e-03)	Tok/s 62248 (69959)	Loss/tok 7.2952 (7.5344)
0: Saving model to gnmt/model_best.pth
4: Running evaluation on test set
7: Running evaluation on test set
6: Running evaluation on test set
1: Running evaluation on test set
5: Running evaluation on test set
0: Running evaluation on test set
3: Running evaluation on test set
2: Running evaluation on test set
4: TEST [0][9/16]	Time 0.4836 (0.5423)	Decoder iters 149.0 (149.0)	Tok/s 6979 (7157)
1: TEST [0][9/16]	Time 0.4844 (0.5423)	Decoder iters 149.0 (149.0)	Tok/s 5590 (7393)
0: TEST [0][9/16]	Time 0.4837 (0.5423)	Decoder iters 149.0 (149.0)	Tok/s 5869 (7566)
6: TEST [0][9/16]	Time 0.4834 (0.5424)	Decoder iters 149.0 (149.0)	Tok/s 5225 (6662)
7: TEST [0][9/16]	Time 0.4835 (0.5425)	Decoder iters 149.0 (149.0)	Tok/s 5977 (7062)
5: TEST [0][9/16]	Time 0.4833 (0.5424)	Decoder iters 149.0 (149.0)	Tok/s 5481 (7062)
2: TEST [0][9/16]	Time 0.4846 (0.5424)	Decoder iters 149.0 (149.0)	Tok/s 6222 (7230)
3: TEST [0][9/16]	Time 0.4843 (0.5424)	Decoder iters 149.0 (149.0)	Tok/s 5703 (7408)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
7: Finished evaluation on test set
4: Finished evaluation on test set
5: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished evaluation on test set
3: Finished evaluation on test set
2: Finished evaluation on test set
6: Finished evaluation on test set
4: Finished epoch 0
1: Finished epoch 0
5: Finished epoch 0
4: Starting epoch 1
1: Starting epoch 1
7: Finished epoch 0
5: Starting epoch 1
7: Starting epoch 1
3: Finished epoch 0
2: Finished epoch 0
3: Starting epoch 1
2: Starting epoch 1
6: Finished epoch 0
4: Executing preallocation
6: Starting epoch 1
7: Executing preallocation
5: Executing preallocation
1: Executing preallocation
3: Executing preallocation
2: Executing preallocation
6: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 8.0166	Validation Loss: 7.4670	Test BLEU: 0.12
0: Performance: Epoch: 0	Training: 83384 Tok/s	Validation: 524239 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
5: Sampler for epoch 1 uses seed 2258090960
0: Sampler for epoch 1 uses seed 2258090960
6: Sampler for epoch 1 uses seed 2258090960
7: Sampler for epoch 1 uses seed 2258090960
4: Sampler for epoch 1 uses seed 2258090960
3: Sampler for epoch 1 uses seed 2258090960
1: Sampler for epoch 1 uses seed 2258090960
2: Sampler for epoch 1 uses seed 2258090960
7: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 1.23e-01 (0.00e+00)	Tok/s 12569 (0)	Loss/tok 7.0047 (7.0047)	LR 2.000e-03
0: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 9.27e-02 (0.00e+00)	Tok/s 12681 (0)	Loss/tok 7.0125 (7.0125)	LR 2.000e-03
6: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 1.16e-01 (0.00e+00)	Tok/s 12575 (0)	Loss/tok 6.9818 (6.9818)	LR 2.000e-03
1: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 1.28e-01 (0.00e+00)	Tok/s 12778 (0)	Loss/tok 6.9616 (6.9616)	LR 2.000e-03
2: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 1.37e-01 (0.00e+00)	Tok/s 12640 (0)	Loss/tok 7.0195 (7.0195)	LR 2.000e-03
5: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 9.23e-02 (0.00e+00)	Tok/s 12738 (0)	Loss/tok 7.0469 (7.0469)	LR 2.000e-03
4: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 1.17e-01 (0.00e+00)	Tok/s 12778 (0)	Loss/tok 7.1302 (7.1302)	LR 2.000e-03
3: TRAIN [1][0/229]	Time 0.514 (0.000)	Data 1.27e-01 (0.00e+00)	Tok/s 12723 (0)	Loss/tok 6.9268 (6.9268)	LR 2.000e-03
0: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 9.68e-05 (1.03e-04)	Tok/s 10097 (12008)	Loss/tok 6.5879 (6.8646)	LR 2.000e-03
7: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 7.63e-05 (8.02e-05)	Tok/s 9970 (11875)	Loss/tok 6.5070 (6.8850)	LR 2.000e-03
6: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 7.70e-05 (8.13e-05)	Tok/s 9893 (11937)	Loss/tok 6.6300 (6.8818)	LR 2.000e-03
1: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 8.30e-05 (8.77e-05)	Tok/s 10089 (12054)	Loss/tok 6.5976 (6.8438)	LR 2.000e-03
2: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 7.68e-05 (8.87e-05)	Tok/s 9951 (12017)	Loss/tok 6.5870 (6.8833)	LR 2.000e-03
5: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 8.15e-05 (9.00e-05)	Tok/s 10237 (12017)	Loss/tok 6.5595 (6.8691)	LR 2.000e-03
4: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 9.32e-05 (9.60e-05)	Tok/s 10008 (11951)	Loss/tok 6.6136 (6.8823)	LR 2.000e-03
3: TRAIN [1][10/229]	Time 0.357 (0.369)	Data 7.56e-05 (8.24e-05)	Tok/s 9988 (12007)	Loss/tok 6.6528 (6.8481)	LR 2.000e-03
0: TRAIN [1][20/229]	Time 0.361 (0.370)	Data 9.51e-05 (1.01e-04)	Tok/s 9893 (12171)	Loss/tok 6.3692 (6.7641)	LR 2.000e-03
7: TRAIN [1][20/229]	Time 0.361 (0.370)	Data 8.06e-05 (8.18e-05)	Tok/s 9934 (12131)	Loss/tok 6.4246 (6.7886)	LR 2.000e-03
6: TRAIN [1][20/229]	Time 0.361 (0.370)	Data 8.18e-05 (8.20e-05)	Tok/s 9950 (12150)	Loss/tok 6.3437 (6.7794)	LR 2.000e-03
1: TRAIN [1][20/229]	Time 0.362 (0.370)	Data 8.49e-05 (8.57e-05)	Tok/s 10037 (12192)	Loss/tok 6.4802 (6.7772)	LR 2.000e-03
5: TRAIN [1][20/229]	Time 0.362 (0.370)	Data 9.47e-05 (9.03e-05)	Tok/s 9883 (12177)	Loss/tok 6.5291 (6.7897)	LR 2.000e-03
2: TRAIN [1][20/229]	Time 0.362 (0.370)	Data 8.06e-05 (8.93e-05)	Tok/s 9757 (12219)	Loss/tok 6.4657 (6.7799)	LR 2.000e-03
4: TRAIN [1][20/229]	Time 0.361 (0.370)	Data 1.07e-04 (9.74e-05)	Tok/s 10101 (12136)	Loss/tok 6.4079 (6.7837)	LR 2.000e-03
3: TRAIN [1][20/229]	Time 0.361 (0.370)	Data 7.84e-05 (7.94e-05)	Tok/s 9700 (12165)	Loss/tok 6.3321 (6.7547)	LR 2.000e-03
0: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 9.56e-05 (1.00e-04)	Tok/s 10018 (11129)	Loss/tok 6.3471 (6.6649)	LR 2.000e-03
7: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 7.53e-05 (8.05e-05)	Tok/s 10428 (11107)	Loss/tok 6.2554 (6.6897)	LR 2.000e-03
6: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 7.18e-05 (8.26e-05)	Tok/s 9969 (11128)	Loss/tok 6.2457 (6.6757)	LR 2.000e-03
1: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 8.51e-05 (8.49e-05)	Tok/s 10205 (11142)	Loss/tok 6.3242 (6.6657)	LR 2.000e-03
2: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 7.99e-05 (8.90e-05)	Tok/s 10313 (11171)	Loss/tok 6.4563 (6.6775)	LR 2.000e-03
5: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 1.03e-04 (8.99e-05)	Tok/s 10077 (11138)	Loss/tok 6.3088 (6.6842)	LR 2.000e-03
4: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 9.44e-05 (9.63e-05)	Tok/s 10137 (11111)	Loss/tok 6.2735 (6.6709)	LR 2.000e-03
3: TRAIN [1][30/229]	Time 0.356 (0.363)	Data 7.89e-05 (7.81e-05)	Tok/s 10497 (11131)	Loss/tok 6.3951 (6.6729)	LR 2.000e-03
0: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 9.75e-05 (9.91e-05)	Tok/s 10277 (10758)	Loss/tok 6.2621 (6.6049)	LR 2.000e-03
7: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 7.15e-05 (7.90e-05)	Tok/s 10365 (10746)	Loss/tok 6.2433 (6.6120)	LR 2.000e-03
1: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 8.94e-05 (8.38e-05)	Tok/s 10301 (10780)	Loss/tok 6.4061 (6.6083)	LR 2.000e-03
5: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 7.89e-05 (8.89e-05)	Tok/s 10261 (10763)	Loss/tok 6.3007 (6.6151)	LR 2.000e-03
2: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 7.70e-05 (8.92e-05)	Tok/s 10211 (10782)	Loss/tok 6.2616 (6.6080)	LR 2.000e-03
6: TRAIN [1][40/229]	Time 0.354 (0.362)	Data 7.53e-05 (8.12e-05)	Tok/s 10399 (10759)	Loss/tok 6.2490 (6.6069)	LR 2.000e-03
4: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 9.25e-05 (9.46e-05)	Tok/s 10148 (10742)	Loss/tok 6.2661 (6.6008)	LR 2.000e-03
3: TRAIN [1][40/229]	Time 0.354 (0.361)	Data 7.70e-05 (7.72e-05)	Tok/s 10189 (10757)	Loss/tok 6.2402 (6.6060)	LR 2.000e-03
0: TRAIN [1][50/229]	Time 0.340 (0.362)	Data 9.94e-05 (9.97e-05)	Tok/s 6186 (10708)	Loss/tok 5.8088 (6.5431)	LR 2.000e-03
7: TRAIN [1][50/229]	Time 0.340 (0.362)	Data 8.37e-05 (7.93e-05)	Tok/s 6492 (10739)	Loss/tok 5.7382 (6.5527)	LR 2.000e-03
1: TRAIN [1][50/229]	Time 0.340 (0.362)	Data 8.85e-05 (8.38e-05)	Tok/s 6421 (10766)	Loss/tok 5.6329 (6.5403)	LR 2.000e-03
6: TRAIN [1][50/229]	Time 0.341 (0.362)	Data 7.84e-05 (8.18e-05)	Tok/s 6347 (10714)	Loss/tok 5.9685 (6.5394)	LR 2.000e-03
5: TRAIN [1][50/229]	Time 0.341 (0.362)	Data 8.46e-05 (8.87e-05)	Tok/s 6389 (10729)	Loss/tok 5.8973 (6.5554)	LR 2.000e-03
2: TRAIN [1][50/229]	Time 0.341 (0.362)	Data 7.77e-05 (8.92e-05)	Tok/s 6361 (10745)	Loss/tok 5.8862 (6.5428)	LR 2.000e-03
3: TRAIN [1][50/229]	Time 0.341 (0.362)	Data 7.68e-05 (7.70e-05)	Tok/s 6455 (10735)	Loss/tok 5.7189 (6.5470)	LR 2.000e-03
4: TRAIN [1][50/229]	Time 0.341 (0.362)	Data 1.15e-04 (9.45e-05)	Tok/s 6379 (10709)	Loss/tok 5.8580 (6.5433)	LR 2.000e-03
0: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 1.03e-04 (1.00e-04)	Tok/s 6497 (10700)	Loss/tok 5.7798 (6.4789)	LR 2.000e-03
7: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 8.46e-05 (7.92e-05)	Tok/s 6503 (10731)	Loss/tok 5.7790 (6.4952)	LR 2.000e-03
6: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 7.37e-05 (8.19e-05)	Tok/s 6539 (10697)	Loss/tok 5.7688 (6.4799)	LR 2.000e-03
1: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 8.37e-05 (8.44e-05)	Tok/s 6358 (10738)	Loss/tok 5.5879 (6.4830)	LR 2.000e-03
5: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 7.92e-05 (8.86e-05)	Tok/s 6602 (10714)	Loss/tok 5.5185 (6.4888)	LR 2.000e-03
2: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 7.65e-05 (8.89e-05)	Tok/s 6781 (10747)	Loss/tok 5.5586 (6.4792)	LR 2.000e-03
4: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 1.01e-04 (9.43e-05)	Tok/s 6453 (10708)	Loss/tok 5.7248 (6.4858)	LR 2.000e-03
3: TRAIN [1][60/229]	Time 0.336 (0.361)	Data 7.89e-05 (7.70e-05)	Tok/s 6343 (10712)	Loss/tok 5.5219 (6.4873)	LR 2.000e-03
0: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 9.82e-05 (1.00e-04)	Tok/s 13225 (11070)	Loss/tok 6.1818 (6.4384)	LR 2.000e-03
7: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 8.11e-05 (7.93e-05)	Tok/s 13379 (11096)	Loss/tok 6.1241 (6.4498)	LR 2.000e-03
6: TRAIN [1][70/229]	Time 0.374 (0.363)	Data 1.31e-04 (8.21e-05)	Tok/s 13307 (11073)	Loss/tok 6.1830 (6.4398)	LR 2.000e-03
1: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 8.30e-05 (8.40e-05)	Tok/s 13200 (11103)	Loss/tok 6.2363 (6.4443)	LR 2.000e-03
5: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 8.51e-05 (8.86e-05)	Tok/s 13423 (11090)	Loss/tok 6.1461 (6.4433)	LR 2.000e-03
2: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 7.61e-05 (8.96e-05)	Tok/s 13416 (11107)	Loss/tok 6.1474 (6.4410)	LR 2.000e-03
4: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 8.94e-05 (9.36e-05)	Tok/s 13260 (11079)	Loss/tok 6.1143 (6.4371)	LR 2.000e-03
3: TRAIN [1][70/229]	Time 0.377 (0.363)	Data 7.77e-05 (7.69e-05)	Tok/s 13101 (11089)	Loss/tok 6.2245 (6.4430)	LR 2.000e-03
0: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 1.05e-04 (1.01e-04)	Tok/s 6294 (10914)	Loss/tok 5.5467 (6.3891)	LR 1.000e-03
7: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 7.75e-05 (7.97e-05)	Tok/s 6610 (10945)	Loss/tok 5.5552 (6.3984)	LR 1.000e-03
6: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 9.27e-05 (8.16e-05)	Tok/s 6408 (10918)	Loss/tok 5.5147 (6.3905)	LR 1.000e-03
1: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 8.49e-05 (8.40e-05)	Tok/s 6326 (10958)	Loss/tok 5.5468 (6.3947)	LR 1.000e-03
2: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 8.27e-05 (8.93e-05)	Tok/s 6627 (10964)	Loss/tok 5.5668 (6.3896)	LR 1.000e-03
5: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 8.23e-05 (8.82e-05)	Tok/s 6475 (10940)	Loss/tok 5.5577 (6.3961)	LR 1.000e-03
4: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 1.01e-04 (9.35e-05)	Tok/s 6358 (10923)	Loss/tok 5.5273 (6.3857)	LR 1.000e-03
3: TRAIN [1][80/229]	Time 0.336 (0.363)	Data 7.56e-05 (7.71e-05)	Tok/s 6508 (10941)	Loss/tok 5.6295 (6.3987)	LR 1.000e-03
7: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 7.39e-05 (8.01e-05)	Tok/s 6331 (10711)	Loss/tok 5.5237 (6.3420)	LR 1.000e-03
0: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 1.01e-04 (1.01e-04)	Tok/s 6764 (10688)	Loss/tok 5.3852 (6.3361)	LR 1.000e-03
6: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 9.13e-05 (8.16e-05)	Tok/s 6322 (10684)	Loss/tok 5.3258 (6.3376)	LR 1.000e-03
1: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 8.77e-05 (8.41e-05)	Tok/s 6320 (10718)	Loss/tok 5.4829 (6.3426)	LR 1.000e-03
5: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 7.94e-05 (8.77e-05)	Tok/s 6609 (10702)	Loss/tok 5.3767 (6.3405)	LR 1.000e-03
2: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 7.63e-05 (8.94e-05)	Tok/s 6639 (10725)	Loss/tok 5.3706 (6.3400)	LR 1.000e-03
4: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 9.16e-05 (9.29e-05)	Tok/s 6587 (10706)	Loss/tok 5.4366 (6.3306)	LR 1.000e-03
3: TRAIN [1][90/229]	Time 0.337 (0.361)	Data 8.51e-05 (7.76e-05)	Tok/s 6144 (10706)	Loss/tok 5.5695 (6.3474)	LR 1.000e-03
0: TRAIN [1][100/229]	Time 0.372 (0.362)	Data 8.70e-05 (1.01e-04)	Tok/s 13470 (10823)	Loss/tok 5.9248 (6.2861)	LR 1.000e-03
7: TRAIN [1][100/229]	Time 0.372 (0.362)	Data 7.92e-05 (8.06e-05)	Tok/s 13335 (10842)	Loss/tok 5.9624 (6.2879)	LR 1.000e-03
6: TRAIN [1][100/229]	Time 0.372 (0.362)	Data 7.82e-05 (8.12e-05)	Tok/s 13601 (10819)	Loss/tok 5.7931 (6.2844)	LR 1.000e-03
1: TRAIN [1][100/229]	Time 0.372 (0.362)	Data 8.96e-05 (8.45e-05)	Tok/s 13522 (10852)	Loss/tok 5.9670 (6.2909)	LR 1.000e-03
5: TRAIN [1][100/229]	Time 0.372 (0.362)	Data 8.08e-05 (8.79e-05)	Tok/s 13558 (10825)	Loss/tok 5.9390 (6.2913)	LR 1.000e-03
2: TRAIN [1][100/229]	Time 0.372 (0.362)	Data 7.89e-05 (8.96e-05)	Tok/s 13653 (10847)	Loss/tok 5.7962 (6.2850)	LR 1.000e-03
3: TRAIN [1][100/229]	Time 0.373 (0.362)	Data 8.15e-05 (7.77e-05)	Tok/s 13377 (10831)	Loss/tok 5.7550 (6.2948)	LR 1.000e-03
4: TRAIN [1][100/229]	Time 0.373 (0.362)	Data 9.01e-05 (9.23e-05)	Tok/s 13724 (10836)	Loss/tok 5.7718 (6.2751)	LR 1.000e-03
0: TRAIN [1][110/229]	Time 0.360 (0.362)	Data 9.16e-05 (1.00e-04)	Tok/s 10185 (10807)	Loss/tok 5.6841 (6.2434)	LR 1.000e-03
6: TRAIN [1][110/229]	Time 0.360 (0.362)	Data 8.03e-05 (8.14e-05)	Tok/s 10075 (10811)	Loss/tok 5.5747 (6.2446)	LR 1.000e-03
1: TRAIN [1][110/229]	Time 0.361 (0.362)	Data 9.13e-05 (8.45e-05)	Tok/s 9929 (10836)	Loss/tok 5.5735 (6.2444)	LR 1.000e-03
5: TRAIN [1][110/229]	Time 0.361 (0.362)	Data 8.61e-05 (8.81e-05)	Tok/s 9895 (10808)	Loss/tok 5.6535 (6.2471)	LR 1.000e-03
2: TRAIN [1][110/229]	Time 0.361 (0.362)	Data 9.51e-05 (8.99e-05)	Tok/s 9926 (10830)	Loss/tok 5.5799 (6.2410)	LR 1.000e-03
4: TRAIN [1][110/229]	Time 0.360 (0.362)	Data 9.58e-05 (9.20e-05)	Tok/s 10099 (10821)	Loss/tok 5.7175 (6.2338)	LR 1.000e-03
3: TRAIN [1][110/229]	Time 0.360 (0.362)	Data 7.89e-05 (7.77e-05)	Tok/s 9906 (10815)	Loss/tok 5.6560 (6.2479)	LR 1.000e-03
7: TRAIN [1][110/229]	Time 0.365 (0.362)	Data 9.01e-05 (8.05e-05)	Tok/s 9874 (10818)	Loss/tok 5.6090 (6.2438)	LR 1.000e-03
0: TRAIN [1][120/229]	Time 0.339 (0.361)	Data 8.46e-05 (9.98e-05)	Tok/s 6495 (10591)	Loss/tok 5.2447 (6.2023)	LR 5.000e-04
7: TRAIN [1][120/229]	Time 0.339 (0.361)	Data 7.61e-05 (8.12e-05)	Tok/s 6527 (10604)	Loss/tok 5.3352 (6.2051)	LR 5.000e-04
6: TRAIN [1][120/229]	Time 0.340 (0.361)	Data 7.99e-05 (8.12e-05)	Tok/s 6241 (10603)	Loss/tok 5.0903 (6.2037)	LR 5.000e-04
1: TRAIN [1][120/229]	Time 0.339 (0.361)	Data 8.75e-05 (8.45e-05)	Tok/s 6398 (10620)	Loss/tok 5.0173 (6.2025)	LR 5.000e-04
2: TRAIN [1][120/229]	Time 0.340 (0.361)	Data 7.44e-05 (9.03e-05)	Tok/s 6474 (10617)	Loss/tok 5.3849 (6.2017)	LR 5.000e-04
5: TRAIN [1][120/229]	Time 0.340 (0.361)	Data 8.23e-05 (8.78e-05)	Tok/s 6399 (10596)	Loss/tok 5.3476 (6.2063)	LR 5.000e-04
4: TRAIN [1][120/229]	Time 0.339 (0.361)	Data 9.56e-05 (9.17e-05)	Tok/s 6384 (10598)	Loss/tok 5.3063 (6.1950)	LR 5.000e-04
3: TRAIN [1][120/229]	Time 0.339 (0.361)	Data 7.82e-05 (7.76e-05)	Tok/s 6121 (10592)	Loss/tok 5.2062 (6.2068)	LR 5.000e-04
0: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 8.30e-05 (9.97e-05)	Tok/s 16486 (10719)	Loss/tok 5.8686 (6.1609)	LR 5.000e-04
7: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 8.39e-05 (8.13e-05)	Tok/s 16970 (10740)	Loss/tok 5.8578 (6.1631)	LR 5.000e-04
6: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 7.56e-05 (8.12e-05)	Tok/s 16573 (10731)	Loss/tok 5.8792 (6.1607)	LR 5.000e-04
1: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 8.82e-05 (8.46e-05)	Tok/s 16550 (10744)	Loss/tok 5.8536 (6.1607)	LR 5.000e-04
5: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 8.54e-05 (8.80e-05)	Tok/s 16597 (10730)	Loss/tok 5.8725 (6.1654)	LR 5.000e-04
2: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 8.96e-05 (9.04e-05)	Tok/s 16642 (10746)	Loss/tok 5.9562 (6.1605)	LR 5.000e-04
4: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 9.42e-05 (9.16e-05)	Tok/s 16668 (10731)	Loss/tok 5.8060 (6.1518)	LR 5.000e-04
3: TRAIN [1][130/229]	Time 0.393 (0.362)	Data 7.68e-05 (7.79e-05)	Tok/s 16461 (10722)	Loss/tok 5.8335 (6.1661)	LR 5.000e-04
0: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 8.32e-05 (9.90e-05)	Tok/s 6622 (10625)	Loss/tok 5.0685 (6.1189)	LR 5.000e-04
7: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 8.03e-05 (8.16e-05)	Tok/s 6268 (10641)	Loss/tok 4.9843 (6.1217)	LR 5.000e-04
6: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 8.18e-05 (8.12e-05)	Tok/s 6432 (10634)	Loss/tok 5.1043 (6.1229)	LR 5.000e-04
1: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 9.78e-05 (8.45e-05)	Tok/s 6344 (10643)	Loss/tok 5.0086 (6.1214)	LR 5.000e-04
2: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 8.68e-05 (9.06e-05)	Tok/s 6582 (10646)	Loss/tok 5.1193 (6.1214)	LR 5.000e-04
5: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 8.46e-05 (8.82e-05)	Tok/s 6350 (10632)	Loss/tok 5.0794 (6.1254)	LR 5.000e-04
4: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 1.13e-04 (9.18e-05)	Tok/s 6512 (10635)	Loss/tok 4.9971 (6.1133)	LR 5.000e-04
3: TRAIN [1][140/229]	Time 0.334 (0.361)	Data 8.87e-05 (7.79e-05)	Tok/s 6523 (10623)	Loss/tok 5.3587 (6.1257)	LR 5.000e-04
0: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 9.63e-05 (9.86e-05)	Tok/s 6053 (10526)	Loss/tok 5.1982 (6.0855)	LR 5.000e-04
7: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 8.87e-05 (8.17e-05)	Tok/s 6019 (10540)	Loss/tok 4.9133 (6.0859)	LR 5.000e-04
6: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 1.22e-04 (8.17e-05)	Tok/s 6099 (10537)	Loss/tok 4.8921 (6.0860)	LR 5.000e-04
1: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 8.75e-05 (8.45e-05)	Tok/s 6174 (10538)	Loss/tok 4.8931 (6.0840)	LR 5.000e-04
2: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 8.42e-05 (9.06e-05)	Tok/s 5919 (10543)	Loss/tok 5.1116 (6.0848)	LR 5.000e-04
5: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 1.20e-04 (8.86e-05)	Tok/s 6432 (10533)	Loss/tok 4.9489 (6.0881)	LR 5.000e-04
3: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 8.89e-05 (7.82e-05)	Tok/s 6513 (10527)	Loss/tok 5.2928 (6.0922)	LR 5.000e-04
4: TRAIN [1][150/229]	Time 0.348 (0.361)	Data 1.06e-04 (9.21e-05)	Tok/s 6389 (10534)	Loss/tok 5.2077 (6.0768)	LR 5.000e-04
0: TRAIN [1][160/229]	Time 0.390 (0.361)	Data 8.73e-05 (9.80e-05)	Tok/s 16714 (10617)	Loss/tok 5.7269 (6.0509)	LR 2.500e-04
7: TRAIN [1][160/229]	Time 0.390 (0.361)	Data 8.03e-05 (8.20e-05)	Tok/s 16545 (10631)	Loss/tok 5.8273 (6.0515)	LR 2.500e-04
6: TRAIN [1][160/229]	Time 0.391 (0.361)	Data 7.44e-05 (8.15e-05)	Tok/s 16881 (10629)	Loss/tok 5.8980 (6.0499)	LR 2.500e-04
1: TRAIN [1][160/229]	Time 0.391 (0.361)	Data 8.94e-05 (8.48e-05)	Tok/s 16836 (10632)	Loss/tok 5.8851 (6.0501)	LR 2.500e-04
5: TRAIN [1][160/229]	Time 0.390 (0.361)	Data 8.37e-05 (8.86e-05)	Tok/s 16756 (10623)	Loss/tok 5.7382 (6.0509)	LR 2.500e-04
2: TRAIN [1][160/229]	Time 0.391 (0.361)	Data 8.08e-05 (9.08e-05)	Tok/s 17032 (10637)	Loss/tok 5.8233 (6.0507)	LR 2.500e-04
3: TRAIN [1][160/229]	Time 0.391 (0.361)	Data 8.03e-05 (7.83e-05)	Tok/s 16701 (10619)	Loss/tok 5.8204 (6.0580)	LR 2.500e-04
4: TRAIN [1][160/229]	Time 0.391 (0.361)	Data 9.18e-05 (9.23e-05)	Tok/s 16704 (10624)	Loss/tok 5.9101 (6.0442)	LR 2.500e-04
0: TRAIN [1][170/229]	Time 0.353 (0.361)	Data 9.27e-05 (9.78e-05)	Tok/s 10362 (10663)	Loss/tok 5.2424 (6.0143)	LR 2.500e-04
7: TRAIN [1][170/229]	Time 0.353 (0.361)	Data 9.27e-05 (8.24e-05)	Tok/s 10274 (10673)	Loss/tok 5.4047 (6.0157)	LR 2.500e-04
6: TRAIN [1][170/229]	Time 0.352 (0.361)	Data 7.53e-05 (8.16e-05)	Tok/s 10164 (10675)	Loss/tok 5.3098 (6.0149)	LR 2.500e-04
1: TRAIN [1][170/229]	Time 0.352 (0.361)	Data 9.08e-05 (8.48e-05)	Tok/s 10365 (10676)	Loss/tok 5.4063 (6.0176)	LR 2.500e-04
5: TRAIN [1][170/229]	Time 0.352 (0.361)	Data 8.37e-05 (8.88e-05)	Tok/s 10059 (10667)	Loss/tok 5.4608 (6.0181)	LR 2.500e-04
2: TRAIN [1][170/229]	Time 0.352 (0.361)	Data 8.65e-05 (9.08e-05)	Tok/s 10343 (10687)	Loss/tok 5.4236 (6.0181)	LR 2.500e-04
4: TRAIN [1][170/229]	Time 0.352 (0.361)	Data 1.01e-04 (9.23e-05)	Tok/s 10343 (10675)	Loss/tok 5.5015 (6.0104)	LR 2.500e-04
3: TRAIN [1][170/229]	Time 0.352 (0.361)	Data 7.63e-05 (7.83e-05)	Tok/s 10192 (10664)	Loss/tok 5.3588 (6.0224)	LR 2.500e-04
0: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 8.30e-05 (9.72e-05)	Tok/s 13311 (10609)	Loss/tok 5.3795 (5.9851)	LR 2.500e-04
7: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 8.15e-05 (8.26e-05)	Tok/s 13484 (10616)	Loss/tok 5.4898 (5.9879)	LR 2.500e-04
6: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 7.41e-05 (8.14e-05)	Tok/s 13580 (10620)	Loss/tok 5.5864 (5.9850)	LR 2.500e-04
1: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 8.56e-05 (8.47e-05)	Tok/s 13737 (10625)	Loss/tok 5.7201 (5.9898)	LR 2.500e-04
2: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 8.37e-05 (9.08e-05)	Tok/s 13599 (10629)	Loss/tok 5.7305 (5.9894)	LR 2.500e-04
5: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 1.00e-04 (8.89e-05)	Tok/s 13262 (10613)	Loss/tok 5.5055 (5.9896)	LR 2.500e-04
3: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 8.42e-05 (7.85e-05)	Tok/s 13675 (10609)	Loss/tok 5.5797 (5.9922)	LR 2.500e-04
4: TRAIN [1][180/229]	Time 0.374 (0.361)	Data 9.66e-05 (9.25e-05)	Tok/s 13520 (10618)	Loss/tok 5.5220 (5.9822)	LR 2.500e-04
0: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 8.23e-05 (9.65e-05)	Tok/s 6364 (10486)	Loss/tok 5.0800 (5.9558)	LR 1.250e-04
7: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 7.84e-05 (8.28e-05)	Tok/s 6368 (10495)	Loss/tok 4.9308 (5.9568)	LR 1.250e-04
6: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 7.77e-05 (8.15e-05)	Tok/s 6527 (10500)	Loss/tok 4.9425 (5.9574)	LR 1.250e-04
1: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 8.68e-05 (8.46e-05)	Tok/s 6462 (10500)	Loss/tok 5.0856 (5.9601)	LR 1.250e-04
5: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 8.27e-05 (8.89e-05)	Tok/s 6643 (10496)	Loss/tok 4.8957 (5.9572)	LR 1.250e-04
2: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 8.42e-05 (9.11e-05)	Tok/s 6573 (10507)	Loss/tok 4.9928 (5.9605)	LR 1.250e-04
4: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 9.23e-05 (9.26e-05)	Tok/s 6645 (10498)	Loss/tok 5.0667 (5.9547)	LR 1.250e-04
3: TRAIN [1][190/229]	Time 0.337 (0.360)	Data 8.06e-05 (7.86e-05)	Tok/s 6412 (10489)	Loss/tok 5.0334 (5.9644)	LR 1.250e-04
0: TRAIN [1][200/229]	Time 0.343 (0.359)	Data 9.04e-05 (9.64e-05)	Tok/s 6386 (10427)	Loss/tok 5.1403 (5.9294)	LR 1.250e-04
7: TRAIN [1][200/229]	Time 0.344 (0.359)	Data 8.44e-05 (8.29e-05)	Tok/s 6422 (10439)	Loss/tok 5.0335 (5.9296)	LR 1.250e-04
6: TRAIN [1][200/229]	Time 0.344 (0.359)	Data 7.56e-05 (8.14e-05)	Tok/s 6326 (10438)	Loss/tok 5.2667 (5.9306)	LR 1.250e-04
1: TRAIN [1][200/229]	Time 0.344 (0.359)	Data 9.18e-05 (8.46e-05)	Tok/s 6370 (10440)	Loss/tok 4.9860 (5.9336)	LR 1.250e-04
5: TRAIN [1][200/229]	Time 0.344 (0.359)	Data 9.20e-05 (8.92e-05)	Tok/s 6374 (10441)	Loss/tok 4.7985 (5.9301)	LR 1.250e-04
2: TRAIN [1][200/229]	Time 0.344 (0.359)	Data 8.20e-05 (9.12e-05)	Tok/s 6501 (10447)	Loss/tok 4.9614 (5.9344)	LR 1.250e-04
4: TRAIN [1][200/229]	Time 0.343 (0.359)	Data 1.05e-04 (9.26e-05)	Tok/s 6047 (10438)	Loss/tok 4.8881 (5.9298)	LR 1.250e-04
3: TRAIN [1][200/229]	Time 0.344 (0.359)	Data 7.72e-05 (7.85e-05)	Tok/s 6337 (10431)	Loss/tok 4.8690 (5.9391)	LR 1.250e-04
0: TRAIN [1][210/229]	Time 0.365 (0.359)	Data 8.58e-05 (9.61e-05)	Tok/s 9791 (10459)	Loss/tok 5.3036 (5.9065)	LR 1.250e-04
7: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 7.37e-05 (8.29e-05)	Tok/s 9907 (10471)	Loss/tok 5.3306 (5.9076)	LR 1.250e-04
6: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 8.30e-05 (8.15e-05)	Tok/s 9936 (10466)	Loss/tok 5.2670 (5.9065)	LR 1.250e-04
1: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 9.18e-05 (8.47e-05)	Tok/s 9923 (10467)	Loss/tok 5.3500 (5.9100)	LR 1.250e-04
2: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 7.70e-05 (9.12e-05)	Tok/s 9730 (10472)	Loss/tok 5.2647 (5.9077)	LR 1.250e-04
5: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 9.06e-05 (8.93e-05)	Tok/s 9631 (10468)	Loss/tok 5.0618 (5.9042)	LR 1.250e-04
4: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 1.11e-04 (9.28e-05)	Tok/s 9890 (10467)	Loss/tok 5.3515 (5.9051)	LR 1.250e-04
3: TRAIN [1][210/229]	Time 0.365 (0.360)	Data 7.63e-05 (7.86e-05)	Tok/s 9957 (10457)	Loss/tok 5.1541 (5.9119)	LR 1.250e-04
0: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 9.20e-05 (9.58e-05)	Tok/s 13181 (10484)	Loss/tok 5.5754 (5.8818)	LR 1.250e-04
7: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 9.01e-05 (8.31e-05)	Tok/s 13419 (10501)	Loss/tok 5.4992 (5.8846)	LR 1.250e-04
6: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 7.63e-05 (8.14e-05)	Tok/s 13215 (10491)	Loss/tok 5.4322 (5.8815)	LR 1.250e-04
1: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 1.03e-04 (8.49e-05)	Tok/s 13172 (10493)	Loss/tok 5.5013 (5.8853)	LR 1.250e-04
2: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 8.77e-05 (9.13e-05)	Tok/s 13216 (10497)	Loss/tok 5.4725 (5.8853)	LR 1.250e-04
5: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 8.65e-05 (8.93e-05)	Tok/s 13034 (10496)	Loss/tok 5.4957 (5.8790)	LR 1.250e-04
4: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 9.37e-05 (9.29e-05)	Tok/s 13343 (10498)	Loss/tok 5.5523 (5.8820)	LR 1.250e-04
3: TRAIN [1][220/229]	Time 0.382 (0.360)	Data 8.06e-05 (7.85e-05)	Tok/s 13194 (10485)	Loss/tok 5.4905 (5.8891)	LR 1.250e-04
3: Running validation on dev set
5: Running validation on dev set
1: Running validation on dev set
6: Running validation on dev set
7: Running validation on dev set
0: Running validation on dev set
2: Running validation on dev set
3: Executing preallocation
6: Executing preallocation
1: Executing preallocation
7: Executing preallocation
5: Executing preallocation
2: Executing preallocation
0: Executing preallocation
4: Running validation on dev set
4: Executing preallocation
3: VALIDATION [1][0/27]	Time 0.044 (0.000)	Data 1.51e-03 (0.00e+00)	Tok/s 73967 (0)	Loss/tok 6.6824 (6.6824)
5: VALIDATION [1][0/26]	Time 0.042 (0.000)	Data 1.51e-03 (0.00e+00)	Tok/s 72800 (0)	Loss/tok 6.5375 (6.5375)
7: VALIDATION [1][0/26]	Time 0.038 (0.000)	Data 1.47e-03 (0.00e+00)	Tok/s 76258 (0)	Loss/tok 6.8032 (6.8032)
1: VALIDATION [1][0/27]	Time 0.051 (0.000)	Data 1.50e-03 (0.00e+00)	Tok/s 72872 (0)	Loss/tok 6.8445 (6.8445)
4: VALIDATION [1][0/27]	Time 0.042 (0.000)	Data 1.48e-03 (0.00e+00)	Tok/s 75720 (0)	Loss/tok 6.5877 (6.5877)
6: VALIDATION [1][0/26]	Time 0.041 (0.000)	Data 1.52e-03 (0.00e+00)	Tok/s 71014 (0)	Loss/tok 6.7336 (6.7336)
2: VALIDATION [1][0/27]	Time 0.048 (0.000)	Data 1.54e-03 (0.00e+00)	Tok/s 71165 (0)	Loss/tok 6.6482 (6.6482)
0: VALIDATION [1][0/27]	Time 0.070 (0.000)	Data 1.42e-03 (0.00e+00)	Tok/s 63282 (0)	Loss/tok 6.7827 (6.7827)
7: VALIDATION [1][10/26]	Time 0.021 (0.026)	Data 1.25e-03 (1.28e-03)	Tok/s 67431 (71385)	Loss/tok 6.1858 (6.4019)
5: VALIDATION [1][10/26]	Time 0.020 (0.026)	Data 1.29e-03 (1.30e-03)	Tok/s 70547 (71965)	Loss/tok 6.2542 (6.4030)
3: VALIDATION [1][10/27]	Time 0.021 (0.027)	Data 1.26e-03 (1.31e-03)	Tok/s 67489 (71472)	Loss/tok 6.2664 (6.4310)
4: VALIDATION [1][10/27]	Time 0.020 (0.026)	Data 1.25e-03 (1.29e-03)	Tok/s 70646 (72491)	Loss/tok 6.2494 (6.4026)
6: VALIDATION [1][10/26]	Time 0.021 (0.026)	Data 1.28e-03 (1.31e-03)	Tok/s 67097 (71773)	Loss/tok 6.3965 (6.3993)
1: VALIDATION [1][10/27]	Time 0.021 (0.027)	Data 1.25e-03 (1.29e-03)	Tok/s 70207 (72770)	Loss/tok 5.9733 (6.4257)
2: VALIDATION [1][10/27]	Time 0.021 (0.027)	Data 1.28e-03 (1.30e-03)	Tok/s 69742 (71913)	Loss/tok 6.0790 (6.3804)
0: VALIDATION [1][10/27]	Time 0.021 (0.027)	Data 1.23e-03 (1.23e-03)	Tok/s 70122 (72937)	Loss/tok 6.2175 (6.4666)
7: VALIDATION [1][20/26]	Time 0.013 (0.021)	Data 1.24e-03 (1.26e-03)	Tok/s 57037 (68163)	Loss/tok 5.8121 (6.2819)
5: VALIDATION [1][20/26]	Time 0.013 (0.021)	Data 1.26e-03 (1.27e-03)	Tok/s 58107 (68904)	Loss/tok 6.0467 (6.2762)
6: VALIDATION [1][20/26]	Time 0.013 (0.021)	Data 1.24e-03 (1.29e-03)	Tok/s 61268 (68387)	Loss/tok 5.9398 (6.2883)
4: VALIDATION [1][20/27]	Time 0.013 (0.021)	Data 1.24e-03 (1.27e-03)	Tok/s 61329 (68976)	Loss/tok 5.7829 (6.2897)
3: VALIDATION [1][20/27]	Time 0.013 (0.022)	Data 1.23e-03 (1.28e-03)	Tok/s 61523 (68085)	Loss/tok 6.0127 (6.3336)
1: VALIDATION [1][20/27]	Time 0.013 (0.022)	Data 1.24e-03 (1.27e-03)	Tok/s 61899 (69072)	Loss/tok 6.0300 (6.2993)
2: VALIDATION [1][20/27]	Time 0.013 (0.022)	Data 1.26e-03 (1.28e-03)	Tok/s 62326 (68310)	Loss/tok 5.9735 (6.2815)
0: VALIDATION [1][20/27]	Time 0.013 (0.022)	Data 1.19e-03 (1.21e-03)	Tok/s 62156 (69777)	Loss/tok 6.0064 (6.3317)
0: Saving model to gnmt/model_best.pth
7: Running evaluation on test set
4: Running evaluation on test set
3: Running evaluation on test set
1: Running evaluation on test set
5: Running evaluation on test set
6: Running evaluation on test set
2: Running evaluation on test set
0: Running evaluation on test set
0: TEST [1][9/16]	Time 0.3800 (0.4349)	Decoder iters 57.0 (122.9)	Tok/s 3400 (4978)
1: TEST [1][9/16]	Time 0.3798 (0.4350)	Decoder iters 149.0 (149.0)	Tok/s 3197 (4931)
4: TEST [1][9/16]	Time 0.3801 (0.4350)	Decoder iters 149.0 (130.5)	Tok/s 3460 (4451)
7: TEST [1][9/16]	Time 0.3799 (0.4351)	Decoder iters 48.0 (119.1)	Tok/s 3038 (4265)
6: TEST [1][9/16]	Time 0.3800 (0.4350)	Decoder iters 40.0 (121.9)	Tok/s 3034 (4441)
5: TEST [1][9/16]	Time 0.3799 (0.4350)	Decoder iters 39.0 (120.1)	Tok/s 3027 (4519)
2: TEST [1][9/16]	Time 0.3802 (0.4350)	Decoder iters 54.0 (120.2)	Tok/s 3085 (4784)
3: TEST [1][9/16]	Time 0.3801 (0.4351)	Decoder iters 54.0 (139.5)	Tok/s 3162 (4698)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
4: Finished evaluation on test set
1: Finished evaluation on test set
5: Finished evaluation on test set
7: Finished evaluation on test set
3: Finished evaluation on test set
6: Finished evaluation on test set
2: Finished evaluation on test set
0: Finished evaluation on test set
5: Finished epoch 1
6: Finished epoch 1
1: Finished epoch 1
4: Finished epoch 1
3: Finished epoch 1
7: Finished epoch 1
2: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 5.8640	Validation Loss: 6.2409	Test BLEU: 1.38
0: Performance: Epoch: 1	Training: 84292 Tok/s	Validation: 523391 Tok/s
0: Finished epoch 1
3: Total training time 225 s
1: Total training time 225 s
2: Total training time 225 s
4: Total training time 225 s
6: Total training time 224 s
5: Total training time 225 s
7: Total training time 224 s
0: Total training time 225 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       8|                  80|                      1.38|                      83838.0|                         3.742|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
