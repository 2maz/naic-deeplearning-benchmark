[2024-09-29 07:51:14,242] torch.distributed.run: [WARNING] 
[2024-09-29 07:51:14,242] torch.distributed.run: [WARNING] *****************************************
[2024-09-29 07:51:14,242] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-29 07:51:14,242] torch.distributed.run: [WARNING] *****************************************
DLL 2024-09-29 07:51:16.629909 - PARAMETER data : /data/ncf/cache/ml-20m  feature_spec_file : feature_spec.yaml  epochs : 2  batch_size : 32000000  valid_batch_size : 1048576  factors : 64  layers : [256, 256, 128, 64]  negative_samples : 4  learning_rate : 0.0045  topk : 10  seed : None  threshold : 1.0  beta1 : 0.25  beta2 : 0.5  eps : 1e-08  dropout : 0.5  checkpoint_dir :   load_checkpoint_path : None  mode : train  grads_accumulated : 1  amp : False  log_path : log.json  world_size : 4  distributed : True  local_rank : 0 
[rank2]: Traceback (most recent call last):
[rank2]:   File "/workspace/benchmark/Recommendation/NCF/ncf.py", line 383, in <module>
[rank2]:     main()
[rank2]:   File "/workspace/benchmark/Recommendation/NCF/ncf.py", line 218, in main
[rank2]:     torch.distributed.broadcast(torch.tensor([1], device="cuda"), 0)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank2]:     return func(*args, **kwargs)
[rank2]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 2045, in broadcast
[rank2]:     work = default_pg.broadcast([tensor], opts)
[rank2]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2006, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank2]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank2]: Last error:
[rank2]: Cuda failure 801 'operation not supported'
[2024-09-29 07:51:24,253] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 11646 closing signal SIGTERM
[2024-09-29 07:51:24,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 11647 closing signal SIGTERM
[2024-09-29 07:51:24,254] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 11649 closing signal SIGTERM
[2024-09-29 07:51:24,682] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 2 (pid: 11648) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 834, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 825, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 137, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 271, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
ncf.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-29_07:51:24
  host      : 4563c6574d95
  rank      : 2 (local_rank: 2)
  exitcode  : 1 (pid: 11648)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE!
