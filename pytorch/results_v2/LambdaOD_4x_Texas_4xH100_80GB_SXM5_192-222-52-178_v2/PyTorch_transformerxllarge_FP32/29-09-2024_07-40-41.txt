[2024-09-29 07:40:44,021] torch.distributed.run: [WARNING] 
[2024-09-29 07:40:44,021] torch.distributed.run: [WARNING] *****************************************
[2024-09-29 07:40:44,021] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
[2024-09-29 07:40:44,021] torch.distributed.run: [WARNING] *****************************************
3: thread affinity: {6, 7, 14, 15, 22, 23, 30, 31, 38, 39, 46, 47, 54, 55, 62, 63, 70, 71, 78, 79, 86, 87, 94, 95, 102, 103}
1: thread affinity: {2, 3, 10, 11, 18, 19, 26, 27, 34, 35, 42, 43, 50, 51, 58, 59, 66, 67, 74, 75, 82, 83, 90, 91, 98, 99}
2: thread affinity: {4, 5, 12, 13, 20, 21, 28, 29, 36, 37, 44, 45, 52, 53, 60, 61, 68, 69, 76, 77, 84, 85, 92, 93, 100, 101}
0: thread affinity: {0, 1, 8, 9, 16, 17, 24, 25, 32, 33, 40, 41, 48, 49, 56, 57, 64, 65, 72, 73, 80, 81, 88, 89, 96, 97}
Experiment dir : LM-TFM
[rank1]: Traceback (most recent call last):
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 1134, in <module>
[rank1]:     main()
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 720, in main
[rank1]:     with utils.distributed.sync_workers() as rank:
[rank1]:   File "/usr/lib/python3.10/contextlib.py", line 142, in __exit__
[rank1]:     next(self.gen)
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/utils/distributed.py", line 121, in sync_workers
[rank1]:     barrier()
[rank1]:   File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/utils/distributed.py", line 43, in barrier
[rank1]:     torch.distributed.barrier()
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/c10d_logger.py", line 75, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/usr/local/lib/python3.10/dist-packages/torch/distributed/distributed_c10d.py", line 3592, in barrier
[rank1]:     work = default_pg.barrier(opts=opts)
[rank1]: torch.distributed.DistBackendError: NCCL error in: /opt/pytorch/pytorch/torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:2006, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.20.5
[rank1]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank1]: Last error:
[rank1]: Cuda failure 801 'operation not supported'
[2024-09-29 07:40:54,037] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2558 closing signal SIGTERM
[2024-09-29 07:40:54,037] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2560 closing signal SIGTERM
[2024-09-29 07:40:54,037] torch.distributed.elastic.multiprocessing.api: [WARNING] Sending process 2561 closing signal SIGTERM
[2024-09-29 07:40:54,465] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 1 (pid: 2559) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 347, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 834, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 825, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 137, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 271, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-29_07:40:54
  host      : 8f5a528b3a5a
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 2559)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE!
