0: thread affinity: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25}
Experiment dir : LM-TFM
Namespace(work_dir='LM-TFM', append_dataset=False, append_time=False, cuda=True, fp16=True, restart='', debug=False, log_all_ranks=False, dllog_file='train_log.json', txtlog_file='train_log.log', save_all=False, no_env=False, no_eval=True, no_test=False, log_interval=10, target_throughput=None, target_perplexity=None, apex_amp_opt_level='O2', amp='apex', affinity='socket_unique_interleaved', data='/data/transformer-xl/wikitext-103', dataset='wt103', vocab='word', n_layer=16, n_head=8, d_head=64, d_embed=512, d_model=512, d_inner=2048, dropout=0.1, dropatt=0.0, pre_lnorm=False, attn_type=0, not_tied=False, clamp_len=-1, adaptive=False, div_val=1, sample_softmax=-1, init='normal', emb_init='normal', init_range=0.1, emb_init_range=0.01, init_std=0.02, proj_init_std=0.01, optim='jitlamb', lr=0.0, mom=0.0, scheduler='cosine', max_step_scheduler=None, warmup_step=1000, decay_rate=0.5, lr_min=0.0, clip=0.25, weight_decay=0.0, clip_nonemb=False, patience=0, eta_min=0.001, max_step=400, batch_size=104, local_batch_size=None, batch_chunk=1, roll=True, tgt_len=192, ext_len=0, mem_len=192, seed=1111, multi_gpu=None, gpu0_bsz=-1, same_length=False, varlen=False, swap_mem=False, eval_tgt_len=192, eval_batch_size=16, eval_max_steps=-1, eval_interval=5000, local_rank=0, tied=True)
world size: 1
Collecting environment information...
PyTorch version: 2.5.0a0+e000cf0ad9.nv24.10
Is debug build: False
CUDA used to build PyTorch: 12.6
ROCM used to build PyTorch: N/A

OS: Ubuntu 22.04.5 LTS (x86_64)
GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
Clang version: Could not collect
CMake version: version 3.30.4
Libc version: glibc-2.35

Python version: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0] (64-bit runtime)
Python platform: Linux-6.8.0-47-generic-x86_64-with-glibc2.35
Is CUDA available: True
CUDA runtime version: 12.6.77
CUDA_MODULE_LOADING set to: LAZY
GPU models and configuration: GPU 0: NVIDIA H100 80GB HBM3
Nvidia driver version: 550.90.12
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.5.0
/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.5.0
HIP runtime version: N/A
MIOpen runtime version: N/A
Is XNNPACK available: True

CPU:
Architecture:                         x86_64
CPU op-mode(s):                       32-bit, 64-bit
Address sizes:                        46 bits physical, 57 bits virtual
Byte Order:                           Little Endian
CPU(s):                               26
On-line CPU(s) list:                  0-25
Vendor ID:                            GenuineIntel
Model name:                           Intel(R) Xeon(R) Platinum 8480+
CPU family:                           6
Model:                                143
Thread(s) per core:                   2
Core(s) per socket:                   13
Socket(s):                            1
Stepping:                             8
BogoMIPS:                             4000.00
Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx_vnni avx512_bf16 wbnoinvd arat vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b fsrm md_clear serialize tsxldtrk avx512_fp16 arch_capabilities
Virtualization:                       VT-x
Hypervisor vendor:                    KVM
Virtualization type:                  full
L1d cache:                            832 KiB (26 instances)
L1i cache:                            832 KiB (26 instances)
L2 cache:                             52 MiB (13 instances)
L3 cache:                             16 MiB (1 instance)
NUMA node(s):                         1
NUMA node0 CPU(s):                    0-25
Vulnerability Gather data sampling:   Not affected
Vulnerability Itlb multihit:          Not affected
Vulnerability L1tf:                   Not affected
Vulnerability Mds:                    Not affected
Vulnerability Meltdown:               Not affected
Vulnerability Mmio stale data:        Unknown: No mitigations
Vulnerability Reg file data sampling: Not affected
Vulnerability Retbleed:               Not affected
Vulnerability Spec rstack overflow:   Not affected
Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl
Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization
Vulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop
Vulnerability Srbds:                  Not affected
Vulnerability Tsx async abort:        Mitigation; TSX disabled

Versions of relevant libraries:
[pip3] mypy-extensions==1.0.0
[pip3] numpy==1.24.4
[pip3] onnx==1.16.2
[pip3] onnxruntime==1.19.2
[pip3] optree==0.13.0
[pip3] pytorch-transformers==1.1.0
[pip3] pytorch-triton==3.0.0+dedb7bdf3
[pip3] torch==2.5.0a0+e000cf0ad9.nv24.10
[pip3] torch_tensorrt==2.5.0a0
[pip3] torchprofile==0.0.4
[pip3] torchvision==0.20.0a0
[conda] Could not collect
Producing dataset wt103...
Traceback (most recent call last):
  File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 1134, in <module>
    main()
  File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/train.py", line 778, in main
    corpus = get_lm_corpus(args.data, args.dataset, args.vocab)
  File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/data_utils.py", line 322, in get_lm_corpus
    corpus = Corpus(datadir, dataset, vocab, **kwargs)
  File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/data_utils.py", line 247, in __init__
    self.vocab.count_file(os.path.join(path, 'train.txt'))
  File "/workspace/benchmark/LanguageModeling/Transformer-XL/pytorch/utils/vocabulary.py", line 58, in count_file
    assert os.path.exists(path)
AssertionError
E1030 05:44:53.148000 2856 torch/distributed/elastic/multiprocessing/api.py:862] failed (exitcode: 1) local_rank: 0 (pid: 2885) of binary: /usr/bin/python
Traceback (most recent call last):
  File "/usr/local/bin/torchrun", line 33, in <module>
    sys.exit(load_entry_point('torch==2.5.0a0+e000cf0ad9.nv24.10', 'console_scripts', 'torchrun')())
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 919, in main
    run(args)
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/run.py", line 910, in run
    elastic_launch(
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/usr/local/lib/python3.10/dist-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-10-30_05:44:53
  host      : 86fe8aa6984e
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 2885)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
DONE!
