1: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 614
0: Scheduler decay interval: 77
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 614
1: Scheduler decay interval: 77
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/461]	Time 0.308 (0.308)	Data 1.40e-01 (1.40e-01)	Tok/s 14076 (14076)	Loss/tok 10.5304 (10.5304)	LR 2.047e-05
1: TRAIN [0][0/461]	Time 0.309 (0.309)	Data 1.47e-01 (1.47e-01)	Tok/s 14111 (14111)	Loss/tok 10.5276 (10.5276)	LR 2.047e-05
0: TRAIN [0][10/461]	Time 0.271 (0.237)	Data 8.87e-05 (1.28e-02)	Tok/s 37332 (30125)	Loss/tok 9.6749 (10.1140)	LR 2.576e-05
1: TRAIN [0][10/461]	Time 0.271 (0.237)	Data 1.29e-04 (1.35e-02)	Tok/s 37123 (30175)	Loss/tok 9.6754 (10.1164)	LR 2.576e-05
0: TRAIN [0][20/461]	Time 0.224 (0.245)	Data 9.78e-05 (6.78e-03)	Tok/s 32023 (31551)	Loss/tok 9.1431 (9.7447)	LR 3.244e-05
1: TRAIN [0][20/461]	Time 0.224 (0.245)	Data 1.15e-04 (7.14e-03)	Tok/s 32246 (31633)	Loss/tok 9.1712 (9.7515)	LR 3.244e-05
0: TRAIN [0][30/461]	Time 0.276 (0.247)	Data 1.00e-04 (4.62e-03)	Tok/s 36713 (32256)	Loss/tok 8.9736 (9.5039)	LR 4.083e-05
1: TRAIN [0][30/461]	Time 0.276 (0.247)	Data 1.15e-04 (4.88e-03)	Tok/s 37068 (32245)	Loss/tok 9.0140 (9.5167)	LR 4.083e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
1: TRAIN [0][40/461]	Time 0.183 (0.246)	Data 1.25e-04 (3.72e-03)	Tok/s 23357 (32165)	Loss/tok 8.5334 (9.3562)	LR 5.141e-05
0: TRAIN [0][40/461]	Time 0.184 (0.246)	Data 1.19e-04 (3.52e-03)	Tok/s 23477 (32186)	Loss/tok 8.5478 (9.3446)	LR 5.141e-05
0: TRAIN [0][50/461]	Time 0.182 (0.240)	Data 9.16e-05 (2.85e-03)	Tok/s 24325 (31517)	Loss/tok 8.3002 (9.2156)	LR 6.472e-05
1: TRAIN [0][50/461]	Time 0.182 (0.240)	Data 1.32e-04 (3.01e-03)	Tok/s 24228 (31490)	Loss/tok 8.3470 (9.2237)	LR 6.472e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][60/461]	Time 0.203 (0.240)	Data 1.02e-04 (2.40e-03)	Tok/s 35427 (31816)	Loss/tok 8.2766 (9.0823)	LR 8.148e-05
1: TRAIN [0][60/461]	Time 0.203 (0.240)	Data 1.34e-04 (2.54e-03)	Tok/s 35400 (31781)	Loss/tok 8.3746 (9.0909)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][70/461]	Time 0.186 (0.235)	Data 9.39e-05 (2.08e-03)	Tok/s 23248 (31151)	Loss/tok 7.9268 (9.0056)	LR 1.026e-04
1: TRAIN [0][70/461]	Time 0.186 (0.235)	Data 1.14e-04 (2.20e-03)	Tok/s 23785 (31123)	Loss/tok 7.8586 (9.0144)	LR 1.026e-04
0: TRAIN [0][80/461]	Time 0.226 (0.237)	Data 9.61e-05 (1.83e-03)	Tok/s 32183 (31386)	Loss/tok 8.0390 (8.8913)	LR 1.291e-04
1: TRAIN [0][80/461]	Time 0.226 (0.237)	Data 1.10e-04 (1.94e-03)	Tok/s 31991 (31393)	Loss/tok 8.0758 (8.8962)	LR 1.291e-04
0: TRAIN [0][90/461]	Time 0.225 (0.240)	Data 1.21e-04 (1.64e-03)	Tok/s 31925 (31681)	Loss/tok 7.7786 (8.7746)	LR 1.626e-04
1: TRAIN [0][90/461]	Time 0.225 (0.240)	Data 1.18e-04 (1.74e-03)	Tok/s 31645 (31665)	Loss/tok 7.8262 (8.7818)	LR 1.626e-04
1: TRAIN [0][100/461]	Time 0.334 (0.245)	Data 1.29e-04 (1.58e-03)	Tok/s 39201 (31997)	Loss/tok 7.9017 (8.6706)	LR 2.047e-04
0: TRAIN [0][100/461]	Time 0.334 (0.245)	Data 1.51e-04 (1.49e-03)	Tok/s 39078 (32019)	Loss/tok 7.9047 (8.6624)	LR 2.047e-04
0: TRAIN [0][110/461]	Time 0.181 (0.242)	Data 1.55e-04 (1.37e-03)	Tok/s 23609 (31659)	Loss/tok 7.4384 (8.5973)	LR 2.576e-04
1: TRAIN [0][110/461]	Time 0.184 (0.242)	Data 1.12e-04 (1.45e-03)	Tok/s 23007 (31632)	Loss/tok 7.4321 (8.6061)	LR 2.576e-04
0: TRAIN [0][120/461]	Time 0.181 (0.240)	Data 1.52e-04 (1.27e-03)	Tok/s 24185 (31517)	Loss/tok 7.3429 (8.5314)	LR 3.244e-04
1: TRAIN [0][120/461]	Time 0.185 (0.240)	Data 1.13e-04 (1.34e-03)	Tok/s 24209 (31505)	Loss/tok 7.2520 (8.5389)	LR 3.244e-04
0: TRAIN [0][130/461]	Time 0.388 (0.241)	Data 9.80e-05 (1.18e-03)	Tok/s 34079 (31378)	Loss/tok 8.0542 (8.4736)	LR 4.083e-04
1: TRAIN [0][130/461]	Time 0.331 (0.241)	Data 1.03e-04 (1.25e-03)	Tok/s 39568 (31377)	Loss/tok 8.0321 (8.4783)	LR 4.083e-04
0: TRAIN [0][140/461]	Time 0.275 (0.239)	Data 9.30e-05 (1.10e-03)	Tok/s 35976 (31068)	Loss/tok 7.7896 (8.4246)	LR 5.141e-04
1: TRAIN [0][140/461]	Time 0.275 (0.239)	Data 1.08e-04 (1.17e-03)	Tok/s 36576 (31080)	Loss/tok 7.8078 (8.4296)	LR 5.141e-04
0: TRAIN [0][150/461]	Time 0.228 (0.239)	Data 9.20e-05 (1.04e-03)	Tok/s 31123 (31104)	Loss/tok 7.5438 (8.3733)	LR 6.472e-04
1: TRAIN [0][150/461]	Time 0.227 (0.239)	Data 1.11e-04 (1.10e-03)	Tok/s 31059 (31116)	Loss/tok 7.5902 (8.3789)	LR 6.472e-04
0: TRAIN [0][160/461]	Time 0.273 (0.238)	Data 1.03e-04 (9.79e-04)	Tok/s 36977 (30945)	Loss/tok 7.9038 (8.3364)	LR 8.148e-04
1: TRAIN [0][160/461]	Time 0.273 (0.238)	Data 1.16e-04 (1.03e-03)	Tok/s 36923 (30950)	Loss/tok 7.9144 (8.3421)	LR 8.148e-04
1: TRAIN [0][170/461]	Time 0.278 (0.237)	Data 1.33e-04 (9.81e-04)	Tok/s 36220 (30816)	Loss/tok 7.7592 (8.3050)	LR 1.026e-03
0: TRAIN [0][170/461]	Time 0.278 (0.237)	Data 1.03e-04 (9.28e-04)	Tok/s 36295 (30803)	Loss/tok 7.7424 (8.2992)	LR 1.026e-03
1: TRAIN [0][180/461]	Time 0.280 (0.237)	Data 1.09e-04 (9.34e-04)	Tok/s 35937 (30869)	Loss/tok 7.6626 (8.2647)	LR 1.291e-03
0: TRAIN [0][180/461]	Time 0.280 (0.237)	Data 1.60e-04 (8.84e-04)	Tok/s 35951 (30853)	Loss/tok 7.6285 (8.2572)	LR 1.291e-03
0: TRAIN [0][190/461]	Time 0.278 (0.239)	Data 9.97e-05 (8.44e-04)	Tok/s 36315 (30983)	Loss/tok 7.5426 (8.2165)	LR 1.626e-03
1: TRAIN [0][190/461]	Time 0.278 (0.239)	Data 1.14e-04 (8.91e-04)	Tok/s 35887 (30992)	Loss/tok 7.5640 (8.2227)	LR 1.626e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][200/461]	Time 0.275 (0.240)	Data 2.22e-04 (8.08e-04)	Tok/s 36466 (31085)	Loss/tok 7.4431 (8.1770)	LR 2.000e-03
1: TRAIN [0][200/461]	Time 0.279 (0.240)	Data 1.16e-04 (8.52e-04)	Tok/s 35792 (31091)	Loss/tok 7.4462 (8.1835)	LR 2.000e-03
0: TRAIN [0][210/461]	Time 0.276 (0.240)	Data 9.06e-05 (7.75e-04)	Tok/s 36562 (31144)	Loss/tok 7.3582 (8.1368)	LR 2.000e-03
1: TRAIN [0][210/461]	Time 0.276 (0.240)	Data 1.15e-04 (8.17e-04)	Tok/s 36435 (31140)	Loss/tok 7.3328 (8.1419)	LR 2.000e-03
0: TRAIN [0][220/461]	Time 0.185 (0.240)	Data 9.75e-05 (7.44e-04)	Tok/s 23994 (31154)	Loss/tok 6.8186 (8.0961)	LR 2.000e-03
1: TRAIN [0][220/461]	Time 0.185 (0.240)	Data 1.17e-04 (7.87e-04)	Tok/s 22832 (31140)	Loss/tok 6.8710 (8.1018)	LR 2.000e-03
1: TRAIN [0][230/461]	Time 0.276 (0.240)	Data 1.07e-04 (7.58e-04)	Tok/s 36472 (31183)	Loss/tok 7.0206 (8.0561)	LR 2.000e-03
0: TRAIN [0][230/461]	Time 0.276 (0.240)	Data 1.13e-04 (7.16e-04)	Tok/s 36386 (31197)	Loss/tok 7.1292 (8.0509)	LR 2.000e-03
0: TRAIN [0][240/461]	Time 0.276 (0.241)	Data 9.39e-05 (6.90e-04)	Tok/s 36717 (31260)	Loss/tok 6.9574 (8.0037)	LR 2.000e-03
1: TRAIN [0][240/461]	Time 0.276 (0.241)	Data 1.11e-04 (7.31e-04)	Tok/s 36182 (31243)	Loss/tok 7.0009 (8.0085)	LR 2.000e-03
0: TRAIN [0][250/461]	Time 0.228 (0.240)	Data 9.18e-05 (6.67e-04)	Tok/s 31674 (31170)	Loss/tok 6.7395 (7.9623)	LR 2.000e-03
1: TRAIN [0][250/461]	Time 0.228 (0.240)	Data 1.15e-04 (7.07e-04)	Tok/s 31649 (31148)	Loss/tok 6.7385 (7.9675)	LR 2.000e-03
1: TRAIN [0][260/461]	Time 0.332 (0.241)	Data 1.13e-04 (6.84e-04)	Tok/s 39341 (31292)	Loss/tok 6.8061 (7.9120)	LR 2.000e-03
0: TRAIN [0][260/461]	Time 0.333 (0.241)	Data 1.07e-04 (6.46e-04)	Tok/s 39226 (31314)	Loss/tok 6.9289 (7.9081)	LR 2.000e-03
0: TRAIN [0][270/461]	Time 0.226 (0.241)	Data 9.58e-05 (6.25e-04)	Tok/s 31758 (31339)	Loss/tok 6.4244 (7.8609)	LR 2.000e-03
1: TRAIN [0][270/461]	Time 0.226 (0.241)	Data 1.19e-04 (6.63e-04)	Tok/s 32015 (31323)	Loss/tok 6.4732 (7.8642)	LR 2.000e-03
0: TRAIN [0][280/461]	Time 0.185 (0.241)	Data 9.37e-05 (6.07e-04)	Tok/s 23383 (31285)	Loss/tok 5.9682 (7.8169)	LR 2.000e-03
1: TRAIN [0][280/461]	Time 0.185 (0.241)	Data 1.12e-04 (6.44e-04)	Tok/s 23762 (31271)	Loss/tok 6.0787 (7.8198)	LR 2.000e-03
0: TRAIN [0][290/461]	Time 0.183 (0.240)	Data 9.92e-05 (5.90e-04)	Tok/s 23619 (31245)	Loss/tok 5.9417 (7.7727)	LR 2.000e-03
1: TRAIN [0][290/461]	Time 0.184 (0.240)	Data 1.29e-04 (6.26e-04)	Tok/s 23941 (31233)	Loss/tok 6.0343 (7.7762)	LR 2.000e-03
1: TRAIN [0][300/461]	Time 0.184 (0.241)	Data 1.15e-04 (6.09e-04)	Tok/s 23479 (31333)	Loss/tok 5.8753 (7.7241)	LR 2.000e-03
0: TRAIN [0][300/461]	Time 0.185 (0.241)	Data 1.04e-04 (5.73e-04)	Tok/s 23479 (31341)	Loss/tok 5.9335 (7.7206)	LR 2.000e-03
0: TRAIN [0][310/461]	Time 0.280 (0.240)	Data 9.87e-05 (5.59e-04)	Tok/s 36003 (31275)	Loss/tok 6.3825 (7.6785)	LR 2.000e-03
1: TRAIN [0][310/461]	Time 0.280 (0.240)	Data 1.11e-04 (5.93e-04)	Tok/s 35795 (31266)	Loss/tok 6.4378 (7.6828)	LR 2.000e-03
1: TRAIN [0][320/461]	Time 0.334 (0.242)	Data 1.12e-04 (5.78e-04)	Tok/s 39050 (31395)	Loss/tok 6.4541 (7.6287)	LR 2.000e-03
0: TRAIN [0][320/461]	Time 0.334 (0.242)	Data 1.02e-04 (5.44e-04)	Tok/s 39091 (31401)	Loss/tok 6.4354 (7.6237)	LR 2.000e-03
0: TRAIN [0][330/461]	Time 0.184 (0.243)	Data 1.04e-04 (5.31e-04)	Tok/s 23519 (31521)	Loss/tok 5.7426 (7.5695)	LR 2.000e-03
1: TRAIN [0][330/461]	Time 0.185 (0.243)	Data 1.17e-04 (5.64e-04)	Tok/s 22977 (31512)	Loss/tok 5.7617 (7.5746)	LR 2.000e-03
1: TRAIN [0][340/461]	Time 0.229 (0.243)	Data 1.19e-04 (5.51e-04)	Tok/s 31558 (31502)	Loss/tok 5.9401 (7.5311)	LR 2.000e-03
0: TRAIN [0][340/461]	Time 0.229 (0.243)	Data 1.09e-04 (5.19e-04)	Tok/s 31349 (31508)	Loss/tok 5.9103 (7.5265)	LR 2.000e-03
0: TRAIN [0][350/461]	Time 0.226 (0.243)	Data 9.23e-05 (5.07e-04)	Tok/s 32004 (31444)	Loss/tok 5.8295 (7.4888)	LR 2.000e-03
1: TRAIN [0][350/461]	Time 0.226 (0.242)	Data 1.21e-04 (5.38e-04)	Tok/s 32106 (31439)	Loss/tok 5.9180 (7.4928)	LR 2.000e-03
0: TRAIN [0][360/461]	Time 0.182 (0.243)	Data 1.18e-04 (4.96e-04)	Tok/s 24239 (31446)	Loss/tok 5.4670 (7.4447)	LR 2.000e-03
1: TRAIN [0][360/461]	Time 0.182 (0.243)	Data 1.15e-04 (5.26e-04)	Tok/s 23715 (31438)	Loss/tok 5.5466 (7.4502)	LR 2.000e-03
0: TRAIN [0][370/461]	Time 0.227 (0.243)	Data 1.00e-04 (4.85e-04)	Tok/s 31706 (31447)	Loss/tok 5.7475 (7.4031)	LR 2.000e-03
1: TRAIN [0][370/461]	Time 0.228 (0.243)	Data 1.10e-04 (5.15e-04)	Tok/s 31545 (31439)	Loss/tok 5.6991 (7.4082)	LR 2.000e-03
0: TRAIN [0][380/461]	Time 0.336 (0.243)	Data 9.63e-05 (4.75e-04)	Tok/s 38722 (31359)	Loss/tok 6.0388 (7.3669)	LR 2.000e-03
1: TRAIN [0][380/461]	Time 0.336 (0.243)	Data 1.32e-04 (5.05e-04)	Tok/s 39521 (31360)	Loss/tok 6.1492 (7.3722)	LR 2.000e-03
0: TRAIN [0][390/461]	Time 0.183 (0.241)	Data 9.25e-05 (4.66e-04)	Tok/s 23484 (31210)	Loss/tok 5.4303 (7.3364)	LR 2.000e-03
1: TRAIN [0][390/461]	Time 0.183 (0.241)	Data 1.18e-04 (4.95e-04)	Tok/s 23667 (31220)	Loss/tok 5.2640 (7.3415)	LR 2.000e-03
0: TRAIN [0][400/461]	Time 0.225 (0.241)	Data 9.78e-05 (4.57e-04)	Tok/s 32134 (31185)	Loss/tok 5.6353 (7.2973)	LR 2.000e-03
1: TRAIN [0][400/461]	Time 0.226 (0.241)	Data 1.11e-04 (4.85e-04)	Tok/s 32482 (31196)	Loss/tok 5.6931 (7.3031)	LR 2.000e-03
0: TRAIN [0][410/461]	Time 0.279 (0.242)	Data 1.00e-04 (4.48e-04)	Tok/s 36016 (31194)	Loss/tok 5.6706 (7.2558)	LR 2.000e-03
1: TRAIN [0][410/461]	Time 0.279 (0.241)	Data 1.15e-04 (4.76e-04)	Tok/s 35947 (31204)	Loss/tok 5.7286 (7.2620)	LR 2.000e-03
0: TRAIN [0][420/461]	Time 0.148 (0.241)	Data 1.09e-04 (4.39e-04)	Tok/s 14544 (31085)	Loss/tok 4.9172 (7.2243)	LR 2.000e-03
1: TRAIN [0][420/461]	Time 0.149 (0.241)	Data 1.14e-04 (4.68e-04)	Tok/s 14408 (31096)	Loss/tok 4.5300 (7.2300)	LR 2.000e-03
0: TRAIN [0][430/461]	Time 0.278 (0.240)	Data 9.54e-05 (4.32e-04)	Tok/s 35842 (31074)	Loss/tok 5.7144 (7.1871)	LR 2.000e-03
1: TRAIN [0][430/461]	Time 0.278 (0.240)	Data 1.27e-04 (4.60e-04)	Tok/s 36387 (31085)	Loss/tok 5.7155 (7.1930)	LR 2.000e-03
0: TRAIN [0][440/461]	Time 0.183 (0.240)	Data 9.37e-05 (4.24e-04)	Tok/s 23271 (31098)	Loss/tok 5.1423 (7.1465)	LR 2.000e-03
1: TRAIN [0][440/461]	Time 0.183 (0.240)	Data 1.15e-04 (4.52e-04)	Tok/s 23960 (31109)	Loss/tok 4.8885 (7.1528)	LR 2.000e-03
0: TRAIN [0][450/461]	Time 0.278 (0.241)	Data 9.44e-05 (4.17e-04)	Tok/s 36177 (31105)	Loss/tok 5.4967 (7.1076)	LR 2.000e-03
1: TRAIN [0][450/461]	Time 0.278 (0.241)	Data 1.15e-04 (4.45e-04)	Tok/s 35803 (31114)	Loss/tok 5.5473 (7.1144)	LR 2.000e-03
0: TRAIN [0][460/461]	Time 0.183 (0.240)	Data 3.65e-05 (4.12e-04)	Tok/s 23780 (31036)	Loss/tok 4.8411 (7.0752)	LR 2.000e-03
1: TRAIN [0][460/461]	Time 0.183 (0.240)	Data 4.10e-05 (4.39e-04)	Tok/s 24090 (31044)	Loss/tok 4.8863 (7.0821)	LR 2.000e-03
1: Running validation on dev set
1: Executing preallocation
0: Running validation on dev set
0: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.057 (0.057)	Data 1.64e-03 (1.64e-03)	Tok/s 83142 (83142)	Loss/tok 6.8418 (6.8418)
0: VALIDATION [0][0/80]	Time 0.085 (0.085)	Data 4.16e-03 (4.16e-03)	Tok/s 67680 (67680)	Loss/tok 6.8778 (6.8778)
1: VALIDATION [0][10/80]	Time 0.033 (0.040)	Data 1.29e-03 (1.40e-03)	Tok/s 89183 (87274)	Loss/tok 6.5595 (6.6971)
0: VALIDATION [0][10/80]	Time 0.033 (0.045)	Data 2.06e-03 (2.59e-03)	Tok/s 87752 (83289)	Loss/tok 6.5515 (6.6605)
1: VALIDATION [0][20/80]	Time 0.027 (0.035)	Data 1.36e-03 (1.37e-03)	Tok/s 85977 (87444)	Loss/tok 6.2040 (6.5861)
0: VALIDATION [0][20/80]	Time 0.027 (0.038)	Data 2.10e-03 (2.34e-03)	Tok/s 85512 (84228)	Loss/tok 6.1846 (6.5712)
1: VALIDATION [0][30/80]	Time 0.022 (0.031)	Data 1.24e-03 (1.35e-03)	Tok/s 86153 (87559)	Loss/tok 6.2621 (6.5025)
0: VALIDATION [0][30/80]	Time 0.024 (0.034)	Data 2.05e-03 (2.24e-03)	Tok/s 82947 (84036)	Loss/tok 6.1318 (6.4902)
1: VALIDATION [0][40/80]	Time 0.019 (0.029)	Data 1.27e-03 (1.33e-03)	Tok/s 83156 (86609)	Loss/tok 6.1138 (6.4333)
0: VALIDATION [0][40/80]	Time 0.021 (0.031)	Data 2.00e-03 (2.18e-03)	Tok/s 77879 (83373)	Loss/tok 6.0523 (6.4428)
1: VALIDATION [0][50/80]	Time 0.016 (0.027)	Data 1.28e-03 (1.33e-03)	Tok/s 82244 (85967)	Loss/tok 6.0364 (6.3901)
0: VALIDATION [0][50/80]	Time 0.017 (0.029)	Data 1.97e-03 (2.15e-03)	Tok/s 79734 (82628)	Loss/tok 6.1697 (6.3907)
1: VALIDATION [0][60/80]	Time 0.013 (0.025)	Data 1.24e-03 (1.32e-03)	Tok/s 80441 (84963)	Loss/tok 5.8700 (6.3489)
0: VALIDATION [0][60/80]	Time 0.015 (0.026)	Data 2.00e-03 (2.12e-03)	Tok/s 74267 (81534)	Loss/tok 6.0287 (6.3508)
1: VALIDATION [0][70/80]	Time 0.011 (0.023)	Data 1.28e-03 (1.31e-03)	Tok/s 70973 (83586)	Loss/tok 5.9297 (6.3102)
0: VALIDATION [0][70/80]	Time 0.012 (0.025)	Data 1.94e-03 (2.11e-03)	Tok/s 68086 (79922)	Loss/tok 5.7830 (6.3152)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
0: TEST [0][9/47]	Time 0.4892 (0.5287)	Decoder iters 149.0 (149.0)	Tok/s 7367 (9023)
1: TEST [0][9/47]	Time 0.4901 (0.5293)	Decoder iters 149.0 (149.0)	Tok/s 8087 (8720)
0: TEST [0][19/47]	Time 0.4391 (0.4922)	Decoder iters 149.0 (149.0)	Tok/s 5607 (7907)
1: TEST [0][19/47]	Time 0.4398 (0.4927)	Decoder iters 149.0 (149.0)	Tok/s 4941 (7665)
1: TEST [0][29/47]	Time 0.3605 (0.4642)	Decoder iters 149.0 (149.0)	Tok/s 4934 (6735)
0: TEST [0][29/47]	Time 0.3613 (0.4638)	Decoder iters 149.0 (149.0)	Tok/s 4454 (6914)
0: TEST [0][39/47]	Time 0.3332 (0.4367)	Decoder iters 35.0 (146.2)	Tok/s 2782 (6095)
1: TEST [0][39/47]	Time 0.3340 (0.4370)	Decoder iters 149.0 (143.3)	Tok/s 2916 (5964)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.0786	Validation Loss: 6.2784	Test BLEU: 1.18
0: Performance: Epoch: 0	Training: 62080 Tok/s	Validation: 158891 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1323436024
1: Sampler for epoch 1 uses seed 1323436024
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [1][0/461]	Time 0.375 (0.375)	Data 1.53e-01 (1.53e-01)	Tok/s 19212 (19212)	Loss/tok 5.0676 (5.0676)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
1: TRAIN [1][0/461]	Time 0.376 (0.376)	Data 1.73e-01 (1.73e-01)	Tok/s 19192 (19192)	Loss/tok 5.1469 (5.1469)	LR 2.000e-03
0: TRAIN [1][10/461]	Time 0.181 (0.234)	Data 1.51e-04 (1.41e-02)	Tok/s 23757 (28447)	Loss/tok 4.6862 (5.0157)	LR 2.000e-03
1: TRAIN [1][10/461]	Time 0.185 (0.234)	Data 1.08e-04 (1.59e-02)	Tok/s 22619 (28233)	Loss/tok 4.6494 (5.0557)	LR 2.000e-03
0: TRAIN [1][20/461]	Time 0.228 (0.252)	Data 9.58e-05 (7.43e-03)	Tok/s 31851 (31106)	Loss/tok 4.9610 (5.1783)	LR 2.000e-03
1: TRAIN [1][20/461]	Time 0.228 (0.252)	Data 1.63e-04 (8.38e-03)	Tok/s 31776 (30889)	Loss/tok 4.9086 (5.1657)	LR 2.000e-03
0: TRAIN [1][30/461]	Time 0.336 (0.253)	Data 1.02e-04 (5.07e-03)	Tok/s 39090 (31648)	Loss/tok 5.3815 (5.1460)	LR 2.000e-03
1: TRAIN [1][30/461]	Time 0.336 (0.253)	Data 1.37e-04 (5.72e-03)	Tok/s 39231 (31617)	Loss/tok 5.4113 (5.1513)	LR 2.000e-03
0: TRAIN [1][40/461]	Time 0.279 (0.246)	Data 9.27e-05 (3.86e-03)	Tok/s 36175 (31295)	Loss/tok 5.1441 (5.0808)	LR 2.000e-03
1: TRAIN [1][40/461]	Time 0.279 (0.246)	Data 1.62e-04 (4.35e-03)	Tok/s 36053 (31310)	Loss/tok 5.1412 (5.0894)	LR 2.000e-03
1: TRAIN [1][50/461]	Time 0.229 (0.246)	Data 1.01e-04 (3.53e-03)	Tok/s 31291 (31370)	Loss/tok 4.8974 (5.0789)	LR 2.000e-03
0: TRAIN [1][50/461]	Time 0.229 (0.246)	Data 1.55e-04 (3.12e-03)	Tok/s 31748 (31336)	Loss/tok 4.7887 (5.0678)	LR 2.000e-03
1: TRAIN [1][60/461]	Time 0.227 (0.244)	Data 1.07e-04 (2.97e-03)	Tok/s 31862 (31257)	Loss/tok 4.7331 (5.0428)	LR 2.000e-03
0: TRAIN [1][60/461]	Time 0.227 (0.244)	Data 1.51e-04 (2.63e-03)	Tok/s 31971 (31211)	Loss/tok 4.5819 (5.0272)	LR 2.000e-03
0: TRAIN [1][70/461]	Time 0.181 (0.241)	Data 9.54e-05 (2.28e-03)	Tok/s 24418 (31000)	Loss/tok 4.3850 (5.0001)	LR 2.000e-03
1: TRAIN [1][70/461]	Time 0.181 (0.241)	Data 1.05e-04 (2.57e-03)	Tok/s 24181 (31046)	Loss/tok 4.3365 (5.0141)	LR 2.000e-03
0: TRAIN [1][80/461]	Time 0.230 (0.240)	Data 1.07e-04 (2.01e-03)	Tok/s 31548 (31005)	Loss/tok 4.6786 (4.9849)	LR 2.000e-03
1: TRAIN [1][80/461]	Time 0.230 (0.240)	Data 1.04e-04 (2.26e-03)	Tok/s 30798 (31049)	Loss/tok 4.6955 (4.9958)	LR 2.000e-03
0: TRAIN [1][90/461]	Time 0.230 (0.239)	Data 1.07e-04 (1.80e-03)	Tok/s 31353 (30786)	Loss/tok 4.6539 (4.9764)	LR 2.000e-03
1: TRAIN [1][90/461]	Time 0.230 (0.239)	Data 1.04e-04 (2.03e-03)	Tok/s 31440 (30816)	Loss/tok 4.6866 (4.9851)	LR 2.000e-03
0: TRAIN [1][100/461]	Time 0.184 (0.238)	Data 9.73e-05 (1.63e-03)	Tok/s 23689 (30579)	Loss/tok 4.2715 (4.9589)	LR 2.000e-03
1: TRAIN [1][100/461]	Time 0.184 (0.238)	Data 1.07e-04 (1.83e-03)	Tok/s 23362 (30615)	Loss/tok 4.3696 (4.9711)	LR 2.000e-03
1: TRAIN [1][110/461]	Time 0.228 (0.237)	Data 1.16e-04 (1.68e-03)	Tok/s 31768 (30540)	Loss/tok 4.5954 (4.9444)	LR 2.000e-03
0: TRAIN [1][110/461]	Time 0.228 (0.237)	Data 1.01e-04 (1.49e-03)	Tok/s 31307 (30511)	Loss/tok 4.6812 (4.9349)	LR 2.000e-03
0: TRAIN [1][120/461]	Time 0.149 (0.237)	Data 9.97e-05 (1.38e-03)	Tok/s 14434 (30546)	Loss/tok 3.7932 (4.9154)	LR 2.000e-03
1: TRAIN [1][120/461]	Time 0.149 (0.237)	Data 1.06e-04 (1.55e-03)	Tok/s 14704 (30548)	Loss/tok 4.0647 (4.9276)	LR 2.000e-03
0: TRAIN [1][130/461]	Time 0.279 (0.237)	Data 9.99e-05 (1.28e-03)	Tok/s 35935 (30560)	Loss/tok 4.7366 (4.8977)	LR 2.000e-03
1: TRAIN [1][130/461]	Time 0.279 (0.237)	Data 1.03e-04 (1.44e-03)	Tok/s 36176 (30547)	Loss/tok 4.8564 (4.9097)	LR 2.000e-03
0: TRAIN [1][140/461]	Time 0.279 (0.239)	Data 1.00e-04 (1.20e-03)	Tok/s 36420 (30908)	Loss/tok 4.7056 (4.8846)	LR 2.000e-03
1: TRAIN [1][140/461]	Time 0.279 (0.239)	Data 1.10e-04 (1.35e-03)	Tok/s 36229 (30871)	Loss/tok 4.6862 (4.8913)	LR 2.000e-03
0: TRAIN [1][150/461]	Time 0.278 (0.240)	Data 9.54e-05 (1.13e-03)	Tok/s 35959 (31021)	Loss/tok 4.7302 (4.8720)	LR 2.000e-03
1: TRAIN [1][150/461]	Time 0.282 (0.240)	Data 1.01e-04 (1.26e-03)	Tok/s 36225 (30996)	Loss/tok 4.7706 (4.8762)	LR 2.000e-03
1: TRAIN [1][160/461]	Time 0.229 (0.242)	Data 1.03e-04 (1.19e-03)	Tok/s 31284 (31099)	Loss/tok 4.4525 (4.8666)	LR 1.000e-03
0: TRAIN [1][160/461]	Time 0.230 (0.242)	Data 1.11e-04 (1.06e-03)	Tok/s 31738 (31118)	Loss/tok 4.3055 (4.8609)	LR 1.000e-03
1: TRAIN [1][170/461]	Time 0.277 (0.242)	Data 1.09e-04 (1.13e-03)	Tok/s 36910 (31141)	Loss/tok 4.6587 (4.8495)	LR 1.000e-03
0: TRAIN [1][170/461]	Time 0.278 (0.242)	Data 1.13e-04 (1.01e-03)	Tok/s 36639 (31155)	Loss/tok 4.4935 (4.8419)	LR 1.000e-03
1: TRAIN [1][180/461]	Time 0.186 (0.243)	Data 1.07e-04 (1.07e-03)	Tok/s 23047 (31145)	Loss/tok 4.1027 (4.8302)	LR 1.000e-03
0: TRAIN [1][180/461]	Time 0.186 (0.243)	Data 1.08e-04 (9.62e-04)	Tok/s 22908 (31161)	Loss/tok 4.0518 (4.8218)	LR 1.000e-03
0: TRAIN [1][190/461]	Time 0.338 (0.244)	Data 1.04e-04 (9.18e-04)	Tok/s 38834 (31205)	Loss/tok 4.7003 (4.8068)	LR 1.000e-03
1: TRAIN [1][190/461]	Time 0.342 (0.244)	Data 1.04e-04 (1.02e-03)	Tok/s 38111 (31183)	Loss/tok 4.7450 (4.8141)	LR 1.000e-03
0: TRAIN [1][200/461]	Time 0.334 (0.244)	Data 9.39e-05 (8.79e-04)	Tok/s 39313 (31270)	Loss/tok 4.6739 (4.7875)	LR 1.000e-03
1: TRAIN [1][200/461]	Time 0.333 (0.244)	Data 1.15e-04 (9.80e-04)	Tok/s 39026 (31256)	Loss/tok 4.7766 (4.7958)	LR 1.000e-03
0: TRAIN [1][210/461]	Time 0.188 (0.243)	Data 9.97e-05 (8.42e-04)	Tok/s 23008 (31259)	Loss/tok 3.9436 (4.7666)	LR 1.000e-03
1: TRAIN [1][210/461]	Time 0.188 (0.243)	Data 1.45e-04 (9.40e-04)	Tok/s 22718 (31242)	Loss/tok 3.9252 (4.7745)	LR 1.000e-03
0: TRAIN [1][220/461]	Time 0.230 (0.244)	Data 1.11e-04 (8.09e-04)	Tok/s 31053 (31297)	Loss/tok 4.1645 (4.7495)	LR 1.000e-03
1: TRAIN [1][220/461]	Time 0.231 (0.244)	Data 1.12e-04 (9.03e-04)	Tok/s 31943 (31277)	Loss/tok 4.1770 (4.7570)	LR 1.000e-03
0: TRAIN [1][230/461]	Time 0.279 (0.244)	Data 9.58e-05 (7.78e-04)	Tok/s 35969 (31382)	Loss/tok 4.2679 (4.7304)	LR 5.000e-04
1: TRAIN [1][230/461]	Time 0.275 (0.244)	Data 1.60e-04 (8.68e-04)	Tok/s 36868 (31348)	Loss/tok 4.4892 (4.7383)	LR 5.000e-04
0: TRAIN [1][240/461]	Time 0.183 (0.243)	Data 9.73e-05 (7.50e-04)	Tok/s 24012 (31248)	Loss/tok 3.9137 (4.7128)	LR 5.000e-04
1: TRAIN [1][240/461]	Time 0.182 (0.243)	Data 1.07e-04 (8.37e-04)	Tok/s 23945 (31212)	Loss/tok 3.9611 (4.7208)	LR 5.000e-04
0: TRAIN [1][250/461]	Time 0.183 (0.242)	Data 9.51e-05 (7.24e-04)	Tok/s 23460 (31221)	Loss/tok 3.9760 (4.6966)	LR 5.000e-04
1: TRAIN [1][250/461]	Time 0.184 (0.242)	Data 1.07e-04 (8.08e-04)	Tok/s 23589 (31191)	Loss/tok 3.6443 (4.7033)	LR 5.000e-04
0: TRAIN [1][260/461]	Time 0.185 (0.243)	Data 1.00e-04 (7.00e-04)	Tok/s 23698 (31268)	Loss/tok 4.0248 (4.6836)	LR 5.000e-04
1: TRAIN [1][260/461]	Time 0.185 (0.243)	Data 1.14e-04 (7.82e-04)	Tok/s 24085 (31247)	Loss/tok 4.0597 (4.6881)	LR 5.000e-04
0: TRAIN [1][270/461]	Time 0.232 (0.243)	Data 1.05e-04 (6.79e-04)	Tok/s 31066 (31331)	Loss/tok 4.1352 (4.6662)	LR 5.000e-04
1: TRAIN [1][270/461]	Time 0.232 (0.243)	Data 1.15e-04 (7.57e-04)	Tok/s 30598 (31301)	Loss/tok 4.2395 (4.6719)	LR 5.000e-04
0: TRAIN [1][280/461]	Time 0.227 (0.243)	Data 9.54e-05 (6.58e-04)	Tok/s 31880 (31327)	Loss/tok 4.0801 (4.6487)	LR 5.000e-04
1: TRAIN [1][280/461]	Time 0.227 (0.243)	Data 1.05e-04 (7.34e-04)	Tok/s 31105 (31290)	Loss/tok 4.0202 (4.6545)	LR 5.000e-04
0: TRAIN [1][290/461]	Time 0.274 (0.243)	Data 1.03e-04 (6.39e-04)	Tok/s 36243 (31336)	Loss/tok 4.3436 (4.6339)	LR 5.000e-04
1: TRAIN [1][290/461]	Time 0.277 (0.243)	Data 1.02e-04 (7.13e-04)	Tok/s 36337 (31306)	Loss/tok 4.4808 (4.6413)	LR 5.000e-04
0: TRAIN [1][300/461]	Time 0.227 (0.244)	Data 9.99e-05 (6.21e-04)	Tok/s 32504 (31371)	Loss/tok 4.2133 (4.6240)	LR 5.000e-04
1: TRAIN [1][300/461]	Time 0.226 (0.244)	Data 1.05e-04 (6.93e-04)	Tok/s 31629 (31327)	Loss/tok 4.0454 (4.6301)	LR 5.000e-04
1: TRAIN [1][310/461]	Time 0.227 (0.243)	Data 1.07e-04 (6.74e-04)	Tok/s 32003 (31273)	Loss/tok 4.0284 (4.6163)	LR 2.500e-04
0: TRAIN [1][310/461]	Time 0.227 (0.243)	Data 9.70e-05 (6.04e-04)	Tok/s 32246 (31312)	Loss/tok 3.9178 (4.6110)	LR 2.500e-04
0: TRAIN [1][320/461]	Time 0.279 (0.244)	Data 1.03e-04 (5.89e-04)	Tok/s 35776 (31394)	Loss/tok 4.2482 (4.5970)	LR 2.500e-04
1: TRAIN [1][320/461]	Time 0.279 (0.244)	Data 1.11e-04 (6.57e-04)	Tok/s 35965 (31362)	Loss/tok 4.2805 (4.6044)	LR 2.500e-04
1: TRAIN [1][330/461]	Time 0.336 (0.244)	Data 1.11e-04 (6.42e-04)	Tok/s 38952 (31348)	Loss/tok 4.5631 (4.5919)	LR 2.500e-04
0: TRAIN [1][330/461]	Time 0.336 (0.244)	Data 1.60e-04 (5.74e-04)	Tok/s 38618 (31376)	Loss/tok 4.4195 (4.5840)	LR 2.500e-04
1: TRAIN [1][340/461]	Time 0.271 (0.245)	Data 1.08e-04 (6.26e-04)	Tok/s 37143 (31439)	Loss/tok 4.3880 (4.5825)	LR 2.500e-04
0: TRAIN [1][340/461]	Time 0.271 (0.245)	Data 1.03e-04 (5.61e-04)	Tok/s 36590 (31469)	Loss/tok 4.2019 (4.5743)	LR 2.500e-04
1: TRAIN [1][350/461]	Time 0.277 (0.245)	Data 1.01e-04 (6.11e-04)	Tok/s 36392 (31434)	Loss/tok 4.3503 (4.5737)	LR 2.500e-04
0: TRAIN [1][350/461]	Time 0.279 (0.245)	Data 1.52e-04 (5.48e-04)	Tok/s 36226 (31466)	Loss/tok 4.2049 (4.5644)	LR 2.500e-04
1: TRAIN [1][360/461]	Time 0.231 (0.245)	Data 1.03e-04 (5.97e-04)	Tok/s 31562 (31331)	Loss/tok 3.9719 (4.5626)	LR 2.500e-04
0: TRAIN [1][360/461]	Time 0.230 (0.245)	Data 1.57e-04 (5.37e-04)	Tok/s 31278 (31360)	Loss/tok 3.9970 (4.5533)	LR 2.500e-04
1: TRAIN [1][370/461]	Time 0.276 (0.244)	Data 1.27e-04 (5.84e-04)	Tok/s 36382 (31288)	Loss/tok 4.2495 (4.5511)	LR 2.500e-04
0: TRAIN [1][370/461]	Time 0.277 (0.244)	Data 1.58e-04 (5.27e-04)	Tok/s 36075 (31315)	Loss/tok 4.2583 (4.5419)	LR 2.500e-04
1: TRAIN [1][380/461]	Time 0.232 (0.244)	Data 1.04e-04 (5.71e-04)	Tok/s 30383 (31244)	Loss/tok 3.9575 (4.5397)	LR 2.500e-04
0: TRAIN [1][380/461]	Time 0.228 (0.244)	Data 1.54e-04 (5.17e-04)	Tok/s 31537 (31279)	Loss/tok 4.0308 (4.5313)	LR 2.500e-04
1: TRAIN [1][390/461]	Time 0.185 (0.244)	Data 1.61e-04 (5.60e-04)	Tok/s 23122 (31227)	Loss/tok 3.8018 (4.5279)	LR 1.250e-04
0: TRAIN [1][390/461]	Time 0.185 (0.244)	Data 1.56e-04 (5.08e-04)	Tok/s 22880 (31263)	Loss/tok 3.6059 (4.5198)	LR 1.250e-04
1: TRAIN [1][400/461]	Time 0.226 (0.243)	Data 1.63e-04 (5.49e-04)	Tok/s 32212 (31117)	Loss/tok 3.8809 (4.5172)	LR 1.250e-04
0: TRAIN [1][400/461]	Time 0.227 (0.243)	Data 1.61e-04 (4.99e-04)	Tok/s 31898 (31155)	Loss/tok 3.9941 (4.5092)	LR 1.250e-04
1: TRAIN [1][410/461]	Time 0.184 (0.241)	Data 1.04e-04 (5.39e-04)	Tok/s 24082 (30925)	Loss/tok 3.6939 (4.5087)	LR 1.250e-04
0: TRAIN [1][410/461]	Time 0.185 (0.242)	Data 1.55e-04 (4.91e-04)	Tok/s 23173 (30965)	Loss/tok 3.7091 (4.5004)	LR 1.250e-04
0: TRAIN [1][420/461]	Time 0.224 (0.241)	Data 9.73e-05 (4.82e-04)	Tok/s 31865 (30955)	Loss/tok 3.8926 (4.4895)	LR 1.250e-04
1: TRAIN [1][420/461]	Time 0.224 (0.241)	Data 1.06e-04 (5.28e-04)	Tok/s 31494 (30913)	Loss/tok 3.9277 (4.4982)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [1][430/461]	Time 0.276 (0.241)	Data 9.66e-05 (4.73e-04)	Tok/s 36933 (31043)	Loss/tok 4.1805 (4.4801)	LR 1.250e-04
1: TRAIN [1][430/461]	Time 0.280 (0.241)	Data 1.06e-04 (5.19e-04)	Tok/s 35966 (31001)	Loss/tok 4.3076 (4.4889)	LR 1.250e-04
0: TRAIN [1][440/461]	Time 0.182 (0.241)	Data 1.09e-04 (4.65e-04)	Tok/s 24169 (31036)	Loss/tok 3.8334 (4.4720)	LR 1.250e-04
1: TRAIN [1][440/461]	Time 0.182 (0.241)	Data 1.07e-04 (5.10e-04)	Tok/s 24113 (31000)	Loss/tok 3.6877 (4.4795)	LR 1.250e-04
0: TRAIN [1][450/461]	Time 0.339 (0.241)	Data 8.77e-05 (4.57e-04)	Tok/s 38542 (31003)	Loss/tok 4.3855 (4.4652)	LR 1.250e-04
1: TRAIN [1][450/461]	Time 0.339 (0.241)	Data 1.04e-04 (5.01e-04)	Tok/s 38332 (30966)	Loss/tok 4.4917 (4.4729)	LR 1.250e-04
0: TRAIN [1][460/461]	Time 0.182 (0.241)	Data 3.24e-05 (4.50e-04)	Tok/s 23555 (30961)	Loss/tok 3.8299 (4.4574)	LR 1.250e-04
1: TRAIN [1][460/461]	Time 0.182 (0.241)	Data 4.29e-05 (4.96e-04)	Tok/s 24173 (30927)	Loss/tok 3.7494 (4.4654)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
0: Executing preallocation
1: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.058 (0.058)	Data 1.56e-03 (1.56e-03)	Tok/s 81624 (81624)	Loss/tok 5.6385 (5.6385)
0: VALIDATION [1][0/80]	Time 0.081 (0.081)	Data 1.56e-03 (1.56e-03)	Tok/s 70330 (70330)	Loss/tok 5.8029 (5.8029)
1: VALIDATION [1][10/80]	Time 0.033 (0.041)	Data 1.32e-03 (1.39e-03)	Tok/s 87833 (86309)	Loss/tok 5.2116 (5.4583)
0: VALIDATION [1][10/80]	Time 0.033 (0.044)	Data 1.33e-03 (1.38e-03)	Tok/s 88927 (85614)	Loss/tok 5.3056 (5.4445)
1: VALIDATION [1][20/80]	Time 0.026 (0.035)	Data 1.34e-03 (1.38e-03)	Tok/s 87186 (86888)	Loss/tok 4.9492 (5.3645)
0: VALIDATION [1][20/80]	Time 0.027 (0.037)	Data 1.29e-03 (1.34e-03)	Tok/s 88015 (86711)	Loss/tok 4.9427 (5.3818)
1: VALIDATION [1][30/80]	Time 0.023 (0.032)	Data 1.25e-03 (1.36e-03)	Tok/s 85240 (86814)	Loss/tok 5.1112 (5.2854)
0: VALIDATION [1][30/80]	Time 0.022 (0.033)	Data 1.22e-03 (1.32e-03)	Tok/s 87068 (86958)	Loss/tok 4.9937 (5.3105)
1: VALIDATION [1][40/80]	Time 0.019 (0.029)	Data 1.56e-03 (1.40e-03)	Tok/s 82205 (85804)	Loss/tok 5.0109 (5.2268)
0: VALIDATION [1][40/80]	Time 0.019 (0.030)	Data 1.24e-03 (1.30e-03)	Tok/s 85986 (86793)	Loss/tok 4.9073 (5.2768)
1: VALIDATION [1][50/80]	Time 0.016 (0.027)	Data 1.50e-03 (1.43e-03)	Tok/s 82560 (85094)	Loss/tok 5.0623 (5.2009)
0: VALIDATION [1][50/80]	Time 0.016 (0.027)	Data 1.29e-03 (1.29e-03)	Tok/s 85456 (86295)	Loss/tok 5.1333 (5.2349)
1: VALIDATION [1][60/80]	Time 0.013 (0.025)	Data 1.49e-03 (1.45e-03)	Tok/s 78455 (84064)	Loss/tok 4.8036 (5.1706)
0: VALIDATION [1][60/80]	Time 0.014 (0.025)	Data 1.19e-03 (1.28e-03)	Tok/s 79732 (85557)	Loss/tok 4.8328 (5.1976)
1: VALIDATION [1][70/80]	Time 0.011 (0.023)	Data 1.50e-03 (1.46e-03)	Tok/s 69692 (82646)	Loss/tok 4.8009 (5.1411)
0: VALIDATION [1][70/80]	Time 0.011 (0.023)	Data 1.20e-03 (1.27e-03)	Tok/s 73252 (84381)	Loss/tok 4.8058 (5.1710)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
1: TEST [1][9/47]	Time 0.3828 (0.4230)	Decoder iters 80.0 (139.3)	Tok/s 6792 (7657)
0: TEST [1][9/47]	Time 0.3834 (0.4231)	Decoder iters 149.0 (141.9)	Tok/s 6752 (7815)
0: TEST [1][19/47]	Time 0.1519 (0.3659)	Decoder iters 45.0 (126.0)	Tok/s 12807 (7904)
1: TEST [1][19/47]	Time 0.1520 (0.3658)	Decoder iters 51.0 (121.1)	Tok/s 13136 (7745)
1: TEST [1][29/47]	Time 0.1224 (0.3159)	Decoder iters 43.0 (105.9)	Tok/s 11922 (8165)
0: TEST [1][29/47]	Time 0.1225 (0.3159)	Decoder iters 37.0 (109.6)	Tok/s 12056 (8312)
0: TEST [1][39/47]	Time 0.0906 (0.2801)	Decoder iters 29.0 (98.0)	Tok/s 11405 (8278)
1: TEST [1][39/47]	Time 0.0900 (0.2801)	Decoder iters 30.0 (88.7)	Tok/s 11375 (8129)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.4614	Validation Loss: 5.1272	Test BLEU: 6.87
0: Performance: Epoch: 1	Training: 61887 Tok/s	Validation: 162665 Tok/s
0: Finished epoch 1
1: Total training time 283 s
0: Total training time 284 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 160|                      6.87|                      61983.5|                         4.725|
DONE!
