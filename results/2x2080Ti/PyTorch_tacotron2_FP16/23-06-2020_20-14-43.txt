:::NVLOGv0.2.2 Tacotron2_PyT 1592943285.977570534 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592943286.006710052 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592943286.024480820 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592943286.414644241 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 2, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592943286.425819159 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 72, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 2, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592943286.541939259 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1592943295.660006762 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592943295.661417484 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943297.322892427 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943301.598315954 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.884544372558594
:::NVLOGv0.2.2 Tacotron2_PyT 1592943303.412523985 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943303.413236380 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 13369.028748850928
:::NVLOGv0.2.2 Tacotron2_PyT 1592943303.413748980 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.09236478805542
Batch: 1/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943303.423188448 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943304.804656982 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.316490173339844
:::NVLOGv0.2.2 Tacotron2_PyT 1592943306.647268772 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943306.648920059 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25435.080740089274
:::NVLOGv0.2.2 Tacotron2_PyT 1592943306.650258780 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2254271507263184
Batch: 2/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943306.659039974 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592943308.125371456 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.704811096191406
:::NVLOGv0.2.2 Tacotron2_PyT 1592943310.028701544 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592943310.030456066 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24506.958717748617
:::NVLOGv0.2.2 Tacotron2_PyT 1592943310.031848192 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.371001720428467
Batch: 3/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943310.040979385 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592943311.503041267 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.86383056640625
:::NVLOGv0.2.2 Tacotron2_PyT 1592943313.369433165 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592943313.370820284 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24232.595041274973
:::NVLOGv0.2.2 Tacotron2_PyT 1592943313.372623682 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.329688787460327
Batch: 4/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943313.384027958 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592943314.780571222 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.818397521972656
:::NVLOGv0.2.2 Tacotron2_PyT 1592943317.369440556 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592943317.371087313 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 20410.26834930546
:::NVLOGv0.2.2 Tacotron2_PyT 1592943317.373387575 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.986473798751831
Batch: 5/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943317.382884026 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592943320.845465660 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.905555725097656
:::NVLOGv0.2.2 Tacotron2_PyT 1592943323.480337381 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592943323.481639385 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 13374.697014845666
:::NVLOGv0.2.2 Tacotron2_PyT 1592943323.485175848 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.098455905914307
Batch: 6/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943323.493786812 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592943326.844038963 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.85337829589844
:::NVLOGv0.2.2 Tacotron2_PyT 1592943328.677520752 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592943328.680144310 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 15861.475930705597
:::NVLOGv0.2.2 Tacotron2_PyT 1592943328.683371544 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.1841959953308105
Batch: 7/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943328.693505287 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592943330.118626118 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.385711669921875
:::NVLOGv0.2.2 Tacotron2_PyT 1592943331.968625307 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592943331.970427990 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24572.191454558
:::NVLOGv0.2.2 Tacotron2_PyT 1592943331.972352266 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2759389877319336
:::NVLOGv0.2.2 Tacotron2_PyT 1592943332.117914438 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943332.119536400 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 17896.232198508435
:::NVLOGv0.2.2 Tacotron2_PyT 1592943332.121078968 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 20220.286999672313
:::NVLOGv0.2.2 Tacotron2_PyT 1592943332.121635437 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.21658992767334
:::NVLOGv0.2.2 Tacotron2_PyT 1592943332.122142315 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 36.457059383392334
:::NVLOGv0.2.2 Tacotron2_PyT 1592943332.122644186 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592943334.652437925 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 45.739349365234375
:::NVLOGv0.2.2 Tacotron2_PyT 1592943334.654797316 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943334.927811146 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943336.717422247 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943338.185118198 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.629554748535156
:::NVLOGv0.2.2 Tacotron2_PyT 1592943340.157042027 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592943340.158970356 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 23604.441828213392
:::NVLOGv0.2.2 Tacotron2_PyT 1592943340.159920931 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.441089630126953
Batch: 1/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943340.171675444 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943341.546581745 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.774200439453125
:::NVLOGv0.2.2 Tacotron2_PyT 1592943343.471344233 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943343.473295927 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 23728.062586079082
:::NVLOGv0.2.2 Tacotron2_PyT 1592943343.475228310 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.3006908893585205
Batch: 2/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943343.486472130 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592943344.862575293 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.751556396484375
:::NVLOGv0.2.2 Tacotron2_PyT 1592943346.692577124 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592943346.694458008 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24628.845569958652
:::NVLOGv0.2.2 Tacotron2_PyT 1592943346.695936441 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.20705246925354
Batch: 3/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943346.706209183 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592943348.202453852 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.203880310058594
:::NVLOGv0.2.2 Tacotron2_PyT 1592943350.064152002 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592943350.065629721 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24334.426587122445
:::NVLOGv0.2.2 Tacotron2_PyT 1592943350.067936182 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.3589038848876953
Batch: 4/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943350.076520443 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592943351.535555840 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 49.48786926269531
:::NVLOGv0.2.2 Tacotron2_PyT 1592943353.379252195 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592943353.380489588 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25884.661154097546
:::NVLOGv0.2.2 Tacotron2_PyT 1592943353.382371426 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.303694009780884
Batch: 5/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943353.390368223 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592943354.783130169 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.713661193847656
:::NVLOGv0.2.2 Tacotron2_PyT 1592943356.638880730 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592943356.640632629 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25001.55829323991
:::NVLOGv0.2.2 Tacotron2_PyT 1592943356.643231153 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2493574619293213
Batch: 6/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943356.653694868 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592943358.023544312 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 49.24126052856445
:::NVLOGv0.2.2 Tacotron2_PyT 1592943359.882828474 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592943359.884470463 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 26444.34442684944
:::NVLOGv0.2.2 Tacotron2_PyT 1592943359.886734962 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.229802131652832
Batch: 7/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943359.896315575 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592943361.247803450 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.831520080566406
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.021964312 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.023516893 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25639.806501973435
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.025334597 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.12642765045166
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.179685354 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.182818890 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 23098.53616122723
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.184564590 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 24908.268368441735
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.185909271 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.204187870025635
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.187283278 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 28.25252628326416
:::NVLOGv0.2.2 Tacotron2_PyT 1592943363.187895298 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943365.827991009 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 45.728782653808594
:::NVLOGv0.2.2 Tacotron2_PyT 1592943365.830816507 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592943365.831934690 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 79.2894811630249
:::NVLOGv0.2.2 Tacotron2_PyT 1592943365.832292795 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 79.2894811630249
:::NVLOGv0.2.2 Tacotron2_PyT 1592943365.832667112 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 79.94023537635803
:::NVLOGv0.2.2 Tacotron2_PyT 1592943365.832989216 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
