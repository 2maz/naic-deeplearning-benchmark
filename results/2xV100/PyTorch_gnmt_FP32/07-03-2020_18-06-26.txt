1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB

Nvidia driver version: 440.33.01
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=372, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Tesla V100-SXM2-32GB
GPU 1: Tesla V100-SXM2-32GB

Nvidia driver version: 440.33.01
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=372, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Pairs before: 160078, after: 148120
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 261
0: Scheduler decay interval: 32
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 261
1: Scheduler decay interval: 32
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
1: TRAIN [0][0/196]	Time 0.988 (0.988)	Data 2.26e-01 (2.26e-01)	Tok/s 17051 (17051)	Loss/tok 10.6778 (10.6778)	LR 2.047e-05
0: TRAIN [0][0/196]	Time 0.982 (0.982)	Data 2.35e-01 (2.35e-01)	Tok/s 17120 (17120)	Loss/tok 10.6764 (10.6764)	LR 2.047e-05
1: TRAIN [0][10/196]	Time 0.998 (0.821)	Data 3.27e-04 (2.07e-02)	Tok/s 23378 (21694)	Loss/tok 9.6832 (10.0840)	LR 2.576e-05
0: TRAIN [0][10/196]	Time 0.998 (0.821)	Data 2.99e-04 (2.16e-02)	Tok/s 23298 (21797)	Loss/tok 9.6734 (10.0783)	LR 2.576e-05
0: TRAIN [0][20/196]	Time 0.754 (0.802)	Data 1.90e-04 (1.14e-02)	Tok/s 22060 (21919)	Loss/tok 9.1596 (9.7497)	LR 3.244e-05
1: TRAIN [0][20/196]	Time 0.754 (0.803)	Data 2.95e-04 (1.10e-02)	Tok/s 22194 (21877)	Loss/tok 9.1065 (9.7496)	LR 3.244e-05
1: TRAIN [0][30/196]	Time 0.258 (0.804)	Data 1.83e-04 (7.49e-03)	Tok/s 19341 (21962)	Loss/tok 8.6999 (9.5188)	LR 4.083e-05
0: TRAIN [0][30/196]	Time 0.257 (0.804)	Data 1.97e-04 (7.81e-03)	Tok/s 19631 (21963)	Loss/tok 8.7343 (9.5138)	LR 4.083e-05
0: TRAIN [0][40/196]	Time 0.260 (0.767)	Data 1.95e-04 (5.96e-03)	Tok/s 19293 (21814)	Loss/tok 8.4542 (9.3649)	LR 5.141e-05
1: TRAIN [0][40/196]	Time 0.260 (0.768)	Data 1.93e-04 (5.72e-03)	Tok/s 18990 (21797)	Loss/tok 8.4290 (9.3680)	LR 5.141e-05
0: TRAIN [0][50/196]	Time 0.759 (0.755)	Data 2.02e-04 (4.83e-03)	Tok/s 22297 (21749)	Loss/tok 8.4091 (9.2160)	LR 6.472e-05
1: TRAIN [0][50/196]	Time 0.759 (0.755)	Data 1.94e-04 (4.63e-03)	Tok/s 22000 (21716)	Loss/tok 8.4670 (9.2194)	LR 6.472e-05
1: TRAIN [0][60/196]	Time 0.757 (0.768)	Data 1.98e-04 (3.91e-03)	Tok/s 21892 (21818)	Loss/tok 8.1635 (9.0633)	LR 8.148e-05
0: TRAIN [0][60/196]	Time 0.757 (0.768)	Data 2.03e-04 (4.07e-03)	Tok/s 22292 (21858)	Loss/tok 8.2396 (9.0593)	LR 8.148e-05
0: TRAIN [0][70/196]	Time 1.000 (0.785)	Data 2.08e-04 (3.53e-03)	Tok/s 23462 (21912)	Loss/tok 8.1909 (8.9356)	LR 1.026e-04
1: TRAIN [0][70/196]	Time 1.000 (0.785)	Data 1.97e-04 (3.39e-03)	Tok/s 23375 (21884)	Loss/tok 8.1744 (8.9381)	LR 1.026e-04
0: TRAIN [0][80/196]	Time 0.764 (0.805)	Data 2.15e-04 (3.12e-03)	Tok/s 21709 (21976)	Loss/tok 7.8242 (8.8006)	LR 1.291e-04
1: TRAIN [0][80/196]	Time 0.764 (0.805)	Data 1.98e-04 (2.99e-03)	Tok/s 21886 (21946)	Loss/tok 7.8370 (8.8058)	LR 1.291e-04
0: TRAIN [0][90/196]	Time 0.498 (0.808)	Data 1.96e-04 (2.80e-03)	Tok/s 20358 (21998)	Loss/tok 7.6054 (8.6991)	LR 1.626e-04
1: TRAIN [0][90/196]	Time 0.498 (0.808)	Data 1.96e-04 (2.69e-03)	Tok/s 20256 (21959)	Loss/tok 7.5739 (8.7030)	LR 1.626e-04
0: TRAIN [0][100/196]	Time 1.286 (0.800)	Data 1.79e-04 (2.55e-03)	Tok/s 23693 (21970)	Loss/tok 8.7037 (8.6298)	LR 2.047e-04
1: TRAIN [0][100/196]	Time 1.286 (0.801)	Data 2.06e-04 (2.44e-03)	Tok/s 23495 (21929)	Loss/tok 8.7202 (8.6348)	LR 2.047e-04
1: TRAIN [0][110/196]	Time 0.258 (0.800)	Data 1.98e-04 (2.24e-03)	Tok/s 19285 (21923)	Loss/tok 7.1472 (8.5582)	LR 2.576e-04
0: TRAIN [0][110/196]	Time 0.261 (0.800)	Data 1.91e-04 (2.34e-03)	Tok/s 19058 (21955)	Loss/tok 7.1892 (8.5533)	LR 2.576e-04
0: TRAIN [0][120/196]	Time 0.257 (0.792)	Data 2.09e-04 (2.16e-03)	Tok/s 19464 (21923)	Loss/tok 6.9721 (8.4936)	LR 3.244e-04
1: TRAIN [0][120/196]	Time 0.257 (0.792)	Data 1.84e-04 (2.07e-03)	Tok/s 19284 (21890)	Loss/tok 7.0222 (8.4979)	LR 3.244e-04
0: TRAIN [0][130/196]	Time 1.290 (0.794)	Data 2.15e-04 (2.01e-03)	Tok/s 23521 (21927)	Loss/tok 7.9517 (8.4368)	LR 4.083e-04
1: TRAIN [0][130/196]	Time 1.290 (0.794)	Data 1.73e-04 (1.93e-03)	Tok/s 23550 (21893)	Loss/tok 7.9034 (8.4403)	LR 4.083e-04
0: TRAIN [0][140/196]	Time 0.999 (0.796)	Data 2.05e-04 (1.88e-03)	Tok/s 23482 (21943)	Loss/tok 7.7656 (8.3823)	LR 5.141e-04
1: TRAIN [0][140/196]	Time 1.000 (0.796)	Data 1.77e-04 (1.80e-03)	Tok/s 23647 (21910)	Loss/tok 7.7071 (8.3850)	LR 5.141e-04
0: TRAIN [0][150/196]	Time 0.260 (0.803)	Data 2.09e-04 (1.77e-03)	Tok/s 19162 (21961)	Loss/tok 6.9892 (8.3323)	LR 6.472e-04
1: TRAIN [0][150/196]	Time 0.261 (0.803)	Data 2.02e-04 (1.70e-03)	Tok/s 19481 (21931)	Loss/tok 6.9907 (8.3357)	LR 6.472e-04
0: TRAIN [0][160/196]	Time 1.005 (0.809)	Data 1.97e-04 (1.68e-03)	Tok/s 23221 (21996)	Loss/tok 7.7314 (8.2885)	LR 8.148e-04
1: TRAIN [0][160/196]	Time 1.006 (0.810)	Data 1.93e-04 (1.60e-03)	Tok/s 23288 (21967)	Loss/tok 7.7411 (8.2906)	LR 8.148e-04
0: TRAIN [0][170/196]	Time 0.499 (0.808)	Data 2.02e-04 (1.59e-03)	Tok/s 20425 (21995)	Loss/tok 7.1734 (8.2462)	LR 1.026e-03
1: TRAIN [0][170/196]	Time 0.499 (0.808)	Data 2.11e-04 (1.52e-03)	Tok/s 20505 (21964)	Loss/tok 7.1672 (8.2491)	LR 1.026e-03
0: TRAIN [0][180/196]	Time 1.001 (0.802)	Data 1.68e-04 (1.51e-03)	Tok/s 23241 (21983)	Loss/tok 7.7281 (8.2170)	LR 1.291e-03
1: TRAIN [0][180/196]	Time 1.001 (0.802)	Data 1.93e-04 (1.45e-03)	Tok/s 23462 (21952)	Loss/tok 7.6955 (8.2193)	LR 1.291e-03
0: TRAIN [0][190/196]	Time 0.758 (0.808)	Data 1.86e-04 (1.45e-03)	Tok/s 22039 (22016)	Loss/tok 7.3599 (8.1774)	LR 1.626e-03
1: TRAIN [0][190/196]	Time 0.758 (0.808)	Data 2.00e-04 (1.38e-03)	Tok/s 22016 (21985)	Loss/tok 7.3099 (8.1791)	LR 1.626e-03
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [0][0/40]	Time 0.143 (0.143)	Data 5.95e-03 (5.95e-03)	Tok/s 59866 (59866)	Loss/tok 8.3250 (8.3250)
0: VALIDATION [0][0/40]	Time 0.215 (0.215)	Data 5.53e-03 (5.53e-03)	Tok/s 48775 (48775)	Loss/tok 8.3475 (8.3475)
1: VALIDATION [0][10/40]	Time 0.072 (0.097)	Data 3.38e-03 (3.88e-03)	Tok/s 62937 (61700)	Loss/tok 8.2149 (8.2211)
0: VALIDATION [0][10/40]	Time 0.073 (0.103)	Data 3.03e-03 (3.61e-03)	Tok/s 63517 (62485)	Loss/tok 8.0423 (8.2267)
1: VALIDATION [0][20/40]	Time 0.053 (0.080)	Data 3.27e-03 (3.68e-03)	Tok/s 59599 (61151)	Loss/tok 7.9618 (8.1607)
0: VALIDATION [0][20/40]	Time 0.052 (0.084)	Data 2.97e-03 (3.35e-03)	Tok/s 61932 (62285)	Loss/tok 8.0424 (8.1487)
1: VALIDATION [0][30/40]	Time 0.036 (0.069)	Data 3.37e-03 (3.59e-03)	Tok/s 57778 (60325)	Loss/tok 7.8399 (8.1157)
0: VALIDATION [0][30/40]	Time 0.037 (0.071)	Data 2.86e-03 (3.22e-03)	Tok/s 58098 (61330)	Loss/tok 7.8672 (8.1084)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [0][9/12]	Time 1.6008 (2.6139)	Decoder iters 149.0 (149.0)	Tok/s 4513 (7041)
0: TEST [0][9/12]	Time 1.6105 (2.6118)	Decoder iters 149.0 (149.0)	Tok/s 5145 (7573)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 8.1620	Validation Loss: 8.0654	Test BLEU: 0.09
0: Performance: Epoch: 0	Training: 43999 Tok/s	Validation: 117603 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1323436024
1: Sampler for epoch 1 uses seed 1323436024
0: TRAIN [1][0/196]	Time 1.000 (1.000)	Data 2.42e-01 (2.42e-01)	Tok/s 16732 (16732)	Loss/tok 7.3743 (7.3743)	LR 1.867e-03
1: TRAIN [1][0/196]	Time 0.980 (0.980)	Data 2.27e-01 (2.27e-01)	Tok/s 17174 (17174)	Loss/tok 7.3710 (7.3710)	LR 1.867e-03
0: TRAIN [1][10/196]	Time 1.002 (0.972)	Data 2.01e-04 (2.22e-02)	Tok/s 23375 (22188)	Loss/tok 7.2827 (7.3911)	LR 2.000e-03
1: TRAIN [1][10/196]	Time 1.002 (0.970)	Data 2.15e-04 (2.09e-02)	Tok/s 23173 (22209)	Loss/tok 7.2969 (7.3701)	LR 2.000e-03
1: TRAIN [1][20/196]	Time 0.499 (0.881)	Data 2.11e-04 (1.10e-02)	Tok/s 19795 (22063)	Loss/tok 6.6544 (7.2810)	LR 2.000e-03
0: TRAIN [1][20/196]	Time 0.499 (0.882)	Data 2.54e-04 (1.17e-02)	Tok/s 20038 (22021)	Loss/tok 6.6690 (7.2820)	LR 2.000e-03
0: TRAIN [1][30/196]	Time 0.497 (0.867)	Data 1.92e-04 (8.02e-03)	Tok/s 20369 (22041)	Loss/tok 6.6373 (7.2110)	LR 2.000e-03
1: TRAIN [1][30/196]	Time 0.499 (0.867)	Data 1.98e-04 (7.54e-03)	Tok/s 20530 (22068)	Loss/tok 6.5901 (7.2130)	LR 2.000e-03
1: TRAIN [1][40/196]	Time 0.492 (0.847)	Data 1.97e-04 (5.75e-03)	Tok/s 20646 (22025)	Loss/tok 6.5916 (7.1263)	LR 2.000e-03
0: TRAIN [1][40/196]	Time 0.493 (0.847)	Data 1.96e-04 (6.11e-03)	Tok/s 20566 (22042)	Loss/tok 6.5314 (7.1220)	LR 2.000e-03
0: TRAIN [1][50/196]	Time 1.002 (0.829)	Data 1.85e-04 (4.96e-03)	Tok/s 23302 (22011)	Loss/tok 6.7964 (7.0539)	LR 2.000e-03
1: TRAIN [1][50/196]	Time 1.002 (0.828)	Data 2.03e-04 (4.67e-03)	Tok/s 23257 (21994)	Loss/tok 6.8114 (7.0539)	LR 2.000e-03
0: TRAIN [1][60/196]	Time 0.764 (0.822)	Data 2.15e-04 (4.18e-03)	Tok/s 21818 (21969)	Loss/tok 6.4886 (6.9883)	LR 2.000e-03
1: TRAIN [1][60/196]	Time 0.765 (0.822)	Data 2.04e-04 (3.93e-03)	Tok/s 21915 (21967)	Loss/tok 6.5162 (6.9873)	LR 2.000e-03
1: TRAIN [1][70/196]	Time 1.008 (0.819)	Data 1.98e-04 (3.41e-03)	Tok/s 23093 (21988)	Loss/tok 6.5109 (6.9191)	LR 1.000e-03
0: TRAIN [1][70/196]	Time 1.009 (0.819)	Data 1.87e-04 (3.62e-03)	Tok/s 23110 (21984)	Loss/tok 6.5640 (6.9188)	LR 1.000e-03
0: TRAIN [1][80/196]	Time 1.292 (0.835)	Data 2.20e-04 (3.20e-03)	Tok/s 23530 (22030)	Loss/tok 6.5719 (6.8549)	LR 1.000e-03
1: TRAIN [1][80/196]	Time 1.292 (0.835)	Data 1.91e-04 (3.01e-03)	Tok/s 23474 (22026)	Loss/tok 6.5482 (6.8521)	LR 1.000e-03
0: TRAIN [1][90/196]	Time 0.759 (0.823)	Data 2.15e-04 (2.87e-03)	Tok/s 22289 (22010)	Loss/tok 6.1799 (6.7946)	LR 1.000e-03
1: TRAIN [1][90/196]	Time 0.759 (0.823)	Data 2.01e-04 (2.70e-03)	Tok/s 22106 (21999)	Loss/tok 6.1618 (6.7898)	LR 1.000e-03
0: TRAIN [1][100/196]	Time 0.501 (0.802)	Data 1.84e-04 (2.61e-03)	Tok/s 20178 (21897)	Loss/tok 5.7036 (6.7439)	LR 5.000e-04
1: TRAIN [1][100/196]	Time 0.501 (0.802)	Data 2.05e-04 (2.46e-03)	Tok/s 19896 (21880)	Loss/tok 5.6863 (6.7391)	LR 5.000e-04
0: TRAIN [1][110/196]	Time 0.502 (0.789)	Data 2.08e-04 (2.39e-03)	Tok/s 20401 (21842)	Loss/tok 5.6068 (6.6910)	LR 5.000e-04
1: TRAIN [1][110/196]	Time 0.502 (0.789)	Data 1.95e-04 (2.25e-03)	Tok/s 20087 (21829)	Loss/tok 5.7395 (6.6868)	LR 5.000e-04
0: TRAIN [1][120/196]	Time 0.757 (0.770)	Data 1.85e-04 (2.21e-03)	Tok/s 22094 (21762)	Loss/tok 6.0074 (6.6401)	LR 5.000e-04
1: TRAIN [1][120/196]	Time 0.757 (0.770)	Data 1.97e-04 (2.08e-03)	Tok/s 22190 (21746)	Loss/tok 5.9944 (6.6378)	LR 5.000e-04
0: TRAIN [1][130/196]	Time 0.499 (0.775)	Data 2.04e-04 (2.06e-03)	Tok/s 20307 (21796)	Loss/tok 5.5249 (6.5879)	LR 2.500e-04
1: TRAIN [1][130/196]	Time 0.502 (0.775)	Data 2.10e-04 (1.94e-03)	Tok/s 19994 (21780)	Loss/tok 5.4742 (6.5868)	LR 2.500e-04
0: TRAIN [1][140/196]	Time 1.000 (0.783)	Data 2.17e-04 (1.93e-03)	Tok/s 23561 (21851)	Loss/tok 6.0561 (6.5433)	LR 2.500e-04
1: TRAIN [1][140/196]	Time 1.000 (0.783)	Data 2.09e-04 (1.82e-03)	Tok/s 23466 (21842)	Loss/tok 6.1095 (6.5425)	LR 2.500e-04
0: TRAIN [1][150/196]	Time 1.004 (0.786)	Data 2.00e-04 (1.81e-03)	Tok/s 23255 (21865)	Loss/tok 6.0583 (6.5023)	LR 2.500e-04
1: TRAIN [1][150/196]	Time 1.003 (0.786)	Data 1.87e-04 (1.71e-03)	Tok/s 23322 (21864)	Loss/tok 6.0238 (6.5032)	LR 2.500e-04
0: TRAIN [1][160/196]	Time 0.996 (0.791)	Data 1.87e-04 (1.71e-03)	Tok/s 23736 (21895)	Loss/tok 6.0946 (6.4647)	LR 1.250e-04
1: TRAIN [1][160/196]	Time 0.997 (0.791)	Data 1.84e-04 (1.62e-03)	Tok/s 23414 (21894)	Loss/tok 5.9893 (6.4657)	LR 1.250e-04
0: TRAIN [1][170/196]	Time 1.005 (0.800)	Data 1.86e-04 (1.62e-03)	Tok/s 23395 (21917)	Loss/tok 6.0082 (6.4332)	LR 1.250e-04
1: TRAIN [1][170/196]	Time 1.004 (0.800)	Data 2.09e-04 (1.53e-03)	Tok/s 23089 (21915)	Loss/tok 5.9735 (6.4321)	LR 1.250e-04
0: TRAIN [1][180/196]	Time 0.768 (0.807)	Data 1.95e-04 (1.55e-03)	Tok/s 21738 (21943)	Loss/tok 5.7378 (6.4032)	LR 1.250e-04
1: TRAIN [1][180/196]	Time 0.769 (0.807)	Data 2.02e-04 (1.46e-03)	Tok/s 22106 (21941)	Loss/tok 5.6773 (6.4027)	LR 1.250e-04
0: TRAIN [1][190/196]	Time 0.491 (0.810)	Data 1.83e-04 (1.48e-03)	Tok/s 20140 (21964)	Loss/tok 5.4331 (6.3735)	LR 1.250e-04
1: TRAIN [1][190/196]	Time 0.490 (0.809)	Data 1.96e-04 (1.39e-03)	Tok/s 20741 (21966)	Loss/tok 5.4502 (6.3735)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [1][0/40]	Time 0.143 (0.143)	Data 5.72e-03 (5.72e-03)	Tok/s 59898 (59898)	Loss/tok 7.0175 (7.0175)
0: VALIDATION [1][0/40]	Time 0.214 (0.214)	Data 5.72e-03 (5.72e-03)	Tok/s 48940 (48940)	Loss/tok 7.0778 (7.0778)
1: VALIDATION [1][10/40]	Time 0.072 (0.097)	Data 3.38e-03 (4.13e-03)	Tok/s 63004 (61392)	Loss/tok 6.7890 (6.8427)
0: VALIDATION [1][10/40]	Time 0.073 (0.104)	Data 3.16e-03 (3.52e-03)	Tok/s 63410 (62332)	Loss/tok 6.5533 (6.8558)
1: VALIDATION [1][20/40]	Time 0.053 (0.081)	Data 3.43e-03 (3.82e-03)	Tok/s 59788 (61007)	Loss/tok 6.4192 (6.7332)
0: VALIDATION [1][20/40]	Time 0.052 (0.084)	Data 3.06e-03 (3.33e-03)	Tok/s 61895 (62154)	Loss/tok 6.4563 (6.7389)
1: VALIDATION [1][30/40]	Time 0.037 (0.069)	Data 3.23e-03 (3.67e-03)	Tok/s 56819 (60054)	Loss/tok 6.2641 (6.6597)
0: VALIDATION [1][30/40]	Time 0.038 (0.071)	Data 3.00e-03 (3.22e-03)	Tok/s 55910 (60993)	Loss/tok 6.3929 (6.6746)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
0: TEST [1][9/12]	Time 0.7003 (1.3020)	Decoder iters 149.0 (149.0)	Tok/s 6398 (6898)
1: TEST [1][9/12]	Time 0.7002 (1.3021)	Decoder iters 53.0 (139.4)	Tok/s 5828 (6335)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 6.3600	Validation Loss: 6.6068	Test BLEU: 0.86
0: Performance: Epoch: 1	Training: 43926 Tok/s	Validation: 117048 Tok/s
0: Finished epoch 1
1: Total training time 405 s
0: Total training time 406 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 372|                      0.86|                      43962.5|                         6.760|
DONE!
