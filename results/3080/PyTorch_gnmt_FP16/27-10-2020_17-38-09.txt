0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: GPU 0: TITAN RTX
Nvidia driver version: 455.28
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1228
0: Scheduler decay interval: 154
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/922]	Time 0.356 (0.000)	Data 1.32e-01 (0.00e+00)	Tok/s 36699 (0)	Loss/tok 10.6733 (10.6733)	LR 2.047e-05
0: TRAIN [0][10/922]	Time 0.161 (0.129)	Data 1.48e-04 (1.47e-04)	Tok/s 44327 (52943)	Loss/tok 9.6428 (10.1975)	LR 2.576e-05
0: TRAIN [0][20/922]	Time 0.095 (0.145)	Data 1.58e-04 (1.52e-04)	Tok/s 45731 (55433)	Loss/tok 9.1065 (9.7967)	LR 3.244e-05
0: TRAIN [0][30/922]	Time 0.161 (0.144)	Data 1.68e-04 (1.55e-04)	Tok/s 62731 (56223)	Loss/tok 8.9489 (9.5646)	LR 4.083e-05
0: TRAIN [0][40/922]	Time 0.106 (0.141)	Data 1.62e-04 (1.54e-04)	Tok/s 41152 (55381)	Loss/tok 8.5578 (9.4024)	LR 5.141e-05
0: TRAIN [0][50/922]	Time 0.127 (0.141)	Data 1.37e-04 (1.54e-04)	Tok/s 56664 (55568)	Loss/tok 8.4678 (9.2481)	LR 6.472e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][60/922]	Time 0.160 (0.141)	Data 1.46e-04 (1.54e-04)	Tok/s 63347 (56184)	Loss/tok 8.5132 (9.1149)	LR 8.148e-05
0: TRAIN [0][70/922]	Time 0.197 (0.142)	Data 1.54e-04 (1.54e-04)	Tok/s 51129 (55528)	Loss/tok 8.2187 (9.0093)	LR 1.026e-04
0: TRAIN [0][80/922]	Time 0.162 (0.143)	Data 1.65e-04 (1.55e-04)	Tok/s 44517 (55159)	Loss/tok 7.9592 (8.8968)	LR 1.291e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][90/922]	Time 0.129 (0.141)	Data 1.60e-04 (1.55e-04)	Tok/s 56449 (54956)	Loss/tok 7.8785 (8.8158)	LR 1.626e-04
0: TRAIN [0][100/922]	Time 0.121 (0.141)	Data 1.95e-04 (1.56e-04)	Tok/s 35700 (54679)	Loss/tok 7.5594 (8.7484)	LR 2.047e-04
0: TRAIN [0][110/922]	Time 0.127 (0.140)	Data 1.64e-04 (1.56e-04)	Tok/s 34375 (54372)	Loss/tok 7.4927 (8.6758)	LR 2.576e-04
0: TRAIN [0][120/922]	Time 0.066 (0.139)	Data 1.56e-04 (1.56e-04)	Tok/s 33271 (54087)	Loss/tok 7.3740 (8.6106)	LR 3.244e-04
0: TRAIN [0][130/922]	Time 0.099 (0.141)	Data 1.60e-04 (1.56e-04)	Tok/s 44295 (54281)	Loss/tok 7.4467 (8.5391)	LR 4.083e-04
0: TRAIN [0][140/922]	Time 0.126 (0.143)	Data 1.66e-04 (1.56e-04)	Tok/s 34052 (54188)	Loss/tok 7.4513 (8.4813)	LR 5.141e-04
0: TRAIN [0][150/922]	Time 0.155 (0.143)	Data 1.48e-04 (1.56e-04)	Tok/s 46299 (54064)	Loss/tok 7.6857 (8.4348)	LR 6.472e-04
0: TRAIN [0][160/922]	Time 0.101 (0.143)	Data 1.71e-04 (1.57e-04)	Tok/s 42802 (54255)	Loss/tok 7.5076 (8.3878)	LR 8.148e-04
0: TRAIN [0][170/922]	Time 0.130 (0.142)	Data 1.45e-04 (1.57e-04)	Tok/s 32977 (54063)	Loss/tok 7.2721 (8.3497)	LR 1.026e-03
0: TRAIN [0][180/922]	Time 0.139 (0.141)	Data 1.38e-04 (1.57e-04)	Tok/s 51790 (53858)	Loss/tok 7.6349 (8.3154)	LR 1.291e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
0: TRAIN [0][190/922]	Time 0.198 (0.141)	Data 1.59e-04 (1.57e-04)	Tok/s 66443 (53987)	Loss/tok 8.0201 (8.3099)	LR 1.626e-03
0: TRAIN [0][200/922]	Time 0.202 (0.141)	Data 1.39e-04 (1.57e-04)	Tok/s 50309 (54040)	Loss/tok 7.5887 (8.2743)	LR 2.000e-03
0: TRAIN [0][210/922]	Time 0.094 (0.141)	Data 1.63e-04 (1.58e-04)	Tok/s 47577 (54077)	Loss/tok 7.0142 (8.2366)	LR 2.000e-03
0: TRAIN [0][220/922]	Time 0.127 (0.141)	Data 1.35e-04 (1.57e-04)	Tok/s 56358 (53968)	Loss/tok 7.5590 (8.2028)	LR 2.000e-03
0: TRAIN [0][230/922]	Time 0.157 (0.141)	Data 1.60e-04 (1.58e-04)	Tok/s 46315 (53988)	Loss/tok 7.2593 (8.1663)	LR 2.000e-03
0: TRAIN [0][240/922]	Time 0.128 (0.141)	Data 1.59e-04 (1.58e-04)	Tok/s 34336 (53902)	Loss/tok 7.0272 (8.1304)	LR 2.000e-03
0: TRAIN [0][250/922]	Time 0.095 (0.139)	Data 1.66e-04 (1.57e-04)	Tok/s 46568 (53656)	Loss/tok 6.7006 (8.1018)	LR 2.000e-03
0: TRAIN [0][260/922]	Time 0.194 (0.140)	Data 2.56e-04 (1.58e-04)	Tok/s 50743 (53503)	Loss/tok 7.1444 (8.0658)	LR 2.000e-03
0: TRAIN [0][270/922]	Time 0.127 (0.140)	Data 1.60e-04 (1.58e-04)	Tok/s 34053 (53475)	Loss/tok 6.4879 (8.0251)	LR 2.000e-03
0: TRAIN [0][280/922]	Time 0.097 (0.140)	Data 1.63e-04 (1.58e-04)	Tok/s 43399 (53420)	Loss/tok 6.3377 (7.9878)	LR 2.000e-03
0: TRAIN [0][290/922]	Time 0.172 (0.139)	Data 1.32e-04 (1.58e-04)	Tok/s 58148 (53285)	Loss/tok 6.9892 (7.9534)	LR 2.000e-03
0: TRAIN [0][300/922]	Time 0.203 (0.139)	Data 1.59e-04 (1.58e-04)	Tok/s 49786 (53221)	Loss/tok 6.8340 (7.9178)	LR 2.000e-03
0: TRAIN [0][310/922]	Time 0.204 (0.139)	Data 1.60e-04 (1.58e-04)	Tok/s 63471 (53173)	Loss/tok 6.8995 (7.8823)	LR 2.000e-03
0: TRAIN [0][320/922]	Time 0.161 (0.139)	Data 1.59e-04 (1.58e-04)	Tok/s 43919 (53167)	Loss/tok 6.6436 (7.8465)	LR 2.000e-03
0: TRAIN [0][330/922]	Time 0.187 (0.139)	Data 1.65e-04 (1.58e-04)	Tok/s 53465 (53076)	Loss/tok 6.6979 (7.8122)	LR 2.000e-03
0: TRAIN [0][340/922]	Time 0.122 (0.138)	Data 1.71e-04 (1.58e-04)	Tok/s 58820 (52971)	Loss/tok 6.4795 (7.7811)	LR 2.000e-03
0: TRAIN [0][350/922]	Time 0.190 (0.138)	Data 2.03e-04 (1.58e-04)	Tok/s 52561 (52981)	Loss/tok 6.6679 (7.7447)	LR 2.000e-03
0: TRAIN [0][360/922]	Time 0.095 (0.138)	Data 1.58e-04 (1.58e-04)	Tok/s 46032 (52863)	Loss/tok 5.9999 (7.7142)	LR 2.000e-03
0: TRAIN [0][370/922]	Time 0.162 (0.138)	Data 1.58e-04 (1.58e-04)	Tok/s 62299 (52944)	Loss/tok 6.4991 (7.6779)	LR 2.000e-03
0: TRAIN [0][380/922]	Time 0.126 (0.138)	Data 2.08e-04 (1.58e-04)	Tok/s 34810 (52985)	Loss/tok 6.0273 (7.6434)	LR 2.000e-03
0: TRAIN [0][390/922]	Time 0.200 (0.138)	Data 1.55e-04 (1.58e-04)	Tok/s 50031 (53049)	Loss/tok 6.4173 (7.6078)	LR 2.000e-03
0: TRAIN [0][400/922]	Time 0.125 (0.138)	Data 1.57e-04 (1.58e-04)	Tok/s 59187 (53148)	Loss/tok 6.0806 (7.5724)	LR 2.000e-03
0: TRAIN [0][410/922]	Time 0.203 (0.138)	Data 1.67e-04 (1.58e-04)	Tok/s 49879 (53139)	Loss/tok 6.2551 (7.5398)	LR 2.000e-03
0: TRAIN [0][420/922]	Time 0.174 (0.138)	Data 1.59e-04 (1.58e-04)	Tok/s 57592 (53114)	Loss/tok 6.3200 (7.5081)	LR 2.000e-03
0: TRAIN [0][430/922]	Time 0.200 (0.138)	Data 1.49e-04 (1.58e-04)	Tok/s 64675 (53156)	Loss/tok 6.4631 (7.4747)	LR 2.000e-03
0: TRAIN [0][440/922]	Time 0.198 (0.138)	Data 1.61e-04 (1.58e-04)	Tok/s 65873 (53173)	Loss/tok 6.3201 (7.4428)	LR 2.000e-03
0: TRAIN [0][450/922]	Time 0.096 (0.138)	Data 1.51e-04 (1.58e-04)	Tok/s 45831 (53111)	Loss/tok 5.6575 (7.4172)	LR 2.000e-03
0: TRAIN [0][460/922]	Time 0.166 (0.138)	Data 1.58e-04 (1.58e-04)	Tok/s 61752 (53173)	Loss/tok 6.1125 (7.3854)	LR 2.000e-03
0: TRAIN [0][470/922]	Time 0.207 (0.138)	Data 1.61e-04 (1.58e-04)	Tok/s 48717 (53145)	Loss/tok 6.0199 (7.3513)	LR 2.000e-03
0: TRAIN [0][480/922]	Time 0.166 (0.140)	Data 1.80e-04 (1.58e-04)	Tok/s 60870 (53022)	Loss/tok 5.9849 (7.3176)	LR 2.000e-03
0: TRAIN [0][490/922]	Time 0.270 (0.140)	Data 1.52e-04 (1.58e-04)	Tok/s 48018 (53049)	Loss/tok 6.0631 (7.2832)	LR 2.000e-03
0: TRAIN [0][500/922]	Time 0.171 (0.141)	Data 1.60e-04 (1.58e-04)	Tok/s 42101 (52958)	Loss/tok 5.5851 (7.2517)	LR 2.000e-03
0: TRAIN [0][510/922]	Time 0.100 (0.140)	Data 1.57e-04 (1.58e-04)	Tok/s 44728 (52827)	Loss/tok 5.2327 (7.2280)	LR 2.000e-03
0: TRAIN [0][520/922]	Time 0.161 (0.141)	Data 1.57e-04 (1.58e-04)	Tok/s 62338 (52826)	Loss/tok 5.8153 (7.1982)	LR 2.000e-03
0: TRAIN [0][530/922]	Time 0.166 (0.141)	Data 1.60e-04 (1.58e-04)	Tok/s 43848 (52876)	Loss/tok 5.5870 (7.1676)	LR 2.000e-03
0: TRAIN [0][540/922]	Time 0.126 (0.140)	Data 1.70e-04 (1.58e-04)	Tok/s 56614 (52880)	Loss/tok 5.5179 (7.1405)	LR 2.000e-03
0: TRAIN [0][550/922]	Time 0.197 (0.140)	Data 1.52e-04 (1.58e-04)	Tok/s 66216 (52935)	Loss/tok 5.9686 (7.1110)	LR 2.000e-03
0: TRAIN [0][560/922]	Time 0.172 (0.140)	Data 1.64e-04 (1.58e-04)	Tok/s 58567 (52963)	Loss/tok 5.7331 (7.0819)	LR 2.000e-03
0: TRAIN [0][570/922]	Time 0.201 (0.140)	Data 1.55e-04 (1.58e-04)	Tok/s 50402 (52928)	Loss/tok 5.4922 (7.0551)	LR 2.000e-03
0: TRAIN [0][580/922]	Time 0.163 (0.140)	Data 1.51e-04 (1.58e-04)	Tok/s 62185 (52970)	Loss/tok 5.5969 (7.0267)	LR 2.000e-03
0: TRAIN [0][590/922]	Time 0.128 (0.140)	Data 1.49e-04 (1.58e-04)	Tok/s 33436 (52897)	Loss/tok 5.1701 (7.0035)	LR 2.000e-03
0: TRAIN [0][600/922]	Time 0.175 (0.140)	Data 1.55e-04 (1.58e-04)	Tok/s 57270 (52909)	Loss/tok 5.5034 (6.9765)	LR 2.000e-03
0: TRAIN [0][610/922]	Time 0.126 (0.140)	Data 1.53e-04 (1.58e-04)	Tok/s 57235 (52867)	Loss/tok 5.2065 (6.9537)	LR 2.000e-03
0: TRAIN [0][620/922]	Time 0.264 (0.140)	Data 1.73e-04 (1.58e-04)	Tok/s 49916 (52848)	Loss/tok 5.6392 (6.9269)	LR 2.000e-03
0: TRAIN [0][630/922]	Time 0.154 (0.140)	Data 1.49e-04 (1.58e-04)	Tok/s 46732 (52863)	Loss/tok 5.1681 (6.9011)	LR 2.000e-03
0: TRAIN [0][640/922]	Time 0.123 (0.140)	Data 1.48e-04 (1.58e-04)	Tok/s 57459 (52814)	Loss/tok 5.1816 (6.8787)	LR 2.000e-03
0: TRAIN [0][650/922]	Time 0.167 (0.140)	Data 1.65e-04 (1.58e-04)	Tok/s 42915 (52831)	Loss/tok 5.0528 (6.8502)	LR 2.000e-03
0: TRAIN [0][660/922]	Time 0.201 (0.140)	Data 1.60e-04 (1.58e-04)	Tok/s 50018 (52791)	Loss/tok 5.3913 (6.8273)	LR 2.000e-03
0: TRAIN [0][670/922]	Time 0.197 (0.140)	Data 1.67e-04 (1.58e-04)	Tok/s 65808 (52785)	Loss/tok 5.4094 (6.8036)	LR 2.000e-03
0: TRAIN [0][680/922]	Time 0.193 (0.140)	Data 1.67e-04 (1.58e-04)	Tok/s 52199 (52844)	Loss/tok 5.2149 (6.7743)	LR 2.000e-03
0: TRAIN [0][690/922]	Time 0.127 (0.141)	Data 1.60e-04 (1.58e-04)	Tok/s 56656 (52872)	Loss/tok 4.9089 (6.7449)	LR 2.000e-03
0: TRAIN [0][700/922]	Time 0.169 (0.141)	Data 2.10e-04 (1.59e-04)	Tok/s 43174 (52874)	Loss/tok 4.9456 (6.7205)	LR 2.000e-03
0: TRAIN [0][710/922]	Time 0.129 (0.141)	Data 1.62e-04 (1.58e-04)	Tok/s 57073 (52883)	Loss/tok 4.8721 (6.6975)	LR 2.000e-03
0: TRAIN [0][720/922]	Time 0.157 (0.141)	Data 1.56e-04 (1.58e-04)	Tok/s 45721 (52869)	Loss/tok 4.7805 (6.6740)	LR 2.000e-03
0: TRAIN [0][730/922]	Time 0.176 (0.141)	Data 1.69e-04 (1.58e-04)	Tok/s 41039 (52870)	Loss/tok 4.7764 (6.6492)	LR 2.000e-03
0: TRAIN [0][740/922]	Time 0.203 (0.141)	Data 1.52e-04 (1.58e-04)	Tok/s 63965 (52915)	Loss/tok 5.3080 (6.6250)	LR 2.000e-03
0: TRAIN [0][750/922]	Time 0.165 (0.141)	Data 1.29e-04 (1.58e-04)	Tok/s 60525 (52948)	Loss/tok 5.0234 (6.6006)	LR 2.000e-03
0: TRAIN [0][760/922]	Time 0.156 (0.141)	Data 1.53e-04 (1.58e-04)	Tok/s 46907 (52977)	Loss/tok 4.8234 (6.5756)	LR 2.000e-03
0: TRAIN [0][770/922]	Time 0.210 (0.141)	Data 1.34e-04 (1.58e-04)	Tok/s 61785 (52944)	Loss/tok 5.1560 (6.5560)	LR 2.000e-03
0: TRAIN [0][780/922]	Time 0.126 (0.141)	Data 1.56e-04 (1.58e-04)	Tok/s 56866 (52984)	Loss/tok 4.7051 (6.5328)	LR 2.000e-03
0: TRAIN [0][790/922]	Time 0.123 (0.141)	Data 1.63e-04 (1.58e-04)	Tok/s 58625 (53021)	Loss/tok 4.7388 (6.5090)	LR 2.000e-03
0: TRAIN [0][800/922]	Time 0.162 (0.142)	Data 1.59e-04 (1.58e-04)	Tok/s 43871 (53016)	Loss/tok 4.6816 (6.4861)	LR 2.000e-03
0: TRAIN [0][810/922]	Time 0.204 (0.142)	Data 1.58e-04 (1.58e-04)	Tok/s 49397 (53005)	Loss/tok 4.9401 (6.4633)	LR 2.000e-03
0: TRAIN [0][820/922]	Time 0.127 (0.142)	Data 1.68e-04 (1.58e-04)	Tok/s 56957 (53018)	Loss/tok 4.7176 (6.4423)	LR 2.000e-03
0: TRAIN [0][830/922]	Time 0.164 (0.142)	Data 1.50e-04 (1.58e-04)	Tok/s 61049 (53013)	Loss/tok 4.7705 (6.4221)	LR 2.000e-03
0: TRAIN [0][840/922]	Time 0.125 (0.142)	Data 1.58e-04 (1.58e-04)	Tok/s 33859 (53026)	Loss/tok 4.3567 (6.4002)	LR 2.000e-03
0: TRAIN [0][850/922]	Time 0.161 (0.142)	Data 1.59e-04 (1.58e-04)	Tok/s 44050 (53039)	Loss/tok 4.5992 (6.3779)	LR 2.000e-03
0: TRAIN [0][860/922]	Time 0.210 (0.142)	Data 1.94e-04 (1.58e-04)	Tok/s 61925 (53080)	Loss/tok 4.9925 (6.3570)	LR 2.000e-03
0: TRAIN [0][870/922]	Time 0.127 (0.142)	Data 1.67e-04 (1.58e-04)	Tok/s 57181 (53088)	Loss/tok 4.4869 (6.3379)	LR 2.000e-03
0: TRAIN [0][880/922]	Time 0.196 (0.142)	Data 1.64e-04 (1.58e-04)	Tok/s 51650 (53081)	Loss/tok 4.7747 (6.3192)	LR 2.000e-03
0: TRAIN [0][890/922]	Time 0.162 (0.142)	Data 1.58e-04 (1.58e-04)	Tok/s 61432 (53061)	Loss/tok 4.6496 (6.3022)	LR 2.000e-03
0: TRAIN [0][900/922]	Time 0.091 (0.142)	Data 1.56e-04 (1.58e-04)	Tok/s 48127 (53058)	Loss/tok 4.1613 (6.2842)	LR 2.000e-03
0: TRAIN [0][910/922]	Time 0.238 (0.142)	Data 1.53e-04 (1.58e-04)	Tok/s 54808 (53054)	Loss/tok 4.9045 (6.2659)	LR 2.000e-03
0: TRAIN [0][920/922]	Time 0.135 (0.142)	Data 5.84e-05 (1.59e-04)	Tok/s 53187 (53060)	Loss/tok 4.3768 (6.2480)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/160]	Time 0.065 (0.000)	Data 2.26e-03 (0.00e+00)	Tok/s 87501 (0)	Loss/tok 6.2137 (6.2137)
0: VALIDATION [0][10/160]	Time 0.033 (0.036)	Data 2.06e-03 (2.07e-03)	Tok/s 105215 (107782)	Loss/tok 5.8223 (5.9975)
0: VALIDATION [0][20/160]	Time 0.026 (0.033)	Data 1.92e-03 (2.04e-03)	Tok/s 113785 (108369)	Loss/tok 5.7806 (5.9264)
0: VALIDATION [0][30/160]	Time 0.024 (0.030)	Data 1.73e-03 (1.97e-03)	Tok/s 107193 (108292)	Loss/tok 5.8797 (5.8778)
0: VALIDATION [0][40/160]	Time 0.021 (0.029)	Data 1.86e-03 (1.94e-03)	Tok/s 110845 (107296)	Loss/tok 5.3960 (5.8440)
0: VALIDATION [0][50/160]	Time 0.020 (0.028)	Data 1.71e-03 (1.90e-03)	Tok/s 107443 (106143)	Loss/tok 5.7036 (5.7972)
0: VALIDATION [0][60/160]	Time 0.018 (0.027)	Data 1.77e-03 (1.88e-03)	Tok/s 106003 (105135)	Loss/tok 5.3822 (5.7614)
0: VALIDATION [0][70/160]	Time 0.017 (0.025)	Data 1.83e-03 (1.86e-03)	Tok/s 103347 (104760)	Loss/tok 5.3924 (5.7363)
0: VALIDATION [0][80/160]	Time 0.016 (0.024)	Data 1.82e-03 (1.85e-03)	Tok/s 102833 (104366)	Loss/tok 5.3917 (5.7118)
0: VALIDATION [0][90/160]	Time 0.014 (0.023)	Data 1.69e-03 (1.84e-03)	Tok/s 105164 (104037)	Loss/tok 5.2484 (5.6895)
0: VALIDATION [0][100/160]	Time 0.013 (0.022)	Data 1.70e-03 (1.84e-03)	Tok/s 101260 (103358)	Loss/tok 5.5906 (5.6735)
0: VALIDATION [0][110/160]	Time 0.012 (0.022)	Data 1.71e-03 (1.83e-03)	Tok/s 97044 (102270)	Loss/tok 5.4571 (5.6547)
0: VALIDATION [0][120/160]	Time 0.012 (0.021)	Data 1.69e-03 (1.82e-03)	Tok/s 92295 (100969)	Loss/tok 5.4021 (5.6394)
0: VALIDATION [0][130/160]	Time 0.011 (0.021)	Data 1.70e-03 (1.82e-03)	Tok/s 88646 (99431)	Loss/tok 5.1051 (5.6213)
0: VALIDATION [0][140/160]	Time 0.010 (0.020)	Data 1.80e-03 (1.81e-03)	Tok/s 81612 (98350)	Loss/tok 5.2177 (5.6092)
0: VALIDATION [0][150/160]	Time 0.008 (0.019)	Data 1.79e-03 (1.82e-03)	Tok/s 75326 (96850)	Loss/tok 4.9500 (5.5909)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/94]	Time 0.5782 (0.5305)	Decoder iters 149.0 (149.0)	Tok/s 5956 (7855)
0: TEST [0][19/94]	Time 0.5029 (0.5227)	Decoder iters 149.0 (149.0)	Tok/s 5804 (6918)
0: TEST [0][29/94]	Time 0.5132 (0.5202)	Decoder iters 149.0 (149.0)	Tok/s 4823 (6313)
0: TEST [0][39/94]	Time 0.3342 (0.5078)	Decoder iters 91.0 (146.3)	Tok/s 5805 (5918)
0: TEST [0][49/94]	Time 0.4829 (0.4936)	Decoder iters 149.0 (143.2)	Tok/s 3564 (5692)
0: TEST [0][59/94]	Time 0.4979 (0.4817)	Decoder iters 149.0 (140.2)	Tok/s 3242 (5478)
0: TEST [0][69/94]	Time 0.1763 (0.4728)	Decoder iters 39.0 (136.4)	Tok/s 6943 (5285)
0: TEST [0][79/94]	Time 0.1416 (0.4474)	Decoder iters 35.0 (128.7)	Tok/s 7335 (5288)
0: TEST [0][89/94]	Time 0.1586 (0.4275)	Decoder iters 35.0 (122.0)	Tok/s 4640 (5207)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 6.2458	Validation Loss: 5.5775	Test BLEU: 4.06
0: Performance: Epoch: 0	Training: 53064 Tok/s	Validation: 94353 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 3588440356
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [1][0/922]	Time 0.279 (0.000)	Data 1.27e-01 (0.00e+00)	Tok/s 35974 (0)	Loss/tok 4.5800 (4.5800)	LR 2.000e-03
0: TRAIN [1][10/922]	Time 0.128 (0.137)	Data 1.54e-04 (1.47e-04)	Tok/s 56624 (57213)	Loss/tok 4.2174 (4.3564)	LR 2.000e-03
0: TRAIN [1][20/922]	Time 0.126 (0.160)	Data 1.51e-04 (1.52e-04)	Tok/s 58138 (55524)	Loss/tok 4.1089 (4.4090)	LR 2.000e-03
0: TRAIN [1][30/922]	Time 0.221 (0.160)	Data 1.53e-04 (1.52e-04)	Tok/s 45298 (53782)	Loss/tok 4.3047 (4.3850)	LR 2.000e-03
0: TRAIN [1][40/922]	Time 0.160 (0.161)	Data 1.60e-04 (1.55e-04)	Tok/s 45364 (52189)	Loss/tok 4.3035 (4.3734)	LR 2.000e-03
0: TRAIN [1][50/922]	Time 0.095 (0.153)	Data 1.50e-04 (1.57e-04)	Tok/s 45350 (50613)	Loss/tok 3.8798 (4.3429)	LR 2.000e-03
0: TRAIN [1][60/922]	Time 0.094 (0.149)	Data 1.69e-04 (1.57e-04)	Tok/s 45707 (50535)	Loss/tok 3.9524 (4.3328)	LR 2.000e-03
0: TRAIN [1][70/922]	Time 0.293 (0.156)	Data 1.59e-04 (1.59e-04)	Tok/s 45036 (49889)	Loss/tok 4.6642 (4.3424)	LR 2.000e-03
0: TRAIN [1][80/922]	Time 0.272 (0.158)	Data 1.63e-04 (1.60e-04)	Tok/s 47979 (49959)	Loss/tok 4.6405 (4.3485)	LR 2.000e-03
0: TRAIN [1][90/922]	Time 0.195 (0.156)	Data 1.56e-04 (1.60e-04)	Tok/s 50776 (50363)	Loss/tok 4.3407 (4.3336)	LR 2.000e-03
0: TRAIN [1][100/922]	Time 0.167 (0.156)	Data 1.58e-04 (1.62e-04)	Tok/s 60783 (49709)	Loss/tok 4.2841 (4.3190)	LR 2.000e-03
0: TRAIN [1][110/922]	Time 0.133 (0.157)	Data 1.67e-04 (1.62e-04)	Tok/s 53596 (49699)	Loss/tok 4.2313 (4.3198)	LR 2.000e-03
0: TRAIN [1][120/922]	Time 0.196 (0.158)	Data 1.75e-04 (1.62e-04)	Tok/s 51423 (49753)	Loss/tok 4.3462 (4.3231)	LR 2.000e-03
0: TRAIN [1][130/922]	Time 0.172 (0.158)	Data 1.95e-04 (1.63e-04)	Tok/s 41851 (49304)	Loss/tok 4.1626 (4.3151)	LR 2.000e-03
0: TRAIN [1][140/922]	Time 0.170 (0.159)	Data 1.75e-04 (1.64e-04)	Tok/s 42096 (48757)	Loss/tok 4.0456 (4.3080)	LR 2.000e-03
0: TRAIN [1][150/922]	Time 0.166 (0.159)	Data 1.54e-04 (1.64e-04)	Tok/s 61060 (48929)	Loss/tok 4.4553 (4.3102)	LR 2.000e-03
0: TRAIN [1][160/922]	Time 0.125 (0.157)	Data 1.71e-04 (1.64e-04)	Tok/s 35908 (49182)	Loss/tok 3.8859 (4.3006)	LR 2.000e-03
0: TRAIN [1][170/922]	Time 0.199 (0.156)	Data 1.61e-04 (1.64e-04)	Tok/s 50506 (49367)	Loss/tok 4.2966 (4.2931)	LR 2.000e-03
0: TRAIN [1][180/922]	Time 0.105 (0.156)	Data 1.57e-04 (1.64e-04)	Tok/s 42052 (49514)	Loss/tok 3.8325 (4.2961)	LR 2.000e-03
0: TRAIN [1][190/922]	Time 0.066 (0.156)	Data 1.49e-04 (1.64e-04)	Tok/s 33155 (49430)	Loss/tok 3.5489 (4.2927)	LR 2.000e-03
0: TRAIN [1][200/922]	Time 0.139 (0.155)	Data 1.90e-04 (1.64e-04)	Tok/s 32587 (49278)	Loss/tok 3.7870 (4.2859)	LR 2.000e-03
0: TRAIN [1][210/922]	Time 0.205 (0.156)	Data 1.57e-04 (1.64e-04)	Tok/s 64075 (49207)	Loss/tok 4.4888 (4.2860)	LR 2.000e-03
0: TRAIN [1][220/922]	Time 0.247 (0.157)	Data 1.66e-04 (1.64e-04)	Tok/s 52744 (49181)	Loss/tok 4.5290 (4.2853)	LR 2.000e-03
0: TRAIN [1][230/922]	Time 0.165 (0.156)	Data 1.53e-04 (1.63e-04)	Tok/s 60390 (49242)	Loss/tok 4.2410 (4.2799)	LR 2.000e-03
0: TRAIN [1][240/922]	Time 0.201 (0.155)	Data 1.60e-04 (1.64e-04)	Tok/s 50362 (49478)	Loss/tok 4.1997 (4.2779)	LR 2.000e-03
0: TRAIN [1][250/922]	Time 0.102 (0.155)	Data 1.52e-04 (1.63e-04)	Tok/s 42692 (49596)	Loss/tok 3.7356 (4.2727)	LR 2.000e-03
0: TRAIN [1][260/922]	Time 0.130 (0.154)	Data 1.56e-04 (1.63e-04)	Tok/s 54971 (49782)	Loss/tok 4.1392 (4.2690)	LR 2.000e-03
0: TRAIN [1][270/922]	Time 0.164 (0.154)	Data 1.60e-04 (1.63e-04)	Tok/s 61072 (49885)	Loss/tok 4.3902 (4.2649)	LR 2.000e-03
0: TRAIN [1][280/922]	Time 0.170 (0.154)	Data 1.54e-04 (1.63e-04)	Tok/s 42619 (49636)	Loss/tok 4.0652 (4.2581)	LR 2.000e-03
0: TRAIN [1][290/922]	Time 0.139 (0.155)	Data 1.57e-04 (1.64e-04)	Tok/s 51568 (49754)	Loss/tok 3.9803 (4.2569)	LR 2.000e-03
0: TRAIN [1][300/922]	Time 0.239 (0.156)	Data 1.76e-04 (1.64e-04)	Tok/s 41733 (49788)	Loss/tok 4.1271 (4.2559)	LR 2.000e-03
0: TRAIN [1][310/922]	Time 0.124 (0.156)	Data 1.70e-04 (1.64e-04)	Tok/s 35490 (49633)	Loss/tok 3.7397 (4.2536)	LR 1.000e-03
0: TRAIN [1][320/922]	Time 0.130 (0.156)	Data 1.73e-04 (1.65e-04)	Tok/s 55262 (49582)	Loss/tok 3.9862 (4.2500)	LR 1.000e-03
0: TRAIN [1][330/922]	Time 0.171 (0.157)	Data 1.79e-04 (1.65e-04)	Tok/s 42428 (49638)	Loss/tok 3.9503 (4.2497)	LR 1.000e-03
0: TRAIN [1][340/922]	Time 0.202 (0.158)	Data 1.72e-04 (1.66e-04)	Tok/s 49393 (49467)	Loss/tok 4.0863 (4.2433)	LR 1.000e-03
0: TRAIN [1][350/922]	Time 0.163 (0.157)	Data 1.65e-04 (1.66e-04)	Tok/s 44669 (49486)	Loss/tok 3.8925 (4.2357)	LR 1.000e-03
0: TRAIN [1][360/922]	Time 0.249 (0.157)	Data 1.60e-04 (1.66e-04)	Tok/s 40879 (49517)	Loss/tok 4.1578 (4.2301)	LR 1.000e-03
0: TRAIN [1][370/922]	Time 0.166 (0.159)	Data 1.66e-04 (1.66e-04)	Tok/s 43453 (49526)	Loss/tok 3.9448 (4.2299)	LR 1.000e-03
0: TRAIN [1][380/922]	Time 0.127 (0.159)	Data 1.62e-04 (1.66e-04)	Tok/s 56269 (49357)	Loss/tok 3.8640 (4.2219)	LR 1.000e-03
0: TRAIN [1][390/922]	Time 0.161 (0.159)	Data 1.57e-04 (1.66e-04)	Tok/s 62979 (49388)	Loss/tok 4.1180 (4.2159)	LR 1.000e-03
0: TRAIN [1][400/922]	Time 0.238 (0.159)	Data 1.57e-04 (1.66e-04)	Tok/s 41471 (49318)	Loss/tok 4.0470 (4.2110)	LR 1.000e-03
0: TRAIN [1][410/922]	Time 0.179 (0.158)	Data 1.67e-04 (1.66e-04)	Tok/s 39812 (49268)	Loss/tok 3.7561 (4.2031)	LR 1.000e-03
0: TRAIN [1][420/922]	Time 0.098 (0.158)	Data 1.62e-04 (1.66e-04)	Tok/s 43789 (49405)	Loss/tok 3.6101 (4.2006)	LR 1.000e-03
0: TRAIN [1][430/922]	Time 0.167 (0.158)	Data 1.59e-04 (1.65e-04)	Tok/s 59683 (49566)	Loss/tok 3.9262 (4.1942)	LR 1.000e-03
0: TRAIN [1][440/922]	Time 0.165 (0.157)	Data 1.56e-04 (1.65e-04)	Tok/s 43754 (49526)	Loss/tok 3.8371 (4.1895)	LR 1.000e-03
0: TRAIN [1][450/922]	Time 0.198 (0.157)	Data 1.63e-04 (1.65e-04)	Tok/s 50714 (49597)	Loss/tok 4.0571 (4.1844)	LR 1.000e-03
0: TRAIN [1][460/922]	Time 0.171 (0.157)	Data 1.39e-04 (1.65e-04)	Tok/s 58913 (49596)	Loss/tok 4.0075 (4.1781)	LR 5.000e-04
0: TRAIN [1][470/922]	Time 0.162 (0.158)	Data 1.81e-04 (1.65e-04)	Tok/s 45247 (49477)	Loss/tok 3.6698 (4.1731)	LR 5.000e-04
0: TRAIN [1][480/922]	Time 0.175 (0.158)	Data 1.76e-04 (1.66e-04)	Tok/s 41068 (49544)	Loss/tok 3.7457 (4.1701)	LR 5.000e-04
0: TRAIN [1][490/922]	Time 0.218 (0.157)	Data 1.44e-04 (1.65e-04)	Tok/s 59555 (49519)	Loss/tok 4.1707 (4.1652)	LR 5.000e-04
0: TRAIN [1][500/922]	Time 0.181 (0.157)	Data 1.72e-04 (1.65e-04)	Tok/s 55358 (49593)	Loss/tok 3.9617 (4.1596)	LR 5.000e-04
0: TRAIN [1][510/922]	Time 0.129 (0.157)	Data 1.58e-04 (1.65e-04)	Tok/s 34075 (49550)	Loss/tok 3.5438 (4.1535)	LR 5.000e-04
0: TRAIN [1][520/922]	Time 0.204 (0.157)	Data 1.60e-04 (1.65e-04)	Tok/s 49737 (49539)	Loss/tok 3.9641 (4.1486)	LR 5.000e-04
0: TRAIN [1][530/922]	Time 0.099 (0.156)	Data 1.57e-04 (1.65e-04)	Tok/s 44646 (49447)	Loss/tok 3.4911 (4.1425)	LR 5.000e-04
0: TRAIN [1][540/922]	Time 0.094 (0.156)	Data 1.54e-04 (1.65e-04)	Tok/s 45796 (49480)	Loss/tok 3.5445 (4.1385)	LR 5.000e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][550/922]	Time 0.096 (0.156)	Data 1.56e-04 (1.65e-04)	Tok/s 44793 (49513)	Loss/tok 3.4942 (4.1354)	LR 5.000e-04
0: TRAIN [1][560/922]	Time 0.204 (0.156)	Data 1.56e-04 (1.65e-04)	Tok/s 49993 (49513)	Loss/tok 4.0224 (4.1309)	LR 5.000e-04
0: TRAIN [1][570/922]	Time 0.125 (0.156)	Data 1.63e-04 (1.65e-04)	Tok/s 56921 (49586)	Loss/tok 3.8222 (4.1257)	LR 5.000e-04
0: TRAIN [1][580/922]	Time 0.096 (0.156)	Data 1.55e-04 (1.65e-04)	Tok/s 45491 (49580)	Loss/tok 3.5333 (4.1207)	LR 5.000e-04
0: TRAIN [1][590/922]	Time 0.174 (0.155)	Data 1.59e-04 (1.65e-04)	Tok/s 57761 (49528)	Loss/tok 3.9598 (4.1168)	LR 5.000e-04
0: TRAIN [1][600/922]	Time 0.195 (0.155)	Data 1.64e-04 (1.64e-04)	Tok/s 51163 (49539)	Loss/tok 3.9742 (4.1122)	LR 5.000e-04
0: TRAIN [1][610/922]	Time 0.078 (0.155)	Data 1.61e-04 (1.65e-04)	Tok/s 27280 (49558)	Loss/tok 3.5005 (4.1087)	LR 5.000e-04
0: TRAIN [1][620/922]	Time 0.127 (0.155)	Data 1.67e-04 (1.64e-04)	Tok/s 57177 (49628)	Loss/tok 3.7035 (4.1049)	LR 2.500e-04
0: TRAIN [1][630/922]	Time 0.165 (0.155)	Data 1.64e-04 (1.64e-04)	Tok/s 43652 (49612)	Loss/tok 3.6067 (4.1000)	LR 2.500e-04
0: TRAIN [1][640/922]	Time 0.191 (0.155)	Data 1.60e-04 (1.64e-04)	Tok/s 52750 (49679)	Loss/tok 3.8618 (4.0969)	LR 2.500e-04
0: TRAIN [1][650/922]	Time 0.219 (0.155)	Data 1.91e-04 (1.64e-04)	Tok/s 60379 (49674)	Loss/tok 4.1852 (4.0930)	LR 2.500e-04
0: TRAIN [1][660/922]	Time 0.068 (0.154)	Data 1.57e-04 (1.64e-04)	Tok/s 30428 (49673)	Loss/tok 3.3142 (4.0885)	LR 2.500e-04
0: TRAIN [1][670/922]	Time 0.198 (0.154)	Data 1.62e-04 (1.64e-04)	Tok/s 50295 (49663)	Loss/tok 3.9965 (4.0843)	LR 2.500e-04
0: TRAIN [1][680/922]	Time 0.202 (0.155)	Data 1.49e-04 (1.64e-04)	Tok/s 49341 (49678)	Loss/tok 3.8943 (4.0808)	LR 2.500e-04
0: TRAIN [1][690/922]	Time 0.160 (0.155)	Data 1.46e-04 (1.64e-04)	Tok/s 45260 (49724)	Loss/tok 3.5875 (4.0766)	LR 2.500e-04
0: TRAIN [1][700/922]	Time 0.205 (0.155)	Data 1.74e-04 (1.64e-04)	Tok/s 63503 (49717)	Loss/tok 4.1262 (4.0730)	LR 2.500e-04
0: TRAIN [1][710/922]	Time 0.202 (0.155)	Data 1.58e-04 (1.64e-04)	Tok/s 50009 (49713)	Loss/tok 3.8871 (4.0687)	LR 2.500e-04
0: TRAIN [1][720/922]	Time 0.094 (0.154)	Data 1.76e-04 (1.64e-04)	Tok/s 45796 (49648)	Loss/tok 3.5215 (4.0647)	LR 2.500e-04
0: TRAIN [1][730/922]	Time 0.195 (0.154)	Data 1.49e-04 (1.64e-04)	Tok/s 51436 (49677)	Loss/tok 3.8801 (4.0612)	LR 2.500e-04
0: TRAIN [1][740/922]	Time 0.206 (0.154)	Data 1.54e-04 (1.64e-04)	Tok/s 48959 (49648)	Loss/tok 3.8773 (4.0576)	LR 2.500e-04
0: TRAIN [1][750/922]	Time 0.274 (0.154)	Data 1.59e-04 (1.64e-04)	Tok/s 47557 (49622)	Loss/tok 4.0077 (4.0546)	LR 2.500e-04
0: TRAIN [1][760/922]	Time 0.127 (0.154)	Data 1.58e-04 (1.64e-04)	Tok/s 33798 (49629)	Loss/tok 3.4460 (4.0512)	LR 2.500e-04
0: TRAIN [1][770/922]	Time 0.099 (0.154)	Data 1.61e-04 (1.64e-04)	Tok/s 45012 (49660)	Loss/tok 3.5387 (4.0481)	LR 1.250e-04
0: TRAIN [1][780/922]	Time 0.103 (0.154)	Data 1.59e-04 (1.64e-04)	Tok/s 41222 (49641)	Loss/tok 3.4476 (4.0453)	LR 1.250e-04
0: TRAIN [1][790/922]	Time 0.133 (0.154)	Data 1.59e-04 (1.64e-04)	Tok/s 32548 (49639)	Loss/tok 3.3814 (4.0431)	LR 1.250e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
0: TRAIN [1][800/922]	Time 0.169 (0.154)	Data 1.60e-04 (1.64e-04)	Tok/s 59899 (49687)	Loss/tok 3.7927 (4.0395)	LR 1.250e-04
0: TRAIN [1][810/922]	Time 0.227 (0.154)	Data 1.67e-04 (1.64e-04)	Tok/s 57459 (49714)	Loss/tok 3.9937 (4.0368)	LR 1.250e-04
0: TRAIN [1][820/922]	Time 0.168 (0.154)	Data 1.70e-04 (1.64e-04)	Tok/s 58774 (49760)	Loss/tok 3.8129 (4.0340)	LR 1.250e-04
0: TRAIN [1][830/922]	Time 0.134 (0.154)	Data 1.63e-04 (1.63e-04)	Tok/s 31853 (49687)	Loss/tok 3.3281 (4.0303)	LR 1.250e-04
0: TRAIN [1][840/922]	Time 0.197 (0.154)	Data 1.66e-04 (1.63e-04)	Tok/s 50413 (49635)	Loss/tok 3.8494 (4.0277)	LR 1.250e-04
0: TRAIN [1][850/922]	Time 0.124 (0.153)	Data 1.57e-04 (1.63e-04)	Tok/s 34731 (49611)	Loss/tok 3.4065 (4.0251)	LR 1.250e-04
0: TRAIN [1][860/922]	Time 0.151 (0.153)	Data 1.67e-04 (1.63e-04)	Tok/s 47925 (49601)	Loss/tok 3.7907 (4.0214)	LR 1.250e-04
0: TRAIN [1][870/922]	Time 0.206 (0.153)	Data 1.58e-04 (1.63e-04)	Tok/s 48634 (49588)	Loss/tok 3.9366 (4.0185)	LR 1.250e-04
0: TRAIN [1][880/922]	Time 0.225 (0.153)	Data 1.64e-04 (1.63e-04)	Tok/s 58130 (49570)	Loss/tok 4.0795 (4.0166)	LR 1.250e-04
0: TRAIN [1][890/922]	Time 0.169 (0.153)	Data 1.70e-04 (1.63e-04)	Tok/s 42506 (49547)	Loss/tok 3.6378 (4.0137)	LR 1.250e-04
0: TRAIN [1][900/922]	Time 0.101 (0.153)	Data 1.55e-04 (1.63e-04)	Tok/s 42816 (49547)	Loss/tok 3.4935 (4.0115)	LR 1.250e-04
0: TRAIN [1][910/922]	Time 0.124 (0.153)	Data 1.49e-04 (1.63e-04)	Tok/s 59010 (49564)	Loss/tok 3.6617 (4.0081)	LR 1.250e-04
0: TRAIN [1][920/922]	Time 0.223 (0.153)	Data 6.87e-05 (1.64e-04)	Tok/s 58911 (49591)	Loss/tok 3.9587 (4.0063)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [1][0/160]	Time 0.070 (0.000)	Data 4.34e-03 (0.00e+00)	Tok/s 81724 (0)	Loss/tok 5.4864 (5.4864)
0: VALIDATION [1][10/160]	Time 0.031 (0.037)	Data 1.97e-03 (2.10e-03)	Tok/s 109700 (105392)	Loss/tok 4.9998 (5.1937)
0: VALIDATION [1][20/160]	Time 0.027 (0.033)	Data 1.80e-03 (1.99e-03)	Tok/s 110080 (106120)	Loss/tok 4.9206 (5.1223)
0: VALIDATION [1][30/160]	Time 0.026 (0.032)	Data 1.87e-03 (1.95e-03)	Tok/s 101013 (104380)	Loss/tok 5.1057 (5.0759)
0: VALIDATION [1][40/160]	Time 0.051 (0.030)	Data 1.86e-03 (1.93e-03)	Tok/s 45928 (103234)	Loss/tok 4.6159 (5.0467)
0: VALIDATION [1][50/160]	Time 0.020 (0.029)	Data 1.73e-03 (1.92e-03)	Tok/s 104517 (103173)	Loss/tok 4.9477 (5.0062)
0: VALIDATION [1][60/160]	Time 0.019 (0.027)	Data 1.73e-03 (1.90e-03)	Tok/s 105162 (103306)	Loss/tok 4.6063 (4.9747)
0: VALIDATION [1][70/160]	Time 0.040 (0.027)	Data 1.86e-03 (1.89e-03)	Tok/s 44362 (101114)	Loss/tok 4.6355 (4.9511)
0: VALIDATION [1][80/160]	Time 0.016 (0.025)	Data 1.85e-03 (1.88e-03)	Tok/s 102355 (100856)	Loss/tok 4.5597 (4.9301)
0: VALIDATION [1][90/160]	Time 0.014 (0.024)	Data 1.73e-03 (1.87e-03)	Tok/s 103553 (100735)	Loss/tok 4.5456 (4.9111)
0: VALIDATION [1][100/160]	Time 0.013 (0.023)	Data 1.74e-03 (1.87e-03)	Tok/s 99932 (100192)	Loss/tok 4.8780 (4.8986)
0: VALIDATION [1][110/160]	Time 0.013 (0.022)	Data 1.76e-03 (1.86e-03)	Tok/s 95316 (99542)	Loss/tok 4.7793 (4.8816)
0: VALIDATION [1][120/160]	Time 0.013 (0.022)	Data 1.82e-03 (1.85e-03)	Tok/s 82361 (98449)	Loss/tok 4.5702 (4.8697)
0: VALIDATION [1][130/160]	Time 0.012 (0.021)	Data 1.74e-03 (1.85e-03)	Tok/s 82293 (96747)	Loss/tok 4.4907 (4.8551)
0: VALIDATION [1][140/160]	Time 0.012 (0.020)	Data 1.92e-03 (1.85e-03)	Tok/s 70075 (95618)	Loss/tok 4.5359 (4.8441)
0: VALIDATION [1][150/160]	Time 0.009 (0.020)	Data 1.82e-03 (1.85e-03)	Tok/s 74003 (94151)	Loss/tok 4.3162 (4.8288)
0: Saving model to gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [1][9/94]	Time 0.4943 (0.5320)	Decoder iters 149.0 (145.8)	Tok/s 6397 (7133)
0: TEST [1][19/94]	Time 0.5631 (0.4918)	Decoder iters 149.0 (137.8)	Tok/s 4868 (7026)
0: TEST [1][29/94]	Time 0.2876 (0.4867)	Decoder iters 69.0 (136.9)	Tok/s 8027 (6459)
0: TEST [1][39/94]	Time 0.2160 (0.4478)	Decoder iters 53.0 (125.5)	Tok/s 9244 (6779)
0: TEST [1][49/94]	Time 0.1939 (0.4090)	Decoder iters 45.0 (113.6)	Tok/s 8820 (7105)
0: TEST [1][59/94]	Time 0.1765 (0.3769)	Decoder iters 51.0 (103.9)	Tok/s 8360 (7306)
0: TEST [1][69/94]	Time 0.1530 (0.3442)	Decoder iters 34.0 (94.2)	Tok/s 7914 (7577)
0: TEST [1][79/94]	Time 0.1389 (0.3189)	Decoder iters 39.0 (86.8)	Tok/s 7404 (7658)
0: TEST [1][89/94]	Time 0.0795 (0.3001)	Decoder iters 21.0 (81.7)	Tok/s 8954 (7618)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 1	Training Loss: 4.0059	Validation Loss: 4.8176	Test BLEU: 8.24
0: Performance: Epoch: 1	Training: 49598 Tok/s	Validation: 91732 Tok/s
0: Finished epoch 1
0: Total training time 377 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 160|                      8.24|                      51331.0|                         6.286|
DONE!
