0: Collecting environment information...
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3080
GPU 1: GeForce RTX 3080

Nvidia driver version: 455.28
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=112, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1758
0: Scheduler decay interval: 220
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/1320]	Time 0.167 (0.000)	Data 1.06e-01 (0.00e+00)	Tok/s 18602 (0)	Loss/tok 10.4868 (10.4868)	LR 2.047e-05
0: TRAIN [0][10/1320]	Time 0.152 (0.101)	Data 8.32e-05 (7.41e-05)	Tok/s 60522 (50338)	Loss/tok 9.8164 (10.1464)	LR 2.576e-05
0: TRAIN [0][20/1320]	Time 0.096 (0.098)	Data 7.20e-05 (7.23e-05)	Tok/s 52361 (49983)	Loss/tok 9.1932 (9.7912)	LR 3.244e-05
0: TRAIN [0][30/1320]	Time 0.096 (0.102)	Data 5.91e-05 (7.27e-05)	Tok/s 52827 (51165)	Loss/tok 8.9394 (9.5315)	LR 4.083e-05
0: TRAIN [0][40/1320]	Time 0.099 (0.104)	Data 8.15e-05 (7.35e-05)	Tok/s 51338 (51264)	Loss/tok 8.6485 (9.3615)	LR 5.141e-05
0: TRAIN [0][50/1320]	Time 0.153 (0.104)	Data 7.44e-05 (7.39e-05)	Tok/s 59380 (51055)	Loss/tok 8.6498 (9.2332)	LR 6.472e-05
0: TRAIN [0][60/1320]	Time 0.152 (0.103)	Data 7.32e-05 (7.49e-05)	Tok/s 61545 (50717)	Loss/tok 8.5124 (9.1134)	LR 8.148e-05
0: TRAIN [0][70/1320]	Time 0.072 (0.102)	Data 7.08e-05 (7.48e-05)	Tok/s 41923 (50602)	Loss/tok 7.9090 (9.0037)	LR 1.026e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][80/1320]	Time 0.124 (0.103)	Data 7.32e-05 (7.45e-05)	Tok/s 57637 (51292)	Loss/tok 8.1500 (8.8963)	LR 1.291e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][90/1320]	Time 0.073 (0.103)	Data 7.30e-05 (7.47e-05)	Tok/s 42194 (51345)	Loss/tok 7.7568 (8.8032)	LR 1.626e-04
0: TRAIN [0][100/1320]	Time 0.053 (0.103)	Data 7.27e-05 (7.48e-05)	Tok/s 28907 (51170)	Loss/tok 7.4198 (8.7184)	LR 2.047e-04
0: TRAIN [0][110/1320]	Time 0.054 (0.104)	Data 7.44e-05 (7.46e-05)	Tok/s 28092 (50947)	Loss/tok 7.3518 (8.6399)	LR 2.576e-04
0: TRAIN [0][120/1320]	Time 0.075 (0.103)	Data 7.15e-05 (7.46e-05)	Tok/s 42246 (50897)	Loss/tok 7.5157 (8.5732)	LR 3.244e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][130/1320]	Time 0.152 (0.104)	Data 7.22e-05 (7.46e-05)	Tok/s 60832 (51060)	Loss/tok 8.2212 (8.5095)	LR 4.083e-04
0: TRAIN [0][140/1320]	Time 0.094 (0.103)	Data 7.65e-05 (7.47e-05)	Tok/s 54164 (50899)	Loss/tok 7.8047 (8.4650)	LR 5.141e-04
0: TRAIN [0][150/1320]	Time 0.124 (0.104)	Data 7.51e-05 (7.46e-05)	Tok/s 56236 (51209)	Loss/tok 7.7970 (8.4106)	LR 6.472e-04
0: TRAIN [0][160/1320]	Time 0.125 (0.104)	Data 7.72e-05 (7.48e-05)	Tok/s 56181 (51079)	Loss/tok 7.7531 (8.3721)	LR 8.148e-04
0: TRAIN [0][170/1320]	Time 0.076 (0.103)	Data 7.32e-05 (7.47e-05)	Tok/s 42052 (50667)	Loss/tok 7.3555 (8.3387)	LR 1.026e-03
0: TRAIN [0][180/1320]	Time 0.096 (0.103)	Data 7.49e-05 (7.47e-05)	Tok/s 52005 (50846)	Loss/tok 7.5232 (8.3041)	LR 1.291e-03
0: TRAIN [0][190/1320]	Time 0.054 (0.103)	Data 7.01e-05 (7.46e-05)	Tok/s 28156 (50719)	Loss/tok 6.8629 (8.2728)	LR 1.626e-03
0: TRAIN [0][200/1320]	Time 0.129 (0.104)	Data 8.23e-05 (7.46e-05)	Tok/s 53478 (50889)	Loss/tok 7.5533 (8.2354)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
0: TRAIN [0][210/1320]	Time 0.100 (0.104)	Data 6.06e-05 (7.46e-05)	Tok/s 50470 (51163)	Loss/tok 7.3775 (8.1956)	LR 2.000e-03
0: TRAIN [0][220/1320]	Time 0.097 (0.104)	Data 7.34e-05 (7.48e-05)	Tok/s 52613 (51096)	Loss/tok 7.4628 (8.1635)	LR 2.000e-03
0: TRAIN [0][230/1320]	Time 0.097 (0.104)	Data 7.39e-05 (7.50e-05)	Tok/s 52044 (50997)	Loss/tok 7.3511 (8.1342)	LR 2.000e-03
0: TRAIN [0][240/1320]	Time 0.075 (0.104)	Data 6.25e-05 (7.49e-05)	Tok/s 40393 (51085)	Loss/tok 6.9354 (8.0950)	LR 2.000e-03
0: TRAIN [0][250/1320]	Time 0.097 (0.104)	Data 7.39e-05 (7.49e-05)	Tok/s 52150 (51062)	Loss/tok 7.0482 (8.0615)	LR 2.000e-03
0: TRAIN [0][260/1320]	Time 0.128 (0.104)	Data 7.49e-05 (7.49e-05)	Tok/s 55075 (51046)	Loss/tok 7.0774 (8.0262)	LR 2.000e-03
0: TRAIN [0][270/1320]	Time 0.121 (0.104)	Data 7.63e-05 (7.48e-05)	Tok/s 58338 (51079)	Loss/tok 7.1225 (7.9912)	LR 2.000e-03
0: TRAIN [0][280/1320]	Time 0.124 (0.104)	Data 9.70e-05 (7.49e-05)	Tok/s 57622 (51072)	Loss/tok 7.1862 (7.9574)	LR 2.000e-03
0: TRAIN [0][290/1320]	Time 0.096 (0.103)	Data 7.49e-05 (7.50e-05)	Tok/s 51699 (50962)	Loss/tok 6.7187 (7.9257)	LR 2.000e-03
0: TRAIN [0][300/1320]	Time 0.122 (0.103)	Data 9.89e-05 (7.50e-05)	Tok/s 57592 (50790)	Loss/tok 7.1451 (7.8979)	LR 2.000e-03
0: TRAIN [0][310/1320]	Time 0.100 (0.103)	Data 8.25e-05 (7.50e-05)	Tok/s 50041 (50779)	Loss/tok 6.7705 (7.8659)	LR 2.000e-03
0: TRAIN [0][320/1320]	Time 0.156 (0.103)	Data 7.61e-05 (7.50e-05)	Tok/s 58869 (50795)	Loss/tok 6.9964 (7.8311)	LR 2.000e-03
0: TRAIN [0][330/1320]	Time 0.101 (0.103)	Data 7.41e-05 (7.51e-05)	Tok/s 49393 (50796)	Loss/tok 6.5905 (7.7997)	LR 2.000e-03
0: TRAIN [0][340/1320]	Time 0.096 (0.104)	Data 7.51e-05 (7.50e-05)	Tok/s 51823 (50842)	Loss/tok 6.4925 (7.7650)	LR 2.000e-03
0: TRAIN [0][350/1320]	Time 0.122 (0.104)	Data 7.32e-05 (7.51e-05)	Tok/s 58200 (50896)	Loss/tok 6.7061 (7.7299)	LR 2.000e-03
0: TRAIN [0][360/1320]	Time 0.155 (0.103)	Data 1.13e-04 (7.52e-05)	Tok/s 58514 (50721)	Loss/tok 6.9218 (7.7041)	LR 2.000e-03
0: TRAIN [0][370/1320]	Time 0.100 (0.104)	Data 7.58e-05 (7.52e-05)	Tok/s 50745 (50785)	Loss/tok 6.4026 (7.6713)	LR 2.000e-03
0: TRAIN [0][380/1320]	Time 0.096 (0.104)	Data 7.68e-05 (7.54e-05)	Tok/s 53391 (50737)	Loss/tok 6.4387 (7.6426)	LR 2.000e-03
0: TRAIN [0][390/1320]	Time 0.073 (0.104)	Data 7.15e-05 (7.55e-05)	Tok/s 42588 (50764)	Loss/tok 6.1170 (7.6116)	LR 2.000e-03
0: TRAIN [0][400/1320]	Time 0.075 (0.104)	Data 6.13e-05 (7.55e-05)	Tok/s 40284 (50784)	Loss/tok 5.9801 (7.5807)	LR 2.000e-03
0: TRAIN [0][410/1320]	Time 0.097 (0.104)	Data 7.56e-05 (7.55e-05)	Tok/s 52642 (50832)	Loss/tok 6.2798 (7.5506)	LR 2.000e-03
0: TRAIN [0][420/1320]	Time 0.054 (0.104)	Data 6.53e-05 (7.55e-05)	Tok/s 28466 (50841)	Loss/tok 5.6707 (7.5220)	LR 2.000e-03
0: TRAIN [0][430/1320]	Time 0.123 (0.104)	Data 7.32e-05 (7.55e-05)	Tok/s 57209 (50825)	Loss/tok 6.4574 (7.4945)	LR 2.000e-03
0: TRAIN [0][440/1320]	Time 0.096 (0.104)	Data 7.68e-05 (7.56e-05)	Tok/s 52402 (50846)	Loss/tok 6.0583 (7.4663)	LR 2.000e-03
0: TRAIN [0][450/1320]	Time 0.093 (0.104)	Data 7.61e-05 (7.57e-05)	Tok/s 55199 (50881)	Loss/tok 6.0918 (7.4384)	LR 2.000e-03
0: TRAIN [0][460/1320]	Time 0.155 (0.104)	Data 7.84e-05 (7.57e-05)	Tok/s 58875 (50926)	Loss/tok 6.3815 (7.4093)	LR 2.000e-03
0: TRAIN [0][470/1320]	Time 0.095 (0.104)	Data 9.99e-05 (7.58e-05)	Tok/s 53621 (50923)	Loss/tok 5.9583 (7.3826)	LR 2.000e-03
0: TRAIN [0][480/1320]	Time 0.092 (0.104)	Data 7.13e-05 (7.58e-05)	Tok/s 54525 (50861)	Loss/tok 5.8360 (7.3583)	LR 2.000e-03
0: TRAIN [0][490/1320]	Time 0.155 (0.104)	Data 7.75e-05 (7.58e-05)	Tok/s 59412 (50930)	Loss/tok 6.3232 (7.3296)	LR 2.000e-03
0: TRAIN [0][500/1320]	Time 0.100 (0.104)	Data 7.49e-05 (7.58e-05)	Tok/s 51460 (50900)	Loss/tok 5.8915 (7.3061)	LR 2.000e-03
0: TRAIN [0][510/1320]	Time 0.125 (0.104)	Data 7.32e-05 (7.58e-05)	Tok/s 56829 (50880)	Loss/tok 6.1291 (7.2813)	LR 2.000e-03
0: TRAIN [0][520/1320]	Time 0.094 (0.104)	Data 7.75e-05 (7.58e-05)	Tok/s 53451 (50922)	Loss/tok 5.8292 (7.2543)	LR 2.000e-03
0: TRAIN [0][530/1320]	Time 0.156 (0.104)	Data 7.61e-05 (7.58e-05)	Tok/s 59527 (50905)	Loss/tok 6.1291 (7.2289)	LR 2.000e-03
0: TRAIN [0][540/1320]	Time 0.097 (0.104)	Data 7.65e-05 (7.58e-05)	Tok/s 51858 (50842)	Loss/tok 5.7905 (7.2076)	LR 2.000e-03
0: TRAIN [0][550/1320]	Time 0.123 (0.104)	Data 7.30e-05 (7.58e-05)	Tok/s 56359 (50836)	Loss/tok 5.9226 (7.1837)	LR 2.000e-03
0: TRAIN [0][560/1320]	Time 0.055 (0.104)	Data 7.63e-05 (7.58e-05)	Tok/s 27512 (50832)	Loss/tok 4.9854 (7.1590)	LR 2.000e-03
0: TRAIN [0][570/1320]	Time 0.071 (0.104)	Data 7.68e-05 (7.58e-05)	Tok/s 42522 (50845)	Loss/tok 5.5201 (7.1346)	LR 2.000e-03
0: TRAIN [0][580/1320]	Time 0.121 (0.104)	Data 7.01e-05 (7.58e-05)	Tok/s 57827 (50809)	Loss/tok 5.9004 (7.1132)	LR 2.000e-03
0: TRAIN [0][590/1320]	Time 0.155 (0.104)	Data 8.46e-05 (7.58e-05)	Tok/s 58951 (50847)	Loss/tok 5.9241 (7.0888)	LR 2.000e-03
0: TRAIN [0][600/1320]	Time 0.076 (0.104)	Data 7.39e-05 (7.59e-05)	Tok/s 39670 (50781)	Loss/tok 5.0916 (7.0679)	LR 2.000e-03
0: TRAIN [0][610/1320]	Time 0.128 (0.104)	Data 7.49e-05 (7.59e-05)	Tok/s 54720 (50876)	Loss/tok 5.7141 (7.0401)	LR 2.000e-03
0: TRAIN [0][620/1320]	Time 0.159 (0.104)	Data 7.49e-05 (7.59e-05)	Tok/s 57962 (50854)	Loss/tok 5.9401 (7.0182)	LR 2.000e-03
0: TRAIN [0][630/1320]	Time 0.125 (0.104)	Data 7.44e-05 (7.59e-05)	Tok/s 56428 (50855)	Loss/tok 5.7247 (6.9957)	LR 2.000e-03
0: TRAIN [0][640/1320]	Time 0.094 (0.104)	Data 7.72e-05 (7.59e-05)	Tok/s 54181 (50861)	Loss/tok 5.5068 (6.9738)	LR 2.000e-03
0: TRAIN [0][650/1320]	Time 0.094 (0.104)	Data 7.25e-05 (7.59e-05)	Tok/s 54585 (50897)	Loss/tok 5.3521 (6.9486)	LR 2.000e-03
0: TRAIN [0][660/1320]	Time 0.126 (0.104)	Data 9.61e-05 (7.60e-05)	Tok/s 56959 (50884)	Loss/tok 5.6534 (6.9273)	LR 2.000e-03
0: TRAIN [0][670/1320]	Time 0.072 (0.104)	Data 9.56e-05 (7.61e-05)	Tok/s 42486 (50865)	Loss/tok 4.9782 (6.9062)	LR 2.000e-03
0: TRAIN [0][680/1320]	Time 0.126 (0.104)	Data 6.77e-05 (7.61e-05)	Tok/s 55259 (50866)	Loss/tok 5.4756 (6.8850)	LR 2.000e-03
0: TRAIN [0][690/1320]	Time 0.097 (0.104)	Data 7.49e-05 (7.61e-05)	Tok/s 51463 (50892)	Loss/tok 5.2984 (6.8627)	LR 2.000e-03
0: TRAIN [0][700/1320]	Time 0.097 (0.104)	Data 7.75e-05 (7.62e-05)	Tok/s 52514 (50888)	Loss/tok 5.2770 (6.8426)	LR 2.000e-03
0: TRAIN [0][710/1320]	Time 0.122 (0.104)	Data 7.34e-05 (7.62e-05)	Tok/s 57162 (50791)	Loss/tok 5.5359 (6.8266)	LR 2.000e-03
0: TRAIN [0][720/1320]	Time 0.075 (0.104)	Data 6.44e-05 (7.63e-05)	Tok/s 40718 (50774)	Loss/tok 4.8353 (6.8070)	LR 2.000e-03
0: TRAIN [0][730/1320]	Time 0.097 (0.104)	Data 7.37e-05 (7.62e-05)	Tok/s 52375 (50841)	Loss/tok 5.2244 (6.7834)	LR 2.000e-03
0: TRAIN [0][740/1320]	Time 0.152 (0.104)	Data 9.75e-05 (7.62e-05)	Tok/s 59741 (50828)	Loss/tok 5.5841 (6.7637)	LR 2.000e-03
0: TRAIN [0][750/1320]	Time 0.153 (0.104)	Data 7.32e-05 (7.63e-05)	Tok/s 58976 (50853)	Loss/tok 5.5370 (6.7423)	LR 2.000e-03
0: TRAIN [0][760/1320]	Time 0.072 (0.104)	Data 7.30e-05 (7.62e-05)	Tok/s 42038 (50824)	Loss/tok 4.7914 (6.7237)	LR 2.000e-03
0: TRAIN [0][770/1320]	Time 0.069 (0.104)	Data 1.02e-04 (7.62e-05)	Tok/s 45710 (50821)	Loss/tok 4.8442 (6.7044)	LR 2.000e-03
0: TRAIN [0][780/1320]	Time 0.102 (0.104)	Data 7.22e-05 (7.62e-05)	Tok/s 50689 (50857)	Loss/tok 4.9894 (6.6831)	LR 2.000e-03
0: TRAIN [0][790/1320]	Time 0.159 (0.105)	Data 7.58e-05 (7.62e-05)	Tok/s 57252 (50892)	Loss/tok 5.4138 (6.6617)	LR 2.000e-03
0: TRAIN [0][800/1320]	Time 0.122 (0.104)	Data 6.10e-05 (7.62e-05)	Tok/s 58102 (50872)	Loss/tok 5.0788 (6.6435)	LR 2.000e-03
0: TRAIN [0][810/1320]	Time 0.098 (0.104)	Data 6.89e-05 (7.62e-05)	Tok/s 52057 (50884)	Loss/tok 4.8803 (6.6235)	LR 2.000e-03
0: TRAIN [0][820/1320]	Time 0.097 (0.104)	Data 7.41e-05 (7.62e-05)	Tok/s 53556 (50884)	Loss/tok 5.0502 (6.6051)	LR 2.000e-03
0: TRAIN [0][830/1320]	Time 0.092 (0.104)	Data 7.41e-05 (7.61e-05)	Tok/s 55128 (50906)	Loss/tok 4.8811 (6.5855)	LR 2.000e-03
0: TRAIN [0][840/1320]	Time 0.121 (0.105)	Data 7.03e-05 (7.61e-05)	Tok/s 57658 (50911)	Loss/tok 5.1483 (6.5668)	LR 2.000e-03
0: TRAIN [0][850/1320]	Time 0.102 (0.104)	Data 7.63e-05 (7.61e-05)	Tok/s 49652 (50865)	Loss/tok 5.0363 (6.5513)	LR 2.000e-03
0: TRAIN [0][860/1320]	Time 0.054 (0.104)	Data 7.61e-05 (7.61e-05)	Tok/s 27405 (50851)	Loss/tok 4.3434 (6.5340)	LR 2.000e-03
0: TRAIN [0][870/1320]	Time 0.126 (0.104)	Data 7.65e-05 (7.61e-05)	Tok/s 56407 (50840)	Loss/tok 5.0538 (6.5178)	LR 2.000e-03
0: TRAIN [0][880/1320]	Time 0.073 (0.104)	Data 7.89e-05 (7.61e-05)	Tok/s 41553 (50818)	Loss/tok 4.6016 (6.5022)	LR 2.000e-03
0: TRAIN [0][890/1320]	Time 0.101 (0.104)	Data 7.72e-05 (7.61e-05)	Tok/s 51268 (50802)	Loss/tok 4.7576 (6.4856)	LR 2.000e-03
0: TRAIN [0][900/1320]	Time 0.122 (0.104)	Data 7.63e-05 (7.61e-05)	Tok/s 57805 (50815)	Loss/tok 5.0163 (6.4682)	LR 2.000e-03
0: TRAIN [0][910/1320]	Time 0.122 (0.104)	Data 7.18e-05 (7.61e-05)	Tok/s 58111 (50791)	Loss/tok 5.0062 (6.4522)	LR 2.000e-03
0: TRAIN [0][920/1320]	Time 0.122 (0.104)	Data 7.75e-05 (7.61e-05)	Tok/s 57729 (50788)	Loss/tok 4.8874 (6.4364)	LR 2.000e-03
0: TRAIN [0][930/1320]	Time 0.051 (0.104)	Data 9.44e-05 (7.61e-05)	Tok/s 29702 (50735)	Loss/tok 4.1468 (6.4220)	LR 2.000e-03
0: TRAIN [0][940/1320]	Time 0.156 (0.104)	Data 7.37e-05 (7.61e-05)	Tok/s 58588 (50717)	Loss/tok 5.2684 (6.4072)	LR 2.000e-03
0: TRAIN [0][950/1320]	Time 0.125 (0.104)	Data 7.18e-05 (7.61e-05)	Tok/s 57364 (50689)	Loss/tok 4.9761 (6.3930)	LR 2.000e-03
0: TRAIN [0][960/1320]	Time 0.072 (0.104)	Data 7.44e-05 (7.60e-05)	Tok/s 42442 (50695)	Loss/tok 4.5093 (6.3765)	LR 2.000e-03
0: TRAIN [0][970/1320]	Time 0.075 (0.104)	Data 7.27e-05 (7.61e-05)	Tok/s 41301 (50654)	Loss/tok 4.4334 (6.3627)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][980/1320]	Time 0.036 (0.104)	Data 7.08e-05 (7.60e-05)	Tok/s 41813 (50678)	Loss/tok 4.3165 (6.3464)	LR 2.000e-03
0: TRAIN [0][990/1320]	Time 0.125 (0.103)	Data 7.30e-05 (7.61e-05)	Tok/s 55968 (50622)	Loss/tok 4.8525 (6.3340)	LR 2.000e-03
0: TRAIN [0][1000/1320]	Time 0.124 (0.103)	Data 7.44e-05 (7.61e-05)	Tok/s 56268 (50602)	Loss/tok 4.9163 (6.3198)	LR 2.000e-03
0: TRAIN [0][1010/1320]	Time 0.155 (0.103)	Data 7.44e-05 (7.61e-05)	Tok/s 59240 (50610)	Loss/tok 5.0185 (6.3040)	LR 2.000e-03
0: TRAIN [0][1020/1320]	Time 0.077 (0.103)	Data 7.41e-05 (7.61e-05)	Tok/s 38059 (50573)	Loss/tok 4.3053 (6.2907)	LR 2.000e-03
0: TRAIN [0][1030/1320]	Time 0.072 (0.103)	Data 7.61e-05 (7.60e-05)	Tok/s 41198 (50533)	Loss/tok 4.2449 (6.2779)	LR 2.000e-03
0: TRAIN [0][1040/1320]	Time 0.125 (0.103)	Data 6.15e-05 (7.60e-05)	Tok/s 56641 (50543)	Loss/tok 4.7786 (6.2633)	LR 2.000e-03
0: TRAIN [0][1050/1320]	Time 0.094 (0.104)	Data 7.30e-05 (7.60e-05)	Tok/s 54425 (50568)	Loss/tok 4.6835 (6.2470)	LR 2.000e-03
0: TRAIN [0][1060/1320]	Time 0.097 (0.104)	Data 7.53e-05 (7.60e-05)	Tok/s 51940 (50576)	Loss/tok 4.5780 (6.2325)	LR 2.000e-03
0: TRAIN [0][1070/1320]	Time 0.094 (0.104)	Data 7.65e-05 (7.60e-05)	Tok/s 52910 (50604)	Loss/tok 4.5863 (6.2167)	LR 2.000e-03
0: TRAIN [0][1080/1320]	Time 0.097 (0.104)	Data 7.58e-05 (7.60e-05)	Tok/s 52056 (50613)	Loss/tok 4.4775 (6.2028)	LR 2.000e-03
0: TRAIN [0][1090/1320]	Time 0.072 (0.104)	Data 7.06e-05 (7.61e-05)	Tok/s 42561 (50607)	Loss/tok 4.1628 (6.1896)	LR 2.000e-03
0: TRAIN [0][1100/1320]	Time 0.124 (0.104)	Data 7.30e-05 (7.61e-05)	Tok/s 57087 (50592)	Loss/tok 4.7683 (6.1765)	LR 2.000e-03
0: TRAIN [0][1110/1320]	Time 0.122 (0.104)	Data 6.68e-05 (7.60e-05)	Tok/s 58281 (50592)	Loss/tok 4.6696 (6.1628)	LR 2.000e-03
0: TRAIN [0][1120/1320]	Time 0.096 (0.103)	Data 7.15e-05 (7.61e-05)	Tok/s 53295 (50571)	Loss/tok 4.4336 (6.1507)	LR 2.000e-03
0: TRAIN [0][1130/1320]	Time 0.068 (0.103)	Data 1.16e-04 (7.61e-05)	Tok/s 46542 (50537)	Loss/tok 4.3410 (6.1396)	LR 2.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
0: TRAIN [0][1140/1320]	Time 0.097 (0.103)	Data 7.58e-05 (7.61e-05)	Tok/s 53009 (50558)	Loss/tok 4.5950 (6.1261)	LR 2.000e-03
0: TRAIN [0][1150/1320]	Time 0.153 (0.103)	Data 7.06e-05 (7.61e-05)	Tok/s 60173 (50590)	Loss/tok 4.9109 (6.1114)	LR 2.000e-03
0: TRAIN [0][1160/1320]	Time 0.077 (0.104)	Data 7.34e-05 (7.61e-05)	Tok/s 38612 (50601)	Loss/tok 4.0798 (6.0975)	LR 2.000e-03
0: TRAIN [0][1170/1320]	Time 0.075 (0.104)	Data 7.77e-05 (7.61e-05)	Tok/s 39269 (50594)	Loss/tok 4.1929 (6.0850)	LR 2.000e-03
0: TRAIN [0][1180/1320]	Time 0.069 (0.103)	Data 7.32e-05 (7.61e-05)	Tok/s 44524 (50544)	Loss/tok 4.0835 (6.0754)	LR 2.000e-03
0: TRAIN [0][1190/1320]	Time 0.097 (0.103)	Data 7.77e-05 (7.61e-05)	Tok/s 52451 (50561)	Loss/tok 4.4824 (6.0626)	LR 2.000e-03
0: TRAIN [0][1200/1320]	Time 0.155 (0.104)	Data 8.80e-05 (7.61e-05)	Tok/s 59259 (50597)	Loss/tok 4.9649 (6.0478)	LR 2.000e-03
0: TRAIN [0][1210/1320]	Time 0.128 (0.104)	Data 7.99e-05 (7.61e-05)	Tok/s 54484 (50618)	Loss/tok 4.7115 (6.0336)	LR 2.000e-03
0: TRAIN [0][1220/1320]	Time 0.124 (0.104)	Data 6.87e-05 (7.61e-05)	Tok/s 55979 (50599)	Loss/tok 4.5166 (6.0224)	LR 2.000e-03
0: TRAIN [0][1230/1320]	Time 0.075 (0.104)	Data 7.27e-05 (7.61e-05)	Tok/s 41065 (50589)	Loss/tok 4.1045 (6.0111)	LR 2.000e-03
0: TRAIN [0][1240/1320]	Time 0.158 (0.104)	Data 7.77e-05 (7.61e-05)	Tok/s 57630 (50587)	Loss/tok 4.8500 (5.9994)	LR 2.000e-03
0: TRAIN [0][1250/1320]	Time 0.051 (0.103)	Data 1.18e-04 (7.62e-05)	Tok/s 29483 (50563)	Loss/tok 3.6511 (5.9890)	LR 2.000e-03
0: TRAIN [0][1260/1320]	Time 0.125 (0.104)	Data 7.84e-05 (7.61e-05)	Tok/s 57191 (50588)	Loss/tok 4.5869 (5.9767)	LR 2.000e-03
0: TRAIN [0][1270/1320]	Time 0.052 (0.103)	Data 6.51e-05 (7.62e-05)	Tok/s 28156 (50542)	Loss/tok 3.7742 (5.9674)	LR 2.000e-03
0: TRAIN [0][1280/1320]	Time 0.122 (0.104)	Data 7.30e-05 (7.61e-05)	Tok/s 58009 (50583)	Loss/tok 4.5813 (5.9538)	LR 2.000e-03
0: TRAIN [0][1290/1320]	Time 0.126 (0.104)	Data 6.25e-05 (7.61e-05)	Tok/s 55816 (50599)	Loss/tok 4.5684 (5.9419)	LR 2.000e-03
0: TRAIN [0][1300/1320]	Time 0.071 (0.103)	Data 7.13e-05 (7.61e-05)	Tok/s 42211 (50575)	Loss/tok 4.1873 (5.9320)	LR 2.000e-03
0: TRAIN [0][1310/1320]	Time 0.077 (0.103)	Data 7.65e-05 (7.61e-05)	Tok/s 39471 (50570)	Loss/tok 4.0928 (5.9213)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
Traceback (most recent call last):
  File "train.py", line 636, in <module>
    main()
  File "train.py", line 567, in main
    val_loss, val_perf = trainer.evaluate(val_loader)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 367, in evaluate
    training=False)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 336, in preallocate
    self.iterate(src, tgt, update=False, training=training)
  File "/workspace/examples/gnmt/seq2seq/train/trainer.py", line 180, in iterate
    output = self.model(src, src_length, tgt[:-1])
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/apex/amp/_initialize.py", line 197, in new_fwd
    **applier(kwargs, input_caster))
  File "/workspace/examples/gnmt/seq2seq/models/gnmt.py", line 68, in forward
    context = self.encode(input_encoder, input_enc_len)
  File "/workspace/examples/gnmt/seq2seq/models/seq2seq_base.py", line 52, in encode
    return self.encoder(inputs, lengths)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/workspace/examples/gnmt/seq2seq/models/encoder.py", line 107, in forward
    x, _ = self.rnn_layers[1](x)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/opt/conda/lib/python3.6/site-packages/torch/nn/modules/rnn.py", line 582, in forward
    self.dropout, self.training, self.bidirectional, self.batch_first)
RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python3', '-u', 'train.py', '--local_rank=0', '--dataset-dir', '/data/gnmt/wmt16_de_en', '--train-batch-size', '112', '--val-batch-size', '32', '--test-batch-size', '32', '--math', 'fp16', '--epochs', '2', '--seed', '2']' returned non-zero exit status 1.
DONE!
