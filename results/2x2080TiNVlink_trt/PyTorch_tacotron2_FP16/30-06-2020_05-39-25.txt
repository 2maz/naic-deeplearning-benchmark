:::NVLOGv0.2.2 Tacotron2_PyT 1593495567.273031235 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593495567.295255184 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593495567.315356731 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593495567.585088015 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 2, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593495567.595555782 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 72, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 2, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593495567.770925760 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1593495576.287097931 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593495576.290472031 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495578.004155397 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495582.909451723 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.709259033203125
:::NVLOGv0.2.2 Tacotron2_PyT 1593495584.830594778 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495584.831368446 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 11926.530796566203
:::NVLOGv0.2.2 Tacotron2_PyT 1593495584.831883192 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.829228162765503
Batch: 1/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495584.840907812 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495586.199696302 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.1450309753418
:::NVLOGv0.2.2 Tacotron2_PyT 1593495588.081910133 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495588.083293200 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25303.440740776965
:::NVLOGv0.2.2 Tacotron2_PyT 1593495588.085687637 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2422072887420654
Batch: 2/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495588.094413042 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593495589.539251804 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.527889251708984
:::NVLOGv0.2.2 Tacotron2_PyT 1593495591.451662064 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593495591.454857588 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24597.514902761442
:::NVLOGv0.2.2 Tacotron2_PyT 1593495591.456365585 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.358591318130493
Batch: 3/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495591.465007544 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593495592.834838629 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.692813873291016
:::NVLOGv0.2.2 Tacotron2_PyT 1593495594.744087458 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593495594.745769739 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24597.322733839177
:::NVLOGv0.2.2 Tacotron2_PyT 1593495594.747919083 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2803163528442383
Batch: 4/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495594.758537531 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593495596.129999399 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.651763916015625
:::NVLOGv0.2.2 Tacotron2_PyT 1593495598.708443642 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593495598.710248470 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 20592.893216350272
:::NVLOGv0.2.2 Tacotron2_PyT 1593495598.712434530 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.951120376586914
Batch: 5/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495598.723222733 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593495602.577785015 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.730262756347656
:::NVLOGv0.2.2 Tacotron2_PyT 1593495605.317937851 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593495605.319780588 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 12366.10319901169
:::NVLOGv0.2.2 Tacotron2_PyT 1593495605.323677778 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.595853090286255
Batch: 6/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495605.333730698 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1593495609.045863867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.68218231201172
:::NVLOGv0.2.2 Tacotron2_PyT 1593495610.909721136 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1593495610.911614180 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 14744.774317114541
:::NVLOGv0.2.2 Tacotron2_PyT 1593495610.914287329 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.5768232345581055
Batch: 7/8 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495610.924590111 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1593495612.315300941 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.21547317504883
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.209109306 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.210877419 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24501.852619017398
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.213346720 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2853434085845947
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.362951994 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.365916491 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 17135.8896875275
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.368166924 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 19828.80406567971
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.369395971 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.044334411621094
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.370572805 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 38.074708223342896
:::NVLOGv0.2.2 Tacotron2_PyT 1593495614.371727943 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593495616.883928776 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 45.64707946777344
:::NVLOGv0.2.2 Tacotron2_PyT 1593495616.886233091 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495617.154057264 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495618.857847214 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495620.239411116 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.45050811767578
:::NVLOGv0.2.2 Tacotron2_PyT 1593495622.164071798 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593495622.165528536 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24547.13643208658
:::NVLOGv0.2.2 Tacotron2_PyT 1593495622.168176174 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.3089399337768555
Batch: 1/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495622.179210186 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495623.594393969 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.60383605957031
:::NVLOGv0.2.2 Tacotron2_PyT 1593495625.524377584 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495625.526703358 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 23404.198115174233
:::NVLOGv0.2.2 Tacotron2_PyT 1593495625.529410124 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.346365451812744
Batch: 2/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495625.540914536 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593495626.906219244 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.58073425292969
:::NVLOGv0.2.2 Tacotron2_PyT 1593495628.782630444 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593495628.783864975 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24358.396775695845
:::NVLOGv0.2.2 Tacotron2_PyT 1593495628.784929037 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.2426600456237793
Batch: 3/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495628.796162367 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593495630.181962967 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.02790069580078
:::NVLOGv0.2.2 Tacotron2_PyT 1593495632.062623978 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593495632.063648224 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 25015.613408428126
:::NVLOGv0.2.2 Tacotron2_PyT 1593495632.065769196 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.267439365386963
Batch: 4/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495632.074351549 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593495633.482349396 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 49.303524017333984
:::NVLOGv0.2.2 Tacotron2_PyT 1593495635.359516621 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593495635.362899542 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 26025.10626858277
:::NVLOGv0.2.2 Tacotron2_PyT 1593495635.364566565 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.285865545272827
Batch: 5/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495635.374989748 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593495636.744966984 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.54300308227539
:::NVLOGv0.2.2 Tacotron2_PyT 1593495638.625139475 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593495638.626831293 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24987.697418510652
:::NVLOGv0.2.2 Tacotron2_PyT 1593495638.629832983 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.251159906387329
Batch: 6/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495638.641215801 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1593495639.991446733 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 49.06257629394531
:::NVLOGv0.2.2 Tacotron2_PyT 1593495641.888278246 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1593495641.890136719 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 26298.505335821093
:::NVLOGv0.2.2 Tacotron2_PyT 1593495641.892395973 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.247713088989258
Batch: 7/8 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495641.899547815 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1593495643.180552959 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.664772033691406
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.736261606 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.737179995 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 28254.310858459954
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.738514423 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8371245861053467
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.889853716 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.891706944 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 23528.478456213266
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.892824650 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 25361.370576594905
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.893877983 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.02960681915283
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.894265413 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 27.736260175704956
:::NVLOGv0.2.2 Tacotron2_PyT 1593495644.894627094 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495647.334333181 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 45.625797271728516
:::NVLOGv0.2.2 Tacotron2_PyT 1593495647.336441755 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593495647.337704420 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 79.56622505187988
:::NVLOGv0.2.2 Tacotron2_PyT 1593495647.338095903 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 79.56622505187988
:::NVLOGv0.2.2 Tacotron2_PyT 1593495647.338490009 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 80.14335370063782
:::NVLOGv0.2.2 Tacotron2_PyT 1593495647.338828087 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
