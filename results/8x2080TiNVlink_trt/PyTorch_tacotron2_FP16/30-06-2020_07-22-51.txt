:::NVLOGv0.2.2 Tacotron2_PyT 1593501774.341778517 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593501774.369634151 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593501774.388216734 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593501777.803361893 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593501777.813909769 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 72, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593501781.080533028 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1593501801.263736010 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593501801.264579773 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/2 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501803.086550236 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501814.499778986 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.08506774902344
:::NVLOGv0.2.2 Tacotron2_PyT 1593501816.502622128 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501816.503273487 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24355.148612124318
:::NVLOGv0.2.2 Tacotron2_PyT 1593501816.503792524 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 13.417614698410034
Batch: 1/2 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501816.511163473 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501818.165053606 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.33183670043945
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.442977905 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.444094896 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 54892.91597085351
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.445769787 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.932568788528442
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.536631584 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.537113905 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 30670.62666187945
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.537538767 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 39624.03229148891
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.537940025 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.208452224731445
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.538327217 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 21.272600889205933
:::NVLOGv0.2.2 Tacotron2_PyT 1593501822.538717270 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593501824.450087309 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.133888244628906
:::NVLOGv0.2.2 Tacotron2_PyT 1593501824.451322556 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501824.739188433 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/2 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501826.474111557 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501832.167112589 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.783016204833984
:::NVLOGv0.2.2 Tacotron2_PyT 1593501835.035765648 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593501835.045589209 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 37395.156908216566
:::NVLOGv0.2.2 Tacotron2_PyT 1593501835.048827410 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.564397811889648
Batch: 1/2 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501835.056249857 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501839.967884779 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.97010040283203
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.628581285 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.632320166 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 38764.49165951585
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.638426304 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.572922945022583
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.799247265 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.802227497 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 34237.69379507374
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.804090261 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 38079.82428386621
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.805056572 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.37655830383301
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.805943251 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 19.06062960624695
:::NVLOGv0.2.2 Tacotron2_PyT 1593501843.806852579 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501845.625446796 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.126407623291016
:::NVLOGv0.2.2 Tacotron2_PyT 1593501845.626805305 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593501845.627950191 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 64.54674887657166
:::NVLOGv0.2.2 Tacotron2_PyT 1593501845.628393173 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 64.54674887657166
:::NVLOGv0.2.2 Tacotron2_PyT 1593501845.628812551 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 71.38543176651001
:::NVLOGv0.2.2 Tacotron2_PyT 1593501845.629172564 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
