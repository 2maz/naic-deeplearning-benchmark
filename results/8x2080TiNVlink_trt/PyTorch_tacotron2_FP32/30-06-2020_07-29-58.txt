:::NVLOGv0.2.2 Tacotron2_PyT 1593502200.592088938 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593502200.617554188 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593502200.627879381 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593502203.641007900 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593502203.648998499 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 36, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593502206.208904982 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593502226.874945164 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593502226.876385689 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502228.978285789 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502240.931513071 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.992393493652344
:::NVLOGv0.2.2 Tacotron2_PyT 1593502242.646673203 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502242.647822380 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 11959.557796550716
:::NVLOGv0.2.2 Tacotron2_PyT 1593502242.648663998 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 13.67007064819336
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502242.656648874 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502243.919482231 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.514102935791016
:::NVLOGv0.2.2 Tacotron2_PyT 1593502245.456830978 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502245.459120750 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 58288.29015529931
:::NVLOGv0.2.2 Tacotron2_PyT 1593502245.460744381 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8015918731689453
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502245.467732191 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593502246.439529896 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.083892822265625
:::NVLOGv0.2.2 Tacotron2_PyT 1593502248.151856899 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593502248.154208422 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 60682.33607715021
:::NVLOGv0.2.2 Tacotron2_PyT 1593502248.156623840 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6849658489227295
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502248.161743402 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593502249.115524769 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.72327423095703
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.876342297 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.879063606 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59925.76758921821
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.881838322 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7154595851898193
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.990683317 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.993410110 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 27055.17545488025
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.995898724 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 47713.98790455461
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.997388840 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.828415870666504
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.998586655 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 24.115312099456787
:::NVLOGv0.2.2 Tacotron2_PyT 1593502250.999742746 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593502252.298678160 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.191627502441406
:::NVLOGv0.2.2 Tacotron2_PyT 1593502252.299755812 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502253.004577875 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502254.083805323 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502255.267742395 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.23465347290039
:::NVLOGv0.2.2 Tacotron2_PyT 1593502256.970507145 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593502256.972056866 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 55241.43670323942
:::NVLOGv0.2.2 Tacotron2_PyT 1593502256.973324060 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8881218433380127
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502256.982483149 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502258.055933237 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.10658264160156
:::NVLOGv0.2.2 Tacotron2_PyT 1593502259.799401999 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502259.802174568 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 57040.01321601335
:::NVLOGv0.2.2 Tacotron2_PyT 1593502259.805344582 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8177237510681152
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502259.813337564 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593502260.951927185 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.50983428955078
:::NVLOGv0.2.2 Tacotron2_PyT 1593502262.436679125 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593502262.438270807 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 63550.37241722706
:::NVLOGv0.2.2 Tacotron2_PyT 1593502262.440732241 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6239657402038574
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502262.448158026 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593502263.527409554 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.927772521972656
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.302192688 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.304035187 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 57999.064737738336
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.305289984 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8547184467315674
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.413354635 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.414695501 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 52588.25694728231
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.415258408 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 58457.72176855455
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.415776730 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.69471073150635
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.416308165 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 12.409462451934814
:::NVLOGv0.2.2 Tacotron2_PyT 1593502265.416814566 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502266.845320940 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.15026092529297
:::NVLOGv0.2.2 Tacotron2_PyT 1593502266.849083424 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593502266.852335930 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 60.64230513572693
:::NVLOGv0.2.2 Tacotron2_PyT 1593502266.852745295 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 60.64230513572693
:::NVLOGv0.2.2 Tacotron2_PyT 1593502266.853179693 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 66.34937953948975
:::NVLOGv0.2.2 Tacotron2_PyT 1593502266.853541851 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
