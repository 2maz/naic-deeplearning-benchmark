3: Collecting environment information...
2: Collecting environment information...
1: Collecting environment information...
0: Collecting environment information...
3: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
3: Saving results to: results/gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
3: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
2: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
2: Saving results to: results/gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
2: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
3: Worker 3 is using worker seed: 1323436024
2: Worker 2 is using worker seed: 3588440356
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31800
2: Size of vocabulary: 31800
3: Size of vocabulary: 31800
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Filtering data, min len: 0, max len: 125
3: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Filtering data, min len: 0, max len: 125
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 125
3: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
0: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Filtering data, min len: 0, max len: 150
2: Pairs before: 3003, after: 3003
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159605817
3: Saving state of the tokenizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 305
3: Scheduler decay interval: 38
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: Initializing amp optimizer
3: Starting epoch 0
3: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 305
1: Scheduler decay interval: 38
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159605817
2: Saving state of the tokenizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 305
2: Scheduler decay interval: 38
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: Initializing amp optimizer
2: Starting epoch 0
2: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 305
0: Scheduler decay interval: 38
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
1: Sampler for epoch 0 uses seed 2602510382
0: Sampler for epoch 0 uses seed 2602510382
3: Sampler for epoch 0 uses seed 2602510382
2: Sampler for epoch 0 uses seed 2602510382
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
0: TRAIN [0][0/229]	Time 0.544 (0.544)	Data 1.54e-01 (1.54e-01)	Tok/s 18479 (18479)	Loss/tok 10.6427 (10.6427)	LR 2.047e-05
3: TRAIN [0][0/229]	Time 0.544 (0.544)	Data 1.59e-01 (1.59e-01)	Tok/s 18560 (18560)	Loss/tok 10.6522 (10.6522)	LR 2.047e-05
2: TRAIN [0][0/229]	Time 0.544 (0.544)	Data 1.82e-01 (1.82e-01)	Tok/s 18488 (18488)	Loss/tok 10.6557 (10.6557)	LR 2.047e-05
1: TRAIN [0][0/229]	Time 0.544 (0.544)	Data 1.32e-01 (1.32e-01)	Tok/s 18769 (18769)	Loss/tok 10.6624 (10.6624)	LR 2.047e-05
0: TRAIN [0][10/229]	Time 0.314 (0.348)	Data 1.23e-04 (1.41e-02)	Tok/s 13946 (16750)	Loss/tok 9.4687 (10.1737)	LR 2.576e-05
2: TRAIN [0][10/229]	Time 0.314 (0.348)	Data 1.21e-04 (1.66e-02)	Tok/s 13839 (16665)	Loss/tok 9.4821 (10.1796)	LR 2.576e-05
3: TRAIN [0][10/229]	Time 0.314 (0.348)	Data 1.13e-04 (1.46e-02)	Tok/s 13904 (16783)	Loss/tok 9.4303 (10.1713)	LR 2.576e-05
1: TRAIN [0][10/229]	Time 0.315 (0.348)	Data 1.36e-04 (1.21e-02)	Tok/s 14018 (16833)	Loss/tok 9.4888 (10.1785)	LR 2.576e-05
0: TRAIN [0][20/229]	Time 0.351 (0.361)	Data 1.60e-04 (7.45e-03)	Tok/s 20854 (19523)	Loss/tok 9.1402 (9.7885)	LR 3.244e-05
3: TRAIN [0][20/229]	Time 0.351 (0.361)	Data 1.12e-04 (7.69e-03)	Tok/s 20372 (19545)	Loss/tok 9.1866 (9.7887)	LR 3.244e-05
1: TRAIN [0][20/229]	Time 0.351 (0.361)	Data 1.31e-04 (6.40e-03)	Tok/s 20735 (19539)	Loss/tok 9.1630 (9.7869)	LR 3.244e-05
2: TRAIN [0][20/229]	Time 0.352 (0.361)	Data 1.41e-04 (8.77e-03)	Tok/s 20708 (19509)	Loss/tok 9.1926 (9.7953)	LR 3.244e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
0: TRAIN [0][30/229]	Time 0.388 (0.360)	Data 1.39e-04 (5.09e-03)	Tok/s 25766 (19816)	Loss/tok 9.0873 (9.5402)	LR 4.083e-05
3: TRAIN [0][30/229]	Time 0.388 (0.360)	Data 1.15e-04 (5.24e-03)	Tok/s 25978 (19853)	Loss/tok 9.1034 (9.5453)	LR 4.083e-05
2: TRAIN [0][30/229]	Time 0.387 (0.360)	Data 1.42e-04 (5.98e-03)	Tok/s 26027 (19828)	Loss/tok 9.0471 (9.5495)	LR 4.083e-05
1: TRAIN [0][30/229]	Time 0.387 (0.360)	Data 1.35e-04 (4.38e-03)	Tok/s 26347 (19866)	Loss/tok 9.0678 (9.5428)	LR 4.083e-05
0: TRAIN [0][40/229]	Time 0.392 (0.359)	Data 1.35e-04 (3.88e-03)	Tok/s 25631 (19931)	Loss/tok 8.8089 (9.3675)	LR 5.141e-05
3: TRAIN [0][40/229]	Time 0.392 (0.359)	Data 1.16e-04 (4.00e-03)	Tok/s 25461 (19965)	Loss/tok 8.7577 (9.3749)	LR 5.141e-05
2: TRAIN [0][40/229]	Time 0.392 (0.359)	Data 1.23e-04 (4.55e-03)	Tok/s 25688 (19923)	Loss/tok 8.7280 (9.3743)	LR 5.141e-05
1: TRAIN [0][40/229]	Time 0.392 (0.359)	Data 1.35e-04 (3.35e-03)	Tok/s 25765 (19991)	Loss/tok 8.7802 (9.3651)	LR 5.141e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
0: TRAIN [0][50/229]	Time 0.315 (0.359)	Data 1.48e-04 (3.15e-03)	Tok/s 13726 (20036)	Loss/tok 8.2690 (9.2085)	LR 6.472e-05
3: TRAIN [0][50/229]	Time 0.316 (0.359)	Data 1.50e-04 (3.24e-03)	Tok/s 13683 (20079)	Loss/tok 8.3245 (9.2158)	LR 6.472e-05
2: TRAIN [0][50/229]	Time 0.316 (0.359)	Data 1.44e-04 (3.69e-03)	Tok/s 13789 (20015)	Loss/tok 8.3163 (9.2140)	LR 6.472e-05
1: TRAIN [0][50/229]	Time 0.312 (0.359)	Data 1.79e-04 (2.72e-03)	Tok/s 14259 (20089)	Loss/tok 8.2778 (9.2100)	LR 6.472e-05
0: TRAIN [0][60/229]	Time 0.317 (0.358)	Data 1.15e-04 (2.66e-03)	Tok/s 13936 (19750)	Loss/tok 8.0398 (9.0791)	LR 8.148e-05
3: TRAIN [0][60/229]	Time 0.317 (0.358)	Data 1.08e-04 (2.73e-03)	Tok/s 13887 (19777)	Loss/tok 7.9698 (9.0810)	LR 8.148e-05
1: TRAIN [0][60/229]	Time 0.317 (0.358)	Data 1.26e-04 (2.30e-03)	Tok/s 13570 (19769)	Loss/tok 8.0149 (9.0815)	LR 8.148e-05
2: TRAIN [0][60/229]	Time 0.317 (0.358)	Data 1.60e-04 (3.11e-03)	Tok/s 13531 (19701)	Loss/tok 8.0014 (9.0829)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
0: TRAIN [0][70/229]	Time 0.393 (0.361)	Data 1.23e-04 (2.30e-03)	Tok/s 25762 (20294)	Loss/tok 8.0971 (8.9381)	LR 1.026e-04
3: TRAIN [0][70/229]	Time 0.393 (0.361)	Data 1.49e-04 (2.36e-03)	Tok/s 25579 (20322)	Loss/tok 8.1589 (8.9455)	LR 1.026e-04
2: TRAIN [0][70/229]	Time 0.393 (0.361)	Data 1.23e-04 (2.69e-03)	Tok/s 25624 (20268)	Loss/tok 8.1150 (8.9431)	LR 1.026e-04
1: TRAIN [0][70/229]	Time 0.393 (0.361)	Data 1.28e-04 (1.99e-03)	Tok/s 25646 (20307)	Loss/tok 8.1649 (8.9427)	LR 1.026e-04
3: TRAIN [0][80/229]	Time 0.323 (0.363)	Data 1.12e-04 (2.09e-03)	Tok/s 13383 (20541)	Loss/tok 7.6617 (8.8344)	LR 1.291e-04
1: TRAIN [0][80/229]	Time 0.323 (0.363)	Data 1.26e-04 (1.76e-03)	Tok/s 13366 (20525)	Loss/tok 7.6639 (8.8297)	LR 1.291e-04
0: TRAIN [0][80/229]	Time 0.334 (0.363)	Data 1.34e-04 (2.03e-03)	Tok/s 12917 (20507)	Loss/tok 7.6561 (8.8286)	LR 1.291e-04
2: TRAIN [0][80/229]	Time 0.334 (0.363)	Data 1.27e-04 (2.37e-03)	Tok/s 12982 (20482)	Loss/tok 7.6769 (8.8277)	LR 1.291e-04
0: TRAIN [0][90/229]	Time 0.352 (0.364)	Data 1.24e-04 (1.82e-03)	Tok/s 20907 (20586)	Loss/tok 7.8364 (8.7295)	LR 1.626e-04
3: TRAIN [0][90/229]	Time 0.352 (0.364)	Data 1.49e-04 (1.87e-03)	Tok/s 20504 (20614)	Loss/tok 7.8595 (8.7373)	LR 1.626e-04
2: TRAIN [0][90/229]	Time 0.352 (0.364)	Data 1.38e-04 (2.13e-03)	Tok/s 20381 (20568)	Loss/tok 7.7811 (8.7281)	LR 1.626e-04
1: TRAIN [0][90/229]	Time 0.352 (0.364)	Data 1.22e-04 (1.59e-03)	Tok/s 20358 (20593)	Loss/tok 7.8296 (8.7319)	LR 1.626e-04
0: TRAIN [0][100/229]	Time 0.354 (0.365)	Data 1.16e-04 (1.66e-03)	Tok/s 20119 (20716)	Loss/tok 7.7422 (8.6326)	LR 2.047e-04
3: TRAIN [0][100/229]	Time 0.354 (0.365)	Data 1.09e-04 (1.70e-03)	Tok/s 20170 (20735)	Loss/tok 7.7206 (8.6385)	LR 2.047e-04
2: TRAIN [0][100/229]	Time 0.354 (0.365)	Data 1.18e-04 (1.93e-03)	Tok/s 20383 (20703)	Loss/tok 7.6276 (8.6344)	LR 2.047e-04
1: TRAIN [0][100/229]	Time 0.354 (0.365)	Data 1.29e-04 (1.45e-03)	Tok/s 20344 (20713)	Loss/tok 7.7178 (8.6368)	LR 2.047e-04
2: TRAIN [0][110/229]	Time 0.391 (0.364)	Data 1.37e-04 (1.77e-03)	Tok/s 25759 (20578)	Loss/tok 7.8331 (8.5614)	LR 2.576e-04
0: TRAIN [0][110/229]	Time 0.392 (0.364)	Data 1.41e-04 (1.52e-03)	Tok/s 25782 (20582)	Loss/tok 7.7594 (8.5577)	LR 2.576e-04
3: TRAIN [0][110/229]	Time 0.392 (0.364)	Data 1.33e-04 (1.56e-03)	Tok/s 25567 (20593)	Loss/tok 7.8214 (8.5641)	LR 2.576e-04
1: TRAIN [0][110/229]	Time 0.392 (0.364)	Data 1.30e-04 (1.33e-03)	Tok/s 25547 (20582)	Loss/tok 7.8244 (8.5619)	LR 2.576e-04
0: TRAIN [0][120/229]	Time 0.394 (0.364)	Data 1.17e-04 (1.40e-03)	Tok/s 25394 (20522)	Loss/tok 7.8715 (8.4990)	LR 3.244e-04
2: TRAIN [0][120/229]	Time 0.390 (0.364)	Data 1.69e-04 (1.63e-03)	Tok/s 25233 (20511)	Loss/tok 7.9500 (8.5027)	LR 3.244e-04
3: TRAIN [0][120/229]	Time 0.395 (0.364)	Data 1.19e-04 (1.44e-03)	Tok/s 25217 (20532)	Loss/tok 7.8786 (8.5054)	LR 3.244e-04
1: TRAIN [0][120/229]	Time 0.403 (0.364)	Data 1.20e-04 (1.23e-03)	Tok/s 24795 (20513)	Loss/tok 7.8546 (8.5017)	LR 3.244e-04
0: TRAIN [0][130/229]	Time 0.351 (0.364)	Data 1.16e-04 (1.31e-03)	Tok/s 20650 (20386)	Loss/tok 7.6696 (8.4508)	LR 4.083e-04
2: TRAIN [0][130/229]	Time 0.351 (0.364)	Data 1.60e-04 (1.52e-03)	Tok/s 20700 (20380)	Loss/tok 7.6805 (8.4545)	LR 4.083e-04
3: TRAIN [0][130/229]	Time 0.352 (0.364)	Data 1.14e-04 (1.34e-03)	Tok/s 20399 (20422)	Loss/tok 7.6775 (8.4576)	LR 4.083e-04
1: TRAIN [0][130/229]	Time 0.352 (0.364)	Data 1.37e-04 (1.15e-03)	Tok/s 20409 (20395)	Loss/tok 7.6824 (8.4546)	LR 4.083e-04
0: TRAIN [0][140/229]	Time 0.317 (0.364)	Data 1.16e-04 (1.22e-03)	Tok/s 14166 (20374)	Loss/tok 7.3360 (8.4000)	LR 5.141e-04
2: TRAIN [0][140/229]	Time 0.316 (0.364)	Data 1.28e-04 (1.42e-03)	Tok/s 13970 (20375)	Loss/tok 7.3089 (8.4011)	LR 5.141e-04
3: TRAIN [0][140/229]	Time 0.317 (0.364)	Data 1.28e-04 (1.25e-03)	Tok/s 13603 (20410)	Loss/tok 7.2760 (8.4038)	LR 5.141e-04
1: TRAIN [0][140/229]	Time 0.317 (0.364)	Data 1.48e-04 (1.08e-03)	Tok/s 13475 (20380)	Loss/tok 7.2170 (8.4034)	LR 5.141e-04
0: TRAIN [0][150/229]	Time 0.390 (0.364)	Data 1.19e-04 (1.15e-03)	Tok/s 25837 (20499)	Loss/tok 7.7361 (8.3467)	LR 6.472e-04
2: TRAIN [0][150/229]	Time 0.390 (0.364)	Data 1.21e-04 (1.33e-03)	Tok/s 25689 (20487)	Loss/tok 7.7306 (8.3481)	LR 6.472e-04
3: TRAIN [0][150/229]	Time 0.390 (0.364)	Data 1.87e-04 (1.18e-03)	Tok/s 26005 (20532)	Loss/tok 7.6728 (8.3509)	LR 6.472e-04
1: TRAIN [0][150/229]	Time 0.390 (0.364)	Data 1.72e-04 (1.02e-03)	Tok/s 26073 (20502)	Loss/tok 7.6603 (8.3491)	LR 6.472e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
2: TRAIN [0][160/229]	Time 0.316 (0.362)	Data 1.18e-04 (1.26e-03)	Tok/s 13403 (20293)	Loss/tok 7.2372 (8.3168)	LR 8.148e-04
3: TRAIN [0][160/229]	Time 0.316 (0.362)	Data 1.27e-04 (1.11e-03)	Tok/s 13390 (20328)	Loss/tok 7.1757 (8.3183)	LR 8.148e-04
1: TRAIN [0][160/229]	Time 0.316 (0.362)	Data 1.25e-04 (9.62e-04)	Tok/s 13797 (20308)	Loss/tok 7.2531 (8.3150)	LR 8.148e-04
0: TRAIN [0][160/229]	Time 0.320 (0.362)	Data 1.15e-04 (1.09e-03)	Tok/s 13690 (20299)	Loss/tok 7.2865 (8.3146)	LR 8.148e-04
2: TRAIN [0][170/229]	Time 0.394 (0.364)	Data 1.23e-04 (1.19e-03)	Tok/s 25720 (20515)	Loss/tok 7.6539 (8.2797)	LR 1.026e-03
0: TRAIN [0][170/229]	Time 0.391 (0.364)	Data 1.76e-04 (1.03e-03)	Tok/s 25993 (20519)	Loss/tok 7.6079 (8.2787)	LR 1.026e-03
3: TRAIN [0][170/229]	Time 0.394 (0.364)	Data 1.20e-04 (1.05e-03)	Tok/s 25496 (20547)	Loss/tok 7.6478 (8.2799)	LR 1.026e-03
1: TRAIN [0][170/229]	Time 0.394 (0.364)	Data 1.27e-04 (9.14e-04)	Tok/s 25543 (20531)	Loss/tok 7.5925 (8.2782)	LR 1.026e-03
0: TRAIN [0][180/229]	Time 0.352 (0.363)	Data 1.24e-04 (9.80e-04)	Tok/s 20435 (20489)	Loss/tok 7.3931 (8.2401)	LR 1.291e-03
3: TRAIN [0][180/229]	Time 0.351 (0.363)	Data 1.20e-04 (1.00e-03)	Tok/s 20315 (20513)	Loss/tok 7.4307 (8.2423)	LR 1.291e-03
2: TRAIN [0][180/229]	Time 0.351 (0.363)	Data 1.19e-04 (1.13e-03)	Tok/s 20515 (20485)	Loss/tok 7.3943 (8.2415)	LR 1.291e-03
1: TRAIN [0][180/229]	Time 0.351 (0.363)	Data 1.33e-04 (8.70e-04)	Tok/s 20521 (20503)	Loss/tok 7.4391 (8.2405)	LR 1.291e-03
0: TRAIN [0][190/229]	Time 0.353 (0.363)	Data 1.22e-04 (9.35e-04)	Tok/s 20252 (20488)	Loss/tok 7.4076 (8.2095)	LR 1.626e-03
3: TRAIN [0][190/229]	Time 0.352 (0.363)	Data 1.20e-04 (9.56e-04)	Tok/s 20378 (20510)	Loss/tok 7.4417 (8.2112)	LR 1.626e-03
2: TRAIN [0][190/229]	Time 0.353 (0.363)	Data 1.18e-04 (1.08e-03)	Tok/s 20322 (20484)	Loss/tok 7.3975 (8.2113)	LR 1.626e-03
1: TRAIN [0][190/229]	Time 0.353 (0.363)	Data 1.81e-04 (8.35e-04)	Tok/s 20659 (20506)	Loss/tok 7.4455 (8.2081)	LR 1.626e-03
0: TRAIN [0][200/229]	Time 0.393 (0.365)	Data 1.32e-04 (8.95e-04)	Tok/s 25610 (20693)	Loss/tok 7.3177 (8.1607)	LR 2.000e-03
3: TRAIN [0][200/229]	Time 0.393 (0.365)	Data 1.13e-04 (9.15e-04)	Tok/s 25517 (20713)	Loss/tok 7.3110 (8.1624)	LR 2.000e-03
2: TRAIN [0][200/229]	Time 0.393 (0.365)	Data 1.25e-04 (1.03e-03)	Tok/s 25711 (20686)	Loss/tok 7.2967 (8.1633)	LR 2.000e-03
1: TRAIN [0][200/229]	Time 0.393 (0.365)	Data 1.34e-04 (8.00e-04)	Tok/s 25550 (20709)	Loss/tok 7.2594 (8.1592)	LR 2.000e-03
0: TRAIN [0][210/229]	Time 0.353 (0.365)	Data 1.31e-04 (8.59e-04)	Tok/s 20693 (20669)	Loss/tok 7.0370 (8.1177)	LR 2.000e-03
3: TRAIN [0][210/229]	Time 0.353 (0.365)	Data 1.21e-04 (8.77e-04)	Tok/s 20264 (20683)	Loss/tok 7.0157 (8.1199)	LR 2.000e-03
2: TRAIN [0][210/229]	Time 0.353 (0.365)	Data 1.24e-04 (9.90e-04)	Tok/s 20399 (20659)	Loss/tok 7.0175 (8.1211)	LR 2.000e-03
1: TRAIN [0][210/229]	Time 0.353 (0.365)	Data 1.33e-04 (7.68e-04)	Tok/s 20427 (20683)	Loss/tok 7.0235 (8.1166)	LR 2.000e-03
0: TRAIN [0][220/229]	Time 0.355 (0.364)	Data 1.22e-04 (8.26e-04)	Tok/s 20190 (20567)	Loss/tok 6.9533 (8.0744)	LR 2.000e-03
2: TRAIN [0][220/229]	Time 0.355 (0.364)	Data 1.38e-04 (9.51e-04)	Tok/s 20608 (20560)	Loss/tok 6.9526 (8.0778)	LR 2.000e-03
3: TRAIN [0][220/229]	Time 0.355 (0.364)	Data 1.23e-04 (8.43e-04)	Tok/s 20146 (20580)	Loss/tok 6.9283 (8.0759)	LR 2.000e-03
1: TRAIN [0][220/229]	Time 0.355 (0.364)	Data 1.29e-04 (7.39e-04)	Tok/s 20239 (20583)	Loss/tok 6.8975 (8.0721)	LR 2.000e-03
3: Running validation on dev set
0: Running validation on dev set
1: Running validation on dev set
3: Executing preallocation
0: Executing preallocation
1: Executing preallocation
2: Running validation on dev set
2: Executing preallocation
3: VALIDATION [0][0/40]	Time 0.051 (0.051)	Data 1.71e-03 (1.71e-03)	Tok/s 81086 (81086)	Loss/tok 7.7512 (7.7512)
1: VALIDATION [0][0/40]	Time 0.071 (0.071)	Data 1.72e-03 (1.72e-03)	Tok/s 66729 (66729)	Loss/tok 7.8509 (7.8509)
2: VALIDATION [0][0/40]	Time 0.068 (0.068)	Data 1.75e-03 (1.75e-03)	Tok/s 65306 (65306)	Loss/tok 7.8900 (7.8900)
0: VALIDATION [0][0/40]	Time 0.095 (0.095)	Data 1.74e-03 (1.74e-03)	Tok/s 59981 (59981)	Loss/tok 7.9251 (7.9251)
3: VALIDATION [0][10/40]	Time 0.029 (0.039)	Data 1.33e-03 (1.45e-03)	Tok/s 78262 (76233)	Loss/tok 7.6403 (7.7038)
2: VALIDATION [0][10/40]	Time 0.029 (0.040)	Data 1.43e-03 (1.52e-03)	Tok/s 78084 (75635)	Loss/tok 7.5889 (7.6562)
1: VALIDATION [0][10/40]	Time 0.031 (0.041)	Data 1.47e-03 (1.47e-03)	Tok/s 75301 (75615)	Loss/tok 7.4370 (7.6758)
0: VALIDATION [0][10/40]	Time 0.031 (0.046)	Data 2.14e-03 (3.13e-03)	Tok/s 76230 (72152)	Loss/tok 7.4515 (7.7175)
3: VALIDATION [0][20/40]	Time 0.020 (0.032)	Data 1.37e-03 (1.41e-03)	Tok/s 79703 (76736)	Loss/tok 7.3646 (7.6010)
2: VALIDATION [0][20/40]	Time 0.021 (0.033)	Data 1.41e-03 (1.47e-03)	Tok/s 75107 (76385)	Loss/tok 7.2533 (7.5705)
1: VALIDATION [0][20/40]	Time 0.022 (0.034)	Data 1.40e-03 (1.44e-03)	Tok/s 73739 (75789)	Loss/tok 7.3370 (7.5713)
0: VALIDATION [0][20/40]	Time 0.022 (0.036)	Data 2.06e-03 (2.64e-03)	Tok/s 73637 (73123)	Loss/tok 7.3742 (7.5989)
3: VALIDATION [0][30/40]	Time 0.014 (0.027)	Data 1.30e-03 (1.39e-03)	Tok/s 72322 (75910)	Loss/tok 7.1402 (7.5524)
2: VALIDATION [0][30/40]	Time 0.014 (0.028)	Data 1.36e-03 (1.45e-03)	Tok/s 74346 (75830)	Loss/tok 7.3238 (7.5091)
1: VALIDATION [0][30/40]	Time 0.015 (0.028)	Data 1.39e-03 (1.42e-03)	Tok/s 72403 (75521)	Loss/tok 7.2901 (7.5146)
0: VALIDATION [0][30/40]	Time 0.015 (0.030)	Data 2.01e-03 (2.45e-03)	Tok/s 71393 (72800)	Loss/tok 7.4408 (7.5567)
0: Saving model to results/gnmt/model_best.pth
3: Running evaluation on test set
2: Running evaluation on test set
0: Running evaluation on test set
1: Running evaluation on test set
1: TEST [0][9/24]	Time 0.4128 (0.4250)	Decoder iters 149.0 (149.0)	Tok/s 6633 (6609)
0: TEST [0][9/24]	Time 0.4131 (0.4251)	Decoder iters 149.0 (149.0)	Tok/s 6236 (6512)
3: TEST [0][9/24]	Time 0.4129 (0.4252)	Decoder iters 149.0 (149.0)	Tok/s 6364 (6548)
2: TEST [0][9/24]	Time 0.4128 (0.4252)	Decoder iters 149.0 (149.0)	Tok/s 5666 (6684)
1: TEST [0][19/24]	Time 0.3873 (0.4346)	Decoder iters 149.0 (149.0)	Tok/s 3798 (5528)
3: TEST [0][19/24]	Time 0.3866 (0.4348)	Decoder iters 149.0 (149.0)	Tok/s 4085 (5541)
2: TEST [0][19/24]	Time 0.3863 (0.4346)	Decoder iters 149.0 (149.0)	Tok/s 4144 (5633)
0: TEST [0][19/24]	Time 0.3862 (0.4345)	Decoder iters 149.0 (149.0)	Tok/s 3532 (5477)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
3: Finished evaluation on test set
2: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
3: Finished epoch 0
2: Finished epoch 0
3: Starting epoch 1
1: Finished epoch 0
2: Starting epoch 1
1: Starting epoch 1
3: Executing preallocation
2: Executing preallocation
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 8.0320	Validation Loss: 7.4954	Test BLEU: 0.29
0: Performance: Epoch: 0	Training: 82460 Tok/s	Validation: 291546 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 2606193617
0: Sampler for epoch 1 uses seed 2606193617
3: Sampler for epoch 1 uses seed 2606193617
2: Sampler for epoch 1 uses seed 2606193617
3: TRAIN [1][0/229]	Time 0.494 (0.494)	Data 1.80e-01 (1.80e-01)	Tok/s 8673 (8673)	Loss/tok 6.6036 (6.6036)	LR 2.000e-03
1: TRAIN [1][0/229]	Time 0.495 (0.495)	Data 1.52e-01 (1.52e-01)	Tok/s 8762 (8762)	Loss/tok 6.4966 (6.4966)	LR 2.000e-03
0: TRAIN [1][0/229]	Time 0.498 (0.498)	Data 1.59e-01 (1.59e-01)	Tok/s 8834 (8834)	Loss/tok 6.5116 (6.5116)	LR 2.000e-03
2: TRAIN [1][0/229]	Time 0.499 (0.499)	Data 1.95e-01 (1.95e-01)	Tok/s 8621 (8621)	Loss/tok 6.5507 (6.5507)	LR 2.000e-03
0: TRAIN [1][10/229]	Time 0.392 (0.389)	Data 1.17e-04 (1.46e-02)	Tok/s 25441 (22088)	Loss/tok 6.8631 (6.8057)	LR 2.000e-03
3: TRAIN [1][10/229]	Time 0.391 (0.389)	Data 1.65e-04 (1.65e-02)	Tok/s 25554 (22006)	Loss/tok 6.7587 (6.7650)	LR 2.000e-03
2: TRAIN [1][10/229]	Time 0.392 (0.389)	Data 1.20e-04 (1.79e-02)	Tok/s 25820 (22040)	Loss/tok 6.7868 (6.8039)	LR 2.000e-03
1: TRAIN [1][10/229]	Time 0.392 (0.389)	Data 1.34e-04 (1.39e-02)	Tok/s 25678 (21913)	Loss/tok 6.8011 (6.7942)	LR 2.000e-03
0: TRAIN [1][20/229]	Time 0.354 (0.377)	Data 1.34e-04 (7.71e-03)	Tok/s 20202 (21363)	Loss/tok 6.4608 (6.7374)	LR 2.000e-03
3: TRAIN [1][20/229]	Time 0.354 (0.377)	Data 1.28e-04 (8.72e-03)	Tok/s 20455 (21342)	Loss/tok 6.6189 (6.7192)	LR 2.000e-03
2: TRAIN [1][20/229]	Time 0.354 (0.377)	Data 1.23e-04 (9.42e-03)	Tok/s 20445 (21356)	Loss/tok 6.4090 (6.7377)	LR 2.000e-03
1: TRAIN [1][20/229]	Time 0.354 (0.377)	Data 1.27e-04 (7.34e-03)	Tok/s 20420 (21281)	Loss/tok 6.4620 (6.7453)	LR 2.000e-03
2: TRAIN [1][30/229]	Time 0.353 (0.380)	Data 1.21e-04 (6.42e-03)	Tok/s 20612 (22072)	Loss/tok 6.4098 (6.6895)	LR 2.000e-03
0: TRAIN [1][30/229]	Time 0.354 (0.380)	Data 1.30e-04 (5.27e-03)	Tok/s 20652 (22055)	Loss/tok 6.3677 (6.6825)	LR 2.000e-03
3: TRAIN [1][30/229]	Time 0.353 (0.380)	Data 1.60e-04 (5.95e-03)	Tok/s 20361 (22018)	Loss/tok 6.3300 (6.6773)	LR 2.000e-03
1: TRAIN [1][30/229]	Time 0.354 (0.380)	Data 1.34e-04 (5.01e-03)	Tok/s 20568 (22014)	Loss/tok 6.3639 (6.6876)	LR 2.000e-03
3: TRAIN [1][40/229]	Time 0.313 (0.375)	Data 1.19e-04 (4.53e-03)	Tok/s 13542 (21378)	Loss/tok 5.9637 (6.6218)	LR 2.000e-03
0: TRAIN [1][40/229]	Time 0.314 (0.375)	Data 1.44e-04 (4.02e-03)	Tok/s 13750 (21423)	Loss/tok 5.9865 (6.6265)	LR 2.000e-03
2: TRAIN [1][40/229]	Time 0.313 (0.375)	Data 1.11e-04 (4.88e-03)	Tok/s 14092 (21412)	Loss/tok 5.9811 (6.6295)	LR 2.000e-03
1: TRAIN [1][40/229]	Time 0.313 (0.375)	Data 1.26e-04 (3.82e-03)	Tok/s 13405 (21368)	Loss/tok 5.8918 (6.6219)	LR 2.000e-03
0: TRAIN [1][50/229]	Time 0.391 (0.373)	Data 1.27e-04 (3.25e-03)	Tok/s 25888 (21426)	Loss/tok 6.2944 (6.5610)	LR 2.000e-03
3: TRAIN [1][50/229]	Time 0.390 (0.373)	Data 1.20e-04 (3.66e-03)	Tok/s 25689 (21354)	Loss/tok 6.4001 (6.5643)	LR 2.000e-03
2: TRAIN [1][50/229]	Time 0.391 (0.373)	Data 1.16e-04 (3.95e-03)	Tok/s 25484 (21377)	Loss/tok 6.3633 (6.5679)	LR 2.000e-03
1: TRAIN [1][50/229]	Time 0.390 (0.373)	Data 1.22e-04 (3.10e-03)	Tok/s 25918 (21388)	Loss/tok 6.3825 (6.5520)	LR 2.000e-03
2: TRAIN [1][60/229]	Time 0.350 (0.371)	Data 1.33e-04 (3.32e-03)	Tok/s 20385 (21213)	Loss/tok 6.0732 (6.5084)	LR 2.000e-03
0: TRAIN [1][60/229]	Time 0.351 (0.371)	Data 1.25e-04 (2.74e-03)	Tok/s 20636 (21268)	Loss/tok 6.1154 (6.5020)	LR 2.000e-03
3: TRAIN [1][60/229]	Time 0.351 (0.371)	Data 1.18e-04 (3.09e-03)	Tok/s 20764 (21177)	Loss/tok 6.1891 (6.5070)	LR 2.000e-03
1: TRAIN [1][60/229]	Time 0.349 (0.371)	Data 1.75e-04 (2.61e-03)	Tok/s 20877 (21218)	Loss/tok 6.1067 (6.4926)	LR 2.000e-03
0: TRAIN [1][70/229]	Time 0.393 (0.371)	Data 1.29e-04 (2.37e-03)	Tok/s 25736 (21228)	Loss/tok 6.0963 (6.4556)	LR 2.000e-03
3: TRAIN [1][70/229]	Time 0.394 (0.371)	Data 1.21e-04 (2.67e-03)	Tok/s 25380 (21142)	Loss/tok 6.1609 (6.4574)	LR 2.000e-03
1: TRAIN [1][70/229]	Time 0.394 (0.371)	Data 1.24e-04 (2.26e-03)	Tok/s 25682 (21165)	Loss/tok 6.2070 (6.4476)	LR 2.000e-03
2: TRAIN [1][70/229]	Time 0.398 (0.371)	Data 1.48e-04 (2.88e-03)	Tok/s 25326 (21167)	Loss/tok 6.2197 (6.4639)	LR 2.000e-03
0: TRAIN [1][80/229]	Time 0.389 (0.367)	Data 1.17e-04 (2.10e-03)	Tok/s 25753 (20729)	Loss/tok 6.0631 (6.4023)	LR 1.000e-03
2: TRAIN [1][80/229]	Time 0.389 (0.367)	Data 1.21e-04 (2.54e-03)	Tok/s 25954 (20684)	Loss/tok 6.1464 (6.4094)	LR 1.000e-03
3: TRAIN [1][80/229]	Time 0.390 (0.367)	Data 1.11e-04 (2.35e-03)	Tok/s 26097 (20671)	Loss/tok 6.1750 (6.4077)	LR 1.000e-03
1: TRAIN [1][80/229]	Time 0.390 (0.367)	Data 1.25e-04 (2.00e-03)	Tok/s 25732 (20672)	Loss/tok 6.0553 (6.3947)	LR 1.000e-03
0: TRAIN [1][90/229]	Time 0.316 (0.367)	Data 1.18e-04 (1.88e-03)	Tok/s 14047 (20796)	Loss/tok 5.4302 (6.3450)	LR 1.000e-03
2: TRAIN [1][90/229]	Time 0.317 (0.367)	Data 1.26e-04 (2.27e-03)	Tok/s 13550 (20758)	Loss/tok 5.4212 (6.3521)	LR 1.000e-03
3: TRAIN [1][90/229]	Time 0.317 (0.367)	Data 1.09e-04 (2.11e-03)	Tok/s 13536 (20744)	Loss/tok 5.4452 (6.3526)	LR 1.000e-03
1: TRAIN [1][90/229]	Time 0.317 (0.367)	Data 1.35e-04 (1.79e-03)	Tok/s 13915 (20737)	Loss/tok 5.3194 (6.3413)	LR 1.000e-03
2: TRAIN [1][100/229]	Time 0.448 (0.368)	Data 1.27e-04 (2.06e-03)	Tok/s 29431 (20865)	Loss/tok 6.1093 (6.2994)	LR 1.000e-03
3: TRAIN [1][100/229]	Time 0.449 (0.368)	Data 1.17e-04 (1.91e-03)	Tok/s 29284 (20856)	Loss/tok 6.0883 (6.3015)	LR 1.000e-03
1: TRAIN [1][100/229]	Time 0.449 (0.368)	Data 1.47e-04 (1.63e-03)	Tok/s 29029 (20850)	Loss/tok 6.1108 (6.2914)	LR 1.000e-03
0: TRAIN [1][100/229]	Time 0.449 (0.368)	Data 2.01e-04 (1.71e-03)	Tok/s 29005 (20902)	Loss/tok 6.1030 (6.2908)	LR 1.000e-03
0: TRAIN [1][110/229]	Time 0.316 (0.367)	Data 1.19e-04 (1.57e-03)	Tok/s 14087 (20806)	Loss/tok 5.2611 (6.2388)	LR 1.000e-03
3: TRAIN [1][110/229]	Time 0.315 (0.367)	Data 1.20e-04 (1.75e-03)	Tok/s 13556 (20745)	Loss/tok 5.2377 (6.2494)	LR 1.000e-03
2: TRAIN [1][110/229]	Time 0.316 (0.367)	Data 1.21e-04 (1.88e-03)	Tok/s 13767 (20764)	Loss/tok 5.1879 (6.2481)	LR 1.000e-03
1: TRAIN [1][110/229]	Time 0.316 (0.367)	Data 1.39e-04 (1.50e-03)	Tok/s 13176 (20745)	Loss/tok 5.1969 (6.2413)	LR 1.000e-03
0: TRAIN [1][120/229]	Time 0.391 (0.365)	Data 1.20e-04 (1.45e-03)	Tok/s 26042 (20655)	Loss/tok 5.7377 (6.1875)	LR 5.000e-04
1: TRAIN [1][120/229]	Time 0.391 (0.365)	Data 1.27e-04 (1.38e-03)	Tok/s 25872 (20604)	Loss/tok 5.7575 (6.1904)	LR 5.000e-04
3: TRAIN [1][120/229]	Time 0.391 (0.365)	Data 1.23e-04 (1.62e-03)	Tok/s 25768 (20594)	Loss/tok 5.7942 (6.1988)	LR 5.000e-04
2: TRAIN [1][120/229]	Time 0.403 (0.365)	Data 1.23e-04 (1.74e-03)	Tok/s 25010 (20621)	Loss/tok 5.7863 (6.1966)	LR 5.000e-04
0: TRAIN [1][130/229]	Time 0.354 (0.365)	Data 1.14e-04 (1.35e-03)	Tok/s 20130 (20683)	Loss/tok 5.3398 (6.1422)	LR 5.000e-04
3: TRAIN [1][130/229]	Time 0.353 (0.365)	Data 1.28e-04 (1.51e-03)	Tok/s 20463 (20631)	Loss/tok 5.4848 (6.1522)	LR 5.000e-04
2: TRAIN [1][130/229]	Time 0.354 (0.365)	Data 1.22e-04 (1.62e-03)	Tok/s 20751 (20661)	Loss/tok 5.4930 (6.1526)	LR 5.000e-04
1: TRAIN [1][130/229]	Time 0.354 (0.365)	Data 1.39e-04 (1.29e-03)	Tok/s 20291 (20631)	Loss/tok 5.4941 (6.1466)	LR 5.000e-04
0: TRAIN [1][140/229]	Time 0.397 (0.367)	Data 1.13e-04 (1.26e-03)	Tok/s 25019 (20937)	Loss/tok 5.6626 (6.1001)	LR 5.000e-04
2: TRAIN [1][140/229]	Time 0.398 (0.367)	Data 1.20e-04 (1.51e-03)	Tok/s 25000 (20923)	Loss/tok 5.5377 (6.1065)	LR 5.000e-04
1: TRAIN [1][140/229]	Time 0.397 (0.367)	Data 1.51e-04 (1.21e-03)	Tok/s 25340 (20894)	Loss/tok 5.6203 (6.1017)	LR 5.000e-04
3: TRAIN [1][140/229]	Time 0.394 (0.367)	Data 2.01e-04 (1.41e-03)	Tok/s 25749 (20900)	Loss/tok 5.6083 (6.1057)	LR 5.000e-04
0: TRAIN [1][150/229]	Time 0.396 (0.366)	Data 1.28e-04 (1.19e-03)	Tok/s 25158 (20869)	Loss/tok 5.5880 (6.0629)	LR 5.000e-04
2: TRAIN [1][150/229]	Time 0.396 (0.366)	Data 1.28e-04 (1.42e-03)	Tok/s 25198 (20852)	Loss/tok 5.5646 (6.0656)	LR 5.000e-04
3: TRAIN [1][150/229]	Time 0.396 (0.366)	Data 1.26e-04 (1.33e-03)	Tok/s 25754 (20837)	Loss/tok 5.6095 (6.0654)	LR 5.000e-04
1: TRAIN [1][150/229]	Time 0.396 (0.366)	Data 1.32e-04 (1.14e-03)	Tok/s 25491 (20828)	Loss/tok 5.6449 (6.0622)	LR 5.000e-04
0: TRAIN [1][160/229]	Time 0.353 (0.366)	Data 1.37e-04 (1.12e-03)	Tok/s 20361 (20878)	Loss/tok 5.3570 (6.0270)	LR 2.500e-04
3: TRAIN [1][160/229]	Time 0.353 (0.366)	Data 1.18e-04 (1.25e-03)	Tok/s 20071 (20851)	Loss/tok 5.3405 (6.0277)	LR 2.500e-04
2: TRAIN [1][160/229]	Time 0.353 (0.366)	Data 1.36e-04 (1.34e-03)	Tok/s 20202 (20867)	Loss/tok 5.2794 (6.0257)	LR 2.500e-04
1: TRAIN [1][160/229]	Time 0.353 (0.366)	Data 1.29e-04 (1.07e-03)	Tok/s 20442 (20852)	Loss/tok 5.2897 (6.0243)	LR 2.500e-04
0: TRAIN [1][170/229]	Time 0.394 (0.367)	Data 1.26e-04 (1.06e-03)	Tok/s 25381 (21006)	Loss/tok 5.5260 (5.9894)	LR 2.500e-04
2: TRAIN [1][170/229]	Time 0.395 (0.367)	Data 1.15e-04 (1.27e-03)	Tok/s 25422 (21004)	Loss/tok 5.4919 (5.9914)	LR 2.500e-04
1: TRAIN [1][170/229]	Time 0.394 (0.367)	Data 1.34e-04 (1.02e-03)	Tok/s 25677 (20989)	Loss/tok 5.4689 (5.9899)	LR 2.500e-04
3: TRAIN [1][170/229]	Time 0.395 (0.367)	Data 1.42e-04 (1.19e-03)	Tok/s 25156 (20986)	Loss/tok 5.5752 (5.9945)	LR 2.500e-04
0: TRAIN [1][180/229]	Time 0.352 (0.367)	Data 1.24e-04 (1.01e-03)	Tok/s 20582 (21014)	Loss/tok 5.3636 (5.9586)	LR 2.500e-04
2: TRAIN [1][180/229]	Time 0.352 (0.367)	Data 1.39e-04 (1.21e-03)	Tok/s 20469 (21007)	Loss/tok 5.2899 (5.9617)	LR 2.500e-04
3: TRAIN [1][180/229]	Time 0.352 (0.367)	Data 1.32e-04 (1.13e-03)	Tok/s 20391 (20987)	Loss/tok 5.2637 (5.9634)	LR 2.500e-04
1: TRAIN [1][180/229]	Time 0.352 (0.367)	Data 1.34e-04 (9.69e-04)	Tok/s 20611 (21001)	Loss/tok 5.2581 (5.9597)	LR 2.500e-04
0: TRAIN [1][190/229]	Time 0.391 (0.367)	Data 1.21e-04 (9.65e-04)	Tok/s 25576 (21006)	Loss/tok 5.5578 (5.9286)	LR 1.250e-04
3: TRAIN [1][190/229]	Time 0.391 (0.367)	Data 1.15e-04 (1.08e-03)	Tok/s 25611 (20974)	Loss/tok 5.4803 (5.9343)	LR 1.250e-04
2: TRAIN [1][190/229]	Time 0.391 (0.367)	Data 1.71e-04 (1.15e-03)	Tok/s 25886 (21001)	Loss/tok 5.4491 (5.9308)	LR 1.250e-04
1: TRAIN [1][190/229]	Time 0.392 (0.367)	Data 1.84e-04 (9.26e-04)	Tok/s 25830 (20994)	Loss/tok 5.5466 (5.9298)	LR 1.250e-04
0: TRAIN [1][200/229]	Time 0.445 (0.366)	Data 1.22e-04 (9.24e-04)	Tok/s 29193 (20927)	Loss/tok 5.6376 (5.9038)	LR 1.250e-04
2: TRAIN [1][200/229]	Time 0.445 (0.366)	Data 1.36e-04 (1.10e-03)	Tok/s 29627 (20928)	Loss/tok 5.6274 (5.9044)	LR 1.250e-04
3: TRAIN [1][200/229]	Time 0.445 (0.366)	Data 1.21e-04 (1.03e-03)	Tok/s 29170 (20901)	Loss/tok 5.6890 (5.9090)	LR 1.250e-04
1: TRAIN [1][200/229]	Time 0.445 (0.366)	Data 1.38e-04 (8.87e-04)	Tok/s 29245 (20918)	Loss/tok 5.6193 (5.9036)	LR 1.250e-04
0: TRAIN [1][210/229]	Time 0.354 (0.366)	Data 1.13e-04 (8.86e-04)	Tok/s 20514 (20878)	Loss/tok 5.2580 (5.8782)	LR 1.250e-04
2: TRAIN [1][210/229]	Time 0.354 (0.366)	Data 1.15e-04 (1.05e-03)	Tok/s 20327 (20878)	Loss/tok 5.2304 (5.8785)	LR 1.250e-04
3: TRAIN [1][210/229]	Time 0.353 (0.366)	Data 1.68e-04 (9.85e-04)	Tok/s 20471 (20852)	Loss/tok 5.2232 (5.8842)	LR 1.250e-04
1: TRAIN [1][210/229]	Time 0.354 (0.366)	Data 1.60e-04 (8.52e-04)	Tok/s 20298 (20867)	Loss/tok 5.3021 (5.8790)	LR 1.250e-04
0: TRAIN [1][220/229]	Time 0.313 (0.365)	Data 1.19e-04 (8.51e-04)	Tok/s 13731 (20766)	Loss/tok 4.9437 (5.8551)	LR 1.250e-04
3: TRAIN [1][220/229]	Time 0.314 (0.365)	Data 1.20e-04 (9.46e-04)	Tok/s 14266 (20735)	Loss/tok 4.9870 (5.8619)	LR 1.250e-04
2: TRAIN [1][220/229]	Time 0.314 (0.365)	Data 1.29e-04 (1.01e-03)	Tok/s 14005 (20767)	Loss/tok 4.8652 (5.8551)	LR 1.250e-04
1: TRAIN [1][220/229]	Time 0.314 (0.365)	Data 1.37e-04 (8.19e-04)	Tok/s 13779 (20753)	Loss/tok 4.8860 (5.8574)	LR 1.250e-04
2: Running validation on dev set
1: Running validation on dev set
2: Executing preallocation
1: Executing preallocation
3: Running validation on dev set
3: Executing preallocation
0: Running validation on dev set
0: Executing preallocation
3: VALIDATION [1][0/40]	Time 0.061 (0.061)	Data 1.67e-03 (1.67e-03)	Tok/s 68902 (68902)	Loss/tok 6.5844 (6.5844)
2: VALIDATION [1][0/40]	Time 0.067 (0.067)	Data 1.74e-03 (1.74e-03)	Tok/s 66162 (66162)	Loss/tok 6.6529 (6.6529)
1: VALIDATION [1][0/40]	Time 0.070 (0.070)	Data 1.84e-03 (1.84e-03)	Tok/s 67958 (67958)	Loss/tok 6.6787 (6.6787)
0: VALIDATION [1][0/40]	Time 0.101 (0.101)	Data 1.67e-03 (1.67e-03)	Tok/s 56825 (56825)	Loss/tok 6.7356 (6.7356)
3: VALIDATION [1][10/40]	Time 0.031 (0.040)	Data 2.14e-03 (2.46e-03)	Tok/s 74182 (74356)	Loss/tok 6.3550 (6.4627)
2: VALIDATION [1][10/40]	Time 0.028 (0.041)	Data 1.42e-03 (1.52e-03)	Tok/s 80046 (74491)	Loss/tok 6.3930 (6.4098)
1: VALIDATION [1][10/40]	Time 0.030 (0.042)	Data 1.39e-03 (1.47e-03)	Tok/s 76681 (74948)	Loss/tok 6.1337 (6.4362)
0: VALIDATION [1][10/40]	Time 0.030 (0.045)	Data 1.40e-03 (1.46e-03)	Tok/s 77805 (74490)	Loss/tok 6.0874 (6.4651)
3: VALIDATION [1][20/40]	Time 0.020 (0.033)	Data 2.04e-03 (2.29e-03)	Tok/s 77327 (74206)	Loss/tok 6.1632 (6.3477)
2: VALIDATION [1][20/40]	Time 0.021 (0.033)	Data 1.39e-03 (1.46e-03)	Tok/s 75330 (75586)	Loss/tok 5.9335 (6.3147)
1: VALIDATION [1][20/40]	Time 0.021 (0.034)	Data 1.36e-03 (1.43e-03)	Tok/s 74867 (75520)	Loss/tok 6.1051 (6.3132)
0: VALIDATION [1][20/40]	Time 0.021 (0.035)	Data 1.36e-03 (1.42e-03)	Tok/s 76591 (75407)	Loss/tok 6.0184 (6.3486)
3: VALIDATION [1][30/40]	Time 0.015 (0.028)	Data 1.31e-03 (2.16e-03)	Tok/s 71362 (73201)	Loss/tok 5.7277 (6.2795)
2: VALIDATION [1][30/40]	Time 0.015 (0.028)	Data 1.39e-03 (1.44e-03)	Tok/s 69099 (74807)	Loss/tok 5.9296 (6.2360)
1: VALIDATION [1][30/40]	Time 0.015 (0.029)	Data 2.09e-03 (1.48e-03)	Tok/s 68737 (74563)	Loss/tok 5.8589 (6.2426)
0: VALIDATION [1][30/40]	Time 0.015 (0.030)	Data 1.33e-03 (1.40e-03)	Tok/s 73518 (75145)	Loss/tok 6.0842 (6.2904)
0: Saving model to results/gnmt/model_best.pth
2: Running evaluation on test set
0: Running evaluation on test set
1: Running evaluation on test set
3: Running evaluation on test set
1: TEST [1][9/24]	Time 0.4040 (0.4856)	Decoder iters 149.0 (149.0)	Tok/s 5049 (6983)
3: TEST [1][9/24]	Time 0.4038 (0.4857)	Decoder iters 149.0 (149.0)	Tok/s 4785 (6418)
0: TEST [1][9/24]	Time 0.4040 (0.4860)	Decoder iters 149.0 (149.0)	Tok/s 5230 (7302)
2: TEST [1][9/24]	Time 0.4038 (0.4860)	Decoder iters 149.0 (149.0)	Tok/s 5227 (6802)
1: TEST [1][19/24]	Time 0.3691 (0.4426)	Decoder iters 132.0 (133.2)	Tok/s 2661 (5380)
0: TEST [1][19/24]	Time 0.3694 (0.4428)	Decoder iters 149.0 (139.3)	Tok/s 2913 (5684)
3: TEST [1][19/24]	Time 0.3693 (0.4427)	Decoder iters 37.0 (126.7)	Tok/s 2765 (5062)
2: TEST [1][19/24]	Time 0.3697 (0.4428)	Decoder iters 149.0 (133.1)	Tok/s 2838 (5279)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
2: Finished evaluation on test set
1: Finished evaluation on test set
3: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
2: Finished epoch 1
3: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 5.8409	Validation Loss: 6.2028	Test BLEU: 1.34
0: Performance: Epoch: 1	Training: 82453 Tok/s	Validation: 289457 Tok/s
0: Finished epoch 1
1: Total training time 219 s
2: Total training time 219 s
0: Total training time 219 s
3: Total training time 219 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       4|                 160|                      1.34|                      82456.8|                         3.657|
DONE!
