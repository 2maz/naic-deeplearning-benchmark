:::NVLOGv0.2.2 Tacotron2_PyT 1587238245.685030699 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1587238245.705904484 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 92, "name": "Intel Xeon Processor (Skylake, IBRS)"}
:::NVLOGv0.2.2 Tacotron2_PyT 1587238245.723937750 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "440G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1587238272.957558632 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.64", "num": 8, "name": ["Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB"], "mem": ["16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1587238272.964318514 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 10, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1587238277.984571218 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1587238339.891389608 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1587238339.903365850 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238340.335047722 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238348.217333078 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021171921398490667
:::NVLOGv0.2.2 Tacotron2_PyT 1587238351.821482420 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238351.822006226 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 55708.491646095914
:::NVLOGv0.2.2 Tacotron2_PyT 1587238351.822426796 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 11.48837423324585
Batch: 1/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238351.826128244 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238353.857448816 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018833238864317536
:::NVLOGv0.2.2 Tacotron2_PyT 1587238355.006654978 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238355.007477283 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 201166.88058433207
:::NVLOGv0.2.2 Tacotron2_PyT 1587238355.008143902 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.1814382076263428
Batch: 2/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238355.012719631 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1587238355.491932154 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0025093352887779474
:::NVLOGv0.2.2 Tacotron2_PyT 1587238356.580765486 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1587238356.581559181 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 407869.8392439299
:::NVLOGv0.2.2 Tacotron2_PyT 1587238356.582232237 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5691280364990234
Batch: 3/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238356.585972786 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1587238357.099824429 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024603167548775673
:::NVLOGv0.2.2 Tacotron2_PyT 1587238358.140177250 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1587238358.141052723 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 411540.84056999156
:::NVLOGv0.2.2 Tacotron2_PyT 1587238358.141687393 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.555131196975708
Batch: 4/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238358.146457911 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1587238358.691217899 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024811262264847755
:::NVLOGv0.2.2 Tacotron2_PyT 1587238359.770431519 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1587238359.771188736 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 393782.08096903365
:::NVLOGv0.2.2 Tacotron2_PyT 1587238359.771761894 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.6252644062042236
Batch: 5/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238359.775268078 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1587238360.303480387 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018393031787127256
:::NVLOGv0.2.2 Tacotron2_PyT 1587238361.420578003 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1587238361.421082973 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 388833.34694939404
:::NVLOGv0.2.2 Tacotron2_PyT 1587238361.421479464 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.645949363708496
Batch: 6/7 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238361.425361633 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1587238361.904143810 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002135458867996931
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.003515482 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.004882336 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 405351.8341031333
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.006072283 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5788753032684326
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.259465218 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.260839462 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 191803.11587449774
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.262026310 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 323464.7591522729
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.263304710 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0022037223347329666
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.264481068 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 23.357284784317017
:::NVLOGv0.2.2 Tacotron2_PyT 1587238363.265620232 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1587238365.542168856 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.001926549943163991
:::NVLOGv0.2.2 Tacotron2_PyT 1587238365.545222998 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238368.980935574 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238369.120254755 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238369.565756083 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022421199828386307
:::NVLOGv0.2.2 Tacotron2_PyT 1587238370.707810640 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1587238370.708352327 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 402115.49526550574
:::NVLOGv0.2.2 Tacotron2_PyT 1587238370.708804607 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5915825366973877
Batch: 1/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238370.711674690 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238371.230370045 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.001955213490873575
:::NVLOGv0.2.2 Tacotron2_PyT 1587238372.265765190 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238372.266276598 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 411628.62262781267
:::NVLOGv0.2.2 Tacotron2_PyT 1587238372.266753674 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5547995567321777
Batch: 2/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238372.269416332 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1587238372.864615679 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019999025389552116
:::NVLOGv0.2.2 Tacotron2_PyT 1587238374.091447353 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1587238374.091985703 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 351089.09697136714
:::NVLOGv0.2.2 Tacotron2_PyT 1587238374.092518568 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.8228991031646729
Batch: 3/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238374.096189976 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1587238374.350812674 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024405994918197393
:::NVLOGv0.2.2 Tacotron2_PyT 1587238375.470747948 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1587238375.471397161 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 465161.8415965767
:::NVLOGv0.2.2 Tacotron2_PyT 1587238375.471840858 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.3758652210235596
Batch: 4/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238375.474614859 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1587238376.047736406 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002040569204837084
:::NVLOGv0.2.2 Tacotron2_PyT 1587238377.137269020 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1587238377.137811422 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 384710.0201286831
:::NVLOGv0.2.2 Tacotron2_PyT 1587238377.138204575 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.663590669631958
Batch: 5/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238377.140714407 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1587238377.760725737 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024786321446299553
:::NVLOGv0.2.2 Tacotron2_PyT 1587238378.989544392 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1587238378.990001678 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 346071.1110561046
:::NVLOGv0.2.2 Tacotron2_PyT 1587238378.990343332 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.8493309020996094
Batch: 6/7 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238378.993795156 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1587238379.401690722 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021586825605481863
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.714316368 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.714892387 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 371862.60688079376
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.715505362 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.7210657596588135
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.773959637 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.775539398 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 379856.86131456215
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.776862860 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 390376.970646692
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.778132677 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002187959916357483
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.779512882 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 11.79391622543335
:::NVLOGv0.2.2 Tacotron2_PyT 1587238380.780791759 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238381.798082829 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.002600262640044093
:::NVLOGv0.2.2 Tacotron2_PyT 1587238381.799702644 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1587238381.801824093 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 103.81638216972351
:::NVLOGv0.2.2 Tacotron2_PyT 1587238381.802659988 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 103.81638216972351
:::NVLOGv0.2.2 Tacotron2_PyT 1587238381.803598166 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 136.22169303894043
:::NVLOGv0.2.2 Tacotron2_PyT 1587238381.804339409 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
