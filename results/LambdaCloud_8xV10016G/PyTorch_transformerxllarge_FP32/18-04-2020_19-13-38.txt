Experiment dir : LM-TFM
Namespace(adaptive=False, append_dataset=False, append_time=False, attn_type=0, batch_chunk=1, batch_size=16, clamp_len=-1, clip=0.25, clip_nonemb=False, cuda=True, d_embed=1024, d_head=64, d_inner=4096, d_model=1024, data='/data/transformer-xl/wikitext-103', dataset='wt103', debug=False, decay_rate=0.5, div_val=1, dropatt=0.2, dropout=0.2, emb_init='normal', emb_init_range=0.01, eta_min=0.001, eval_batch_size=16, eval_interval=5000, eval_max_steps=-1, eval_tgt_len=128, ext_len=0, fp16=False, gpu0_bsz=-1, init='normal', init_range=0.1, init_std=0.02, local_rank=0, log_all_ranks=False, log_interval=10, lr=0.0, lr_min=0.0, max_step=400, max_step_scheduler=None, mem_len=256, mom=0.0, multi_gpu=None, n_head=16, n_layer=18, not_tied=False, optim='adam', patience=0, pre_lnorm=False, proj_init_std=0.01, restart='', roll=True, same_length=False, sample_softmax=-1, save_all=False, scheduler='cosine', seed=1111, target_perplexity=None, target_throughput=None, tgt_len=256, tied=True, varlen=False, vocab='word', warmup_step=16000, weight_decay=0.0, work_dir='LM-TFM')
Producing dataset wt103...
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
====================================================================================================
    - work_dir : LM-TFM
    - append_dataset : False
    - append_time : False
    - cuda : True
    - fp16 : False
    - restart : 
    - debug : False
    - log_all_ranks : False
    - save_all : False
    - log_interval : 10
    - target_throughput : None
    - target_perplexity : None
    - data : /data/transformer-xl/wikitext-103
    - dataset : wt103
    - vocab : word
    - n_layer : 18
    - n_head : 16
    - d_head : 64
    - d_embed : 1024
    - d_model : 1024
    - d_inner : 4096
    - dropout : 0.2
    - dropatt : 0.2
    - pre_lnorm : False
    - attn_type : 0
    - not_tied : False
    - clamp_len : -1
    - adaptive : False
    - div_val : 1
    - sample_softmax : -1
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.0
    - mom : 0.0
    - scheduler : cosine
    - max_step_scheduler : None
    - warmup_step : 16000
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - weight_decay : 0.0
    - clip_nonemb : False
    - patience : 0
    - eta_min : 0.001
    - max_step : 400
    - batch_size : 16
    - batch_chunk : 1
    - roll : True
    - tgt_len : 256
    - ext_len : 0
    - mem_len : 256
    - seed : 1111
    - multi_gpu : None
    - gpu0_bsz : -1
    - same_length : False
    - varlen : False
    - eval_tgt_len : 128
    - eval_batch_size : 16
    - eval_max_steps : -1
    - eval_interval : 5000
    - local_rank : 0
    - tied : True
    - n_token : 267735
    - n_all_param : 519963095
    - n_nonemb_param : 245532672
====================================================================================================
#params = 519963095
#non emb params = 245532672
| epoch   1 step       10 | batches     10 / 25202 | lr 0.000e+00 | ms/batch 761.1 | tok/s    5515 | loss 12.76 | ppl 347134.06
| epoch   1 step       20 | batches     20 / 25202 | lr 0.000e+00 | ms/batch 305.6 | tok/s   13402 | loss 12.76 | ppl 346843.85
| epoch   1 step       30 | batches     30 / 25202 | lr 0.000e+00 | ms/batch 305.6 | tok/s   13401 | loss 12.75 | ppl 343776.64
| epoch   1 step       40 | batches     40 / 25202 | lr 0.000e+00 | ms/batch 305.3 | tok/s   13417 | loss 12.76 | ppl 349295.92
| epoch   1 step       50 | batches     50 / 25202 | lr 0.000e+00 | ms/batch 307.1 | tok/s   13339 | loss 12.76 | ppl 346794.56
| epoch   1 step       60 | batches     60 / 25202 | lr 0.000e+00 | ms/batch 306.3 | tok/s   13374 | loss 12.75 | ppl 345093.51
| epoch   1 step       70 | batches     70 / 25202 | lr 0.000e+00 | ms/batch 306.6 | tok/s   13359 | loss 12.76 | ppl 348134.28
| epoch   1 step       80 | batches     80 / 25202 | lr 0.000e+00 | ms/batch 306.9 | tok/s   13348 | loss 12.76 | ppl 347832.29
| epoch   1 step       90 | batches     90 / 25202 | lr 0.000e+00 | ms/batch 306.1 | tok/s   13380 | loss 12.76 | ppl 347689.01
| epoch   1 step      100 | batches    100 / 25202 | lr 0.000e+00 | ms/batch 306.6 | tok/s   13358 | loss 12.76 | ppl 346826.65
| epoch   1 step      110 | batches    110 / 25202 | lr 0.000e+00 | ms/batch 307.3 | tok/s   13328 | loss 12.75 | ppl 342983.17
| epoch   1 step      120 | batches    120 / 25202 | lr 0.000e+00 | ms/batch 308.4 | tok/s   13281 | loss 12.75 | ppl 343197.49
| epoch   1 step      130 | batches    130 / 25202 | lr 0.000e+00 | ms/batch 308.5 | tok/s   13277 | loss 12.75 | ppl 343612.43
| epoch   1 step      140 | batches    140 / 25202 | lr 0.000e+00 | ms/batch 308.4 | tok/s   13283 | loss 12.75 | ppl 345860.85
| epoch   1 step      150 | batches    150 / 25202 | lr 0.000e+00 | ms/batch 307.9 | tok/s   13304 | loss 12.76 | ppl 347412.25
| epoch   1 step      160 | batches    160 / 25202 | lr 0.000e+00 | ms/batch 308.3 | tok/s   13284 | loss 12.75 | ppl 346272.73
| epoch   1 step      170 | batches    170 / 25202 | lr 0.000e+00 | ms/batch 308.4 | tok/s   13283 | loss 12.76 | ppl 347544.81
| epoch   1 step      180 | batches    180 / 25202 | lr 0.000e+00 | ms/batch 309.5 | tok/s   13235 | loss 12.75 | ppl 346200.75
| epoch   1 step      190 | batches    190 / 25202 | lr 0.000e+00 | ms/batch 310.2 | tok/s   13203 | loss 12.74 | ppl 342083.86
| epoch   1 step      200 | batches    200 / 25202 | lr 0.000e+00 | ms/batch 308.1 | tok/s   13294 | loss 12.75 | ppl 343574.74
| epoch   1 step      210 | batches    210 / 25202 | lr 0.000e+00 | ms/batch 308.3 | tok/s   13287 | loss 12.76 | ppl 349074.46
| epoch   1 step      220 | batches    220 / 25202 | lr 0.000e+00 | ms/batch 309.2 | tok/s   13246 | loss 12.76 | ppl 346385.69
| epoch   1 step      230 | batches    230 / 25202 | lr 0.000e+00 | ms/batch 308.2 | tok/s   13291 | loss 12.75 | ppl 345810.72
| epoch   1 step      240 | batches    240 / 25202 | lr 0.000e+00 | ms/batch 308.3 | tok/s   13284 | loss 12.75 | ppl 346004.03
| epoch   1 step      250 | batches    250 / 25202 | lr 0.000e+00 | ms/batch 308.7 | tok/s   13268 | loss 12.75 | ppl 343304.20
| epoch   1 step      260 | batches    260 / 25202 | lr 0.000e+00 | ms/batch 308.3 | tok/s   13287 | loss 12.76 | ppl 346911.00
| epoch   1 step      270 | batches    270 / 25202 | lr 0.000e+00 | ms/batch 308.4 | tok/s   13282 | loss 12.75 | ppl 345475.81
| epoch   1 step      280 | batches    280 / 25202 | lr 0.000e+00 | ms/batch 309.6 | tok/s   13232 | loss 12.76 | ppl 349091.11
| epoch   1 step      290 | batches    290 / 25202 | lr 0.000e+00 | ms/batch 309.1 | tok/s   13251 | loss 12.76 | ppl 347840.91
| epoch   1 step      300 | batches    300 / 25202 | lr 0.000e+00 | ms/batch 307.5 | tok/s   13319 | loss 12.76 | ppl 349284.26
| epoch   1 step      310 | batches    310 / 25202 | lr 0.000e+00 | ms/batch 309.1 | tok/s   13251 | loss 12.76 | ppl 347821.01
| epoch   1 step      320 | batches    320 / 25202 | lr 0.000e+00 | ms/batch 308.6 | tok/s   13273 | loss 12.75 | ppl 343667.15
| epoch   1 step      330 | batches    330 / 25202 | lr 0.000e+00 | ms/batch 309.3 | tok/s   13242 | loss 12.75 | ppl 345618.50
| epoch   1 step      340 | batches    340 / 25202 | lr 0.000e+00 | ms/batch 308.2 | tok/s   13290 | loss 12.75 | ppl 344688.62
| epoch   1 step      350 | batches    350 / 25202 | lr 0.000e+00 | ms/batch 309.4 | tok/s   13240 | loss 12.76 | ppl 347124.46
| epoch   1 step      360 | batches    360 / 25202 | lr 0.000e+00 | ms/batch 309.0 | tok/s   13255 | loss 12.75 | ppl 344284.53
| epoch   1 step      370 | batches    370 / 25202 | lr 0.000e+00 | ms/batch 307.6 | tok/s   13317 | loss 12.75 | ppl 344568.98
| epoch   1 step      380 | batches    380 / 25202 | lr 0.000e+00 | ms/batch 308.7 | tok/s   13266 | loss 12.75 | ppl 344851.37
| epoch   1 step      390 | batches    390 / 25202 | lr 0.000e+00 | ms/batch 308.7 | tok/s   13268 | loss 12.76 | ppl 346668.58
| epoch   1 step      400 | batches    400 / 25202 | lr 0.000e+00 | ms/batch 308.7 | tok/s   13270 | loss 12.76 | ppl 348179.11
----------------------------------------------------------------------------------------------------
End of training
Training time: 2.13 minutes
Training throughput: 13294.34 tok/s
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
