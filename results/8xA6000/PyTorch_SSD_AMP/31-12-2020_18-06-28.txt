DLL 2020-12-31 18:06:31.131742 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.187885 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.200558 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.204763 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.234418 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.238532 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.240096 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
DLL 2020-12-31 18:06:31.255033 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 256  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : True  precision : amp 
Using seed = 7241
loading annotations into memory...
Using seed = 5202
loading annotations into memory...
Using seed = 596
Using seed = 3033
Using seed = 2553
loading annotations into memory...
loading annotations into memory...
loading annotations into memory...
Done (t=0.64s)
creating index...
Done (t=0.64s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.64s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.64s)
creating index...
Done (t=0.65s)
creating index...
index created!
Using seed = 2301
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
loading annotations into memory...
Using seed = 6029
Using seed = 4023
loading annotations into memory...
loading annotations into memory...
Done (t=0.65s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
Done (t=0.66s)
creating index...
Done (t=0.67s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
DLL 2020-12-31 18:07:22.425557 - () avg_img/sec : 225.16464331845492  med_img/sec : 225.29108441154904  min_img/sec : 222.64740490161864  max_img/sec : 227.08623230458699 
Done benchmarking. Total images: 2560	total time: 11.369	Average images/sec: 225.165	Median images/sec: 225.291
DLL 2020-12-31 18:07:22.425791 - () avg_img/sec : 225.21110049269538  med_img/sec : 225.39187336631107  min_img/sec : 222.60018566914488  max_img/sec : 227.20766038972732 
Done benchmarking. Total images: 2560	total time: 11.367	Average images/sec: 225.211	Median images/sec: 225.392
DLL 2020-12-31 18:07:22.425881 - () avg_img/sec : 225.41704707067083  med_img/sec : 225.63282009895158  min_img/sec : 222.7989832611587  max_img/sec : 227.1238913942922 
Done benchmarking. Total images: 2560	total time: 11.357	Average images/sec: 225.417	Median images/sec: 225.633
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.426400 - () total time : 38.66677284240723 
DLL 2020-12-31 18:07:22.426424 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.426465 - () avg_img/sec : 225.48944160619007  med_img/sec : 225.64146052281683  min_img/sec : 222.84471439443575  max_img/sec : 227.43813932050392 
DLL 2020-12-31 18:07:22.426562 - () total time : 38.66692495346069 
DLL 2020-12-31 18:07:22.426584 - () 
Done benchmarking. Total images: 2560	total time: 11.353	Average images/sec: 225.489	Median images/sec: 225.641
DLL 2020-12-31 18:07:22.426520 - () avg_img/sec : 225.69909531756835  med_img/sec : 225.91262557778415  min_img/sec : 223.59945998366956  max_img/sec : 227.49712149063816 
DLL 2020-12-31 18:07:22.426481 - () avg_img/sec : 225.3228376754153  med_img/sec : 225.58637523993445  min_img/sec : 223.26930381865674  max_img/sec : 227.39450064316867 
Done benchmarking. Total images: 2560	total time: 11.343	Average images/sec: 225.699	Median images/sec: 225.913
Done benchmarking. Total images: 2560	total time: 11.361	Average images/sec: 225.323	Median images/sec: 225.586
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.426691 - () total time : 38.66703534126282 
DLL 2020-12-31 18:07:22.426714 - () 
DLL 2020-12-31 18:07:22.426651 - () avg_img/sec : 225.42511595651843  med_img/sec : 225.32607427095581  min_img/sec : 223.12227775760974  max_img/sec : 227.30732162722586 
Done benchmarking. Total images: 2560	total time: 11.356	Average images/sec: 225.425	Median images/sec: 225.326
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.427522 - () total time : 38.66784405708313 
DLL 2020-12-31 18:07:22.427547 - () 
DLL 2020-12-31 18:07:22.427562 - () avg_img/sec : 225.37359859845205  med_img/sec : 225.49356144380556  min_img/sec : 222.50405514785572  max_img/sec : 227.34269544210153 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
Done benchmarking. Total images: 2560	total time: 11.359	Average images/sec: 225.374	Median images/sec: 225.494
DLL 2020-12-31 18:07:22.427742 - () total time : 38.66780686378479 
DLL 2020-12-31 18:07:22.427766 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.427998 - () total time : 38.6681866645813 
DLL 2020-12-31 18:07:22.428024 - () 
Training performance = 1804.27587890625 FPS
DLL 2020-12-31 18:07:22.428491 - (0,) time : 38.66978669166565 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.428809 - () total time : 38.66978669166565 
DLL 2020-12-31 18:07:22.428826 - () 
WARNING:root:DALI iterator does not support resetting while epoch is not finished. Ignoring...
DLL 2020-12-31 18:07:22.428909 - () total time : 38.66911792755127 
DLL 2020-12-31 18:07:22.428939 - () 
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
