:::NVLOGv0.2.2 Tacotron2_PyT 1593208420.066996336 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593208420.094249249 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593208420.112843513 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593208421.212055445 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 4, "name": ["Quadro RTX 8000", "Quadro RTX 8000", "Quadro RTX 8000", "Quadro RTX 8000"], "mem": ["48601 MiB", "48601 MiB", "48601 MiB", "48601 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593208421.216054678 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 26, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 4, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593208422.144019127 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593208455.507643223 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593208455.509130955 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208456.008928299 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208463.613039732 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020482533145695925
:::NVLOGv0.2.2 Tacotron2_PyT 1593208470.333297968 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208470.333837986 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 58076.161355246535
:::NVLOGv0.2.2 Tacotron2_PyT 1593208470.334212303 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 14.326015710830688
Batch: 1/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208470.337357759 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208472.484770298 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002192853717133403
:::NVLOGv0.2.2 Tacotron2_PyT 1593208476.489840031 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208476.490335703 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 135213.70696497682
:::NVLOGv0.2.2 Tacotron2_PyT 1593208476.490688562 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.153222322463989
Batch: 2/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208476.494020462 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593208478.521472216 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018795839278027415
:::NVLOGv0.2.2 Tacotron2_PyT 1593208482.485632658 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593208482.486165762 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 138842.50785550955
:::NVLOGv0.2.2 Tacotron2_PyT 1593208482.486528873 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.992401123046875
Batch: 3/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208482.489899635 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593208484.596459866 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020167462062090635
:::NVLOGv0.2.2 Tacotron2_PyT 1593208488.698420525 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593208488.698818207 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 133994.90992574464
:::NVLOGv0.2.2 Tacotron2_PyT 1593208488.699141979 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.209191083908081
Batch: 4/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208488.702201605 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593208490.655892372 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021039999555796385
:::NVLOGv0.2.2 Tacotron2_PyT 1593208494.726016045 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593208494.726579905 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 138089.26775542498
:::NVLOGv0.2.2 Tacotron2_PyT 1593208494.726910353 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.02508807182312
Batch: 5/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208494.729505062 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593208496.700674534 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002100194338709116
:::NVLOGv0.2.2 Tacotron2_PyT 1593208500.928795815 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593208500.929322243 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 134198.0112152753
:::NVLOGv0.2.2 Tacotron2_PyT 1593208500.929651976 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.199793815612793
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593208501.097877979 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208501.098308325 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 109498.30119941471
:::NVLOGv0.2.2 Tacotron2_PyT 1593208501.098619699 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 123069.0941786963
:::NVLOGv0.2.2 Tacotron2_PyT 1593208501.098921776 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002056938576667259
:::NVLOGv0.2.2 Tacotron2_PyT 1593208501.099221230 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 45.58974838256836
:::NVLOGv0.2.2 Tacotron2_PyT 1593208501.099508762 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593208505.425257921 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0018019031267613173
:::NVLOGv0.2.2 Tacotron2_PyT 1593208505.426493645 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208513.641294479 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208513.893533945 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208516.246210337 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024825576692819595
:::NVLOGv0.2.2 Tacotron2_PyT 1593208520.437175989 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593208520.437704086 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 127091.8817180413
:::NVLOGv0.2.2 Tacotron2_PyT 1593208520.438042164 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.546444892883301
Batch: 1/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208520.441441059 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208522.397881746 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022427786607295275
:::NVLOGv0.2.2 Tacotron2_PyT 1593208526.457011700 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208526.457438469 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 138293.5144581094
:::NVLOGv0.2.2 Tacotron2_PyT 1593208526.457771540 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.0161895751953125
Batch: 2/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208526.460867167 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593208528.581216574 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002051363466307521
:::NVLOGv0.2.2 Tacotron2_PyT 1593208532.808244467 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593208532.808667660 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 131064.37952000086
:::NVLOGv0.2.2 Tacotron2_PyT 1593208532.809000254 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.348025321960449
Batch: 3/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208532.812217712 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593208534.836352587 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022509589325636625
:::NVLOGv0.2.2 Tacotron2_PyT 1593208538.965547800 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593208538.966376305 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 135189.29708100273
:::NVLOGv0.2.2 Tacotron2_PyT 1593208538.966914654 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.1543333530426025
Batch: 4/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208538.970401049 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593208540.947846174 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0026025152765214443
:::NVLOGv0.2.2 Tacotron2_PyT 1593208545.111991405 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593208545.112524033 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 135453.33330642118
:::NVLOGv0.2.2 Tacotron2_PyT 1593208545.112863779 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.142336845397949
Batch: 5/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208545.115695000 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593208547.134270668 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002492276020348072
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.268674612 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.269183636 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 135207.85512306623
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.269522429 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.153488636016846
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.341112375 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.342605591 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 132410.06494459393
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.343956232 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 133716.71020110694
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.345293522 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002353741670958698
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.346653461 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 37.701061487197876
:::NVLOGv0.2.2 Tacotron2_PyT 1593208551.347970963 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208553.362786531 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0019574062898755074
:::NVLOGv0.2.2 Tacotron2_PyT 1593208553.363587141 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593208553.364585876 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 131.2199366092682
:::NVLOGv0.2.2 Tacotron2_PyT 1593208553.364916086 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 131.2199366092682
:::NVLOGv0.2.2 Tacotron2_PyT 1593208553.365260839 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 133.3775291442871
:::NVLOGv0.2.2 Tacotron2_PyT 1593208553.365559816 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
