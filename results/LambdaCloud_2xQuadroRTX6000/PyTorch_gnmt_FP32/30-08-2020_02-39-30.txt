1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Quadro RTX 6000
GPU 1: Quadro RTX 6000

Nvidia driver version: 450.57
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=288, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: Quadro RTX 6000
GPU 1: Quadro RTX 6000

Nvidia driver version: 450.57
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=288, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
1: Worker 1 is using worker seed: 364522461
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
1: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 125
0: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
0: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Number of parameters: 159593523
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 339
0: Scheduler decay interval: 42
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 339
1: Scheduler decay interval: 42
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: Starting epoch 0
1: Executing preallocation
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
1: TRAIN [0][0/255]	Time 1.019 (1.019)	Data 3.47e-01 (3.47e-01)	Tok/s 7728 (7728)	Loss/tok 10.6137 (10.6137)	LR 2.047e-05
0: TRAIN [0][0/255]	Time 1.012 (1.012)	Data 3.98e-01 (3.98e-01)	Tok/s 7948 (7948)	Loss/tok 10.6113 (10.6113)	LR 2.047e-05
1: TRAIN [0][10/255]	Time 0.619 (0.829)	Data 2.09e-04 (3.17e-02)	Tok/s 12882 (14838)	Loss/tok 9.4478 (10.1071)	LR 2.576e-05
0: TRAIN [0][10/255]	Time 0.619 (0.828)	Data 2.01e-04 (3.64e-02)	Tok/s 12597 (14757)	Loss/tok 9.4700 (10.1090)	LR 2.576e-05
1: TRAIN [0][20/255]	Time 0.801 (0.903)	Data 2.06e-04 (1.67e-02)	Tok/s 16120 (15844)	Loss/tok 9.1773 (9.7406)	LR 3.244e-05
0: TRAIN [0][20/255]	Time 0.802 (0.902)	Data 2.16e-04 (1.92e-02)	Tok/s 16266 (15780)	Loss/tok 9.2402 (9.7366)	LR 3.244e-05
1: TRAIN [0][30/255]	Time 0.490 (0.865)	Data 2.10e-04 (1.14e-02)	Tok/s 7806 (15449)	Loss/tok 8.7360 (9.5362)	LR 4.083e-05
0: TRAIN [0][30/255]	Time 0.489 (0.865)	Data 2.03e-04 (1.30e-02)	Tok/s 7871 (15420)	Loss/tok 8.6683 (9.5352)	LR 4.083e-05
1: TRAIN [0][40/255]	Time 0.817 (0.857)	Data 2.18e-04 (8.67e-03)	Tok/s 15984 (15531)	Loss/tok 8.6765 (9.3608)	LR 5.141e-05
0: TRAIN [0][40/255]	Time 0.817 (0.857)	Data 2.19e-04 (9.92e-03)	Tok/s 15757 (15500)	Loss/tok 8.7282 (9.3625)	LR 5.141e-05
1: TRAIN [0][50/255]	Time 0.798 (0.845)	Data 2.08e-04 (7.01e-03)	Tok/s 16312 (15496)	Loss/tok 8.4301 (9.2165)	LR 6.472e-05
0: TRAIN [0][50/255]	Time 0.795 (0.845)	Data 1.91e-04 (8.01e-03)	Tok/s 16170 (15445)	Loss/tok 8.4618 (9.2163)	LR 6.472e-05
1: TRAIN [0][60/255]	Time 1.247 (0.846)	Data 1.92e-04 (5.89e-03)	Tok/s 19006 (15429)	Loss/tok 8.4427 (9.0845)	LR 8.148e-05
0: TRAIN [0][60/255]	Time 1.247 (0.846)	Data 2.00e-04 (6.73e-03)	Tok/s 18727 (15393)	Loss/tok 8.4578 (9.0844)	LR 8.148e-05
1: TRAIN [0][70/255]	Time 1.010 (0.861)	Data 2.08e-04 (5.09e-03)	Tok/s 17998 (15654)	Loss/tok 8.1357 (8.9427)	LR 1.026e-04
0: TRAIN [0][70/255]	Time 1.010 (0.861)	Data 2.14e-04 (5.81e-03)	Tok/s 17797 (15624)	Loss/tok 8.1147 (8.9362)	LR 1.026e-04
1: TRAIN [0][80/255]	Time 0.803 (0.853)	Data 2.22e-04 (4.49e-03)	Tok/s 16383 (15619)	Loss/tok 7.9153 (8.8319)	LR 1.291e-04
0: TRAIN [0][80/255]	Time 0.802 (0.853)	Data 2.17e-04 (5.12e-03)	Tok/s 16102 (15593)	Loss/tok 7.8679 (8.8261)	LR 1.291e-04
1: TRAIN [0][90/255]	Time 0.614 (0.854)	Data 2.12e-04 (4.10e-03)	Tok/s 12693 (15686)	Loss/tok 7.6907 (8.7241)	LR 1.626e-04
0: TRAIN [0][90/255]	Time 0.614 (0.854)	Data 2.01e-04 (4.66e-03)	Tok/s 12902 (15671)	Loss/tok 7.6460 (8.7166)	LR 1.626e-04
1: TRAIN [0][100/255]	Time 0.799 (0.856)	Data 2.14e-04 (3.71e-03)	Tok/s 16042 (15756)	Loss/tok 7.7173 (8.6292)	LR 2.047e-04
0: TRAIN [0][100/255]	Time 0.799 (0.855)	Data 2.36e-04 (4.22e-03)	Tok/s 16280 (15740)	Loss/tok 7.6554 (8.6242)	LR 2.047e-04
1: TRAIN [0][110/255]	Time 0.793 (0.858)	Data 2.31e-04 (3.40e-03)	Tok/s 16414 (15770)	Loss/tok 7.5903 (8.5512)	LR 2.576e-04
0: TRAIN [0][110/255]	Time 0.793 (0.858)	Data 2.09e-04 (3.86e-03)	Tok/s 16457 (15752)	Loss/tok 7.6861 (8.5471)	LR 2.576e-04
1: TRAIN [0][120/255]	Time 1.002 (0.861)	Data 2.47e-04 (3.13e-03)	Tok/s 18268 (15787)	Loss/tok 7.7976 (8.4832)	LR 3.244e-04
0: TRAIN [0][120/255]	Time 1.002 (0.861)	Data 2.07e-04 (3.55e-03)	Tok/s 18247 (15768)	Loss/tok 7.7718 (8.4805)	LR 3.244e-04
1: TRAIN [0][130/255]	Time 0.622 (0.859)	Data 2.04e-04 (2.91e-03)	Tok/s 12807 (15769)	Loss/tok 7.3590 (8.4252)	LR 4.083e-04
0: TRAIN [0][130/255]	Time 0.622 (0.859)	Data 2.01e-04 (3.30e-03)	Tok/s 12784 (15753)	Loss/tok 7.3374 (8.4228)	LR 4.083e-04
1: TRAIN [0][140/255]	Time 1.018 (0.858)	Data 1.96e-04 (2.72e-03)	Tok/s 17784 (15744)	Loss/tok 7.7526 (8.3787)	LR 5.141e-04
0: TRAIN [0][140/255]	Time 1.020 (0.858)	Data 2.54e-04 (3.08e-03)	Tok/s 17593 (15731)	Loss/tok 7.7521 (8.3764)	LR 5.141e-04
1: TRAIN [0][150/255]	Time 0.998 (0.858)	Data 2.14e-04 (2.55e-03)	Tok/s 18190 (15753)	Loss/tok 7.6956 (8.3286)	LR 6.472e-04
0: TRAIN [0][150/255]	Time 0.998 (0.858)	Data 1.91e-04 (2.89e-03)	Tok/s 18132 (15745)	Loss/tok 7.7143 (8.3267)	LR 6.472e-04
1: TRAIN [0][160/255]	Time 1.010 (0.864)	Data 2.28e-04 (2.41e-03)	Tok/s 17983 (15847)	Loss/tok 7.7017 (8.2812)	LR 8.148e-04
0: TRAIN [0][160/255]	Time 1.010 (0.864)	Data 2.13e-04 (2.72e-03)	Tok/s 17959 (15839)	Loss/tok 7.6322 (8.2794)	LR 8.148e-04
1: TRAIN [0][170/255]	Time 1.010 (0.861)	Data 1.95e-04 (2.28e-03)	Tok/s 17983 (15831)	Loss/tok 7.6453 (8.2392)	LR 1.026e-03
0: TRAIN [0][170/255]	Time 1.010 (0.861)	Data 1.94e-04 (2.58e-03)	Tok/s 17971 (15827)	Loss/tok 7.6496 (8.2370)	LR 1.026e-03
1: TRAIN [0][180/255]	Time 1.021 (0.859)	Data 2.17e-04 (2.16e-03)	Tok/s 17912 (15831)	Loss/tok 7.9710 (8.2034)	LR 1.291e-03
0: TRAIN [0][180/255]	Time 1.021 (0.859)	Data 2.07e-04 (2.45e-03)	Tok/s 17702 (15825)	Loss/tok 7.9533 (8.2025)	LR 1.291e-03
1: TRAIN [0][190/255]	Time 0.999 (0.854)	Data 1.95e-04 (2.06e-03)	Tok/s 18174 (15743)	Loss/tok 7.6008 (8.1738)	LR 1.626e-03
0: TRAIN [0][190/255]	Time 0.999 (0.854)	Data 2.11e-04 (2.33e-03)	Tok/s 18395 (15739)	Loss/tok 7.6233 (8.1731)	LR 1.626e-03
1: TRAIN [0][200/255]	Time 1.000 (0.854)	Data 1.93e-04 (1.97e-03)	Tok/s 18233 (15748)	Loss/tok 7.4934 (8.1356)	LR 2.000e-03
0: TRAIN [0][200/255]	Time 1.000 (0.854)	Data 2.49e-04 (2.22e-03)	Tok/s 18044 (15744)	Loss/tok 7.4617 (8.1344)	LR 2.000e-03
1: TRAIN [0][210/255]	Time 0.600 (0.854)	Data 2.08e-04 (1.88e-03)	Tok/s 13038 (15785)	Loss/tok 6.8033 (8.0917)	LR 2.000e-03
0: TRAIN [0][210/255]	Time 0.600 (0.854)	Data 2.19e-04 (2.13e-03)	Tok/s 12849 (15778)	Loss/tok 6.7057 (8.0906)	LR 2.000e-03
1: TRAIN [0][220/255]	Time 0.599 (0.853)	Data 2.17e-04 (1.81e-03)	Tok/s 13060 (15775)	Loss/tok 6.7217 (8.0521)	LR 2.000e-03
0: TRAIN [0][220/255]	Time 0.599 (0.853)	Data 2.01e-04 (2.06e-03)	Tok/s 13082 (15772)	Loss/tok 6.6893 (8.0509)	LR 2.000e-03
1: TRAIN [0][230/255]	Time 0.600 (0.855)	Data 2.21e-04 (1.74e-03)	Tok/s 13039 (15788)	Loss/tok 6.6714 (8.0070)	LR 2.000e-03
0: TRAIN [0][230/255]	Time 0.600 (0.855)	Data 2.00e-04 (1.98e-03)	Tok/s 12943 (15786)	Loss/tok 6.5507 (8.0046)	LR 2.000e-03
1: TRAIN [0][240/255]	Time 0.788 (0.855)	Data 2.01e-04 (1.68e-03)	Tok/s 16542 (15791)	Loss/tok 6.7916 (7.9600)	LR 2.000e-03
0: TRAIN [0][240/255]	Time 0.788 (0.855)	Data 2.65e-04 (1.91e-03)	Tok/s 16308 (15792)	Loss/tok 6.7365 (7.9568)	LR 2.000e-03
1: TRAIN [0][250/255]	Time 0.597 (0.851)	Data 2.15e-04 (1.62e-03)	Tok/s 13256 (15766)	Loss/tok 6.2967 (7.9162)	LR 2.000e-03
0: TRAIN [0][250/255]	Time 0.597 (0.851)	Data 2.13e-04 (1.84e-03)	Tok/s 12941 (15765)	Loss/tok 6.4273 (7.9133)	LR 2.000e-03
0: Running validation on dev set
1: Running validation on dev set
0: Executing preallocation
1: Executing preallocation
1: VALIDATION [0][0/40]	Time 0.149 (0.149)	Data 3.75e-03 (3.75e-03)	Tok/s 57820 (57820)	Loss/tok 7.5920 (7.5920)
0: VALIDATION [0][0/40]	Time 0.232 (0.232)	Data 3.57e-03 (3.57e-03)	Tok/s 45113 (45113)	Loss/tok 7.6613 (7.6613)
1: VALIDATION [0][10/40]	Time 0.075 (0.101)	Data 3.45e-03 (3.53e-03)	Tok/s 60452 (59441)	Loss/tok 7.4950 (7.4954)
0: VALIDATION [0][10/40]	Time 0.079 (0.111)	Data 4.00e-03 (3.46e-03)	Tok/s 58750 (58308)	Loss/tok 7.3048 (7.5116)
1: VALIDATION [0][20/40]	Time 0.055 (0.083)	Data 3.36e-03 (3.49e-03)	Tok/s 57810 (58982)	Loss/tok 7.1870 (7.4291)
0: VALIDATION [0][20/40]	Time 0.055 (0.089)	Data 3.34e-03 (3.40e-03)	Tok/s 58452 (58363)	Loss/tok 7.2603 (7.4307)
1: VALIDATION [0][30/40]	Time 0.038 (0.071)	Data 3.36e-03 (3.46e-03)	Tok/s 55110 (58054)	Loss/tok 7.1068 (7.3805)
0: VALIDATION [0][30/40]	Time 0.039 (0.076)	Data 3.31e-03 (3.36e-03)	Tok/s 54375 (57500)	Loss/tok 7.1683 (7.3858)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
0: TEST [0][9/12]	Time 2.0950 (2.9004)	Decoder iters 149.0 (149.0)	Tok/s 5819 (7108)
1: TEST [0][9/12]	Time 2.0940 (2.9006)	Decoder iters 149.0 (149.0)	Tok/s 5112 (6665)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.8944	Validation Loss: 7.3373	Test BLEU: 0.10
0: Performance: Epoch: 0	Training: 31550 Tok/s	Validation: 111812 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/255]	Time 1.080 (1.080)	Data 2.36e-01 (2.36e-01)	Tok/s 12053 (12053)	Loss/tok 6.5688 (6.5688)	LR 2.000e-03
0: TRAIN [1][0/255]	Time 1.068 (1.068)	Data 2.85e-01 (2.85e-01)	Tok/s 12200 (12200)	Loss/tok 6.5567 (6.5567)	LR 2.000e-03
1: TRAIN [1][10/255]	Time 0.614 (0.712)	Data 2.10e-04 (2.16e-02)	Tok/s 12718 (13508)	Loss/tok 6.1029 (6.4111)	LR 2.000e-03
0: TRAIN [1][10/255]	Time 0.614 (0.711)	Data 2.31e-04 (2.61e-02)	Tok/s 12907 (13575)	Loss/tok 6.2132 (6.4450)	LR 2.000e-03
1: TRAIN [1][20/255]	Time 0.798 (0.768)	Data 2.24e-04 (1.14e-02)	Tok/s 16255 (14633)	Loss/tok 6.3383 (6.4630)	LR 2.000e-03
0: TRAIN [1][20/255]	Time 0.798 (0.767)	Data 2.28e-04 (1.38e-02)	Tok/s 16356 (14650)	Loss/tok 6.3612 (6.4780)	LR 2.000e-03
1: TRAIN [1][30/255]	Time 1.003 (0.808)	Data 2.35e-04 (7.81e-03)	Tok/s 18099 (15234)	Loss/tok 6.4051 (6.4390)	LR 2.000e-03
0: TRAIN [1][30/255]	Time 1.004 (0.807)	Data 2.20e-04 (9.98e-03)	Tok/s 18077 (15245)	Loss/tok 6.4224 (6.4554)	LR 2.000e-03
1: TRAIN [1][40/255]	Time 1.239 (0.855)	Data 2.05e-04 (5.96e-03)	Tok/s 18961 (15665)	Loss/tok 6.4399 (6.4240)	LR 2.000e-03
0: TRAIN [1][40/255]	Time 1.239 (0.854)	Data 1.98e-04 (7.60e-03)	Tok/s 18970 (15669)	Loss/tok 6.5234 (6.4407)	LR 2.000e-03
1: TRAIN [1][50/255]	Time 0.789 (0.845)	Data 2.21e-04 (4.84e-03)	Tok/s 16579 (15654)	Loss/tok 5.9911 (6.3691)	LR 2.000e-03
0: TRAIN [1][50/255]	Time 0.789 (0.845)	Data 2.21e-04 (6.15e-03)	Tok/s 16647 (15665)	Loss/tok 6.1375 (6.3920)	LR 2.000e-03
1: TRAIN [1][60/255]	Time 0.787 (0.825)	Data 2.10e-04 (4.08e-03)	Tok/s 16450 (15478)	Loss/tok 6.0240 (6.3127)	LR 2.000e-03
0: TRAIN [1][60/255]	Time 0.787 (0.825)	Data 2.15e-04 (5.18e-03)	Tok/s 16597 (15497)	Loss/tok 5.9943 (6.3277)	LR 2.000e-03
1: TRAIN [1][70/255]	Time 0.819 (0.828)	Data 1.95e-04 (3.54e-03)	Tok/s 15743 (15474)	Loss/tok 5.9060 (6.2650)	LR 2.000e-03
0: TRAIN [1][70/255]	Time 0.819 (0.828)	Data 2.06e-04 (4.48e-03)	Tok/s 15858 (15505)	Loss/tok 5.8570 (6.2800)	LR 2.000e-03
1: TRAIN [1][80/255]	Time 0.815 (0.837)	Data 2.23e-04 (3.12e-03)	Tok/s 15928 (15555)	Loss/tok 5.7447 (6.2228)	LR 2.000e-03
0: TRAIN [1][80/255]	Time 0.815 (0.837)	Data 2.28e-04 (3.95e-03)	Tok/s 15949 (15593)	Loss/tok 5.7800 (6.2368)	LR 2.000e-03
1: TRAIN [1][90/255]	Time 0.602 (0.841)	Data 2.57e-04 (2.81e-03)	Tok/s 13059 (15627)	Loss/tok 5.3092 (6.1754)	LR 1.000e-03
0: TRAIN [1][90/255]	Time 0.602 (0.841)	Data 2.14e-04 (3.55e-03)	Tok/s 12973 (15665)	Loss/tok 5.3659 (6.1895)	LR 1.000e-03
1: TRAIN [1][100/255]	Time 0.594 (0.835)	Data 2.08e-04 (2.55e-03)	Tok/s 13412 (15587)	Loss/tok 5.2825 (6.1279)	LR 1.000e-03
0: TRAIN [1][100/255]	Time 0.594 (0.835)	Data 2.08e-04 (3.22e-03)	Tok/s 13411 (15620)	Loss/tok 5.2081 (6.1406)	LR 1.000e-03
1: TRAIN [1][110/255]	Time 0.602 (0.839)	Data 2.06e-04 (2.34e-03)	Tok/s 13147 (15609)	Loss/tok 5.1434 (6.0866)	LR 1.000e-03
0: TRAIN [1][110/255]	Time 0.602 (0.839)	Data 2.11e-04 (2.95e-03)	Tok/s 13213 (15633)	Loss/tok 5.0523 (6.0967)	LR 1.000e-03
1: TRAIN [1][120/255]	Time 0.997 (0.834)	Data 2.28e-04 (2.17e-03)	Tok/s 18376 (15606)	Loss/tok 5.6471 (6.0386)	LR 1.000e-03
0: TRAIN [1][120/255]	Time 0.997 (0.834)	Data 2.09e-04 (2.72e-03)	Tok/s 18015 (15625)	Loss/tok 5.6450 (6.0488)	LR 1.000e-03
1: TRAIN [1][130/255]	Time 1.039 (0.844)	Data 2.07e-04 (2.02e-03)	Tok/s 17578 (15680)	Loss/tok 5.5118 (6.0009)	LR 5.000e-04
0: TRAIN [1][130/255]	Time 1.038 (0.844)	Data 2.16e-04 (2.53e-03)	Tok/s 17698 (15701)	Loss/tok 5.5874 (6.0123)	LR 5.000e-04
1: TRAIN [1][140/255]	Time 0.791 (0.842)	Data 1.95e-04 (1.89e-03)	Tok/s 16144 (15653)	Loss/tok 5.4020 (5.9607)	LR 5.000e-04
0: TRAIN [1][140/255]	Time 0.791 (0.842)	Data 1.95e-04 (2.37e-03)	Tok/s 16570 (15678)	Loss/tok 5.3638 (5.9718)	LR 5.000e-04
1: TRAIN [1][150/255]	Time 1.006 (0.848)	Data 2.20e-04 (1.78e-03)	Tok/s 17969 (15761)	Loss/tok 5.4410 (5.9213)	LR 5.000e-04
0: TRAIN [1][150/255]	Time 1.006 (0.847)	Data 2.20e-04 (2.23e-03)	Tok/s 17884 (15782)	Loss/tok 5.4755 (5.9312)	LR 5.000e-04
1: TRAIN [1][160/255]	Time 0.997 (0.846)	Data 2.13e-04 (1.68e-03)	Tok/s 18275 (15725)	Loss/tok 5.5363 (5.8877)	LR 5.000e-04
0: TRAIN [1][160/255]	Time 0.997 (0.846)	Data 2.06e-04 (2.10e-03)	Tok/s 18049 (15742)	Loss/tok 5.4639 (5.8974)	LR 5.000e-04
1: TRAIN [1][170/255]	Time 1.007 (0.842)	Data 2.22e-04 (1.60e-03)	Tok/s 17900 (15659)	Loss/tok 5.4074 (5.8551)	LR 2.500e-04
0: TRAIN [1][170/255]	Time 1.007 (0.842)	Data 2.13e-04 (1.99e-03)	Tok/s 18041 (15676)	Loss/tok 5.4643 (5.8658)	LR 2.500e-04
1: TRAIN [1][180/255]	Time 0.790 (0.838)	Data 2.24e-04 (1.52e-03)	Tok/s 16460 (15664)	Loss/tok 5.1960 (5.8208)	LR 2.500e-04
0: TRAIN [1][180/255]	Time 0.790 (0.838)	Data 2.25e-04 (1.89e-03)	Tok/s 16537 (15683)	Loss/tok 5.1620 (5.8296)	LR 2.500e-04
1: TRAIN [1][190/255]	Time 0.809 (0.842)	Data 2.60e-04 (1.45e-03)	Tok/s 16116 (15727)	Loss/tok 5.0968 (5.7904)	LR 2.500e-04
0: TRAIN [1][190/255]	Time 0.809 (0.842)	Data 2.07e-04 (1.80e-03)	Tok/s 15923 (15747)	Loss/tok 5.0577 (5.7976)	LR 2.500e-04
1: TRAIN [1][200/255]	Time 0.797 (0.843)	Data 2.07e-04 (1.39e-03)	Tok/s 16406 (15762)	Loss/tok 5.1233 (5.7627)	LR 2.500e-04
0: TRAIN [1][200/255]	Time 0.797 (0.843)	Data 2.07e-04 (1.72e-03)	Tok/s 16164 (15782)	Loss/tok 5.0712 (5.7681)	LR 2.500e-04
1: TRAIN [1][210/255]	Time 1.004 (0.843)	Data 1.99e-04 (1.34e-03)	Tok/s 17911 (15793)	Loss/tok 5.2953 (5.7361)	LR 1.250e-04
0: TRAIN [1][210/255]	Time 1.004 (0.843)	Data 2.00e-04 (1.66e-03)	Tok/s 18149 (15810)	Loss/tok 5.4098 (5.7393)	LR 1.250e-04
1: TRAIN [1][220/255]	Time 1.000 (0.838)	Data 2.28e-04 (1.29e-03)	Tok/s 18236 (15702)	Loss/tok 5.3506 (5.7128)	LR 1.250e-04
0: TRAIN [1][220/255]	Time 1.000 (0.838)	Data 2.21e-04 (1.59e-03)	Tok/s 18099 (15716)	Loss/tok 5.2799 (5.7160)	LR 1.250e-04
1: TRAIN [1][230/255]	Time 0.994 (0.838)	Data 1.95e-04 (1.24e-03)	Tok/s 18119 (15693)	Loss/tok 5.2946 (5.6903)	LR 1.250e-04
0: TRAIN [1][230/255]	Time 0.994 (0.838)	Data 2.10e-04 (1.53e-03)	Tok/s 18215 (15709)	Loss/tok 5.2567 (5.6924)	LR 1.250e-04
1: TRAIN [1][240/255]	Time 0.997 (0.843)	Data 2.23e-04 (1.20e-03)	Tok/s 18245 (15743)	Loss/tok 5.3321 (5.6712)	LR 1.250e-04
0: TRAIN [1][240/255]	Time 0.998 (0.843)	Data 2.28e-04 (1.48e-03)	Tok/s 17979 (15757)	Loss/tok 5.2626 (5.6721)	LR 1.250e-04
1: TRAIN [1][250/255]	Time 1.238 (0.848)	Data 1.89e-04 (1.16e-03)	Tok/s 18934 (15784)	Loss/tok 5.4929 (5.6533)	LR 1.250e-04
0: TRAIN [1][250/255]	Time 1.239 (0.847)	Data 1.89e-04 (1.49e-03)	Tok/s 18953 (15798)	Loss/tok 5.4658 (5.6540)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
0: Executing preallocation
1: Executing preallocation
1: VALIDATION [1][0/40]	Time 0.149 (0.149)	Data 3.87e-03 (3.87e-03)	Tok/s 57572 (57572)	Loss/tok 6.5140 (6.5140)
0: VALIDATION [1][0/40]	Time 0.233 (0.233)	Data 3.72e-03 (3.72e-03)	Tok/s 45011 (45011)	Loss/tok 6.6049 (6.6049)
1: VALIDATION [1][10/40]	Time 0.075 (0.107)	Data 3.51e-03 (7.72e-03)	Tok/s 60629 (56937)	Loss/tok 6.2228 (6.3134)
0: VALIDATION [1][10/40]	Time 0.078 (0.117)	Data 3.38e-03 (8.05e-03)	Tok/s 59518 (55872)	Loss/tok 5.9858 (6.3282)
1: VALIDATION [1][20/40]	Time 0.056 (0.087)	Data 3.45e-03 (5.69e-03)	Tok/s 57143 (57638)	Loss/tok 5.8917 (6.2069)
0: VALIDATION [1][20/40]	Time 0.055 (0.093)	Data 3.33e-03 (5.84e-03)	Tok/s 58517 (57079)	Loss/tok 5.9713 (6.2081)
1: VALIDATION [1][30/40]	Time 0.038 (0.074)	Data 3.35e-03 (4.95e-03)	Tok/s 55614 (57130)	Loss/tok 5.7058 (6.1337)
0: VALIDATION [1][30/40]	Time 0.039 (0.078)	Data 3.26e-03 (5.03e-03)	Tok/s 54298 (56561)	Loss/tok 5.8402 (6.1438)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
0: TEST [1][9/12]	Time 0.8040 (1.6968)	Decoder iters 41.0 (137.1)	Tok/s 5693 (6481)
1: TEST [1][9/12]	Time 0.8035 (1.6968)	Decoder iters 149.0 (129.8)	Tok/s 5339 (5784)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 5.6473	Validation Loss: 6.0799	Test BLEU: 1.62
0: Performance: Epoch: 1	Training: 31603 Tok/s	Validation: 109869 Tok/s
0: Finished epoch 1
1: Total training time 542 s
0: Total training time 542 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 288|                      1.62|                      31576.2|                         9.031|
DONE!
