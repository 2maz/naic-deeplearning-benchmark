:::NVLOGv0.2.2 Tacotron2_PyT 1586634982.700850010 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1586634982.709229231 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 32, "name": "Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1586634982.715952158 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "240G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1586634984.477924585 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.64", "num": 4, "name": ["Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB"], "mem": ["16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1586634984.480837107 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 52, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 4, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1586634985.761122704 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1586634995.079189062 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1586634995.080446243 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/3 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586634996.657301664 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635003.260599136 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.18251037597656
:::NVLOGv0.2.2 Tacotron2_PyT 1586635005.290115595 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635005.290643454 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 13313.111587440626
:::NVLOGv0.2.2 Tacotron2_PyT 1586635005.291095734 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.63396954536438
Batch: 1/3 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635005.339106321 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635006.699504137 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.79275894165039
:::NVLOGv0.2.2 Tacotron2_PyT 1586635008.691787004 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635008.692812681 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 35551.01705464299
:::NVLOGv0.2.2 Tacotron2_PyT 1586635008.694490194 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.3532936573028564
Batch: 2/3 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635008.700241804 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586635010.078276396 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.267539978027344
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.086376190 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.088285446 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 34783.23327798117
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.089665651 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.3866891860961914
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.161149740 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.161724567 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 20604.688770470744
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.162257195 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 27882.453973354928
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.162739992 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.080936431884766
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.163213015 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 17.081451892852783
:::NVLOGv0.2.2 Tacotron2_PyT 1586635012.163688421 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586635014.197872877 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 47.121341705322266
:::NVLOGv0.2.2 Tacotron2_PyT 1586635014.199924231 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635014.883190155 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/3 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635016.529265642 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635018.264323950 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.47166442871094
:::NVLOGv0.2.2 Tacotron2_PyT 1586635020.292943716 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586635020.294240475 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 30747.685539437516
:::NVLOGv0.2.2 Tacotron2_PyT 1586635020.296636820 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.76486873626709
Batch: 1/3 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635020.304027557 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635021.633610010 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.90375900268555
:::NVLOGv0.2.2 Tacotron2_PyT 1586635023.663341999 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635023.666038275 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 35123.57365746927
:::NVLOGv0.2.2 Tacotron2_PyT 1586635023.667454243 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.359880208969116
Batch: 2/3 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635023.673790693 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586635024.966527462 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.50807571411133
:::NVLOGv0.2.2 Tacotron2_PyT 1586635026.987120867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586635026.988380909 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 35658.37094069264
:::NVLOGv0.2.2 Tacotron2_PyT 1586635026.990370274 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.313864231109619
:::NVLOGv0.2.2 Tacotron2_PyT 1586635027.070453644 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635027.072326183 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 28875.658239975062
:::NVLOGv0.2.2 Tacotron2_PyT 1586635027.073266745 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 33843.21004586647
:::NVLOGv0.2.2 Tacotron2_PyT 1586635027.074650288 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 46.96116638183594
:::NVLOGv0.2.2 Tacotron2_PyT 1586635027.075661421 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 12.188085794448853
:::NVLOGv0.2.2 Tacotron2_PyT 1586635027.076191902 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635029.027276516 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 47.10116195678711
:::NVLOGv0.2.2 Tacotron2_PyT 1586635029.029231787 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586635029.030430794 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 43.26877546310425
:::NVLOGv0.2.2 Tacotron2_PyT 1586635029.030915737 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 43.26877546310425
:::NVLOGv0.2.2 Tacotron2_PyT 1586635029.031442404 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 46.44314432144165
:::NVLOGv0.2.2 Tacotron2_PyT 1586635029.031939983 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
