0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: TITAN RTX
GPU 1: GeForce GTX 1080 Ti

Nvidia driver version: 440.44
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=1, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=128, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=96, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=64, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 1026
0: Scheduler decay interval: 128
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1541]	Time 0.198 (0.198)	Data 1.10e-01 (1.10e-01)	Tok/s 6497 (6497)	Loss/tok 10.4501 (10.4501)	LR 2.047e-05
0: TRAIN [0][10/1541]	Time 0.166 (0.220)	Data 8.39e-05 (1.00e-02)	Tok/s 15552 (15877)	Loss/tok 9.5590 (10.1765)	LR 2.576e-05
0: TRAIN [0][20/1541]	Time 0.169 (0.231)	Data 8.27e-05 (5.29e-03)	Tok/s 15474 (16572)	Loss/tok 9.1739 (9.8423)	LR 3.244e-05
0: TRAIN [0][30/1541]	Time 0.160 (0.241)	Data 1.43e-04 (3.62e-03)	Tok/s 16730 (16907)	Loss/tok 8.8594 (9.5889)	LR 4.083e-05
0: TRAIN [0][40/1541]	Time 0.397 (0.264)	Data 1.53e-04 (2.77e-03)	Tok/s 19710 (17253)	Loss/tok 8.8518 (9.3770)	LR 5.141e-05
0: TRAIN [0][50/1541]	Time 0.246 (0.259)	Data 1.01e-04 (2.26e-03)	Tok/s 17850 (17263)	Loss/tok 8.5478 (9.2324)	LR 6.472e-05
0: TRAIN [0][60/1541]	Time 0.100 (0.257)	Data 8.20e-05 (1.91e-03)	Tok/s 13129 (17236)	Loss/tok 8.2182 (9.1104)	LR 8.148e-05
0: TRAIN [0][70/1541]	Time 0.240 (0.257)	Data 7.58e-05 (1.65e-03)	Tok/s 18114 (17232)	Loss/tok 8.1367 (8.9956)	LR 1.026e-04
0: TRAIN [0][80/1541]	Time 0.237 (0.260)	Data 1.43e-04 (1.46e-03)	Tok/s 18194 (17354)	Loss/tok 7.9679 (8.8830)	LR 1.291e-04
0: TRAIN [0][90/1541]	Time 0.244 (0.261)	Data 1.44e-04 (1.31e-03)	Tok/s 17619 (17372)	Loss/tok 7.8392 (8.7819)	LR 1.626e-04
0: TRAIN [0][100/1541]	Time 0.174 (0.258)	Data 1.39e-04 (1.20e-03)	Tok/s 15205 (17314)	Loss/tok 7.8609 (8.7004)	LR 2.047e-04
0: TRAIN [0][110/1541]	Time 0.334 (0.262)	Data 1.00e-04 (1.10e-03)	Tok/s 18039 (17281)	Loss/tok 7.9686 (8.6398)	LR 2.576e-04
0: TRAIN [0][120/1541]	Time 0.252 (0.265)	Data 9.32e-05 (1.02e-03)	Tok/s 17365 (17276)	Loss/tok 7.6795 (8.5670)	LR 3.244e-04
0: TRAIN [0][130/1541]	Time 0.170 (0.267)	Data 9.32e-05 (9.47e-04)	Tok/s 15257 (17298)	Loss/tok 7.4266 (8.5140)	LR 4.083e-04
0: TRAIN [0][140/1541]	Time 0.317 (0.268)	Data 7.82e-05 (8.87e-04)	Tok/s 19114 (17325)	Loss/tok 7.8427 (8.4584)	LR 5.141e-04
0: TRAIN [0][150/1541]	Time 0.235 (0.265)	Data 9.20e-05 (8.35e-04)	Tok/s 18215 (17301)	Loss/tok 7.6969 (8.4152)	LR 6.472e-04
0: TRAIN [0][160/1541]	Time 0.314 (0.264)	Data 1.51e-04 (7.89e-04)	Tok/s 19368 (17301)	Loss/tok 7.7713 (8.3731)	LR 8.148e-04
0: TRAIN [0][170/1541]	Time 0.234 (0.264)	Data 7.89e-05 (7.48e-04)	Tok/s 18544 (17324)	Loss/tok 7.6357 (8.3371)	LR 1.026e-03
0: TRAIN [0][180/1541]	Time 0.397 (0.264)	Data 9.04e-05 (7.12e-04)	Tok/s 19809 (17331)	Loss/tok 7.9550 (8.3040)	LR 1.291e-03
0: TRAIN [0][190/1541]	Time 0.390 (0.268)	Data 8.99e-05 (6.80e-04)	Tok/s 20035 (17342)	Loss/tok 8.2086 (8.2726)	LR 1.626e-03
0: TRAIN [0][200/1541]	Time 0.234 (0.267)	Data 8.56e-05 (6.51e-04)	Tok/s 18461 (17318)	Loss/tok 7.5346 (8.2468)	LR 2.000e-03
0: TRAIN [0][210/1541]	Time 0.165 (0.265)	Data 1.40e-04 (6.26e-04)	Tok/s 16163 (17288)	Loss/tok 7.2572 (8.2167)	LR 2.000e-03
0: TRAIN [0][220/1541]	Time 0.315 (0.265)	Data 1.03e-04 (6.03e-04)	Tok/s 18801 (17264)	Loss/tok 7.5114 (8.1871)	LR 2.000e-03
0: TRAIN [0][230/1541]	Time 0.235 (0.265)	Data 9.01e-05 (5.82e-04)	Tok/s 18545 (17252)	Loss/tok 7.2427 (8.1547)	LR 2.000e-03
0: TRAIN [0][240/1541]	Time 0.250 (0.265)	Data 1.32e-04 (5.63e-04)	Tok/s 17137 (17238)	Loss/tok 7.2514 (8.1226)	LR 2.000e-03
0: TRAIN [0][250/1541]	Time 0.306 (0.266)	Data 1.23e-04 (5.46e-04)	Tok/s 19887 (17284)	Loss/tok 7.2621 (8.0872)	LR 2.000e-03
0: TRAIN [0][260/1541]	Time 0.393 (0.265)	Data 1.59e-04 (5.30e-04)	Tok/s 19998 (17265)	Loss/tok 7.4768 (8.0568)	LR 2.000e-03
0: TRAIN [0][270/1541]	Time 0.431 (0.265)	Data 9.25e-05 (5.16e-04)	Tok/s 18461 (17264)	Loss/tok 7.3847 (8.0235)	LR 2.000e-03
0: TRAIN [0][280/1541]	Time 0.307 (0.265)	Data 7.61e-05 (5.01e-04)	Tok/s 19669 (17270)	Loss/tok 7.2373 (7.9915)	LR 2.000e-03
0: TRAIN [0][290/1541]	Time 0.234 (0.266)	Data 9.23e-05 (4.87e-04)	Tok/s 18440 (17301)	Loss/tok 7.0621 (7.9565)	LR 2.000e-03
0: TRAIN [0][300/1541]	Time 0.175 (0.266)	Data 1.27e-04 (4.75e-04)	Tok/s 14799 (17284)	Loss/tok 6.7476 (7.9260)	LR 2.000e-03
0: TRAIN [0][310/1541]	Time 0.237 (0.265)	Data 1.27e-04 (4.63e-04)	Tok/s 18387 (17286)	Loss/tok 6.9329 (7.8957)	LR 2.000e-03
0: TRAIN [0][320/1541]	Time 0.167 (0.264)	Data 1.30e-04 (4.53e-04)	Tok/s 15504 (17261)	Loss/tok 6.5231 (7.8678)	LR 2.000e-03
0: TRAIN [0][330/1541]	Time 0.399 (0.264)	Data 9.73e-05 (4.43e-04)	Tok/s 19764 (17272)	Loss/tok 7.0772 (7.8398)	LR 2.000e-03
0: TRAIN [0][340/1541]	Time 0.308 (0.263)	Data 9.39e-05 (4.33e-04)	Tok/s 19522 (17256)	Loss/tok 6.8988 (7.8139)	LR 2.000e-03
0: TRAIN [0][350/1541]	Time 0.395 (0.265)	Data 8.82e-05 (4.25e-04)	Tok/s 20057 (17285)	Loss/tok 7.0672 (7.7823)	LR 2.000e-03
0: TRAIN [0][360/1541]	Time 0.164 (0.264)	Data 7.99e-05 (4.16e-04)	Tok/s 15794 (17281)	Loss/tok 6.5553 (7.7569)	LR 2.000e-03
0: TRAIN [0][370/1541]	Time 0.097 (0.263)	Data 8.54e-05 (4.07e-04)	Tok/s 13736 (17286)	Loss/tok 5.9684 (7.7277)	LR 2.000e-03
0: TRAIN [0][380/1541]	Time 0.308 (0.264)	Data 7.94e-05 (3.99e-04)	Tok/s 19501 (17302)	Loss/tok 6.6867 (7.6992)	LR 2.000e-03
0: TRAIN [0][390/1541]	Time 0.095 (0.264)	Data 7.61e-05 (3.91e-04)	Tok/s 13466 (17321)	Loss/tok 6.6652 (7.6722)	LR 2.000e-03
0: TRAIN [0][400/1541]	Time 0.163 (0.263)	Data 8.51e-05 (3.83e-04)	Tok/s 15950 (17308)	Loss/tok 6.3087 (7.6483)	LR 2.000e-03
0: TRAIN [0][410/1541]	Time 0.235 (0.262)	Data 6.96e-05 (3.76e-04)	Tok/s 18389 (17297)	Loss/tok 6.5223 (7.6241)	LR 2.000e-03
0: TRAIN [0][420/1541]	Time 0.317 (0.262)	Data 9.92e-05 (3.70e-04)	Tok/s 19102 (17304)	Loss/tok 6.6620 (7.5988)	LR 2.000e-03
0: TRAIN [0][430/1541]	Time 0.168 (0.261)	Data 1.45e-04 (3.65e-04)	Tok/s 15209 (17283)	Loss/tok 6.1286 (7.5778)	LR 2.000e-03
0: TRAIN [0][440/1541]	Time 0.235 (0.261)	Data 1.57e-04 (3.60e-04)	Tok/s 18224 (17275)	Loss/tok 6.3281 (7.5553)	LR 2.000e-03
0: TRAIN [0][450/1541]	Time 0.097 (0.261)	Data 8.51e-05 (3.55e-04)	Tok/s 14090 (17286)	Loss/tok 5.4608 (7.5305)	LR 2.000e-03
0: TRAIN [0][460/1541]	Time 0.236 (0.261)	Data 8.75e-05 (3.50e-04)	Tok/s 18528 (17293)	Loss/tok 6.4458 (7.5064)	LR 2.000e-03
0: TRAIN [0][470/1541]	Time 0.235 (0.261)	Data 8.42e-05 (3.44e-04)	Tok/s 18435 (17294)	Loss/tok 6.2920 (7.4834)	LR 2.000e-03
0: TRAIN [0][480/1541]	Time 0.311 (0.261)	Data 7.80e-05 (3.39e-04)	Tok/s 19766 (17313)	Loss/tok 6.4385 (7.4586)	LR 2.000e-03
0: TRAIN [0][490/1541]	Time 0.096 (0.260)	Data 7.65e-05 (3.33e-04)	Tok/s 13400 (17298)	Loss/tok 5.5100 (7.4379)	LR 2.000e-03
0: TRAIN [0][500/1541]	Time 0.234 (0.260)	Data 8.18e-05 (3.28e-04)	Tok/s 18416 (17299)	Loss/tok 6.1839 (7.4161)	LR 2.000e-03
0: TRAIN [0][510/1541]	Time 0.238 (0.259)	Data 7.80e-05 (3.24e-04)	Tok/s 18230 (17308)	Loss/tok 6.1174 (7.3933)	LR 2.000e-03
0: TRAIN [0][520/1541]	Time 0.164 (0.260)	Data 7.75e-05 (3.19e-04)	Tok/s 16010 (17315)	Loss/tok 5.8882 (7.3695)	LR 2.000e-03
0: TRAIN [0][530/1541]	Time 0.232 (0.260)	Data 1.72e-04 (3.15e-04)	Tok/s 18003 (17311)	Loss/tok 5.9634 (7.3479)	LR 2.000e-03
0: TRAIN [0][540/1541]	Time 0.239 (0.259)	Data 8.94e-05 (3.11e-04)	Tok/s 18063 (17304)	Loss/tok 6.1768 (7.3289)	LR 2.000e-03
0: TRAIN [0][550/1541]	Time 0.240 (0.259)	Data 9.01e-05 (3.08e-04)	Tok/s 17921 (17308)	Loss/tok 6.1088 (7.3074)	LR 2.000e-03
0: TRAIN [0][560/1541]	Time 0.239 (0.259)	Data 1.40e-04 (3.06e-04)	Tok/s 18403 (17317)	Loss/tok 6.0129 (7.2855)	LR 2.000e-03
0: TRAIN [0][570/1541]	Time 0.307 (0.259)	Data 7.49e-05 (3.02e-04)	Tok/s 19544 (17321)	Loss/tok 6.1507 (7.2643)	LR 2.000e-03
0: TRAIN [0][580/1541]	Time 0.163 (0.259)	Data 6.87e-05 (2.99e-04)	Tok/s 15619 (17324)	Loss/tok 5.6160 (7.2439)	LR 2.000e-03
0: TRAIN [0][590/1541]	Time 0.236 (0.259)	Data 7.92e-05 (2.95e-04)	Tok/s 18155 (17332)	Loss/tok 5.8388 (7.2225)	LR 2.000e-03
0: TRAIN [0][600/1541]	Time 0.305 (0.259)	Data 1.27e-04 (2.92e-04)	Tok/s 19855 (17331)	Loss/tok 6.2164 (7.2035)	LR 2.000e-03
0: TRAIN [0][610/1541]	Time 0.165 (0.259)	Data 1.21e-04 (2.89e-04)	Tok/s 16317 (17330)	Loss/tok 5.6224 (7.1839)	LR 2.000e-03
0: TRAIN [0][620/1541]	Time 0.235 (0.258)	Data 1.11e-04 (2.87e-04)	Tok/s 18091 (17321)	Loss/tok 5.8862 (7.1660)	LR 2.000e-03
0: TRAIN [0][630/1541]	Time 0.232 (0.258)	Data 1.39e-04 (2.84e-04)	Tok/s 18536 (17322)	Loss/tok 5.8086 (7.1463)	LR 2.000e-03
0: TRAIN [0][640/1541]	Time 0.164 (0.258)	Data 1.18e-04 (2.82e-04)	Tok/s 16214 (17322)	Loss/tok 5.3988 (7.1261)	LR 2.000e-03
0: TRAIN [0][650/1541]	Time 0.233 (0.258)	Data 1.49e-04 (2.80e-04)	Tok/s 18642 (17322)	Loss/tok 5.8284 (7.1087)	LR 2.000e-03
0: TRAIN [0][660/1541]	Time 0.234 (0.258)	Data 1.43e-04 (2.77e-04)	Tok/s 18133 (17329)	Loss/tok 5.7810 (7.0889)	LR 2.000e-03
0: TRAIN [0][670/1541]	Time 0.239 (0.258)	Data 1.42e-04 (2.75e-04)	Tok/s 18348 (17322)	Loss/tok 5.5861 (7.0705)	LR 2.000e-03
0: TRAIN [0][680/1541]	Time 0.237 (0.258)	Data 1.74e-04 (2.73e-04)	Tok/s 18031 (17318)	Loss/tok 5.5402 (7.0519)	LR 2.000e-03
0: TRAIN [0][690/1541]	Time 0.166 (0.257)	Data 1.12e-04 (2.71e-04)	Tok/s 16054 (17303)	Loss/tok 5.1435 (7.0358)	LR 2.000e-03
0: TRAIN [0][700/1541]	Time 0.171 (0.257)	Data 1.20e-04 (2.69e-04)	Tok/s 15416 (17295)	Loss/tok 5.3578 (7.0190)	LR 2.000e-03
0: TRAIN [0][710/1541]	Time 0.161 (0.257)	Data 1.06e-04 (2.67e-04)	Tok/s 16854 (17301)	Loss/tok 5.1541 (7.0001)	LR 2.000e-03
0: TRAIN [0][720/1541]	Time 0.396 (0.257)	Data 1.44e-04 (2.65e-04)	Tok/s 19925 (17307)	Loss/tok 5.9487 (6.9803)	LR 2.000e-03
0: TRAIN [0][730/1541]	Time 0.165 (0.257)	Data 1.26e-04 (2.63e-04)	Tok/s 15427 (17309)	Loss/tok 5.3452 (6.9616)	LR 2.000e-03
0: TRAIN [0][740/1541]	Time 0.160 (0.258)	Data 1.45e-04 (2.61e-04)	Tok/s 16121 (17324)	Loss/tok 5.0333 (6.9404)	LR 2.000e-03
0: TRAIN [0][750/1541]	Time 0.394 (0.258)	Data 7.06e-05 (2.59e-04)	Tok/s 19964 (17331)	Loss/tok 5.8196 (6.9209)	LR 2.000e-03
0: TRAIN [0][760/1541]	Time 0.163 (0.258)	Data 7.72e-05 (2.57e-04)	Tok/s 16019 (17331)	Loss/tok 5.1007 (6.9024)	LR 2.000e-03
0: TRAIN [0][770/1541]	Time 0.235 (0.258)	Data 7.49e-05 (2.55e-04)	Tok/s 18344 (17334)	Loss/tok 5.3788 (6.8843)	LR 2.000e-03
0: TRAIN [0][780/1541]	Time 0.163 (0.258)	Data 7.39e-05 (2.53e-04)	Tok/s 16207 (17338)	Loss/tok 5.0753 (6.8659)	LR 2.000e-03
0: TRAIN [0][790/1541]	Time 0.165 (0.258)	Data 7.84e-05 (2.50e-04)	Tok/s 15852 (17344)	Loss/tok 5.1373 (6.8476)	LR 2.000e-03
0: TRAIN [0][800/1541]	Time 0.165 (0.258)	Data 7.87e-05 (2.48e-04)	Tok/s 15842 (17350)	Loss/tok 5.0094 (6.8292)	LR 2.000e-03
0: TRAIN [0][810/1541]	Time 0.395 (0.258)	Data 7.63e-05 (2.46e-04)	Tok/s 19698 (17343)	Loss/tok 5.7402 (6.8129)	LR 2.000e-03
0: TRAIN [0][820/1541]	Time 0.393 (0.258)	Data 7.51e-05 (2.44e-04)	Tok/s 19733 (17355)	Loss/tok 5.6753 (6.7934)	LR 2.000e-03
0: TRAIN [0][830/1541]	Time 0.392 (0.258)	Data 8.18e-05 (2.42e-04)	Tok/s 20128 (17359)	Loss/tok 5.5929 (6.7756)	LR 2.000e-03
0: TRAIN [0][840/1541]	Time 0.234 (0.258)	Data 7.46e-05 (2.41e-04)	Tok/s 18176 (17360)	Loss/tok 5.1112 (6.7590)	LR 2.000e-03
0: TRAIN [0][850/1541]	Time 0.164 (0.258)	Data 7.44e-05 (2.39e-04)	Tok/s 16104 (17353)	Loss/tok 4.8615 (6.7435)	LR 2.000e-03
0: TRAIN [0][860/1541]	Time 0.314 (0.259)	Data 6.99e-05 (2.37e-04)	Tok/s 19511 (17371)	Loss/tok 5.4028 (6.7230)	LR 2.000e-03
0: TRAIN [0][870/1541]	Time 0.394 (0.259)	Data 7.25e-05 (2.35e-04)	Tok/s 20242 (17380)	Loss/tok 5.4840 (6.7045)	LR 2.000e-03
0: TRAIN [0][880/1541]	Time 0.165 (0.260)	Data 1.39e-04 (2.34e-04)	Tok/s 15999 (17388)	Loss/tok 4.8720 (6.6865)	LR 2.000e-03
0: TRAIN [0][890/1541]	Time 0.310 (0.260)	Data 7.53e-05 (2.33e-04)	Tok/s 19889 (17392)	Loss/tok 5.2087 (6.6689)	LR 2.000e-03
0: TRAIN [0][900/1541]	Time 0.168 (0.260)	Data 1.49e-04 (2.31e-04)	Tok/s 15474 (17395)	Loss/tok 4.7959 (6.6516)	LR 2.000e-03
0: TRAIN [0][910/1541]	Time 0.312 (0.259)	Data 1.42e-04 (2.30e-04)	Tok/s 19205 (17388)	Loss/tok 5.2691 (6.6369)	LR 2.000e-03
0: TRAIN [0][920/1541]	Time 0.235 (0.260)	Data 7.89e-05 (2.29e-04)	Tok/s 18382 (17399)	Loss/tok 5.0396 (6.6192)	LR 2.000e-03
0: TRAIN [0][930/1541]	Time 0.391 (0.260)	Data 1.50e-04 (2.28e-04)	Tok/s 20322 (17399)	Loss/tok 5.3670 (6.6027)	LR 2.000e-03
0: TRAIN [0][940/1541]	Time 0.238 (0.260)	Data 1.43e-04 (2.27e-04)	Tok/s 18081 (17402)	Loss/tok 5.0166 (6.5868)	LR 2.000e-03
0: TRAIN [0][950/1541]	Time 0.239 (0.261)	Data 1.15e-04 (2.26e-04)	Tok/s 18206 (17411)	Loss/tok 4.9816 (6.5698)	LR 2.000e-03
0: TRAIN [0][960/1541]	Time 0.167 (0.260)	Data 1.55e-04 (2.25e-04)	Tok/s 15645 (17400)	Loss/tok 4.4537 (6.5561)	LR 2.000e-03
0: TRAIN [0][970/1541]	Time 0.095 (0.261)	Data 1.09e-04 (2.24e-04)	Tok/s 13704 (17397)	Loss/tok 4.4765 (6.5395)	LR 2.000e-03
0: TRAIN [0][980/1541]	Time 0.311 (0.261)	Data 1.00e-04 (2.23e-04)	Tok/s 19338 (17398)	Loss/tok 4.9739 (6.5230)	LR 2.000e-03
0: TRAIN [0][990/1541]	Time 0.186 (0.261)	Data 1.10e-04 (2.22e-04)	Tok/s 14193 (17387)	Loss/tok 4.6126 (6.5078)	LR 2.000e-03
0: TRAIN [0][1000/1541]	Time 0.323 (0.262)	Data 1.60e-04 (2.21e-04)	Tok/s 19033 (17391)	Loss/tok 4.9822 (6.4907)	LR 2.000e-03
0: TRAIN [0][1010/1541]	Time 0.312 (0.262)	Data 9.04e-05 (2.20e-04)	Tok/s 19711 (17393)	Loss/tok 5.1948 (6.4754)	LR 2.000e-03
0: TRAIN [0][1020/1541]	Time 0.254 (0.262)	Data 1.58e-04 (2.19e-04)	Tok/s 16547 (17372)	Loss/tok 4.7526 (6.4630)	LR 2.000e-03
0: TRAIN [0][1030/1541]	Time 0.180 (0.262)	Data 1.62e-04 (2.19e-04)	Tok/s 14539 (17360)	Loss/tok 4.5193 (6.4496)	LR 1.000e-03
0: TRAIN [0][1040/1541]	Time 0.177 (0.261)	Data 1.02e-04 (2.18e-04)	Tok/s 15381 (17346)	Loss/tok 4.4027 (6.4374)	LR 1.000e-03
0: TRAIN [0][1050/1541]	Time 0.248 (0.261)	Data 1.59e-04 (2.17e-04)	Tok/s 17337 (17339)	Loss/tok 4.7086 (6.4233)	LR 1.000e-03
0: TRAIN [0][1060/1541]	Time 0.400 (0.261)	Data 1.81e-04 (2.16e-04)	Tok/s 19968 (17339)	Loss/tok 5.0968 (6.4068)	LR 1.000e-03
0: TRAIN [0][1070/1541]	Time 0.258 (0.262)	Data 9.73e-05 (2.15e-04)	Tok/s 16600 (17337)	Loss/tok 4.7107 (6.3903)	LR 1.000e-03
0: TRAIN [0][1080/1541]	Time 0.267 (0.262)	Data 8.20e-05 (2.14e-04)	Tok/s 16387 (17339)	Loss/tok 4.5089 (6.3742)	LR 1.000e-03
0: TRAIN [0][1090/1541]	Time 0.267 (0.262)	Data 1.08e-04 (2.13e-04)	Tok/s 16232 (17323)	Loss/tok 4.5634 (6.3606)	LR 1.000e-03
0: TRAIN [0][1100/1541]	Time 0.236 (0.262)	Data 7.84e-05 (2.12e-04)	Tok/s 18524 (17322)	Loss/tok 4.7295 (6.3471)	LR 1.000e-03
0: TRAIN [0][1110/1541]	Time 0.235 (0.262)	Data 1.07e-04 (2.11e-04)	Tok/s 18487 (17327)	Loss/tok 4.5308 (6.3322)	LR 1.000e-03
0: TRAIN [0][1120/1541]	Time 0.167 (0.262)	Data 8.54e-05 (2.10e-04)	Tok/s 15308 (17311)	Loss/tok 4.3643 (6.3207)	LR 1.000e-03
0: TRAIN [0][1130/1541]	Time 0.412 (0.262)	Data 1.41e-04 (2.10e-04)	Tok/s 18856 (17307)	Loss/tok 5.1164 (6.3071)	LR 1.000e-03
0: TRAIN [0][1140/1541]	Time 0.330 (0.262)	Data 1.06e-04 (2.09e-04)	Tok/s 18092 (17299)	Loss/tok 4.8303 (6.2936)	LR 1.000e-03
0: TRAIN [0][1150/1541]	Time 0.165 (0.262)	Data 8.80e-05 (2.08e-04)	Tok/s 16708 (17289)	Loss/tok 4.2663 (6.2817)	LR 1.000e-03
0: TRAIN [0][1160/1541]	Time 0.416 (0.262)	Data 1.04e-04 (2.07e-04)	Tok/s 18660 (17291)	Loss/tok 4.9910 (6.2669)	LR 5.000e-04
0: TRAIN [0][1170/1541]	Time 0.405 (0.262)	Data 9.70e-05 (2.06e-04)	Tok/s 19096 (17282)	Loss/tok 4.9671 (6.2548)	LR 5.000e-04
0: TRAIN [0][1180/1541]	Time 0.235 (0.262)	Data 9.87e-05 (2.05e-04)	Tok/s 18492 (17286)	Loss/tok 4.4701 (6.2397)	LR 5.000e-04
0: TRAIN [0][1190/1541]	Time 0.239 (0.262)	Data 9.18e-05 (2.04e-04)	Tok/s 18161 (17283)	Loss/tok 4.5319 (6.2279)	LR 5.000e-04
0: TRAIN [0][1200/1541]	Time 0.244 (0.262)	Data 8.34e-05 (2.04e-04)	Tok/s 17676 (17284)	Loss/tok 4.4400 (6.2139)	LR 5.000e-04
0: TRAIN [0][1210/1541]	Time 0.308 (0.262)	Data 7.58e-05 (2.03e-04)	Tok/s 19661 (17292)	Loss/tok 4.7064 (6.1991)	LR 5.000e-04
0: TRAIN [0][1220/1541]	Time 0.232 (0.262)	Data 8.06e-05 (2.02e-04)	Tok/s 18551 (17298)	Loss/tok 4.3879 (6.1850)	LR 5.000e-04
0: TRAIN [0][1230/1541]	Time 0.427 (0.263)	Data 1.63e-04 (2.01e-04)	Tok/s 18674 (17297)	Loss/tok 4.8768 (6.1718)	LR 5.000e-04
0: TRAIN [0][1240/1541]	Time 0.188 (0.263)	Data 1.70e-04 (2.01e-04)	Tok/s 13794 (17287)	Loss/tok 4.1907 (6.1590)	LR 5.000e-04
0: TRAIN [0][1250/1541]	Time 0.107 (0.263)	Data 1.25e-04 (2.00e-04)	Tok/s 12155 (17271)	Loss/tok 3.9365 (6.1487)	LR 5.000e-04
0: TRAIN [0][1260/1541]	Time 0.264 (0.263)	Data 1.44e-04 (1.99e-04)	Tok/s 16242 (17260)	Loss/tok 4.7396 (6.1367)	LR 5.000e-04
0: TRAIN [0][1270/1541]	Time 0.269 (0.262)	Data 1.24e-04 (1.99e-04)	Tok/s 16114 (17248)	Loss/tok 4.5402 (6.1258)	LR 5.000e-04
0: TRAIN [0][1280/1541]	Time 0.331 (0.263)	Data 2.12e-04 (1.98e-04)	Tok/s 18158 (17250)	Loss/tok 4.6449 (6.1128)	LR 5.000e-04
0: TRAIN [0][1290/1541]	Time 0.237 (0.263)	Data 1.04e-04 (1.98e-04)	Tok/s 18567 (17250)	Loss/tok 4.5240 (6.1015)	LR 2.500e-04
0: TRAIN [0][1300/1541]	Time 0.110 (0.262)	Data 1.08e-04 (1.97e-04)	Tok/s 11967 (17233)	Loss/tok 3.8961 (6.0933)	LR 2.500e-04
0: TRAIN [0][1310/1541]	Time 0.413 (0.262)	Data 1.00e-04 (1.97e-04)	Tok/s 19228 (17235)	Loss/tok 4.8262 (6.0811)	LR 2.500e-04
0: TRAIN [0][1320/1541]	Time 0.313 (0.262)	Data 1.77e-04 (1.96e-04)	Tok/s 19414 (17235)	Loss/tok 4.6899 (6.0691)	LR 2.500e-04
0: TRAIN [0][1330/1541]	Time 0.271 (0.262)	Data 1.14e-04 (1.96e-04)	Tok/s 16095 (17232)	Loss/tok 4.4061 (6.0580)	LR 2.500e-04
0: TRAIN [0][1340/1541]	Time 0.164 (0.262)	Data 1.03e-04 (1.95e-04)	Tok/s 15887 (17227)	Loss/tok 4.2567 (6.0472)	LR 2.500e-04
0: TRAIN [0][1350/1541]	Time 0.453 (0.263)	Data 1.14e-04 (1.95e-04)	Tok/s 17217 (17216)	Loss/tok 4.8961 (6.0353)	LR 2.500e-04
0: TRAIN [0][1360/1541]	Time 0.339 (0.263)	Data 1.10e-04 (1.94e-04)	Tok/s 17823 (17221)	Loss/tok 4.6476 (6.0230)	LR 2.500e-04
0: TRAIN [0][1370/1541]	Time 0.106 (0.263)	Data 1.08e-04 (1.94e-04)	Tok/s 12219 (17211)	Loss/tok 3.6010 (6.0130)	LR 2.500e-04
0: TRAIN [0][1380/1541]	Time 0.326 (0.263)	Data 1.70e-04 (1.93e-04)	Tok/s 18456 (17202)	Loss/tok 4.6140 (6.0022)	LR 2.500e-04
0: TRAIN [0][1390/1541]	Time 0.254 (0.263)	Data 1.06e-04 (1.93e-04)	Tok/s 17139 (17193)	Loss/tok 4.4753 (5.9922)	LR 2.500e-04
0: TRAIN [0][1400/1541]	Time 0.309 (0.263)	Data 1.03e-04 (1.92e-04)	Tok/s 19714 (17191)	Loss/tok 4.6340 (5.9803)	LR 2.500e-04
0: TRAIN [0][1410/1541]	Time 0.332 (0.263)	Data 1.10e-04 (1.92e-04)	Tok/s 18211 (17186)	Loss/tok 4.5462 (5.9713)	LR 1.250e-04
0: TRAIN [0][1420/1541]	Time 0.176 (0.263)	Data 1.10e-04 (1.91e-04)	Tok/s 15122 (17187)	Loss/tok 4.1853 (5.9603)	LR 1.250e-04
0: TRAIN [0][1430/1541]	Time 0.339 (0.263)	Data 1.08e-04 (1.90e-04)	Tok/s 17676 (17177)	Loss/tok 4.5464 (5.9504)	LR 1.250e-04
0: TRAIN [0][1440/1541]	Time 0.336 (0.264)	Data 1.36e-04 (1.90e-04)	Tok/s 17847 (17173)	Loss/tok 4.4881 (5.9402)	LR 1.250e-04
0: TRAIN [0][1450/1541]	Time 0.393 (0.264)	Data 8.70e-05 (1.89e-04)	Tok/s 20094 (17175)	Loss/tok 4.7537 (5.9296)	LR 1.250e-04
0: TRAIN [0][1460/1541]	Time 0.323 (0.264)	Data 1.66e-04 (1.89e-04)	Tok/s 18755 (17174)	Loss/tok 4.6485 (5.9199)	LR 1.250e-04
0: TRAIN [0][1470/1541]	Time 0.173 (0.264)	Data 1.68e-04 (1.89e-04)	Tok/s 15041 (17171)	Loss/tok 4.1188 (5.9096)	LR 1.250e-04
0: TRAIN [0][1480/1541]	Time 0.318 (0.264)	Data 1.34e-04 (1.89e-04)	Tok/s 18900 (17164)	Loss/tok 4.5760 (5.9017)	LR 1.250e-04
0: TRAIN [0][1490/1541]	Time 0.252 (0.264)	Data 1.58e-04 (1.88e-04)	Tok/s 16964 (17159)	Loss/tok 4.1763 (5.8932)	LR 1.250e-04
0: TRAIN [0][1500/1541]	Time 0.250 (0.264)	Data 1.61e-04 (1.88e-04)	Tok/s 17143 (17165)	Loss/tok 4.5523 (5.8824)	LR 1.250e-04
0: TRAIN [0][1510/1541]	Time 0.167 (0.264)	Data 1.76e-04 (1.88e-04)	Tok/s 15966 (17167)	Loss/tok 4.3270 (5.8729)	LR 1.250e-04
0: TRAIN [0][1520/1541]	Time 0.093 (0.264)	Data 1.57e-04 (1.88e-04)	Tok/s 13284 (17167)	Loss/tok 3.5420 (5.8637)	LR 1.250e-04
0: TRAIN [0][1530/1541]	Time 0.167 (0.264)	Data 1.10e-04 (1.88e-04)	Tok/s 15332 (17166)	Loss/tok 3.7804 (5.8544)	LR 1.250e-04
0: TRAIN [0][1540/1541]	Time 0.234 (0.264)	Data 3.89e-05 (1.88e-04)	Tok/s 18056 (17173)	Loss/tok 4.3783 (5.8445)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/80]	Time 0.235 (0.235)	Data 2.71e-03 (2.71e-03)	Tok/s 44534 (44534)	Loss/tok 6.0208 (6.0208)
0: VALIDATION [0][10/80]	Time 0.100 (0.127)	Data 2.55e-03 (2.61e-03)	Tok/s 58543 (57573)	Loss/tok 5.6111 (5.7908)
0: VALIDATION [0][20/80]	Time 0.089 (0.111)	Data 2.61e-03 (2.63e-03)	Tok/s 52348 (56750)	Loss/tok 5.3506 (5.7197)
0: VALIDATION [0][30/80]	Time 0.068 (0.100)	Data 2.59e-03 (2.64e-03)	Tok/s 57255 (55894)	Loss/tok 5.4148 (5.6486)
0: VALIDATION [0][40/80]	Time 0.055 (0.091)	Data 2.42e-03 (2.61e-03)	Tok/s 58630 (56150)	Loss/tok 5.3739 (5.6038)
0: VALIDATION [0][50/80]	Time 0.045 (0.083)	Data 2.31e-03 (2.56e-03)	Tok/s 58546 (56399)	Loss/tok 5.4330 (5.5681)
0: VALIDATION [0][60/80]	Time 0.039 (0.076)	Data 2.60e-03 (2.54e-03)	Tok/s 55059 (56307)	Loss/tok 5.1818 (5.5331)
0: VALIDATION [0][70/80]	Time 0.031 (0.070)	Data 2.31e-03 (2.53e-03)	Tok/s 52001 (55888)	Loss/tok 5.1497 (5.5042)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/24]	Time 1.0046 (1.6344)	Decoder iters 149.0 (149.0)	Tok/s 8484 (7761)
0: TEST [0][19/24]	Time 0.6235 (1.2270)	Decoder iters 149.0 (149.0)	Tok/s 6946 (7721)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 5.8445	Validation Loss: 5.4745	Test BLEU: 4.21
0: Performance: Epoch: 0	Training: 17173 Tok/s	Validation: 55011 Tok/s
0: Finished epoch 0
0: Total training time 460 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                  96|                      4.21|                      17173.1|                         7.669|
