1: Collecting environment information...
0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
1: Worker 1 is using worker seed: 364522461
0: Worker 0 is using worker seed: 242886303
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
0: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 909
1: Scheduler decay interval: 114
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 909
0: Scheduler decay interval: 114
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: Sampler for epoch 0 uses seed 3588440356
0: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/683]	Time 0.567 (0.567)	Data 1.67e-01 (1.67e-01)	Tok/s 11992 (11992)	Loss/tok 10.7036 (10.7036)	LR 2.047e-05
1: TRAIN [0][0/683]	Time 0.571 (0.571)	Data 1.73e-01 (1.73e-01)	Tok/s 11860 (11860)	Loss/tok 10.6986 (10.6986)	LR 2.047e-05
1: TRAIN [0][10/683]	Time 0.317 (0.314)	Data 8.51e-05 (1.58e-02)	Tok/s 15084 (14222)	Loss/tok 9.6614 (10.1808)	LR 2.576e-05
0: TRAIN [0][10/683]	Time 0.317 (0.315)	Data 1.02e-04 (1.53e-02)	Tok/s 15527 (14275)	Loss/tok 9.7140 (10.1935)	LR 2.576e-05
1: TRAIN [0][20/683]	Time 0.223 (0.314)	Data 9.08e-05 (8.33e-03)	Tok/s 13329 (14357)	Loss/tok 9.1135 (9.8124)	LR 3.244e-05
0: TRAIN [0][20/683]	Time 0.223 (0.314)	Data 1.20e-04 (8.08e-03)	Tok/s 13735 (14400)	Loss/tok 9.1079 (9.8237)	LR 3.244e-05
1: TRAIN [0][30/683]	Time 0.221 (0.335)	Data 1.11e-04 (5.68e-03)	Tok/s 13775 (14764)	Loss/tok 8.7528 (9.5369)	LR 4.083e-05
0: TRAIN [0][30/683]	Time 0.225 (0.335)	Data 1.08e-04 (5.51e-03)	Tok/s 13561 (14781)	Loss/tok 8.8299 (9.5483)	LR 4.083e-05
1: TRAIN [0][40/683]	Time 0.526 (0.337)	Data 1.08e-04 (4.32e-03)	Tok/s 16640 (14785)	Loss/tok 8.8338 (9.3544)	LR 5.141e-05
0: TRAIN [0][40/683]	Time 0.530 (0.337)	Data 1.20e-04 (4.19e-03)	Tok/s 16668 (14778)	Loss/tok 8.7691 (9.3681)	LR 5.141e-05
0: TRAIN [0][50/683]	Time 0.528 (0.338)	Data 9.49e-05 (3.39e-03)	Tok/s 16764 (14810)	Loss/tok 8.6714 (9.2263)	LR 6.472e-05
1: TRAIN [0][50/683]	Time 0.530 (0.338)	Data 1.05e-04 (3.49e-03)	Tok/s 16850 (14825)	Loss/tok 8.6443 (9.2155)	LR 6.472e-05
1: TRAIN [0][60/683]	Time 0.415 (0.338)	Data 1.12e-04 (2.94e-03)	Tok/s 16384 (14845)	Loss/tok 8.3308 (9.0796)	LR 8.148e-05
0: TRAIN [0][60/683]	Time 0.417 (0.338)	Data 9.51e-05 (2.85e-03)	Tok/s 16380 (14841)	Loss/tok 8.3371 (9.0865)	LR 8.148e-05
1: TRAIN [0][70/683]	Time 0.316 (0.337)	Data 9.49e-05 (2.54e-03)	Tok/s 15217 (14855)	Loss/tok 8.0898 (8.9825)	LR 1.026e-04
0: TRAIN [0][70/683]	Time 0.316 (0.337)	Data 8.70e-05 (2.47e-03)	Tok/s 15287 (14852)	Loss/tok 8.1363 (8.9897)	LR 1.026e-04
1: TRAIN [0][80/683]	Time 0.320 (0.343)	Data 9.73e-05 (2.24e-03)	Tok/s 15473 (14953)	Loss/tok 7.9535 (8.8594)	LR 1.291e-04
0: TRAIN [0][80/683]	Time 0.321 (0.343)	Data 1.35e-04 (2.17e-03)	Tok/s 15114 (14944)	Loss/tok 7.9632 (8.8651)	LR 1.291e-04
1: TRAIN [0][90/683]	Time 0.223 (0.346)	Data 8.82e-05 (2.00e-03)	Tok/s 12686 (14978)	Loss/tok 7.6269 (8.7506)	LR 1.626e-04
0: TRAIN [0][90/683]	Time 0.223 (0.346)	Data 9.51e-05 (1.95e-03)	Tok/s 12986 (14980)	Loss/tok 7.5812 (8.7541)	LR 1.626e-04
1: TRAIN [0][100/683]	Time 0.417 (0.342)	Data 9.23e-05 (1.81e-03)	Tok/s 16139 (14937)	Loss/tok 7.9330 (8.6726)	LR 2.047e-04
0: TRAIN [0][100/683]	Time 0.416 (0.342)	Data 8.87e-05 (1.76e-03)	Tok/s 16435 (14943)	Loss/tok 7.9487 (8.6730)	LR 2.047e-04
1: TRAIN [0][110/683]	Time 0.422 (0.339)	Data 8.30e-05 (1.66e-03)	Tok/s 16124 (14887)	Loss/tok 7.8757 (8.6022)	LR 2.576e-04
0: TRAIN [0][110/683]	Time 0.421 (0.339)	Data 8.96e-05 (1.61e-03)	Tok/s 15938 (14890)	Loss/tok 7.8416 (8.6038)	LR 2.576e-04
1: TRAIN [0][120/683]	Time 0.224 (0.339)	Data 8.44e-05 (1.53e-03)	Tok/s 13001 (14896)	Loss/tok 7.5120 (8.5332)	LR 3.244e-04
0: TRAIN [0][120/683]	Time 0.224 (0.339)	Data 1.42e-04 (1.49e-03)	Tok/s 13176 (14912)	Loss/tok 7.5888 (8.5328)	LR 3.244e-04
1: TRAIN [0][130/683]	Time 0.318 (0.339)	Data 7.70e-05 (1.42e-03)	Tok/s 15524 (14899)	Loss/tok 7.8405 (8.4756)	LR 4.083e-04
0: TRAIN [0][130/683]	Time 0.321 (0.339)	Data 9.04e-05 (1.38e-03)	Tok/s 14956 (14917)	Loss/tok 7.6545 (8.4735)	LR 4.083e-04
1: TRAIN [0][140/683]	Time 0.225 (0.344)	Data 8.39e-05 (1.33e-03)	Tok/s 12918 (14955)	Loss/tok 7.3877 (8.4149)	LR 5.141e-04
0: TRAIN [0][140/683]	Time 0.225 (0.344)	Data 1.29e-04 (1.29e-03)	Tok/s 12600 (14978)	Loss/tok 7.4502 (8.4118)	LR 5.141e-04
1: TRAIN [0][150/683]	Time 0.421 (0.340)	Data 1.31e-04 (1.25e-03)	Tok/s 16025 (14888)	Loss/tok 7.7664 (8.3746)	LR 6.472e-04
0: TRAIN [0][150/683]	Time 0.424 (0.340)	Data 8.94e-05 (1.21e-03)	Tok/s 15660 (14910)	Loss/tok 7.8251 (8.3722)	LR 6.472e-04
1: TRAIN [0][160/683]	Time 0.225 (0.339)	Data 8.32e-05 (1.17e-03)	Tok/s 12910 (14881)	Loss/tok 7.2699 (8.3305)	LR 8.148e-04
0: TRAIN [0][160/683]	Time 0.225 (0.339)	Data 9.32e-05 (1.14e-03)	Tok/s 13236 (14902)	Loss/tok 7.2673 (8.3279)	LR 8.148e-04
1: TRAIN [0][170/683]	Time 0.225 (0.339)	Data 8.13e-05 (1.11e-03)	Tok/s 13119 (14870)	Loss/tok 7.3472 (8.2933)	LR 1.026e-03
0: TRAIN [0][170/683]	Time 0.225 (0.339)	Data 9.20e-05 (1.08e-03)	Tok/s 13216 (14893)	Loss/tok 7.2676 (8.2896)	LR 1.026e-03
1: TRAIN [0][180/683]	Time 0.524 (0.341)	Data 8.18e-05 (1.05e-03)	Tok/s 16616 (14888)	Loss/tok 7.7574 (8.2546)	LR 1.291e-03
0: TRAIN [0][180/683]	Time 0.524 (0.341)	Data 9.32e-05 (1.03e-03)	Tok/s 16739 (14915)	Loss/tok 7.7312 (8.2483)	LR 1.291e-03
1: TRAIN [0][190/683]	Time 0.418 (0.346)	Data 9.35e-05 (1.00e-03)	Tok/s 16213 (14960)	Loss/tok 7.5413 (8.2112)	LR 1.626e-03
0: TRAIN [0][190/683]	Time 0.420 (0.346)	Data 1.04e-04 (9.80e-04)	Tok/s 16132 (14989)	Loss/tok 7.6216 (8.2066)	LR 1.626e-03
0: TRAIN [0][200/683]	Time 0.527 (0.348)	Data 1.14e-04 (9.37e-04)	Tok/s 16742 (14984)	Loss/tok 7.5600 (8.1685)	LR 2.000e-03
1: TRAIN [0][200/683]	Time 0.531 (0.348)	Data 1.52e-04 (9.60e-04)	Tok/s 16643 (14955)	Loss/tok 7.5841 (8.1732)	LR 2.000e-03
0: TRAIN [0][210/683]	Time 0.227 (0.350)	Data 9.54e-05 (8.97e-04)	Tok/s 12738 (15011)	Loss/tok 7.0487 (8.1291)	LR 2.000e-03
1: TRAIN [0][210/683]	Time 0.231 (0.350)	Data 9.82e-05 (9.21e-04)	Tok/s 12901 (14991)	Loss/tok 6.9937 (8.1307)	LR 2.000e-03
1: TRAIN [0][220/683]	Time 0.226 (0.349)	Data 8.03e-05 (8.83e-04)	Tok/s 13291 (14959)	Loss/tok 7.1520 (8.0965)	LR 2.000e-03
0: TRAIN [0][220/683]	Time 0.226 (0.349)	Data 9.39e-05 (8.61e-04)	Tok/s 12664 (14977)	Loss/tok 6.8456 (8.0958)	LR 2.000e-03
1: TRAIN [0][230/683]	Time 0.420 (0.350)	Data 7.89e-05 (8.49e-04)	Tok/s 16526 (14985)	Loss/tok 7.2888 (8.0546)	LR 2.000e-03
0: TRAIN [0][230/683]	Time 0.420 (0.350)	Data 9.42e-05 (8.28e-04)	Tok/s 16168 (15002)	Loss/tok 7.3439 (8.0540)	LR 2.000e-03
1: TRAIN [0][240/683]	Time 0.223 (0.349)	Data 7.99e-05 (8.17e-04)	Tok/s 12916 (14967)	Loss/tok 6.7538 (8.0186)	LR 2.000e-03
0: TRAIN [0][240/683]	Time 0.223 (0.349)	Data 9.11e-05 (7.98e-04)	Tok/s 13285 (14984)	Loss/tok 6.6886 (8.0186)	LR 2.000e-03
1: TRAIN [0][250/683]	Time 0.532 (0.350)	Data 8.39e-05 (7.88e-04)	Tok/s 16609 (14975)	Loss/tok 7.2179 (7.9781)	LR 2.000e-03
0: TRAIN [0][250/683]	Time 0.533 (0.350)	Data 1.01e-04 (7.70e-04)	Tok/s 16483 (14989)	Loss/tok 7.1913 (7.9782)	LR 2.000e-03
1: TRAIN [0][260/683]	Time 0.325 (0.350)	Data 1.09e-04 (7.61e-04)	Tok/s 15077 (14974)	Loss/tok 6.8986 (7.9401)	LR 2.000e-03
0: TRAIN [0][260/683]	Time 0.325 (0.350)	Data 1.21e-04 (7.45e-04)	Tok/s 15123 (14983)	Loss/tok 6.7950 (7.9405)	LR 2.000e-03
1: TRAIN [0][270/683]	Time 0.533 (0.349)	Data 7.77e-05 (7.36e-04)	Tok/s 16662 (14966)	Loss/tok 7.1311 (7.9027)	LR 2.000e-03
0: TRAIN [0][270/683]	Time 0.532 (0.349)	Data 9.30e-05 (7.21e-04)	Tok/s 16590 (14972)	Loss/tok 7.0672 (7.9035)	LR 2.000e-03
1: TRAIN [0][280/683]	Time 0.225 (0.350)	Data 8.75e-05 (7.14e-04)	Tok/s 13172 (14971)	Loss/tok 6.4440 (7.8622)	LR 2.000e-03
0: TRAIN [0][280/683]	Time 0.225 (0.350)	Data 9.75e-05 (6.99e-04)	Tok/s 12976 (14979)	Loss/tok 6.4643 (7.8639)	LR 2.000e-03
1: TRAIN [0][290/683]	Time 0.224 (0.350)	Data 8.30e-05 (6.92e-04)	Tok/s 13418 (14975)	Loss/tok 6.1766 (7.8227)	LR 2.000e-03
0: TRAIN [0][290/683]	Time 0.225 (0.350)	Data 9.58e-05 (6.79e-04)	Tok/s 13371 (14983)	Loss/tok 6.3632 (7.8248)	LR 2.000e-03
0: TRAIN [0][300/683]	Time 0.224 (0.350)	Data 9.73e-05 (6.60e-04)	Tok/s 12958 (14991)	Loss/tok 6.1492 (7.7831)	LR 2.000e-03
1: TRAIN [0][300/683]	Time 0.226 (0.350)	Data 9.49e-05 (6.72e-04)	Tok/s 12983 (14982)	Loss/tok 6.2602 (7.7819)	LR 2.000e-03
1: TRAIN [0][310/683]	Time 0.223 (0.351)	Data 8.32e-05 (6.53e-04)	Tok/s 12991 (14992)	Loss/tok 6.2746 (7.7408)	LR 2.000e-03
0: TRAIN [0][310/683]	Time 0.226 (0.351)	Data 1.03e-04 (6.42e-04)	Tok/s 12969 (14998)	Loss/tok 6.1668 (7.7429)	LR 2.000e-03
1: TRAIN [0][320/683]	Time 0.320 (0.350)	Data 9.61e-05 (6.36e-04)	Tok/s 15401 (14988)	Loss/tok 6.3545 (7.7046)	LR 2.000e-03
0: TRAIN [0][320/683]	Time 0.320 (0.350)	Data 9.18e-05 (6.25e-04)	Tok/s 15384 (14994)	Loss/tok 6.4341 (7.7072)	LR 2.000e-03
1: TRAIN [0][330/683]	Time 0.422 (0.350)	Data 8.01e-05 (6.20e-04)	Tok/s 16237 (14981)	Loss/tok 6.5724 (7.6673)	LR 2.000e-03
0: TRAIN [0][330/683]	Time 0.423 (0.350)	Data 9.58e-05 (6.09e-04)	Tok/s 16331 (14988)	Loss/tok 6.4545 (7.6700)	LR 2.000e-03
1: TRAIN [0][340/683]	Time 0.418 (0.349)	Data 9.39e-05 (6.04e-04)	Tok/s 16218 (14981)	Loss/tok 6.4665 (7.6306)	LR 2.000e-03
0: TRAIN [0][340/683]	Time 0.418 (0.349)	Data 9.80e-05 (5.94e-04)	Tok/s 16104 (14985)	Loss/tok 6.4800 (7.6342)	LR 2.000e-03
0: TRAIN [0][350/683]	Time 0.322 (0.348)	Data 9.54e-05 (5.80e-04)	Tok/s 14944 (14958)	Loss/tok 6.2318 (7.6041)	LR 2.000e-03
1: TRAIN [0][350/683]	Time 0.325 (0.348)	Data 8.82e-05 (5.90e-04)	Tok/s 15095 (14952)	Loss/tok 6.1688 (7.5989)	LR 2.000e-03
1: TRAIN [0][360/683]	Time 0.419 (0.347)	Data 8.25e-05 (5.76e-04)	Tok/s 16068 (14939)	Loss/tok 6.3098 (7.5658)	LR 2.000e-03
0: TRAIN [0][360/683]	Time 0.419 (0.347)	Data 1.07e-04 (5.67e-04)	Tok/s 16110 (14938)	Loss/tok 6.3994 (7.5707)	LR 2.000e-03
1: TRAIN [0][370/683]	Time 0.320 (0.347)	Data 9.75e-05 (5.63e-04)	Tok/s 15292 (14953)	Loss/tok 6.0183 (7.5254)	LR 2.000e-03
0: TRAIN [0][370/683]	Time 0.320 (0.347)	Data 1.03e-04 (5.54e-04)	Tok/s 15390 (14951)	Loss/tok 6.0527 (7.5327)	LR 2.000e-03
1: TRAIN [0][380/683]	Time 0.421 (0.346)	Data 9.42e-05 (5.51e-04)	Tok/s 16181 (14942)	Loss/tok 6.3794 (7.4920)	LR 2.000e-03
0: TRAIN [0][380/683]	Time 0.421 (0.346)	Data 9.32e-05 (5.42e-04)	Tok/s 16103 (14939)	Loss/tok 6.3964 (7.4998)	LR 2.000e-03
1: TRAIN [0][390/683]	Time 0.322 (0.347)	Data 8.73e-05 (5.40e-04)	Tok/s 15211 (14956)	Loss/tok 5.9935 (7.4526)	LR 2.000e-03
0: TRAIN [0][390/683]	Time 0.322 (0.347)	Data 9.13e-05 (5.31e-04)	Tok/s 15213 (14953)	Loss/tok 5.9212 (7.4613)	LR 2.000e-03
1: TRAIN [0][400/683]	Time 0.420 (0.348)	Data 9.11e-05 (5.28e-04)	Tok/s 16133 (14964)	Loss/tok 6.0881 (7.4156)	LR 2.000e-03
0: TRAIN [0][400/683]	Time 0.421 (0.348)	Data 9.35e-05 (5.20e-04)	Tok/s 16237 (14962)	Loss/tok 6.1412 (7.4226)	LR 2.000e-03
1: TRAIN [0][410/683]	Time 0.421 (0.349)	Data 1.04e-04 (5.18e-04)	Tok/s 16009 (14969)	Loss/tok 6.0856 (7.3787)	LR 2.000e-03
0: TRAIN [0][410/683]	Time 0.421 (0.349)	Data 1.00e-04 (5.10e-04)	Tok/s 16155 (14969)	Loss/tok 6.1017 (7.3860)	LR 2.000e-03
1: TRAIN [0][420/683]	Time 0.226 (0.348)	Data 9.20e-05 (5.08e-04)	Tok/s 13532 (14963)	Loss/tok 5.4439 (7.3471)	LR 2.000e-03
0: TRAIN [0][420/683]	Time 0.226 (0.348)	Data 9.56e-05 (5.00e-04)	Tok/s 13034 (14963)	Loss/tok 5.4726 (7.3528)	LR 2.000e-03
1: TRAIN [0][430/683]	Time 0.323 (0.348)	Data 9.63e-05 (4.98e-04)	Tok/s 14702 (14960)	Loss/tok 5.7278 (7.3123)	LR 2.000e-03
0: TRAIN [0][430/683]	Time 0.325 (0.348)	Data 1.47e-04 (4.92e-04)	Tok/s 14995 (14961)	Loss/tok 5.6598 (7.3188)	LR 2.000e-03
1: TRAIN [0][440/683]	Time 0.318 (0.347)	Data 9.11e-05 (4.89e-04)	Tok/s 15142 (14939)	Loss/tok 5.7178 (7.2838)	LR 2.000e-03
0: TRAIN [0][440/683]	Time 0.320 (0.347)	Data 1.48e-04 (4.84e-04)	Tok/s 15410 (14940)	Loss/tok 5.7265 (7.2900)	LR 2.000e-03
1: TRAIN [0][450/683]	Time 0.421 (0.347)	Data 9.37e-05 (4.80e-04)	Tok/s 16349 (14951)	Loss/tok 5.8542 (7.2472)	LR 2.000e-03
0: TRAIN [0][450/683]	Time 0.419 (0.347)	Data 1.05e-04 (4.76e-04)	Tok/s 16000 (14949)	Loss/tok 5.8553 (7.2547)	LR 2.000e-03
1: TRAIN [0][460/683]	Time 0.319 (0.348)	Data 8.85e-05 (4.72e-04)	Tok/s 15255 (14958)	Loss/tok 5.6283 (7.2112)	LR 2.000e-03
0: TRAIN [0][460/683]	Time 0.322 (0.348)	Data 1.05e-04 (4.68e-04)	Tok/s 15237 (14957)	Loss/tok 5.5354 (7.2180)	LR 2.000e-03
1: TRAIN [0][470/683]	Time 0.526 (0.348)	Data 1.02e-04 (4.64e-04)	Tok/s 16841 (14954)	Loss/tok 5.9658 (7.1793)	LR 2.000e-03
0: TRAIN [0][470/683]	Time 0.526 (0.348)	Data 1.06e-04 (4.60e-04)	Tok/s 16962 (14952)	Loss/tok 5.8838 (7.1863)	LR 2.000e-03
1: TRAIN [0][480/683]	Time 0.317 (0.347)	Data 8.92e-05 (4.56e-04)	Tok/s 15488 (14950)	Loss/tok 5.4410 (7.1481)	LR 2.000e-03
0: TRAIN [0][480/683]	Time 0.317 (0.347)	Data 1.05e-04 (4.53e-04)	Tok/s 15534 (14948)	Loss/tok 5.5648 (7.1555)	LR 2.000e-03
1: TRAIN [0][490/683]	Time 0.226 (0.346)	Data 8.89e-05 (4.49e-04)	Tok/s 12664 (14941)	Loss/tok 5.0749 (7.1198)	LR 2.000e-03
0: TRAIN [0][490/683]	Time 0.231 (0.346)	Data 1.00e-04 (4.45e-04)	Tok/s 12981 (14939)	Loss/tok 5.1567 (7.1272)	LR 2.000e-03
1: TRAIN [0][500/683]	Time 0.323 (0.346)	Data 1.02e-04 (4.42e-04)	Tok/s 14926 (14942)	Loss/tok 5.3673 (7.0874)	LR 2.000e-03
0: TRAIN [0][500/683]	Time 0.325 (0.346)	Data 9.92e-05 (4.39e-04)	Tok/s 14734 (14940)	Loss/tok 5.3295 (7.0958)	LR 2.000e-03
1: TRAIN [0][510/683]	Time 0.221 (0.346)	Data 9.35e-05 (4.35e-04)	Tok/s 13547 (14943)	Loss/tok 5.0512 (7.0559)	LR 2.000e-03
0: TRAIN [0][510/683]	Time 0.221 (0.346)	Data 1.02e-04 (4.32e-04)	Tok/s 13807 (14940)	Loss/tok 4.9625 (7.0638)	LR 2.000e-03
1: TRAIN [0][520/683]	Time 0.320 (0.344)	Data 8.87e-05 (4.29e-04)	Tok/s 15541 (14930)	Loss/tok 5.2483 (7.0286)	LR 2.000e-03
0: TRAIN [0][520/683]	Time 0.320 (0.344)	Data 8.77e-05 (4.26e-04)	Tok/s 15378 (14926)	Loss/tok 5.3270 (7.0368)	LR 2.000e-03
1: TRAIN [0][530/683]	Time 0.226 (0.345)	Data 8.27e-05 (4.22e-04)	Tok/s 12789 (14935)	Loss/tok 4.9433 (6.9967)	LR 2.000e-03
0: TRAIN [0][530/683]	Time 0.226 (0.345)	Data 1.08e-04 (4.20e-04)	Tok/s 13024 (14931)	Loss/tok 4.7402 (7.0038)	LR 2.000e-03
1: TRAIN [0][540/683]	Time 0.322 (0.344)	Data 8.73e-05 (4.16e-04)	Tok/s 14944 (14921)	Loss/tok 5.0062 (6.9700)	LR 2.000e-03
0: TRAIN [0][540/683]	Time 0.323 (0.344)	Data 9.66e-05 (4.14e-04)	Tok/s 15069 (14916)	Loss/tok 5.0687 (6.9761)	LR 2.000e-03
1: TRAIN [0][550/683]	Time 0.222 (0.343)	Data 9.27e-05 (4.10e-04)	Tok/s 13458 (14906)	Loss/tok 4.8714 (6.9436)	LR 2.000e-03
0: TRAIN [0][550/683]	Time 0.222 (0.343)	Data 9.42e-05 (4.08e-04)	Tok/s 13201 (14899)	Loss/tok 4.7016 (6.9498)	LR 2.000e-03
1: TRAIN [0][560/683]	Time 0.321 (0.343)	Data 9.49e-05 (4.05e-04)	Tok/s 15329 (14907)	Loss/tok 5.1029 (6.9133)	LR 2.000e-03
0: TRAIN [0][560/683]	Time 0.318 (0.343)	Data 8.82e-05 (4.02e-04)	Tok/s 15387 (14899)	Loss/tok 4.9679 (6.9198)	LR 2.000e-03
1: TRAIN [0][570/683]	Time 0.224 (0.343)	Data 9.37e-05 (3.99e-04)	Tok/s 13205 (14905)	Loss/tok 4.7891 (6.8841)	LR 2.000e-03
0: TRAIN [0][570/683]	Time 0.224 (0.343)	Data 9.97e-05 (3.97e-04)	Tok/s 12349 (14895)	Loss/tok 4.5424 (6.8910)	LR 2.000e-03
1: TRAIN [0][580/683]	Time 0.224 (0.343)	Data 9.63e-05 (3.94e-04)	Tok/s 13077 (14900)	Loss/tok 4.7959 (6.8564)	LR 2.000e-03
0: TRAIN [0][580/683]	Time 0.224 (0.343)	Data 1.05e-04 (3.92e-04)	Tok/s 13238 (14893)	Loss/tok 4.7190 (6.8623)	LR 2.000e-03
1: TRAIN [0][590/683]	Time 0.320 (0.342)	Data 9.54e-05 (3.89e-04)	Tok/s 15290 (14902)	Loss/tok 5.0238 (6.8261)	LR 2.000e-03
0: TRAIN [0][590/683]	Time 0.323 (0.342)	Data 1.39e-04 (3.87e-04)	Tok/s 15181 (14894)	Loss/tok 4.9849 (6.8329)	LR 2.000e-03
1: TRAIN [0][600/683]	Time 0.221 (0.343)	Data 8.87e-05 (3.84e-04)	Tok/s 12970 (14901)	Loss/tok 4.7491 (6.7972)	LR 2.000e-03
0: TRAIN [0][600/683]	Time 0.221 (0.343)	Data 1.51e-04 (3.83e-04)	Tok/s 13159 (14896)	Loss/tok 4.6618 (6.8036)	LR 2.000e-03
1: TRAIN [0][610/683]	Time 0.319 (0.342)	Data 9.30e-05 (3.80e-04)	Tok/s 15505 (14893)	Loss/tok 4.9416 (6.7702)	LR 2.000e-03
0: TRAIN [0][610/683]	Time 0.319 (0.342)	Data 8.03e-05 (3.78e-04)	Tok/s 15343 (14889)	Loss/tok 4.9192 (6.7766)	LR 2.000e-03
1: TRAIN [0][620/683]	Time 0.139 (0.342)	Data 9.47e-05 (3.75e-04)	Tok/s 10489 (14887)	Loss/tok 4.3144 (6.7432)	LR 2.000e-03
0: TRAIN [0][620/683]	Time 0.139 (0.342)	Data 9.94e-05 (3.74e-04)	Tok/s 10291 (14881)	Loss/tok 4.1044 (6.7499)	LR 2.000e-03
1: TRAIN [0][630/683]	Time 0.322 (0.342)	Data 1.06e-04 (3.71e-04)	Tok/s 15034 (14888)	Loss/tok 5.0138 (6.7158)	LR 2.000e-03
0: TRAIN [0][630/683]	Time 0.320 (0.342)	Data 9.47e-05 (3.69e-04)	Tok/s 15111 (14883)	Loss/tok 4.8875 (6.7226)	LR 2.000e-03
1: TRAIN [0][640/683]	Time 0.319 (0.341)	Data 9.11e-05 (3.66e-04)	Tok/s 15074 (14885)	Loss/tok 4.8720 (6.6899)	LR 2.000e-03
0: TRAIN [0][640/683]	Time 0.317 (0.341)	Data 8.89e-05 (3.65e-04)	Tok/s 15351 (14879)	Loss/tok 4.7549 (6.6966)	LR 2.000e-03
1: TRAIN [0][650/683]	Time 0.224 (0.341)	Data 9.27e-05 (3.62e-04)	Tok/s 13139 (14885)	Loss/tok 4.4506 (6.6629)	LR 2.000e-03
0: TRAIN [0][650/683]	Time 0.226 (0.341)	Data 9.51e-05 (3.61e-04)	Tok/s 13550 (14879)	Loss/tok 4.4762 (6.6695)	LR 2.000e-03
1: TRAIN [0][660/683]	Time 0.322 (0.341)	Data 9.63e-05 (3.58e-04)	Tok/s 15244 (14887)	Loss/tok 4.6386 (6.6364)	LR 2.000e-03
0: TRAIN [0][660/683]	Time 0.321 (0.341)	Data 8.56e-05 (3.57e-04)	Tok/s 15661 (14882)	Loss/tok 4.6639 (6.6421)	LR 2.000e-03
0: TRAIN [0][670/683]	Time 0.127 (0.341)	Data 1.60e-04 (3.53e-04)	Tok/s 11381 (14877)	Loss/tok 4.1540 (6.6177)	LR 2.000e-03
1: TRAIN [0][670/683]	Time 0.133 (0.341)	Data 9.42e-05 (3.54e-04)	Tok/s 11345 (14882)	Loss/tok 4.0912 (6.6123)	LR 2.000e-03
1: TRAIN [0][680/683]	Time 0.527 (0.341)	Data 4.39e-05 (3.51e-04)	Tok/s 16562 (14880)	Loss/tok 5.1290 (6.5863)	LR 2.000e-03
0: TRAIN [0][680/683]	Time 0.528 (0.341)	Data 3.91e-05 (3.51e-04)	Tok/s 16913 (14878)	Loss/tok 5.1185 (6.5917)	LR 2.000e-03
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [0][0/80]	Time 0.107 (0.107)	Data 1.67e-03 (1.67e-03)	Tok/s 44248 (44248)	Loss/tok 6.3001 (6.3001)
0: VALIDATION [0][0/80]	Time 0.149 (0.149)	Data 1.68e-03 (1.68e-03)	Tok/s 38495 (38495)	Loss/tok 6.3955 (6.3955)
1: VALIDATION [0][10/80]	Time 0.063 (0.076)	Data 1.30e-03 (1.40e-03)	Tok/s 46128 (45988)	Loss/tok 5.9415 (6.1507)
0: VALIDATION [0][10/80]	Time 0.062 (0.082)	Data 1.32e-03 (1.44e-03)	Tok/s 47643 (45564)	Loss/tok 6.0295 (6.1378)
1: VALIDATION [0][20/80]	Time 0.050 (0.066)	Data 1.34e-03 (1.36e-03)	Tok/s 46112 (46187)	Loss/tok 5.6344 (6.0574)
0: VALIDATION [0][20/80]	Time 0.050 (0.070)	Data 1.29e-03 (1.40e-03)	Tok/s 46381 (45861)	Loss/tok 5.6395 (6.0664)
1: VALIDATION [0][30/80]	Time 0.042 (0.059)	Data 1.26e-03 (1.34e-03)	Tok/s 45534 (46354)	Loss/tok 5.7747 (5.9791)
0: VALIDATION [0][30/80]	Time 0.043 (0.062)	Data 1.30e-03 (1.37e-03)	Tok/s 45698 (46086)	Loss/tok 5.6674 (5.9955)
1: VALIDATION [0][40/80]	Time 0.036 (0.054)	Data 1.23e-03 (1.32e-03)	Tok/s 44341 (45939)	Loss/tok 5.7546 (5.9218)
0: VALIDATION [0][40/80]	Time 0.036 (0.056)	Data 1.31e-03 (1.36e-03)	Tok/s 45325 (45889)	Loss/tok 5.6602 (5.9574)
1: VALIDATION [0][50/80]	Time 0.029 (0.050)	Data 1.26e-03 (1.31e-03)	Tok/s 44758 (45770)	Loss/tok 5.6096 (5.8893)
0: VALIDATION [0][50/80]	Time 0.030 (0.052)	Data 1.26e-03 (1.35e-03)	Tok/s 45031 (45634)	Loss/tok 5.7456 (5.9119)
1: VALIDATION [0][60/80]	Time 0.025 (0.046)	Data 1.23e-03 (1.31e-03)	Tok/s 42200 (45360)	Loss/tok 5.4751 (5.8543)
0: VALIDATION [0][60/80]	Time 0.026 (0.048)	Data 1.31e-03 (1.34e-03)	Tok/s 42036 (45262)	Loss/tok 5.6186 (5.8755)
1: VALIDATION [0][70/80]	Time 0.020 (0.043)	Data 1.21e-03 (1.30e-03)	Tok/s 39924 (44846)	Loss/tok 5.4193 (5.8215)
0: VALIDATION [0][70/80]	Time 0.020 (0.044)	Data 1.24e-03 (1.33e-03)	Tok/s 39605 (44732)	Loss/tok 5.3240 (5.8450)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
1: Running evaluation on test set
1: TEST [0][9/47]	Time 0.6958 (0.9435)	Decoder iters 149.0 (149.0)	Tok/s 4650 (4335)
0: TEST [0][9/47]	Time 0.6956 (0.9432)	Decoder iters 149.0 (149.0)	Tok/s 4037 (4400)
1: TEST [0][19/47]	Time 0.6365 (0.8021)	Decoder iters 149.0 (149.0)	Tok/s 3430 (4016)
0: TEST [0][19/47]	Time 0.6339 (0.8017)	Decoder iters 149.0 (149.0)	Tok/s 3595 (4196)
1: TEST [0][29/47]	Time 0.4960 (0.7188)	Decoder iters 149.0 (149.0)	Tok/s 3018 (3773)
0: TEST [0][29/47]	Time 0.4960 (0.7184)	Decoder iters 149.0 (149.0)	Tok/s 3056 (3900)
1: TEST [0][39/47]	Time 0.4370 (0.6561)	Decoder iters 149.0 (142.3)	Tok/s 2153 (3462)
0: TEST [0][39/47]	Time 0.4311 (0.6558)	Decoder iters 149.0 (147.2)	Tok/s 2292 (3581)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.5828	Validation Loss: 5.8020	Test BLEU: 2.62
0: Performance: Epoch: 0	Training: 29763 Tok/s	Validation: 87804 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
1: Sampler for epoch 1 uses seed 1323436024
0: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/683]	Time 0.575 (0.575)	Data 1.68e-01 (1.68e-01)	Tok/s 11691 (11691)	Loss/tok 4.7634 (4.7634)	LR 2.000e-03
0: TRAIN [1][0/683]	Time 0.565 (0.565)	Data 1.59e-01 (1.59e-01)	Tok/s 11931 (11931)	Loss/tok 4.8130 (4.8130)	LR 2.000e-03
1: TRAIN [1][10/683]	Time 0.221 (0.408)	Data 8.85e-05 (1.53e-02)	Tok/s 13605 (15352)	Loss/tok 4.1402 (4.7222)	LR 2.000e-03
0: TRAIN [1][10/683]	Time 0.221 (0.407)	Data 1.06e-04 (1.45e-02)	Tok/s 13228 (15279)	Loss/tok 4.0181 (4.7079)	LR 2.000e-03
0: TRAIN [1][20/683]	Time 0.317 (0.399)	Data 1.13e-04 (7.66e-03)	Tok/s 15248 (15536)	Loss/tok 4.4900 (4.7145)	LR 2.000e-03
1: TRAIN [1][20/683]	Time 0.319 (0.400)	Data 1.09e-04 (8.07e-03)	Tok/s 15298 (15599)	Loss/tok 4.4855 (4.7094)	LR 2.000e-03
1: TRAIN [1][30/683]	Time 0.420 (0.395)	Data 8.08e-05 (5.49e-03)	Tok/s 16379 (15489)	Loss/tok 4.8283 (4.7276)	LR 2.000e-03
0: TRAIN [1][30/683]	Time 0.418 (0.395)	Data 9.73e-05 (5.23e-03)	Tok/s 16316 (15447)	Loss/tok 4.5546 (4.7220)	LR 2.000e-03
1: TRAIN [1][40/683]	Time 0.224 (0.377)	Data 8.42e-05 (4.17e-03)	Tok/s 12879 (15275)	Loss/tok 4.1039 (4.6753)	LR 2.000e-03
0: TRAIN [1][40/683]	Time 0.224 (0.377)	Data 9.51e-05 (3.98e-03)	Tok/s 13636 (15258)	Loss/tok 3.9182 (4.6844)	LR 2.000e-03
1: TRAIN [1][50/683]	Time 0.417 (0.374)	Data 7.68e-05 (3.37e-03)	Tok/s 16125 (15252)	Loss/tok 4.4641 (4.6518)	LR 2.000e-03
0: TRAIN [1][50/683]	Time 0.417 (0.374)	Data 8.70e-05 (3.22e-03)	Tok/s 16350 (15258)	Loss/tok 4.6161 (4.6695)	LR 2.000e-03
0: TRAIN [1][60/683]	Time 0.134 (0.369)	Data 9.18e-05 (2.71e-03)	Tok/s 11120 (15202)	Loss/tok 3.6297 (4.6540)	LR 2.000e-03
1: TRAIN [1][60/683]	Time 0.139 (0.369)	Data 8.18e-05 (2.83e-03)	Tok/s 10404 (15196)	Loss/tok 3.8305 (4.6383)	LR 2.000e-03
1: TRAIN [1][70/683]	Time 0.420 (0.369)	Data 8.49e-05 (2.45e-03)	Tok/s 16061 (15244)	Loss/tok 4.5504 (4.6179)	LR 2.000e-03
0: TRAIN [1][70/683]	Time 0.418 (0.369)	Data 9.70e-05 (2.34e-03)	Tok/s 16158 (15242)	Loss/tok 4.7684 (4.6408)	LR 2.000e-03
1: TRAIN [1][80/683]	Time 0.313 (0.367)	Data 1.39e-04 (2.16e-03)	Tok/s 15988 (15231)	Loss/tok 4.3695 (4.6103)	LR 2.000e-03
0: TRAIN [1][80/683]	Time 0.316 (0.367)	Data 9.08e-05 (2.06e-03)	Tok/s 15329 (15213)	Loss/tok 4.4247 (4.6269)	LR 2.000e-03
1: TRAIN [1][90/683]	Time 0.319 (0.362)	Data 7.82e-05 (1.93e-03)	Tok/s 15087 (15187)	Loss/tok 4.3375 (4.5921)	LR 2.000e-03
0: TRAIN [1][90/683]	Time 0.319 (0.362)	Data 9.85e-05 (1.85e-03)	Tok/s 15320 (15160)	Loss/tok 4.1085 (4.6039)	LR 2.000e-03
0: TRAIN [1][100/683]	Time 0.323 (0.358)	Data 9.73e-05 (1.68e-03)	Tok/s 14983 (15152)	Loss/tok 4.2755 (4.5798)	LR 2.000e-03
1: TRAIN [1][100/683]	Time 0.323 (0.358)	Data 1.36e-04 (1.75e-03)	Tok/s 14875 (15169)	Loss/tok 4.4543 (4.5719)	LR 2.000e-03
1: TRAIN [1][110/683]	Time 0.222 (0.350)	Data 7.89e-05 (1.60e-03)	Tok/s 13086 (15072)	Loss/tok 3.9905 (4.5495)	LR 2.000e-03
0: TRAIN [1][110/683]	Time 0.222 (0.350)	Data 9.47e-05 (1.53e-03)	Tok/s 13464 (15055)	Loss/tok 3.9604 (4.5527)	LR 2.000e-03
1: TRAIN [1][120/683]	Time 0.424 (0.350)	Data 8.42e-05 (1.47e-03)	Tok/s 16043 (15077)	Loss/tok 4.5866 (4.5435)	LR 2.000e-03
0: TRAIN [1][120/683]	Time 0.424 (0.350)	Data 9.61e-05 (1.42e-03)	Tok/s 16185 (15075)	Loss/tok 4.6555 (4.5461)	LR 2.000e-03
0: TRAIN [1][130/683]	Time 0.419 (0.349)	Data 1.03e-04 (1.32e-03)	Tok/s 16022 (15069)	Loss/tok 4.6416 (4.5346)	LR 2.000e-03
1: TRAIN [1][130/683]	Time 0.420 (0.349)	Data 8.44e-05 (1.37e-03)	Tok/s 16104 (15070)	Loss/tok 4.5644 (4.5326)	LR 2.000e-03
1: TRAIN [1][140/683]	Time 0.226 (0.350)	Data 8.99e-05 (1.28e-03)	Tok/s 12898 (15079)	Loss/tok 3.6485 (4.5258)	LR 2.000e-03
0: TRAIN [1][140/683]	Time 0.228 (0.350)	Data 1.56e-04 (1.23e-03)	Tok/s 13307 (15085)	Loss/tok 4.0011 (4.5276)	LR 2.000e-03
1: TRAIN [1][150/683]	Time 0.319 (0.351)	Data 7.99e-05 (1.20e-03)	Tok/s 15189 (15085)	Loss/tok 4.2453 (4.5205)	LR 2.000e-03
0: TRAIN [1][150/683]	Time 0.319 (0.351)	Data 9.51e-05 (1.16e-03)	Tok/s 15241 (15090)	Loss/tok 4.2726 (4.5230)	LR 2.000e-03
1: TRAIN [1][160/683]	Time 0.422 (0.351)	Data 8.32e-05 (1.13e-03)	Tok/s 15881 (15083)	Loss/tok 4.3840 (4.5110)	LR 2.000e-03
0: TRAIN [1][160/683]	Time 0.422 (0.351)	Data 1.02e-04 (1.09e-03)	Tok/s 16038 (15090)	Loss/tok 4.5297 (4.5200)	LR 2.000e-03
1: TRAIN [1][170/683]	Time 0.418 (0.350)	Data 7.65e-05 (1.07e-03)	Tok/s 16211 (15066)	Loss/tok 4.5584 (4.5016)	LR 2.000e-03
0: TRAIN [1][170/683]	Time 0.418 (0.350)	Data 9.94e-05 (1.03e-03)	Tok/s 16343 (15079)	Loss/tok 4.6152 (4.5122)	LR 2.000e-03
1: TRAIN [1][180/683]	Time 0.422 (0.353)	Data 9.04e-05 (1.01e-03)	Tok/s 16065 (15096)	Loss/tok 4.4917 (4.5043)	LR 2.000e-03
0: TRAIN [1][180/683]	Time 0.422 (0.353)	Data 1.05e-04 (9.81e-04)	Tok/s 16222 (15108)	Loss/tok 4.4926 (4.5137)	LR 2.000e-03
1: TRAIN [1][190/683]	Time 0.225 (0.354)	Data 8.32e-05 (9.65e-04)	Tok/s 13008 (15116)	Loss/tok 3.8042 (4.4955)	LR 2.000e-03
0: TRAIN [1][190/683]	Time 0.225 (0.354)	Data 9.94e-05 (9.35e-04)	Tok/s 12817 (15125)	Loss/tok 3.8634 (4.5069)	LR 2.000e-03
1: TRAIN [1][200/683]	Time 0.420 (0.354)	Data 8.27e-05 (9.22e-04)	Tok/s 15984 (15119)	Loss/tok 4.2911 (4.4874)	LR 2.000e-03
0: TRAIN [1][200/683]	Time 0.420 (0.354)	Data 9.87e-05 (8.94e-04)	Tok/s 16081 (15128)	Loss/tok 4.4193 (4.4980)	LR 2.000e-03
0: TRAIN [1][210/683]	Time 0.134 (0.349)	Data 9.42e-05 (8.56e-04)	Tok/s 10829 (15059)	Loss/tok 3.8314 (4.4842)	LR 2.000e-03
1: TRAIN [1][210/683]	Time 0.132 (0.350)	Data 9.04e-05 (8.82e-04)	Tok/s 11324 (15065)	Loss/tok 3.7549 (4.4739)	LR 2.000e-03
0: TRAIN [1][220/683]	Time 0.319 (0.347)	Data 1.03e-04 (8.22e-04)	Tok/s 15139 (15031)	Loss/tok 4.1823 (4.4736)	LR 2.000e-03
1: TRAIN [1][220/683]	Time 0.323 (0.347)	Data 8.73e-05 (8.46e-04)	Tok/s 15045 (15036)	Loss/tok 4.2001 (4.4644)	LR 2.000e-03
1: TRAIN [1][230/683]	Time 0.220 (0.347)	Data 1.32e-04 (8.13e-04)	Tok/s 13194 (15029)	Loss/tok 3.8256 (4.4572)	LR 1.000e-03
0: TRAIN [1][230/683]	Time 0.224 (0.347)	Data 9.99e-05 (7.92e-04)	Tok/s 12686 (15021)	Loss/tok 3.8932 (4.4654)	LR 1.000e-03
1: TRAIN [1][240/683]	Time 0.533 (0.348)	Data 8.01e-05 (7.83e-04)	Tok/s 16496 (15028)	Loss/tok 4.5563 (4.4544)	LR 1.000e-03
0: TRAIN [1][240/683]	Time 0.534 (0.348)	Data 9.87e-05 (7.63e-04)	Tok/s 16459 (15019)	Loss/tok 4.7004 (4.4625)	LR 1.000e-03
1: TRAIN [1][250/683]	Time 0.322 (0.349)	Data 8.44e-05 (7.56e-04)	Tok/s 15271 (15048)	Loss/tok 3.9071 (4.4445)	LR 1.000e-03
0: TRAIN [1][250/683]	Time 0.322 (0.349)	Data 9.87e-05 (7.37e-04)	Tok/s 15476 (15041)	Loss/tok 4.1312 (4.4558)	LR 1.000e-03
1: TRAIN [1][260/683]	Time 0.423 (0.349)	Data 8.37e-05 (7.30e-04)	Tok/s 15960 (15048)	Loss/tok 4.2290 (4.4325)	LR 1.000e-03
0: TRAIN [1][260/683]	Time 0.423 (0.349)	Data 1.06e-04 (7.13e-04)	Tok/s 16035 (15041)	Loss/tok 4.3713 (4.4453)	LR 1.000e-03
1: TRAIN [1][270/683]	Time 0.318 (0.349)	Data 9.04e-05 (7.06e-04)	Tok/s 15240 (15045)	Loss/tok 3.9220 (4.4232)	LR 1.000e-03
0: TRAIN [1][270/683]	Time 0.318 (0.349)	Data 1.09e-04 (6.90e-04)	Tok/s 15189 (15036)	Loss/tok 4.0449 (4.4342)	LR 1.000e-03
1: TRAIN [1][280/683]	Time 0.225 (0.348)	Data 8.63e-05 (6.84e-04)	Tok/s 13294 (15031)	Loss/tok 3.8361 (4.4151)	LR 1.000e-03
0: TRAIN [1][280/683]	Time 0.225 (0.348)	Data 9.92e-05 (6.69e-04)	Tok/s 12618 (15020)	Loss/tok 3.7230 (4.4248)	LR 1.000e-03
0: TRAIN [1][290/683]	Time 0.417 (0.347)	Data 1.05e-04 (6.50e-04)	Tok/s 16144 (15014)	Loss/tok 4.2774 (4.4144)	LR 1.000e-03
1: TRAIN [1][290/683]	Time 0.420 (0.347)	Data 8.46e-05 (6.64e-04)	Tok/s 16089 (15029)	Loss/tok 4.2221 (4.4039)	LR 1.000e-03
0: TRAIN [1][300/683]	Time 0.534 (0.346)	Data 9.04e-05 (6.32e-04)	Tok/s 16422 (14985)	Loss/tok 4.3801 (4.4076)	LR 1.000e-03
1: TRAIN [1][300/683]	Time 0.535 (0.346)	Data 8.80e-05 (6.44e-04)	Tok/s 16528 (15001)	Loss/tok 4.5471 (4.3970)	LR 1.000e-03
1: TRAIN [1][310/683]	Time 0.222 (0.345)	Data 1.29e-04 (6.27e-04)	Tok/s 13266 (14980)	Loss/tok 3.7131 (4.3881)	LR 1.000e-03
0: TRAIN [1][310/683]	Time 0.226 (0.345)	Data 1.00e-04 (6.15e-04)	Tok/s 13360 (14966)	Loss/tok 3.5209 (4.3985)	LR 1.000e-03
0: TRAIN [1][320/683]	Time 0.323 (0.344)	Data 8.85e-05 (5.99e-04)	Tok/s 15201 (14951)	Loss/tok 3.8625 (4.3878)	LR 1.000e-03
1: TRAIN [1][320/683]	Time 0.328 (0.344)	Data 7.72e-05 (6.10e-04)	Tok/s 15037 (14971)	Loss/tok 4.0465 (4.3773)	LR 1.000e-03
1: TRAIN [1][330/683]	Time 0.319 (0.341)	Data 8.32e-05 (5.94e-04)	Tok/s 15396 (14937)	Loss/tok 4.0434 (4.3652)	LR 1.000e-03
0: TRAIN [1][330/683]	Time 0.319 (0.341)	Data 9.78e-05 (5.84e-04)	Tok/s 15230 (14917)	Loss/tok 3.9388 (4.3769)	LR 1.000e-03
1: TRAIN [1][340/683]	Time 0.319 (0.341)	Data 9.04e-05 (5.80e-04)	Tok/s 15317 (14928)	Loss/tok 3.8856 (4.3565)	LR 5.000e-04
0: TRAIN [1][340/683]	Time 0.319 (0.341)	Data 1.03e-04 (5.70e-04)	Tok/s 15241 (14904)	Loss/tok 3.9663 (4.3686)	LR 5.000e-04
1: TRAIN [1][350/683]	Time 0.529 (0.342)	Data 8.85e-05 (5.66e-04)	Tok/s 16766 (14945)	Loss/tok 4.4045 (4.3525)	LR 5.000e-04
0: TRAIN [1][350/683]	Time 0.529 (0.342)	Data 1.04e-04 (5.57e-04)	Tok/s 16815 (14922)	Loss/tok 4.3058 (4.3608)	LR 5.000e-04
1: TRAIN [1][360/683]	Time 0.226 (0.342)	Data 8.34e-05 (5.53e-04)	Tok/s 12595 (14939)	Loss/tok 3.6925 (4.3424)	LR 5.000e-04
0: TRAIN [1][360/683]	Time 0.223 (0.342)	Data 9.89e-05 (5.44e-04)	Tok/s 13656 (14917)	Loss/tok 3.5431 (4.3518)	LR 5.000e-04
1: TRAIN [1][370/683]	Time 0.317 (0.341)	Data 8.37e-05 (5.40e-04)	Tok/s 15384 (14934)	Loss/tok 3.8466 (4.3322)	LR 5.000e-04
0: TRAIN [1][370/683]	Time 0.317 (0.341)	Data 9.87e-05 (5.33e-04)	Tok/s 15222 (14913)	Loss/tok 3.8745 (4.3422)	LR 5.000e-04
1: TRAIN [1][380/683]	Time 0.419 (0.342)	Data 8.65e-05 (5.28e-04)	Tok/s 16210 (14950)	Loss/tok 4.1212 (4.3268)	LR 5.000e-04
0: TRAIN [1][380/683]	Time 0.419 (0.342)	Data 9.61e-05 (5.21e-04)	Tok/s 16084 (14932)	Loss/tok 4.0963 (4.3344)	LR 5.000e-04
1: TRAIN [1][390/683]	Time 0.423 (0.342)	Data 9.08e-05 (5.17e-04)	Tok/s 16117 (14955)	Loss/tok 4.0539 (4.3185)	LR 5.000e-04
0: TRAIN [1][390/683]	Time 0.423 (0.342)	Data 1.11e-04 (5.10e-04)	Tok/s 16470 (14933)	Loss/tok 4.0246 (4.3255)	LR 5.000e-04
1: TRAIN [1][400/683]	Time 0.525 (0.341)	Data 1.14e-04 (5.06e-04)	Tok/s 16634 (14949)	Loss/tok 4.3232 (4.3110)	LR 5.000e-04
0: TRAIN [1][400/683]	Time 0.525 (0.341)	Data 1.03e-04 (5.01e-04)	Tok/s 16577 (14928)	Loss/tok 4.2447 (4.3164)	LR 5.000e-04
1: TRAIN [1][410/683]	Time 0.419 (0.341)	Data 8.70e-05 (4.96e-04)	Tok/s 16299 (14949)	Loss/tok 3.9323 (4.3031)	LR 5.000e-04
0: TRAIN [1][410/683]	Time 0.417 (0.341)	Data 1.02e-04 (4.92e-04)	Tok/s 16337 (14927)	Loss/tok 4.0712 (4.3089)	LR 5.000e-04
1: TRAIN [1][420/683]	Time 0.226 (0.341)	Data 8.56e-05 (4.86e-04)	Tok/s 13094 (14935)	Loss/tok 3.5831 (4.2968)	LR 5.000e-04
0: TRAIN [1][420/683]	Time 0.227 (0.341)	Data 1.04e-04 (4.82e-04)	Tok/s 12822 (14914)	Loss/tok 3.5997 (4.3010)	LR 5.000e-04
1: TRAIN [1][430/683]	Time 0.317 (0.340)	Data 8.89e-05 (4.77e-04)	Tok/s 15622 (14926)	Loss/tok 3.7729 (4.2885)	LR 5.000e-04
0: TRAIN [1][430/683]	Time 0.317 (0.340)	Data 1.02e-04 (4.74e-04)	Tok/s 15351 (14907)	Loss/tok 3.7550 (4.2929)	LR 5.000e-04
1: TRAIN [1][440/683]	Time 0.225 (0.340)	Data 9.06e-05 (4.68e-04)	Tok/s 12909 (14915)	Loss/tok 3.6210 (4.2823)	LR 5.000e-04
0: TRAIN [1][440/683]	Time 0.226 (0.340)	Data 1.06e-04 (4.65e-04)	Tok/s 13352 (14898)	Loss/tok 3.8161 (4.2877)	LR 5.000e-04
1: TRAIN [1][450/683]	Time 0.320 (0.340)	Data 8.68e-05 (4.60e-04)	Tok/s 15282 (14906)	Loss/tok 3.7871 (4.2758)	LR 5.000e-04
0: TRAIN [1][450/683]	Time 0.320 (0.340)	Data 1.05e-04 (4.57e-04)	Tok/s 15282 (14891)	Loss/tok 3.9175 (4.2805)	LR 5.000e-04
1: TRAIN [1][460/683]	Time 0.134 (0.340)	Data 8.96e-05 (4.52e-04)	Tok/s 10628 (14908)	Loss/tok 3.3866 (4.2692)	LR 2.500e-04
0: TRAIN [1][460/683]	Time 0.134 (0.340)	Data 1.11e-04 (4.50e-04)	Tok/s 10701 (14892)	Loss/tok 3.2421 (4.2749)	LR 2.500e-04
1: TRAIN [1][470/683]	Time 0.224 (0.340)	Data 8.13e-05 (4.44e-04)	Tok/s 13486 (14916)	Loss/tok 3.5899 (4.2619)	LR 2.500e-04
0: TRAIN [1][470/683]	Time 0.224 (0.340)	Data 9.82e-05 (4.43e-04)	Tok/s 13136 (14902)	Loss/tok 3.4673 (4.2675)	LR 2.500e-04
1: TRAIN [1][480/683]	Time 0.320 (0.340)	Data 8.96e-05 (4.37e-04)	Tok/s 15483 (14913)	Loss/tok 3.7500 (4.2545)	LR 2.500e-04
0: TRAIN [1][480/683]	Time 0.320 (0.340)	Data 9.70e-05 (4.36e-04)	Tok/s 15145 (14900)	Loss/tok 3.7659 (4.2601)	LR 2.500e-04
1: TRAIN [1][490/683]	Time 0.226 (0.339)	Data 8.65e-05 (4.30e-04)	Tok/s 13140 (14906)	Loss/tok 3.4941 (4.2489)	LR 2.500e-04
0: TRAIN [1][490/683]	Time 0.226 (0.339)	Data 1.02e-04 (4.29e-04)	Tok/s 12981 (14896)	Loss/tok 3.6201 (4.2543)	LR 2.500e-04
1: TRAIN [1][500/683]	Time 0.226 (0.340)	Data 9.44e-05 (4.23e-04)	Tok/s 13274 (14907)	Loss/tok 3.5583 (4.2427)	LR 2.500e-04
0: TRAIN [1][500/683]	Time 0.226 (0.340)	Data 1.01e-04 (4.22e-04)	Tok/s 13350 (14896)	Loss/tok 3.5920 (4.2493)	LR 2.500e-04
0: TRAIN [1][510/683]	Time 0.317 (0.340)	Data 9.39e-05 (4.16e-04)	Tok/s 15727 (14893)	Loss/tok 3.8322 (4.2434)	LR 2.500e-04
1: TRAIN [1][510/683]	Time 0.320 (0.340)	Data 7.92e-05 (4.16e-04)	Tok/s 15411 (14901)	Loss/tok 3.9302 (4.2369)	LR 2.500e-04
1: TRAIN [1][520/683]	Time 0.420 (0.340)	Data 8.54e-05 (4.10e-04)	Tok/s 15978 (14910)	Loss/tok 3.9808 (4.2311)	LR 2.500e-04
0: TRAIN [1][520/683]	Time 0.420 (0.340)	Data 9.30e-05 (4.10e-04)	Tok/s 16286 (14902)	Loss/tok 4.0693 (4.2379)	LR 2.500e-04
1: TRAIN [1][530/683]	Time 0.532 (0.340)	Data 8.01e-05 (4.04e-04)	Tok/s 16728 (14912)	Loss/tok 4.1876 (4.2255)	LR 2.500e-04
0: TRAIN [1][530/683]	Time 0.535 (0.340)	Data 7.92e-05 (4.04e-04)	Tok/s 16357 (14903)	Loss/tok 4.2311 (4.2309)	LR 2.500e-04
1: TRAIN [1][540/683]	Time 0.320 (0.339)	Data 8.18e-05 (3.98e-04)	Tok/s 15258 (14901)	Loss/tok 3.8817 (4.2192)	LR 2.500e-04
0: TRAIN [1][540/683]	Time 0.319 (0.339)	Data 8.96e-05 (3.98e-04)	Tok/s 15575 (14892)	Loss/tok 3.7801 (4.2247)	LR 2.500e-04
1: TRAIN [1][550/683]	Time 0.422 (0.341)	Data 9.04e-05 (3.93e-04)	Tok/s 16095 (14921)	Loss/tok 4.1529 (4.2154)	LR 2.500e-04
0: TRAIN [1][550/683]	Time 0.424 (0.341)	Data 9.61e-05 (3.93e-04)	Tok/s 15956 (14912)	Loss/tok 4.0076 (4.2207)	LR 2.500e-04
1: TRAIN [1][560/683]	Time 0.319 (0.341)	Data 9.16e-05 (3.87e-04)	Tok/s 15137 (14932)	Loss/tok 3.7069 (4.2093)	LR 2.500e-04
0: TRAIN [1][560/683]	Time 0.319 (0.341)	Data 1.03e-04 (3.88e-04)	Tok/s 15019 (14920)	Loss/tok 3.7979 (4.2155)	LR 2.500e-04
1: TRAIN [1][570/683]	Time 0.224 (0.341)	Data 8.82e-05 (3.82e-04)	Tok/s 12840 (14934)	Loss/tok 3.5340 (4.2027)	LR 1.250e-04
0: TRAIN [1][570/683]	Time 0.222 (0.341)	Data 1.11e-04 (3.83e-04)	Tok/s 13551 (14926)	Loss/tok 3.7262 (4.2094)	LR 1.250e-04
1: TRAIN [1][580/683]	Time 0.134 (0.340)	Data 8.92e-05 (3.77e-04)	Tok/s 10808 (14929)	Loss/tok 3.2094 (4.1975)	LR 1.250e-04
0: TRAIN [1][580/683]	Time 0.134 (0.340)	Data 1.03e-04 (3.78e-04)	Tok/s 10962 (14922)	Loss/tok 3.1217 (4.2042)	LR 1.250e-04
1: TRAIN [1][590/683]	Time 0.422 (0.339)	Data 9.32e-05 (3.72e-04)	Tok/s 16308 (14914)	Loss/tok 3.9370 (4.1918)	LR 1.250e-04
0: TRAIN [1][590/683]	Time 0.423 (0.339)	Data 8.87e-05 (3.73e-04)	Tok/s 15899 (14907)	Loss/tok 4.1070 (4.1982)	LR 1.250e-04
1: TRAIN [1][600/683]	Time 0.532 (0.340)	Data 8.18e-05 (3.67e-04)	Tok/s 16668 (14923)	Loss/tok 4.0479 (4.1876)	LR 1.250e-04
0: TRAIN [1][600/683]	Time 0.532 (0.340)	Data 1.04e-04 (3.68e-04)	Tok/s 16566 (14914)	Loss/tok 4.2196 (4.1951)	LR 1.250e-04
1: TRAIN [1][610/683]	Time 0.535 (0.340)	Data 8.87e-05 (3.63e-04)	Tok/s 16575 (14917)	Loss/tok 4.1423 (4.1826)	LR 1.250e-04
0: TRAIN [1][610/683]	Time 0.535 (0.340)	Data 1.05e-04 (3.64e-04)	Tok/s 16393 (14909)	Loss/tok 4.1837 (4.1899)	LR 1.250e-04
0: TRAIN [1][620/683]	Time 0.144 (0.340)	Data 1.27e-04 (3.60e-04)	Tok/s 9717 (14909)	Loss/tok 3.0054 (4.1868)	LR 1.250e-04
1: TRAIN [1][620/683]	Time 0.147 (0.340)	Data 9.99e-05 (3.58e-04)	Tok/s 10080 (14917)	Loss/tok 3.1565 (4.1789)	LR 1.250e-04
1: TRAIN [1][630/683]	Time 0.317 (0.341)	Data 8.23e-05 (3.54e-04)	Tok/s 15319 (14923)	Loss/tok 3.8481 (4.1745)	LR 1.250e-04
0: TRAIN [1][630/683]	Time 0.317 (0.340)	Data 9.54e-05 (3.56e-04)	Tok/s 15583 (14917)	Loss/tok 3.7591 (4.1821)	LR 1.250e-04
1: TRAIN [1][640/683]	Time 0.318 (0.341)	Data 8.68e-05 (3.50e-04)	Tok/s 15296 (14930)	Loss/tok 3.8345 (4.1695)	LR 1.250e-04
0: TRAIN [1][640/683]	Time 0.318 (0.341)	Data 1.05e-04 (3.52e-04)	Tok/s 15293 (14924)	Loss/tok 3.8147 (4.1768)	LR 1.250e-04
1: TRAIN [1][650/683]	Time 0.318 (0.341)	Data 7.87e-05 (3.46e-04)	Tok/s 14981 (14942)	Loss/tok 3.7097 (4.1653)	LR 1.250e-04
0: TRAIN [1][650/683]	Time 0.318 (0.341)	Data 9.75e-05 (3.48e-04)	Tok/s 15788 (14938)	Loss/tok 3.9227 (4.1723)	LR 1.250e-04
1: TRAIN [1][660/683]	Time 0.318 (0.341)	Data 7.94e-05 (3.42e-04)	Tok/s 14910 (14930)	Loss/tok 3.8223 (4.1622)	LR 1.250e-04
0: TRAIN [1][660/683]	Time 0.321 (0.341)	Data 8.89e-05 (3.44e-04)	Tok/s 15088 (14925)	Loss/tok 3.7274 (4.1685)	LR 1.250e-04
1: TRAIN [1][670/683]	Time 0.533 (0.341)	Data 8.44e-05 (3.38e-04)	Tok/s 16403 (14929)	Loss/tok 4.2220 (4.1582)	LR 1.250e-04
0: TRAIN [1][670/683]	Time 0.533 (0.341)	Data 9.47e-05 (3.41e-04)	Tok/s 16923 (14925)	Loss/tok 4.1760 (4.1647)	LR 1.250e-04
1: TRAIN [1][680/683]	Time 0.317 (0.341)	Data 3.12e-05 (3.36e-04)	Tok/s 15336 (14925)	Loss/tok 3.6633 (4.1544)	LR 1.250e-04
0: TRAIN [1][680/683]	Time 0.317 (0.341)	Data 3.65e-05 (3.38e-04)	Tok/s 15315 (14919)	Loss/tok 3.6854 (4.1610)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [1][0/80]	Time 0.109 (0.109)	Data 2.71e-03 (2.71e-03)	Tok/s 43668 (43668)	Loss/tok 5.4454 (5.4454)
0: VALIDATION [1][0/80]	Time 0.149 (0.149)	Data 1.63e-03 (1.63e-03)	Tok/s 38308 (38308)	Loss/tok 5.5757 (5.5757)
1: VALIDATION [1][10/80]	Time 0.063 (0.077)	Data 1.31e-03 (1.64e-03)	Tok/s 45994 (45873)	Loss/tok 4.9902 (5.2404)
0: VALIDATION [1][10/80]	Time 0.062 (0.082)	Data 1.33e-03 (1.44e-03)	Tok/s 47176 (45534)	Loss/tok 5.0729 (5.2213)
1: VALIDATION [1][20/80]	Time 0.050 (0.066)	Data 1.25e-03 (1.48e-03)	Tok/s 46234 (46171)	Loss/tok 4.7137 (5.1462)
0: VALIDATION [1][20/80]	Time 0.050 (0.070)	Data 1.35e-03 (1.39e-03)	Tok/s 46847 (45756)	Loss/tok 4.7657 (5.1620)
1: VALIDATION [1][30/80]	Time 0.042 (0.059)	Data 1.24e-03 (1.41e-03)	Tok/s 46039 (46365)	Loss/tok 4.9146 (5.0727)
0: VALIDATION [1][30/80]	Time 0.043 (0.062)	Data 1.32e-03 (1.37e-03)	Tok/s 45812 (45984)	Loss/tok 4.7959 (5.0953)
1: VALIDATION [1][40/80]	Time 0.036 (0.055)	Data 1.23e-03 (1.38e-03)	Tok/s 44491 (45849)	Loss/tok 4.7941 (5.0137)
0: VALIDATION [1][40/80]	Time 0.036 (0.056)	Data 1.27e-03 (1.35e-03)	Tok/s 44602 (45818)	Loss/tok 4.6454 (5.0613)
1: VALIDATION [1][50/80]	Time 0.029 (0.050)	Data 1.27e-03 (1.36e-03)	Tok/s 44524 (45694)	Loss/tok 4.8719 (4.9899)
0: VALIDATION [1][50/80]	Time 0.029 (0.052)	Data 1.32e-03 (1.35e-03)	Tok/s 45667 (45603)	Loss/tok 4.9301 (5.0201)
1: VALIDATION [1][60/80]	Time 0.025 (0.046)	Data 1.30e-03 (1.35e-03)	Tok/s 42524 (45260)	Loss/tok 4.6083 (4.9611)
0: VALIDATION [1][60/80]	Time 0.026 (0.048)	Data 1.30e-03 (1.34e-03)	Tok/s 42130 (45225)	Loss/tok 4.7275 (4.9866)
1: VALIDATION [1][70/80]	Time 0.020 (0.043)	Data 1.26e-03 (1.34e-03)	Tok/s 39342 (44759)	Loss/tok 4.6114 (4.9322)
0: VALIDATION [1][70/80]	Time 0.020 (0.044)	Data 1.22e-03 (1.34e-03)	Tok/s 39589 (44714)	Loss/tok 4.5358 (4.9618)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [1][9/47]	Time 0.5718 (0.7177)	Decoder iters 149.0 (149.0)	Tok/s 4731 (4687)
0: TEST [1][9/47]	Time 0.5716 (0.7177)	Decoder iters 64.0 (134.1)	Tok/s 4571 (4811)
1: TEST [1][19/47]	Time 0.5158 (0.6331)	Decoder iters 50.0 (126.3)	Tok/s 3889 (4380)
0: TEST [1][19/47]	Time 0.5159 (0.6331)	Decoder iters 149.0 (135.3)	Tok/s 3831 (4528)
1: TEST [1][29/47]	Time 0.5073 (0.5685)	Decoder iters 42.0 (111.1)	Tok/s 2835 (4292)
0: TEST [1][29/47]	Time 0.5069 (0.5684)	Decoder iters 149.0 (122.8)	Tok/s 2963 (4411)
1: TEST [1][39/47]	Time 0.1637 (0.4843)	Decoder iters 24.0 (95.7)	Tok/s 6280 (4642)
0: TEST [1][39/47]	Time 0.1634 (0.4840)	Decoder iters 31.0 (101.8)	Tok/s 6132 (4748)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.1564	Validation Loss: 4.9206	Test BLEU: 7.69
0: Performance: Epoch: 1	Training: 29841 Tok/s	Validation: 87638 Tok/s
0: Finished epoch 1
1: Total training time 554 s
0: Total training time 554 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 108|                      7.69|                      29802.3|                         9.225|
DONE!
