:::NVLOGv0.2.2 Tacotron2_PyT 1593111808.657912731 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593111808.685851336 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593111808.705298901 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593111809.530880690 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 4, "name": ["Quadro RTX 8000", "Quadro RTX 8000", "Quadro RTX 8000", "Quadro RTX 8000"], "mem": ["48601 MiB", "48601 MiB", "48601 MiB", "48601 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593111809.535852194 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 26, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 4, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593111811.257105350 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593111836.503228188 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593111836.504791260 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111836.911353827 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111840.956569910 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021027610637247562
:::NVLOGv0.2.2 Tacotron2_PyT 1593111845.564737797 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111845.565262079 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 96117.7657846284
:::NVLOGv0.2.2 Tacotron2_PyT 1593111845.565633297 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.656048059463501
Batch: 1/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111845.569130421 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111847.007050037 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019730746280401945
:::NVLOGv0.2.2 Tacotron2_PyT 1593111850.274709463 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111850.275148869 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176783.67377375692
:::NVLOGv0.2.2 Tacotron2_PyT 1593111850.275485992 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.706316947937012
Batch: 2/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111850.278883934 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593111851.683824301 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021065808832645416
:::NVLOGv0.2.2 Tacotron2_PyT 1593111854.964419603 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593111854.964827061 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177540.7327934994
:::NVLOGv0.2.2 Tacotron2_PyT 1593111854.965153456 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.686248540878296
Batch: 3/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111854.968267679 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593111856.393314600 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020077875815331936
:::NVLOGv0.2.2 Tacotron2_PyT 1593111859.675485373 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593111859.675863504 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176724.0397672789
:::NVLOGv0.2.2 Tacotron2_PyT 1593111859.676204681 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.707905054092407
Batch: 4/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111859.678716421 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593111861.098970175 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002095119096338749
:::NVLOGv0.2.2 Tacotron2_PyT 1593111864.390863419 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593111864.391332626 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176546.5132191801
:::NVLOGv0.2.2 Tacotron2_PyT 1593111864.391702414 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.712639093399048
Batch: 5/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111864.394453049 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593111865.811590433 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002037794329226017
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.068765402 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.069144726 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177978.236399892
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.069509983 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.674728870391846
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.264168024 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.265584469 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 152378.96625351266
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.266930342 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 163615.16028970596
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.268226385 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002053852930354575
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.269520283 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 32.76042699813843
:::NVLOGv0.2.2 Tacotron2_PyT 1593111869.270828485 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593111872.500482321 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.001756242010742426
:::NVLOGv0.2.2 Tacotron2_PyT 1593111872.501290083 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111881.131907701 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111881.350952625 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111882.774475336 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021183956414461136
:::NVLOGv0.2.2 Tacotron2_PyT 1593111886.076717615 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593111886.077172756 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176013.8121219528
:::NVLOGv0.2.2 Tacotron2_PyT 1593111886.077489138 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.7269017696380615
Batch: 1/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111886.080881596 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111887.508898497 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020562843419611454
:::NVLOGv0.2.2 Tacotron2_PyT 1593111890.819407940 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111890.819781065 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175548.39695661058
:::NVLOGv0.2.2 Tacotron2_PyT 1593111890.820103645 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.739433765411377
Batch: 2/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111890.822925568 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593111892.251269102 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021308534778654575
:::NVLOGv0.2.2 Tacotron2_PyT 1593111895.557977200 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593111895.558385611 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175686.42815831318
:::NVLOGv0.2.2 Tacotron2_PyT 1593111895.558736563 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.735710144042969
Batch: 3/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111895.561660528 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593111896.986141682 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018993044504895806
:::NVLOGv0.2.2 Tacotron2_PyT 1593111900.293386936 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593111900.293773174 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175808.30699898908
:::NVLOGv0.2.2 Tacotron2_PyT 1593111900.294092894 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.73242712020874
Batch: 4/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111900.296646833 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593111901.732752085 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002197335008531809
:::NVLOGv0.2.2 Tacotron2_PyT 1593111905.070917368 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1593111905.071364164 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 174250.83103002782
:::NVLOGv0.2.2 Tacotron2_PyT 1593111905.071674109 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.774726152420044
Batch: 5/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111905.074149609 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593111906.482536793 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020899754017591476
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.816265106 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.816674709 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175433.05146026766
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.817013979 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.742549896240234
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.872254372 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.872675180 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 173690.01254074147
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.873017311 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 175456.8044543602
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.873360395 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002082024720342209
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.873701572 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 28.74085807800293
:::NVLOGv0.2.2 Tacotron2_PyT 1593111909.874037981 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111911.527157068 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0019413715926930308
:::NVLOGv0.2.2 Tacotron2_PyT 1593111911.528013229 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593111911.529054642 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 100.2713098526001
:::NVLOGv0.2.2 Tacotron2_PyT 1593111911.529384613 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 100.2713098526001
:::NVLOGv0.2.2 Tacotron2_PyT 1593111911.529725552 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 102.95927858352661
:::NVLOGv0.2.2 Tacotron2_PyT 1593111911.530056238 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
