:::NVLOGv0.2.2 Tacotron2_PyT 1593499368.193748474 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593499368.218698502 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593499368.240310431 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593499369.215433121 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 4, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593499369.222281933 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 36, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 4, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593499370.495914698 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593499383.293816328 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593499383.295670986 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499384.144319296 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499390.172979355 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 45.821929931640625
:::NVLOGv0.2.2 Tacotron2_PyT 1593499391.794391632 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499391.794807434 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 10205.065808855668
:::NVLOGv0.2.2 Tacotron2_PyT 1593499391.795152187 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 7.652865886688232
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499391.826471329 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499393.007853746 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.05475616455078
:::NVLOGv0.2.2 Tacotron2_PyT 1593499394.480198622 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499394.481458187 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 30668.53426091466
:::NVLOGv0.2.2 Tacotron2_PyT 1593499394.481804132 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6555230617523193
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499394.487238884 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593499395.563610077 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.87899398803711
:::NVLOGv0.2.2 Tacotron2_PyT 1593499397.075319529 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593499397.077326536 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 31990.30625566917
:::NVLOGv0.2.2 Tacotron2_PyT 1593499397.078071833 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.588502883911133
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499397.083369017 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593499398.116074324 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.21710205078125
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.574283123 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.575553894 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 33017.58053385821
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.578169584 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.4914302825927734
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.683067560 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.684091568 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 19807.354136203383
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.685294867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 26470.371714824425
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.685780287 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.74319553375244
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.686154604 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 16.388206005096436
:::NVLOGv0.2.2 Tacotron2_PyT 1593499399.686508656 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593499401.218011379 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 47.19036102294922
:::NVLOGv0.2.2 Tacotron2_PyT 1593499401.219334364 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499401.900368214 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499402.854413986 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499403.807695627 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.189781188964844
:::NVLOGv0.2.2 Tacotron2_PyT 1593499405.299651146 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593499405.302352667 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 32460.3452260687
:::NVLOGv0.2.2 Tacotron2_PyT 1593499405.304636240 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.448310375213623
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499405.311450958 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499406.500198603 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.953521728515625
:::NVLOGv0.2.2 Tacotron2_PyT 1593499407.981557131 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499407.983472347 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 30371.2066678949
:::NVLOGv0.2.2 Tacotron2_PyT 1593499407.985951185 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6710166931152344
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499407.992213964 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593499409.130803108 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.73014831542969
:::NVLOGv0.2.2 Tacotron2_PyT 1593499410.591016769 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593499410.593291759 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 31747.33283391477
:::NVLOGv0.2.2 Tacotron2_PyT 1593499410.595351696 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.5992419719696045
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499410.601459503 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593499411.617406368 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.716556549072266
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.093690157 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.095918655 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 32663.15522219696
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.098510504 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.4927475452423096
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.175486803 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.176555634 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 28782.086658411365
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.177428484 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 31810.509987518835
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.177811861 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.397501945495605
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.178169966 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 11.27558970451355
:::NVLOGv0.2.2 Tacotron2_PyT 1593499413.178522825 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499414.790242195 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 47.18479919433594
:::NVLOGv0.2.2 Tacotron2_PyT 1593499414.791736841 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593499414.792858362 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 44.29618716239929
:::NVLOGv0.2.2 Tacotron2_PyT 1593499414.793183327 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 44.29618716239929
:::NVLOGv0.2.2 Tacotron2_PyT 1593499414.793525457 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 46.68936514854431
:::NVLOGv0.2.2 Tacotron2_PyT 1593499414.793823242 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
