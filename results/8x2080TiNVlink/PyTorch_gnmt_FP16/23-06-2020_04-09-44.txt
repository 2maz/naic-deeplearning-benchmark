6: Collecting environment information...
7: Collecting environment information...
4: Collecting environment information...
3: Collecting environment information...
5: Collecting environment information...
1: Collecting environment information...
2: Collecting environment information...
0: Collecting environment information...
6: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
6: Saving results to: results/gnmt
6: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=6, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=6, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
6: Using master seed from command line: 2
4: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
4: Saving results to: results/gnmt
4: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=4, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=4, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
4: Using master seed from command line: 2
7: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
7: Saving results to: results/gnmt
7: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=7, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=7, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
7: Using master seed from command line: 2
5: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
5: Saving results to: results/gnmt
5: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=5, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=5, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
5: Using master seed from command line: 2
2: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
2: Saving results to: results/gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
2: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
3: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
3: Saving results to: results/gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
3: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
2: Worker 2 is using worker seed: 3588440356
3: Worker 3 is using worker seed: 1323436024
6: Worker 6 is using worker seed: 4077622522
4: Worker 4 is using worker seed: 2602510382
7: Worker 7 is using worker seed: 117874757
5: Worker 5 is using worker seed: 2606193617
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
7: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
6: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
4: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Size of vocabulary: 31800
1: Size of vocabulary: 31800
7: Size of vocabulary: 31800
5: Size of vocabulary: 31800
6: Size of vocabulary: 31800
0: Size of vocabulary: 31800
4: Size of vocabulary: 31800
2: Size of vocabulary: 31800
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
5: Filtering data, min len: 0, max len: 50
4: Filtering data, min len: 0, max len: 50
7: Filtering data, min len: 0, max len: 50
6: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
4: Pairs before: 160078, after: 148120
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Pairs before: 160078, after: 148120
5: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Pairs before: 160078, after: 148120
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Filtering data, min len: 0, max len: 125
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Pairs before: 5100, after: 5100
3: Filtering data, min len: 0, max len: 125
4: Filtering data, min len: 0, max len: 125
5: Filtering data, min len: 0, max len: 125
3: Pairs before: 5100, after: 5100
7: Filtering data, min len: 0, max len: 125
5: Pairs before: 5100, after: 5100
4: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
7: Pairs before: 5100, after: 5100
6: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
6: Pairs before: 5100, after: 5100
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Filtering data, min len: 0, max len: 150
7: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Pairs before: 3003, after: 3003
6: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
4: Filtering data, min len: 0, max len: 150
5: Filtering data, min len: 0, max len: 150
3: Filtering data, min len: 0, max len: 150
4: Pairs before: 3003, after: 3003
5: Pairs before: 3003, after: 3003
3: Pairs before: 3003, after: 3003
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Filtering data, min len: 0, max len: 150
7: Pairs before: 3003, after: 3003
6: Filtering data, min len: 0, max len: 150
6: Pairs before: 3003, after: 3003
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
5: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
5: Building LabelSmoothingLoss (smoothing: 0.1)
5: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
5: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
5: Number of parameters: 159605817
7: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: Building LabelSmoothingLoss (smoothing: 0.1)
7: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
7: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
7: Number of parameters: 159605817
5: Saving state of the tokenizer
5: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
5: Scheduler warmup steps: 200
5: Scheduler remain steps: 150
5: Scheduler decay interval: 19
5: Scheduler decay factor: 0.5
5: Scheduler max decay steps: 4
5: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
7: Saving state of the tokenizer
7: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
7: Scheduler warmup steps: 200
7: Scheduler remain steps: 150
7: Scheduler decay interval: 19
7: Scheduler decay factor: 0.5
7: Scheduler max decay steps: 4
7: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
5: Initializing amp optimizer
7: Initializing amp optimizer
5: Starting epoch 0
5: Executing preallocation
7: Starting epoch 0
7: Executing preallocation
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159605817
3: Saving state of the tokenizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 150
3: Scheduler decay interval: 19
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
3: Initializing amp optimizer
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159605817
3: Starting epoch 0
3: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
2: Saving state of the tokenizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 150
2: Scheduler decay interval: 19
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
2: Initializing amp optimizer
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 150
0: Scheduler decay interval: 19
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
2: Starting epoch 0
2: Executing preallocation
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 150
1: Scheduler decay interval: 19
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
1: Initializing amp optimizer
6: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
6: Building LabelSmoothingLoss (smoothing: 0.1)
6: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
6: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Starting epoch 0
6: Number of parameters: 159605817
1: Executing preallocation
6: Saving state of the tokenizer
6: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Scheduler warmup steps: 200
6: Scheduler remain steps: 150
6: Scheduler decay interval: 19
6: Scheduler decay factor: 0.5
6: Scheduler max decay steps: 4
6: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
6: Initializing amp optimizer
6: Starting epoch 0
6: Executing preallocation
4: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
4: Building LabelSmoothingLoss (smoothing: 0.1)
4: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
4: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
4: Number of parameters: 159605817
4: Saving state of the tokenizer
4: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
4: Scheduler warmup steps: 200
4: Scheduler remain steps: 150
4: Scheduler decay interval: 19
4: Scheduler decay factor: 0.5
4: Scheduler max decay steps: 4
4: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
4: Initializing amp optimizer
4: Starting epoch 0
4: Executing preallocation
1: Sampler for epoch 0 uses seed 1632151663
3: Sampler for epoch 0 uses seed 1632151663
2: Sampler for epoch 0 uses seed 1632151663
6: Sampler for epoch 0 uses seed 1632151663
0: Sampler for epoch 0 uses seed 1632151663
7: Sampler for epoch 0 uses seed 1632151663
5: Sampler for epoch 0 uses seed 1632151663
4: Sampler for epoch 0 uses seed 1632151663
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
1: TRAIN [0][0/113]	Time 0.441 (0.441)	Data 1.59e-01 (1.59e-01)	Tok/s 16562 (16562)	Loss/tok 10.6053 (10.6053)	LR 2.062e-05
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
6: TRAIN [0][0/113]	Time 0.442 (0.442)	Data 1.66e-01 (1.66e-01)	Tok/s 16205 (16205)	Loss/tok 10.5966 (10.5966)	LR 2.062e-05
5: TRAIN [0][0/113]	Time 0.442 (0.442)	Data 1.67e-01 (1.67e-01)	Tok/s 16681 (16681)	Loss/tok 10.5953 (10.5953)	LR 2.062e-05
7: TRAIN [0][0/113]	Time 0.441 (0.441)	Data 1.68e-01 (1.68e-01)	Tok/s 16178 (16178)	Loss/tok 10.6177 (10.6177)	LR 2.062e-05
3: TRAIN [0][0/113]	Time 0.442 (0.442)	Data 1.63e-01 (1.63e-01)	Tok/s 16423 (16423)	Loss/tok 10.6108 (10.6108)	LR 2.062e-05
4: TRAIN [0][0/113]	Time 0.442 (0.442)	Data 1.83e-01 (1.83e-01)	Tok/s 16539 (16539)	Loss/tok 10.6075 (10.6075)	LR 2.062e-05
2: TRAIN [0][0/113]	Time 0.442 (0.442)	Data 1.86e-01 (1.86e-01)	Tok/s 16327 (16327)	Loss/tok 10.5937 (10.5937)	LR 2.062e-05
0: TRAIN [0][0/113]	Time 0.442 (0.442)	Data 1.75e-01 (1.75e-01)	Tok/s 16234 (16234)	Loss/tok 10.5957 (10.5957)	LR 2.062e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
6: TRAIN [0][10/113]	Time 0.329 (0.311)	Data 1.40e-04 (1.52e-02)	Tok/s 30251 (24194)	Loss/tok 9.8048 (10.1862)	LR 2.803e-05
1: TRAIN [0][10/113]	Time 0.329 (0.311)	Data 1.33e-04 (1.46e-02)	Tok/s 30680 (24270)	Loss/tok 9.8029 (10.1823)	LR 2.803e-05
3: TRAIN [0][10/113]	Time 0.329 (0.311)	Data 1.37e-04 (1.50e-02)	Tok/s 30443 (24373)	Loss/tok 9.8346 (10.1826)	LR 2.803e-05
5: TRAIN [0][10/113]	Time 0.329 (0.311)	Data 1.34e-04 (1.54e-02)	Tok/s 30850 (24393)	Loss/tok 9.7929 (10.1748)	LR 2.803e-05
7: TRAIN [0][10/113]	Time 0.329 (0.311)	Data 1.51e-04 (1.54e-02)	Tok/s 30671 (24267)	Loss/tok 9.8006 (10.1757)	LR 2.803e-05
2: TRAIN [0][10/113]	Time 0.329 (0.311)	Data 1.96e-04 (1.71e-02)	Tok/s 30593 (24210)	Loss/tok 9.7965 (10.1737)	LR 2.803e-05
0: TRAIN [0][10/113]	Time 0.328 (0.311)	Data 1.94e-04 (1.61e-02)	Tok/s 30685 (24250)	Loss/tok 9.8340 (10.1753)	LR 2.803e-05
4: TRAIN [0][10/113]	Time 0.336 (0.312)	Data 1.86e-04 (1.68e-02)	Tok/s 29621 (24125)	Loss/tok 9.7661 (10.1685)	LR 2.803e-05
5: TRAIN [0][20/113]	Time 0.326 (0.306)	Data 1.29e-04 (8.13e-03)	Tok/s 31124 (25068)	Loss/tok 9.3169 (9.8180)	LR 3.811e-05
2: TRAIN [0][20/113]	Time 0.325 (0.306)	Data 1.95e-04 (9.02e-03)	Tok/s 30644 (24922)	Loss/tok 9.3330 (9.8160)	LR 3.811e-05
3: TRAIN [0][20/113]	Time 0.326 (0.306)	Data 1.42e-04 (7.93e-03)	Tok/s 31317 (25032)	Loss/tok 9.2673 (9.8191)	LR 3.811e-05
1: TRAIN [0][20/113]	Time 0.326 (0.306)	Data 1.37e-04 (7.70e-03)	Tok/s 30890 (24973)	Loss/tok 9.3850 (9.8292)	LR 3.811e-05
0: TRAIN [0][20/113]	Time 0.326 (0.306)	Data 1.41e-04 (8.51e-03)	Tok/s 30450 (24940)	Loss/tok 9.2871 (9.8221)	LR 3.811e-05
4: TRAIN [0][20/113]	Time 0.326 (0.306)	Data 1.40e-04 (8.86e-03)	Tok/s 30819 (24859)	Loss/tok 9.3373 (9.8187)	LR 3.811e-05
6: TRAIN [0][20/113]	Time 0.327 (0.306)	Data 1.92e-04 (8.05e-03)	Tok/s 30815 (24865)	Loss/tok 9.3087 (9.8232)	LR 3.811e-05
7: TRAIN [0][20/113]	Time 0.327 (0.306)	Data 2.35e-04 (8.20e-03)	Tok/s 30709 (24923)	Loss/tok 9.2907 (9.8177)	LR 3.811e-05
5: TRAIN [0][30/113]	Time 0.287 (0.303)	Data 1.28e-04 (5.55e-03)	Tok/s 25141 (25159)	Loss/tok 8.8274 (9.5637)	LR 5.180e-05
6: TRAIN [0][30/113]	Time 0.287 (0.303)	Data 1.33e-04 (5.50e-03)	Tok/s 25428 (25042)	Loss/tok 8.8623 (9.5669)	LR 5.180e-05
2: TRAIN [0][30/113]	Time 0.288 (0.303)	Data 1.56e-04 (6.16e-03)	Tok/s 25041 (25009)	Loss/tok 8.8492 (9.5569)	LR 5.180e-05
3: TRAIN [0][30/113]	Time 0.287 (0.303)	Data 1.28e-04 (5.42e-03)	Tok/s 25403 (25177)	Loss/tok 8.8134 (9.5674)	LR 5.180e-05
1: TRAIN [0][30/113]	Time 0.288 (0.303)	Data 1.33e-04 (5.27e-03)	Tok/s 24766 (25086)	Loss/tok 8.8046 (9.5667)	LR 5.180e-05
0: TRAIN [0][30/113]	Time 0.288 (0.303)	Data 1.67e-04 (5.82e-03)	Tok/s 25002 (25085)	Loss/tok 8.8233 (9.5670)	LR 5.180e-05
4: TRAIN [0][30/113]	Time 0.289 (0.303)	Data 1.61e-04 (6.05e-03)	Tok/s 25495 (25001)	Loss/tok 8.8543 (9.5624)	LR 5.180e-05
7: TRAIN [0][30/113]	Time 0.287 (0.303)	Data 2.38e-04 (5.63e-03)	Tok/s 25391 (25047)	Loss/tok 8.7790 (9.5649)	LR 5.180e-05
2: TRAIN [0][40/113]	Time 0.286 (0.304)	Data 1.49e-04 (4.70e-03)	Tok/s 25171 (25457)	Loss/tok 8.4350 (9.3294)	LR 7.042e-05
3: TRAIN [0][40/113]	Time 0.286 (0.304)	Data 1.50e-04 (4.14e-03)	Tok/s 25600 (25590)	Loss/tok 8.4688 (9.3369)	LR 7.042e-05
0: TRAIN [0][40/113]	Time 0.286 (0.304)	Data 1.84e-04 (4.43e-03)	Tok/s 25403 (25522)	Loss/tok 8.4666 (9.3377)	LR 7.042e-05
5: TRAIN [0][40/113]	Time 0.283 (0.304)	Data 1.38e-04 (4.23e-03)	Tok/s 25252 (25558)	Loss/tok 8.4685 (9.3355)	LR 7.042e-05
6: TRAIN [0][40/113]	Time 0.286 (0.304)	Data 1.41e-04 (4.20e-03)	Tok/s 25107 (25521)	Loss/tok 8.5465 (9.3402)	LR 7.042e-05
7: TRAIN [0][40/113]	Time 0.286 (0.304)	Data 1.77e-04 (4.30e-03)	Tok/s 25311 (25521)	Loss/tok 8.4452 (9.3382)	LR 7.042e-05
4: TRAIN [0][40/113]	Time 0.286 (0.304)	Data 1.55e-04 (4.61e-03)	Tok/s 25290 (25491)	Loss/tok 8.4559 (9.3387)	LR 7.042e-05
1: TRAIN [0][40/113]	Time 0.290 (0.304)	Data 1.49e-04 (4.02e-03)	Tok/s 24829 (25541)	Loss/tok 8.4444 (9.3422)	LR 7.042e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
5: TRAIN [0][50/113]	Time 0.382 (0.298)	Data 1.32e-04 (3.43e-03)	Tok/s 34230 (25073)	Loss/tok 8.8281 (9.2644)	LR 9.573e-05
4: TRAIN [0][50/113]	Time 0.382 (0.298)	Data 1.60e-04 (3.74e-03)	Tok/s 34367 (25047)	Loss/tok 8.8521 (9.2697)	LR 9.573e-05
2: TRAIN [0][50/113]	Time 0.383 (0.298)	Data 1.60e-04 (3.81e-03)	Tok/s 34460 (25025)	Loss/tok 8.8234 (9.2518)	LR 9.573e-05
6: TRAIN [0][50/113]	Time 0.382 (0.298)	Data 1.39e-04 (3.40e-03)	Tok/s 33785 (25055)	Loss/tok 8.8017 (9.2663)	LR 9.573e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
7: TRAIN [0][50/113]	Time 0.382 (0.298)	Data 1.86e-04 (3.49e-03)	Tok/s 34074 (25046)	Loss/tok 8.8921 (9.2634)	LR 9.573e-05
1: TRAIN [0][50/113]	Time 0.379 (0.298)	Data 1.93e-04 (3.27e-03)	Tok/s 34761 (25116)	Loss/tok 8.8408 (9.2645)	LR 9.573e-05
3: TRAIN [0][50/113]	Time 0.383 (0.298)	Data 1.33e-04 (3.35e-03)	Tok/s 34190 (25135)	Loss/tok 8.8745 (9.2656)	LR 9.573e-05
0: TRAIN [0][50/113]	Time 0.379 (0.298)	Data 1.79e-04 (3.59e-03)	Tok/s 34706 (25060)	Loss/tok 8.8324 (9.2622)	LR 9.573e-05
5: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.31e-04 (2.89e-03)	Tok/s 17836 (25012)	Loss/tok 7.8097 (9.1075)	LR 1.301e-04
7: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.49e-04 (2.95e-03)	Tok/s 16934 (24954)	Loss/tok 7.8272 (9.1085)	LR 1.301e-04
6: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.30e-04 (2.86e-03)	Tok/s 17552 (24980)	Loss/tok 7.9024 (9.1083)	LR 1.301e-04
2: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.33e-04 (3.21e-03)	Tok/s 18092 (24988)	Loss/tok 7.8152 (9.0946)	LR 1.301e-04
3: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.30e-04 (2.83e-03)	Tok/s 17965 (25064)	Loss/tok 7.9047 (9.1058)	LR 1.301e-04
4: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.43e-04 (3.15e-03)	Tok/s 17564 (24978)	Loss/tok 7.9191 (9.1146)	LR 1.301e-04
0: TRAIN [0][60/113]	Time 0.243 (0.297)	Data 1.24e-04 (3.03e-03)	Tok/s 18009 (24992)	Loss/tok 7.8715 (9.1029)	LR 1.301e-04
1: TRAIN [0][60/113]	Time 0.242 (0.297)	Data 1.84e-04 (2.76e-03)	Tok/s 17907 (25056)	Loss/tok 7.9013 (9.1089)	LR 1.301e-04
5: TRAIN [0][70/113]	Time 0.242 (0.297)	Data 1.29e-04 (2.50e-03)	Tok/s 18066 (25168)	Loss/tok 7.7217 (8.9481)	LR 1.769e-04
3: TRAIN [0][70/113]	Time 0.242 (0.297)	Data 1.29e-04 (2.45e-03)	Tok/s 17882 (25205)	Loss/tok 7.7420 (8.9453)	LR 1.769e-04
2: TRAIN [0][70/113]	Time 0.242 (0.297)	Data 1.83e-04 (2.78e-03)	Tok/s 17756 (25154)	Loss/tok 7.7180 (8.9324)	LR 1.769e-04
4: TRAIN [0][70/113]	Time 0.241 (0.297)	Data 1.53e-04 (2.73e-03)	Tok/s 17747 (25136)	Loss/tok 7.6342 (8.9519)	LR 1.769e-04
1: TRAIN [0][70/113]	Time 0.243 (0.297)	Data 1.35e-04 (2.39e-03)	Tok/s 17777 (25202)	Loss/tok 7.5885 (8.9487)	LR 1.769e-04
6: TRAIN [0][70/113]	Time 0.242 (0.297)	Data 1.28e-04 (2.48e-03)	Tok/s 18084 (25162)	Loss/tok 7.6447 (8.9455)	LR 1.769e-04
0: TRAIN [0][70/113]	Time 0.243 (0.297)	Data 1.27e-04 (2.62e-03)	Tok/s 17478 (25118)	Loss/tok 7.6733 (8.9456)	LR 1.769e-04
7: TRAIN [0][70/113]	Time 0.242 (0.297)	Data 1.75e-04 (2.56e-03)	Tok/s 17961 (25109)	Loss/tok 7.6993 (8.9494)	LR 1.769e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
5: TRAIN [0][80/113]	Time 0.258 (0.296)	Data 1.39e-04 (2.21e-03)	Tok/s 28347 (25251)	Loss/tok 8.1088 (8.8239)	LR 2.405e-04
1: TRAIN [0][80/113]	Time 0.258 (0.296)	Data 1.69e-04 (2.12e-03)	Tok/s 27772 (25275)	Loss/tok 8.0172 (8.8288)	LR 2.405e-04
2: TRAIN [0][80/113]	Time 0.258 (0.296)	Data 1.59e-04 (2.45e-03)	Tok/s 27839 (25209)	Loss/tok 7.9936 (8.8126)	LR 2.405e-04
4: TRAIN [0][80/113]	Time 0.258 (0.296)	Data 1.50e-04 (2.41e-03)	Tok/s 28202 (25230)	Loss/tok 7.9359 (8.8274)	LR 2.405e-04
6: TRAIN [0][80/113]	Time 0.258 (0.296)	Data 1.40e-04 (2.19e-03)	Tok/s 27876 (25243)	Loss/tok 8.0826 (8.8246)	LR 2.405e-04
3: TRAIN [0][80/113]	Time 0.258 (0.296)	Data 1.41e-04 (2.16e-03)	Tok/s 27878 (25291)	Loss/tok 7.9797 (8.8252)	LR 2.405e-04
7: TRAIN [0][80/113]	Time 0.259 (0.296)	Data 1.55e-04 (2.26e-03)	Tok/s 27910 (25195)	Loss/tok 7.9689 (8.8264)	LR 2.405e-04
0: TRAIN [0][80/113]	Time 0.262 (0.296)	Data 1.40e-04 (2.32e-03)	Tok/s 27539 (25186)	Loss/tok 8.0762 (8.8222)	LR 2.405e-04
5: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.30e-04 (1.98e-03)	Tok/s 18129 (25053)	Loss/tok 7.5290 (8.7247)	LR 3.269e-04
0: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.33e-04 (2.08e-03)	Tok/s 18537 (25007)	Loss/tok 7.5108 (8.7208)	LR 3.269e-04
3: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.55e-04 (1.94e-03)	Tok/s 18495 (25074)	Loss/tok 7.4223 (8.7270)	LR 3.269e-04
6: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.20e-04 (1.96e-03)	Tok/s 17977 (25054)	Loss/tok 7.4965 (8.7234)	LR 3.269e-04
4: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.51e-04 (2.16e-03)	Tok/s 18218 (25039)	Loss/tok 7.4710 (8.7254)	LR 3.269e-04
7: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.61e-04 (2.03e-03)	Tok/s 18518 (25003)	Loss/tok 7.4808 (8.7274)	LR 3.269e-04
1: TRAIN [0][90/113]	Time 0.241 (0.294)	Data 1.74e-04 (1.90e-03)	Tok/s 17809 (25049)	Loss/tok 7.4606 (8.7256)	LR 3.269e-04
2: TRAIN [0][90/113]	Time 0.244 (0.294)	Data 1.37e-04 (2.20e-03)	Tok/s 17698 (25006)	Loss/tok 7.5393 (8.7156)	LR 3.269e-04
1: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.64e-04 (1.73e-03)	Tok/s 17370 (25163)	Loss/tok 7.4180 (8.6591)	LR 4.443e-04
5: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.51e-04 (1.80e-03)	Tok/s 17731 (25149)	Loss/tok 7.3931 (8.6596)	LR 4.443e-04
2: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.67e-04 (2.00e-03)	Tok/s 17692 (25123)	Loss/tok 7.3504 (8.6496)	LR 4.443e-04
0: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.34e-04 (1.89e-03)	Tok/s 17920 (25107)	Loss/tok 7.4439 (8.6533)	LR 4.443e-04
6: TRAIN [0][100/113]	Time 0.246 (0.296)	Data 1.23e-04 (1.78e-03)	Tok/s 17786 (25149)	Loss/tok 7.3485 (8.6561)	LR 4.443e-04
7: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.42e-04 (1.84e-03)	Tok/s 17312 (25111)	Loss/tok 7.3393 (8.6612)	LR 4.443e-04
4: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.44e-04 (1.96e-03)	Tok/s 17739 (25148)	Loss/tok 7.3741 (8.6563)	LR 4.443e-04
3: TRAIN [0][100/113]	Time 0.245 (0.296)	Data 1.77e-04 (1.76e-03)	Tok/s 17536 (25175)	Loss/tok 7.3053 (8.6585)	LR 4.443e-04
5: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.32e-05 (1.65e-03)	Tok/s 30284 (25201)	Loss/tok 7.7745 (8.5693)	LR 6.040e-04
3: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.08e-05 (1.62e-03)	Tok/s 30164 (25234)	Loss/tok 7.8296 (8.5728)	LR 6.040e-04
1: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.53e-05 (1.59e-03)	Tok/s 30332 (25234)	Loss/tok 7.7469 (8.5698)	LR 6.040e-04
7: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.60e-05 (1.69e-03)	Tok/s 30244 (25184)	Loss/tok 7.7554 (8.5735)	LR 6.040e-04
2: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.82e-05 (1.84e-03)	Tok/s 30765 (25182)	Loss/tok 7.7851 (8.5619)	LR 6.040e-04
6: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 4.89e-05 (1.64e-03)	Tok/s 30046 (25216)	Loss/tok 7.7816 (8.5676)	LR 6.040e-04
4: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.75e-05 (1.80e-03)	Tok/s 30181 (25214)	Loss/tok 7.7797 (8.5680)	LR 6.040e-04
0: TRAIN [0][110/113]	Time 0.333 (0.296)	Data 5.10e-05 (1.74e-03)	Tok/s 29670 (25169)	Loss/tok 7.7315 (8.5638)	LR 6.040e-04
3: Running validation on dev set
6: Running validation on dev set
3: Executing preallocation
7: Running validation on dev set
6: Executing preallocation
7: Executing preallocation
4: Running validation on dev set
2: Running validation on dev set
5: Running validation on dev set
1: Running validation on dev set
4: Executing preallocation
2: Executing preallocation
5: Executing preallocation
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
3: VALIDATION [0][0/20]	Time 0.059 (0.059)	Data 1.95e-03 (1.95e-03)	Tok/s 71156 (71156)	Loss/tok 8.6336 (8.6336)
7: VALIDATION [0][0/20]	Time 0.051 (0.051)	Data 1.94e-03 (1.94e-03)	Tok/s 71845 (71845)	Loss/tok 8.6584 (8.6584)
6: VALIDATION [0][0/20]	Time 0.051 (0.051)	Data 1.82e-03 (1.82e-03)	Tok/s 73958 (73958)	Loss/tok 8.5554 (8.5554)
1: VALIDATION [0][0/20]	Time 0.066 (0.066)	Data 2.99e-03 (2.99e-03)	Tok/s 71948 (71948)	Loss/tok 8.6818 (8.6818)
5: VALIDATION [0][0/20]	Time 0.051 (0.051)	Data 2.16e-03 (2.16e-03)	Tok/s 75228 (75228)	Loss/tok 8.7157 (8.7157)
4: VALIDATION [0][0/20]	Time 0.055 (0.055)	Data 1.94e-03 (1.94e-03)	Tok/s 72252 (72252)	Loss/tok 8.6085 (8.6085)
2: VALIDATION [0][0/20]	Time 0.064 (0.064)	Data 1.76e-03 (1.76e-03)	Tok/s 68682 (68682)	Loss/tok 8.7334 (8.7334)
0: VALIDATION [0][0/20]	Time 0.096 (0.096)	Data 2.85e-03 (2.85e-03)	Tok/s 59747 (59747)	Loss/tok 8.7184 (8.7184)
7: VALIDATION [0][10/20]	Time 0.020 (0.030)	Data 1.42e-03 (1.59e-03)	Tok/s 75212 (76823)	Loss/tok 8.2662 (8.4209)
3: VALIDATION [0][10/20]	Time 0.020 (0.033)	Data 1.44e-03 (1.57e-03)	Tok/s 80378 (76222)	Loss/tok 8.0328 (8.4063)
6: VALIDATION [0][10/20]	Time 0.020 (0.031)	Data 1.41e-03 (1.98e-03)	Tok/s 77150 (75945)	Loss/tok 7.8950 (8.3490)
5: VALIDATION [0][10/20]	Time 0.020 (0.031)	Data 1.47e-03 (1.60e-03)	Tok/s 77358 (78154)	Loss/tok 7.9905 (8.3626)
4: VALIDATION [0][10/20]	Time 0.021 (0.032)	Data 1.44e-03 (1.68e-03)	Tok/s 76456 (76667)	Loss/tok 8.0616 (8.3978)
1: VALIDATION [0][10/20]	Time 0.021 (0.033)	Data 1.44e-03 (1.82e-03)	Tok/s 77298 (77352)	Loss/tok 8.0694 (8.3898)
2: VALIDATION [0][10/20]	Time 0.021 (0.033)	Data 1.42e-03 (1.53e-03)	Tok/s 75782 (76982)	Loss/tok 8.0279 (8.4170)
0: VALIDATION [0][10/20]	Time 0.021 (0.037)	Data 1.48e-03 (2.17e-03)	Tok/s 77309 (75080)	Loss/tok 8.1638 (8.4312)
0: Saving model to results/gnmt/model_best.pth
3: Running evaluation on test set
5: Running evaluation on test set
7: Running evaluation on test set
2: Running evaluation on test set
0: Running evaluation on test set
6: Running evaluation on test set
4: Running evaluation on test set
1: Running evaluation on test set
4: TEST [0][9/12]	Time 0.0939 (0.1583)	Decoder iters 18.0 (25.8)	Tok/s 10604 (10789)
3: TEST [0][9/12]	Time 0.0942 (0.1585)	Decoder iters 18.0 (26.5)	Tok/s 10586 (10946)
5: TEST [0][9/12]	Time 0.0939 (0.1586)	Decoder iters 17.0 (26.1)	Tok/s 10015 (10607)
0: TEST [0][9/12]	Time 0.0944 (0.1585)	Decoder iters 20.0 (26.7)	Tok/s 11415 (11749)
7: TEST [0][9/12]	Time 0.0944 (0.1586)	Decoder iters 18.0 (25.9)	Tok/s 10051 (10449)
2: TEST [0][9/12]	Time 0.0944 (0.1585)	Decoder iters 19.0 (26.4)	Tok/s 10990 (11076)
1: TEST [0][9/12]	Time 0.0954 (0.1580)	Decoder iters 21.0 (26.9)	Tok/s 11367 (11395)
6: TEST [0][9/12]	Time 0.0963 (0.1571)	Decoder iters 17.0 (25.6)	Tok/s 9652 (10585)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
5: Finished evaluation on test set
3: Finished evaluation on test set
7: Finished evaluation on test set
2: Finished evaluation on test set
4: Finished evaluation on test set
6: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
5: Finished epoch 0
3: Finished epoch 0
2: Finished epoch 0
7: Finished epoch 0
5: Starting epoch 1
3: Starting epoch 1
4: Finished epoch 0
2: Starting epoch 1
7: Starting epoch 1
6: Finished epoch 0
4: Starting epoch 1
6: Starting epoch 1
3: Executing preallocation
5: Executing preallocation
7: Executing preallocation
2: Executing preallocation
1: Finished epoch 0
1: Starting epoch 1
6: Executing preallocation
4: Executing preallocation
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 8.5511	Validation Loss: 8.2639	Test BLEU: 0.03
0: Performance: Epoch: 0	Training: 202017 Tok/s	Validation: 592658 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
3: Sampler for epoch 1 uses seed 2258090960
2: Sampler for epoch 1 uses seed 2258090960
5: Sampler for epoch 1 uses seed 2258090960
7: Sampler for epoch 1 uses seed 2258090960
1: Sampler for epoch 1 uses seed 2258090960
0: Sampler for epoch 1 uses seed 2258090960
4: Sampler for epoch 1 uses seed 2258090960
6: Sampler for epoch 1 uses seed 2258090960
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
5: TRAIN [1][0/113]	Time 0.489 (0.489)	Data 1.63e-01 (1.63e-01)	Tok/s 20608 (20608)	Loss/tok 7.7499 (7.7499)	LR 6.623e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
3: TRAIN [1][0/113]	Time 0.490 (0.490)	Data 1.63e-01 (1.63e-01)	Tok/s 20354 (20354)	Loss/tok 7.6980 (7.6980)	LR 6.623e-04
7: TRAIN [1][0/113]	Time 0.489 (0.489)	Data 1.75e-01 (1.75e-01)	Tok/s 20809 (20809)	Loss/tok 7.8456 (7.8456)	LR 6.623e-04
4: TRAIN [1][0/113]	Time 0.490 (0.490)	Data 1.78e-01 (1.78e-01)	Tok/s 20491 (20491)	Loss/tok 7.7398 (7.7398)	LR 6.623e-04
6: TRAIN [1][0/113]	Time 0.489 (0.489)	Data 1.76e-01 (1.76e-01)	Tok/s 20633 (20633)	Loss/tok 7.8006 (7.8006)	LR 6.623e-04
2: TRAIN [1][0/113]	Time 0.490 (0.490)	Data 1.67e-01 (1.67e-01)	Tok/s 20455 (20455)	Loss/tok 7.7893 (7.7893)	LR 6.623e-04
1: TRAIN [1][0/113]	Time 0.490 (0.490)	Data 1.60e-01 (1.60e-01)	Tok/s 20512 (20512)	Loss/tok 7.8306 (7.8306)	LR 6.623e-04
0: TRAIN [1][0/113]	Time 0.489 (0.489)	Data 1.84e-01 (1.84e-01)	Tok/s 20642 (20642)	Loss/tok 7.7728 (7.7728)	LR 6.623e-04
6: TRAIN [1][10/113]	Time 0.399 (0.317)	Data 1.21e-04 (1.61e-02)	Tok/s 32774 (25994)	Loss/tok 7.9248 (7.6836)	LR 9.003e-04
7: TRAIN [1][10/113]	Time 0.399 (0.317)	Data 1.43e-04 (1.61e-02)	Tok/s 32795 (25889)	Loss/tok 7.9250 (7.6780)	LR 9.003e-04
0: TRAIN [1][10/113]	Time 0.399 (0.317)	Data 1.45e-04 (1.68e-02)	Tok/s 32404 (25883)	Loss/tok 7.9346 (7.6788)	LR 9.003e-04
4: TRAIN [1][10/113]	Time 0.395 (0.317)	Data 1.84e-04 (1.63e-02)	Tok/s 33142 (26000)	Loss/tok 7.9591 (7.6719)	LR 9.003e-04
1: TRAIN [1][10/113]	Time 0.400 (0.317)	Data 1.52e-04 (1.46e-02)	Tok/s 32968 (26009)	Loss/tok 7.9132 (7.6650)	LR 9.003e-04
5: TRAIN [1][10/113]	Time 0.399 (0.317)	Data 1.79e-04 (1.49e-02)	Tok/s 32619 (25864)	Loss/tok 7.9829 (7.6965)	LR 9.003e-04
2: TRAIN [1][10/113]	Time 0.403 (0.317)	Data 1.25e-04 (1.54e-02)	Tok/s 32498 (25901)	Loss/tok 7.9574 (7.6798)	LR 9.003e-04
3: TRAIN [1][10/113]	Time 0.399 (0.318)	Data 1.83e-04 (1.50e-02)	Tok/s 32693 (25880)	Loss/tok 7.9081 (7.6442)	LR 9.003e-04
6: TRAIN [1][20/113]	Time 0.331 (0.301)	Data 1.19e-04 (8.51e-03)	Tok/s 30485 (24778)	Loss/tok 7.7042 (7.6368)	LR 1.224e-03
1: TRAIN [1][20/113]	Time 0.329 (0.301)	Data 1.55e-04 (7.74e-03)	Tok/s 30696 (24759)	Loss/tok 7.6797 (7.6226)	LR 1.224e-03
4: TRAIN [1][20/113]	Time 0.332 (0.301)	Data 1.19e-04 (8.61e-03)	Tok/s 30458 (24795)	Loss/tok 7.7328 (7.6250)	LR 1.224e-03
7: TRAIN [1][20/113]	Time 0.331 (0.301)	Data 1.33e-04 (8.48e-03)	Tok/s 30502 (24730)	Loss/tok 7.7033 (7.6350)	LR 1.224e-03
0: TRAIN [1][20/113]	Time 0.333 (0.301)	Data 1.78e-04 (8.93e-03)	Tok/s 29874 (24742)	Loss/tok 7.6711 (7.6305)	LR 1.224e-03
5: TRAIN [1][20/113]	Time 0.332 (0.301)	Data 1.53e-04 (7.90e-03)	Tok/s 30517 (24749)	Loss/tok 7.7330 (7.6452)	LR 1.224e-03
2: TRAIN [1][20/113]	Time 0.331 (0.301)	Data 2.13e-04 (8.13e-03)	Tok/s 30551 (24732)	Loss/tok 7.7068 (7.6357)	LR 1.224e-03
3: TRAIN [1][20/113]	Time 0.332 (0.301)	Data 1.74e-04 (7.93e-03)	Tok/s 29991 (24692)	Loss/tok 7.7068 (7.6119)	LR 1.224e-03
5: TRAIN [1][30/113]	Time 0.332 (0.301)	Data 1.13e-04 (5.39e-03)	Tok/s 29871 (25149)	Loss/tok 7.5528 (7.6130)	LR 1.664e-03
4: TRAIN [1][30/113]	Time 0.333 (0.301)	Data 1.36e-04 (5.87e-03)	Tok/s 30564 (25201)	Loss/tok 7.5352 (7.5976)	LR 1.664e-03
2: TRAIN [1][30/113]	Time 0.333 (0.301)	Data 1.22e-04 (5.55e-03)	Tok/s 30325 (25181)	Loss/tok 7.5977 (7.6025)	LR 1.664e-03
7: TRAIN [1][30/113]	Time 0.332 (0.301)	Data 1.64e-04 (5.79e-03)	Tok/s 29809 (25186)	Loss/tok 7.5673 (7.6046)	LR 1.664e-03
1: TRAIN [1][30/113]	Time 0.333 (0.301)	Data 1.81e-04 (5.30e-03)	Tok/s 30271 (25180)	Loss/tok 7.5820 (7.6015)	LR 1.664e-03
6: TRAIN [1][30/113]	Time 0.332 (0.301)	Data 1.56e-04 (5.81e-03)	Tok/s 29997 (25220)	Loss/tok 7.5608 (7.6032)	LR 1.664e-03
0: TRAIN [1][30/113]	Time 0.333 (0.301)	Data 1.45e-04 (6.11e-03)	Tok/s 30202 (25182)	Loss/tok 7.6254 (7.5993)	LR 1.664e-03
3: TRAIN [1][30/113]	Time 0.332 (0.301)	Data 1.68e-04 (5.43e-03)	Tok/s 30075 (25160)	Loss/tok 7.5434 (7.5931)	LR 1.664e-03
6: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 1.18e-04 (4.42e-03)	Tok/s 10402 (24921)	Loss/tok 6.9685 (7.5615)	LR 1.000e-03
5: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 1.05e-04 (4.10e-03)	Tok/s 9845 (24852)	Loss/tok 7.0974 (7.5700)	LR 1.000e-03
2: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 1.27e-04 (4.24e-03)	Tok/s 10430 (24898)	Loss/tok 7.1121 (7.5609)	LR 1.000e-03
7: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 1.33e-04 (4.41e-03)	Tok/s 10577 (24908)	Loss/tok 6.7683 (7.5568)	LR 1.000e-03
4: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 1.32e-04 (4.48e-03)	Tok/s 10219 (24863)	Loss/tok 6.9247 (7.5545)	LR 1.000e-03
0: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 1.80e-04 (4.66e-03)	Tok/s 10525 (24889)	Loss/tok 6.9164 (7.5530)	LR 1.000e-03
1: TRAIN [1][40/113]	Time 0.207 (0.297)	Data 1.93e-04 (4.05e-03)	Tok/s 10572 (24913)	Loss/tok 6.9691 (7.5557)	LR 1.000e-03
3: TRAIN [1][40/113]	Time 0.208 (0.297)	Data 2.20e-04 (4.15e-03)	Tok/s 10341 (24886)	Loss/tok 7.0088 (7.5529)	LR 1.000e-03
5: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.08e-04 (3.32e-03)	Tok/s 30009 (24833)	Loss/tok 7.3309 (7.5272)	LR 1.000e-03
1: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.39e-04 (3.28e-03)	Tok/s 30222 (24892)	Loss/tok 7.4172 (7.5132)	LR 1.000e-03
6: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.22e-04 (3.59e-03)	Tok/s 30437 (24919)	Loss/tok 7.3806 (7.5225)	LR 1.000e-03
2: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.34e-04 (3.43e-03)	Tok/s 30535 (24891)	Loss/tok 7.4046 (7.5211)	LR 1.000e-03
4: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.47e-04 (3.63e-03)	Tok/s 30508 (24858)	Loss/tok 7.3259 (7.5115)	LR 1.000e-03
7: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.30e-04 (3.58e-03)	Tok/s 30135 (24883)	Loss/tok 7.3559 (7.5133)	LR 1.000e-03
3: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.33e-04 (3.37e-03)	Tok/s 30454 (24859)	Loss/tok 7.3571 (7.5121)	LR 1.000e-03
0: TRAIN [1][50/113]	Time 0.332 (0.297)	Data 1.53e-04 (3.77e-03)	Tok/s 29981 (24875)	Loss/tok 7.3401 (7.5144)	LR 1.000e-03
5: TRAIN [1][60/113]	Time 0.284 (0.297)	Data 1.13e-04 (2.79e-03)	Tok/s 25392 (24947)	Loss/tok 7.0939 (7.4748)	LR 5.000e-04
1: TRAIN [1][60/113]	Time 0.285 (0.297)	Data 1.63e-04 (2.77e-03)	Tok/s 25520 (24962)	Loss/tok 7.0529 (7.4642)	LR 5.000e-04
3: TRAIN [1][60/113]	Time 0.285 (0.297)	Data 1.65e-04 (2.84e-03)	Tok/s 25214 (24930)	Loss/tok 6.9822 (7.4607)	LR 5.000e-04
6: TRAIN [1][60/113]	Time 0.285 (0.297)	Data 1.23e-04 (3.02e-03)	Tok/s 24742 (24961)	Loss/tok 7.1179 (7.4712)	LR 5.000e-04
4: TRAIN [1][60/113]	Time 0.284 (0.297)	Data 1.42e-04 (3.06e-03)	Tok/s 25241 (24924)	Loss/tok 6.9130 (7.4590)	LR 5.000e-04
2: TRAIN [1][60/113]	Time 0.284 (0.297)	Data 1.27e-04 (2.89e-03)	Tok/s 24995 (24958)	Loss/tok 6.9707 (7.4647)	LR 5.000e-04
7: TRAIN [1][60/113]	Time 0.285 (0.297)	Data 1.47e-04 (3.01e-03)	Tok/s 25491 (24971)	Loss/tok 7.0746 (7.4609)	LR 5.000e-04
0: TRAIN [1][60/113]	Time 0.284 (0.297)	Data 1.69e-04 (3.18e-03)	Tok/s 25144 (24908)	Loss/tok 7.0891 (7.4646)	LR 5.000e-04
5: TRAIN [1][70/113]	Time 0.389 (0.300)	Data 1.59e-04 (2.42e-03)	Tok/s 33490 (25231)	Loss/tok 7.1514 (7.4133)	LR 5.000e-04
4: TRAIN [1][70/113]	Time 0.389 (0.300)	Data 1.40e-04 (2.65e-03)	Tok/s 33876 (25220)	Loss/tok 7.1619 (7.3996)	LR 5.000e-04
2: TRAIN [1][70/113]	Time 0.389 (0.300)	Data 1.38e-04 (2.51e-03)	Tok/s 33685 (25229)	Loss/tok 7.1463 (7.4011)	LR 5.000e-04
3: TRAIN [1][70/113]	Time 0.389 (0.300)	Data 1.49e-04 (2.46e-03)	Tok/s 32908 (25227)	Loss/tok 7.1904 (7.4008)	LR 5.000e-04
1: TRAIN [1][70/113]	Time 0.389 (0.300)	Data 1.57e-04 (2.40e-03)	Tok/s 33653 (25246)	Loss/tok 7.2101 (7.4014)	LR 5.000e-04
0: TRAIN [1][70/113]	Time 0.389 (0.300)	Data 2.23e-04 (2.75e-03)	Tok/s 33406 (25211)	Loss/tok 7.1540 (7.4056)	LR 5.000e-04
6: TRAIN [1][70/113]	Time 0.394 (0.300)	Data 1.19e-04 (2.61e-03)	Tok/s 33166 (25255)	Loss/tok 7.1488 (7.4096)	LR 5.000e-04
7: TRAIN [1][70/113]	Time 0.394 (0.300)	Data 1.52e-04 (2.62e-03)	Tok/s 32996 (25239)	Loss/tok 7.1941 (7.4014)	LR 5.000e-04
3: TRAIN [1][80/113]	Time 0.203 (0.299)	Data 1.53e-04 (2.17e-03)	Tok/s 10885 (25195)	Loss/tok 6.2488 (7.3459)	LR 2.500e-04
5: TRAIN [1][80/113]	Time 0.204 (0.299)	Data 1.21e-04 (2.13e-03)	Tok/s 10420 (25165)	Loss/tok 6.1391 (7.3560)	LR 2.500e-04
1: TRAIN [1][80/113]	Time 0.203 (0.299)	Data 2.11e-04 (2.13e-03)	Tok/s 10748 (25188)	Loss/tok 6.1161 (7.3432)	LR 2.500e-04
4: TRAIN [1][80/113]	Time 0.203 (0.299)	Data 1.44e-04 (2.34e-03)	Tok/s 10321 (25155)	Loss/tok 6.1424 (7.3421)	LR 2.500e-04
6: TRAIN [1][80/113]	Time 0.205 (0.299)	Data 1.24e-04 (2.31e-03)	Tok/s 10323 (25200)	Loss/tok 6.1896 (7.3491)	LR 2.500e-04
7: TRAIN [1][80/113]	Time 0.205 (0.299)	Data 1.68e-04 (2.32e-03)	Tok/s 10297 (25193)	Loss/tok 6.3115 (7.3432)	LR 2.500e-04
0: TRAIN [1][80/113]	Time 0.204 (0.299)	Data 1.36e-04 (2.43e-03)	Tok/s 10710 (25158)	Loss/tok 6.0378 (7.3448)	LR 2.500e-04
2: TRAIN [1][80/113]	Time 0.203 (0.299)	Data 1.66e-04 (2.21e-03)	Tok/s 10757 (25176)	Loss/tok 6.2297 (7.3417)	LR 2.500e-04
5: TRAIN [1][90/113]	Time 0.241 (0.299)	Data 1.20e-04 (1.91e-03)	Tok/s 18565 (25217)	Loss/tok 6.5054 (7.3003)	LR 2.500e-04
2: TRAIN [1][90/113]	Time 0.242 (0.299)	Data 1.32e-04 (1.99e-03)	Tok/s 18204 (25224)	Loss/tok 6.4122 (7.2898)	LR 2.500e-04
6: TRAIN [1][90/113]	Time 0.241 (0.299)	Data 1.32e-04 (2.07e-03)	Tok/s 17863 (25240)	Loss/tok 6.4472 (7.2936)	LR 2.500e-04
4: TRAIN [1][90/113]	Time 0.242 (0.299)	Data 1.36e-04 (2.10e-03)	Tok/s 18281 (25217)	Loss/tok 6.5645 (7.2892)	LR 2.500e-04
3: TRAIN [1][90/113]	Time 0.242 (0.299)	Data 1.28e-04 (1.95e-03)	Tok/s 17726 (25255)	Loss/tok 6.5019 (7.2930)	LR 2.500e-04
1: TRAIN [1][90/113]	Time 0.242 (0.299)	Data 1.48e-04 (1.91e-03)	Tok/s 17804 (25225)	Loss/tok 6.3836 (7.2895)	LR 2.500e-04
7: TRAIN [1][90/113]	Time 0.242 (0.299)	Data 2.03e-04 (2.08e-03)	Tok/s 18000 (25236)	Loss/tok 6.5080 (7.2918)	LR 2.500e-04
0: TRAIN [1][90/113]	Time 0.243 (0.299)	Data 1.36e-04 (2.18e-03)	Tok/s 18097 (25223)	Loss/tok 6.5352 (7.2916)	LR 2.500e-04
4: TRAIN [1][100/113]	Time 0.396 (0.299)	Data 1.30e-04 (1.91e-03)	Tok/s 33055 (25252)	Loss/tok 7.0194 (7.2452)	LR 1.250e-04
3: TRAIN [1][100/113]	Time 0.396 (0.299)	Data 1.21e-04 (1.77e-03)	Tok/s 32981 (25280)	Loss/tok 7.0529 (7.2469)	LR 1.250e-04
1: TRAIN [1][100/113]	Time 0.397 (0.299)	Data 1.39e-04 (1.74e-03)	Tok/s 32998 (25245)	Loss/tok 7.0308 (7.2449)	LR 1.250e-04
6: TRAIN [1][100/113]	Time 0.397 (0.299)	Data 1.24e-04 (1.88e-03)	Tok/s 32876 (25274)	Loss/tok 7.0124 (7.2472)	LR 1.250e-04
0: TRAIN [1][100/113]	Time 0.397 (0.299)	Data 1.29e-04 (1.98e-03)	Tok/s 33286 (25272)	Loss/tok 7.0323 (7.2445)	LR 1.250e-04
7: TRAIN [1][100/113]	Time 0.397 (0.299)	Data 1.34e-04 (1.89e-03)	Tok/s 33076 (25271)	Loss/tok 6.9727 (7.2457)	LR 1.250e-04
2: TRAIN [1][100/113]	Time 0.398 (0.299)	Data 1.24e-04 (1.80e-03)	Tok/s 32611 (25264)	Loss/tok 7.0588 (7.2425)	LR 1.250e-04
5: TRAIN [1][100/113]	Time 0.396 (0.299)	Data 1.58e-04 (1.73e-03)	Tok/s 32671 (25251)	Loss/tok 7.0253 (7.2555)	LR 1.250e-04
5: TRAIN [1][110/113]	Time 0.200 (0.297)	Data 4.41e-05 (1.59e-03)	Tok/s 10696 (25070)	Loss/tok 5.8842 (7.2145)	LR 1.250e-04
4: TRAIN [1][110/113]	Time 0.201 (0.297)	Data 5.36e-05 (1.75e-03)	Tok/s 10902 (25053)	Loss/tok 6.1800 (7.2053)	LR 1.250e-04
6: TRAIN [1][110/113]	Time 0.201 (0.297)	Data 5.03e-05 (1.72e-03)	Tok/s 10654 (25080)	Loss/tok 5.8553 (7.2045)	LR 1.250e-04
7: TRAIN [1][110/113]	Time 0.201 (0.297)	Data 5.58e-05 (1.73e-03)	Tok/s 10387 (25073)	Loss/tok 5.9992 (7.2069)	LR 1.250e-04
2: TRAIN [1][110/113]	Time 0.201 (0.297)	Data 5.67e-05 (1.66e-03)	Tok/s 10648 (25074)	Loss/tok 5.9722 (7.2029)	LR 1.250e-04
3: TRAIN [1][110/113]	Time 0.201 (0.297)	Data 4.82e-05 (1.62e-03)	Tok/s 11077 (25074)	Loss/tok 5.8451 (7.2060)	LR 1.250e-04
0: TRAIN [1][110/113]	Time 0.202 (0.297)	Data 4.70e-05 (1.81e-03)	Tok/s 10663 (25069)	Loss/tok 5.9116 (7.2039)	LR 1.250e-04
1: TRAIN [1][110/113]	Time 0.201 (0.297)	Data 7.08e-05 (1.60e-03)	Tok/s 10655 (25060)	Loss/tok 6.1118 (7.2023)	LR 1.250e-04
0: Running validation on dev set
2: Running validation on dev set
7: Running validation on dev set
0: Executing preallocation
2: Executing preallocation
3: Running validation on dev set
4: Running validation on dev set
7: Executing preallocation
6: Running validation on dev set
3: Executing preallocation
4: Executing preallocation
6: Executing preallocation
5: Running validation on dev set
5: Executing preallocation
1: Running validation on dev set
1: Executing preallocation
7: VALIDATION [1][0/20]	Time 0.045 (0.045)	Data 1.83e-03 (1.83e-03)	Tok/s 80691 (80691)	Loss/tok 7.7406 (7.7406)
6: VALIDATION [1][0/20]	Time 0.050 (0.050)	Data 1.73e-03 (1.73e-03)	Tok/s 75886 (75886)	Loss/tok 7.5917 (7.5917)
4: VALIDATION [1][0/20]	Time 0.055 (0.055)	Data 2.42e-03 (2.42e-03)	Tok/s 72876 (72876)	Loss/tok 7.6619 (7.6619)
5: VALIDATION [1][0/20]	Time 0.051 (0.051)	Data 2.82e-03 (2.82e-03)	Tok/s 75329 (75329)	Loss/tok 7.7351 (7.7351)
2: VALIDATION [1][0/20]	Time 0.063 (0.063)	Data 2.85e-03 (2.85e-03)	Tok/s 69601 (69601)	Loss/tok 7.8283 (7.8283)
3: VALIDATION [1][0/20]	Time 0.060 (0.060)	Data 3.00e-03 (3.00e-03)	Tok/s 69877 (69877)	Loss/tok 7.6737 (7.6737)
0: VALIDATION [1][0/20]	Time 0.095 (0.095)	Data 1.79e-03 (1.79e-03)	Tok/s 60347 (60347)	Loss/tok 7.7995 (7.7995)
1: VALIDATION [1][0/20]	Time 0.068 (0.068)	Data 1.94e-03 (1.94e-03)	Tok/s 70033 (70033)	Loss/tok 7.7621 (7.7621)
7: VALIDATION [1][10/20]	Time 0.021 (0.030)	Data 1.45e-03 (1.56e-03)	Tok/s 73253 (77642)	Loss/tok 7.4866 (7.5620)
6: VALIDATION [1][10/20]	Time 0.020 (0.030)	Data 1.73e-03 (1.58e-03)	Tok/s 76410 (77831)	Loss/tok 7.1475 (7.4693)
4: VALIDATION [1][10/20]	Time 0.019 (0.031)	Data 1.49e-03 (1.60e-03)	Tok/s 81183 (78041)	Loss/tok 7.2912 (7.5207)
5: VALIDATION [1][10/20]	Time 0.021 (0.032)	Data 2.39e-03 (2.49e-03)	Tok/s 74260 (75784)	Loss/tok 7.2683 (7.5010)
2: VALIDATION [1][10/20]	Time 0.022 (0.033)	Data 1.50e-03 (1.93e-03)	Tok/s 72949 (76127)	Loss/tok 7.2433 (7.5467)
3: VALIDATION [1][10/20]	Time 0.021 (0.034)	Data 2.26e-03 (2.47e-03)	Tok/s 74439 (72222)	Loss/tok 7.2670 (7.5303)
0: VALIDATION [1][10/20]	Time 0.022 (0.038)	Data 1.51e-03 (1.56e-03)	Tok/s 73767 (73455)	Loss/tok 7.3601 (7.5512)
1: VALIDATION [1][10/20]	Time 0.024 (0.035)	Data 1.43e-03 (1.52e-03)	Tok/s 68070 (73824)	Loss/tok 7.2948 (7.5059)
0: Saving model to results/gnmt/model_best.pth
3: Running evaluation on test set
4: Running evaluation on test set
2: Running evaluation on test set
7: Running evaluation on test set
5: Running evaluation on test set
6: Running evaluation on test set
1: Running evaluation on test set
0: Running evaluation on test set
3: TEST [1][9/12]	Time 0.1699 (0.3850)	Decoder iters 33.0 (85.2)	Tok/s 6353 (5730)
4: TEST [1][9/12]	Time 0.1700 (0.3856)	Decoder iters 48.0 (87.3)	Tok/s 6589 (5593)
6: TEST [1][9/12]	Time 0.1695 (0.3856)	Decoder iters 43.0 (82.7)	Tok/s 6055 (5247)
5: TEST [1][9/12]	Time 0.1697 (0.3856)	Decoder iters 34.0 (92.9)	Tok/s 5875 (5481)
7: TEST [1][9/12]	Time 0.1699 (0.3856)	Decoder iters 27.0 (62.8)	Tok/s 5821 (5117)
0: TEST [1][9/12]	Time 0.1698 (0.3851)	Decoder iters 46.0 (88.0)	Tok/s 6845 (6216)
2: TEST [1][9/12]	Time 0.1699 (0.3857)	Decoder iters 51.0 (83.9)	Tok/s 6503 (5673)
1: TEST [1][9/12]	Time 0.1853 (0.3871)	Decoder iters 30.0 (86.0)	Tok/s 6059 (5782)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
5: Finished evaluation on test set
7: Finished evaluation on test set
2: Finished evaluation on test set
6: Finished evaluation on test set
4: Finished evaluation on test set
3: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
5: Finished epoch 1
7: Finished epoch 1
2: Finished epoch 1
4: Finished epoch 1
6: Finished epoch 1
3: Finished epoch 1
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 7.1987	Validation Loss: 7.4207	Test BLEU: 0.41
0: Performance: Epoch: 1	Training: 200679 Tok/s	Validation: 579986 Tok/s
0: Finished epoch 1
5: Total training time 109 s
2: Total training time 109 s
6: Total training time 109 s
4: Total training time 109 s
7: Total training time 109 s
1: Total training time 109 s
3: Total training time 109 s
0: Total training time 109 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       8|                 160|                      0.41|                     201348.0|                         1.817|
DONE!
