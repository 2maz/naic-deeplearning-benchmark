:::NVLOGv0.2.2 Tacotron2_PyT 1592886978.111024380 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592886978.134937048 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592886978.156185865 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592886982.808965445 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592886982.815971851 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 36, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592886985.896797180 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592887015.577217817 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592887015.578599215 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887017.048237801 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887027.376911640 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.092201232910156
:::NVLOGv0.2.2 Tacotron2_PyT 1592887029.037165642 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887029.039616585 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 13633.231451374179
:::NVLOGv0.2.2 Tacotron2_PyT 1592887029.040251732 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 11.99187445640564
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887029.053032160 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887030.175836325 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.60291290283203
:::NVLOGv0.2.2 Tacotron2_PyT 1592887031.678223133 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887031.680355787 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 62174.156412250886
:::NVLOGv0.2.2 Tacotron2_PyT 1592887031.680748224 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.626493215560913
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887031.686084509 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592887032.745056152 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.17601776123047
:::NVLOGv0.2.2 Tacotron2_PyT 1592887034.450207472 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592887034.452624321 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 58933.098057870135
:::NVLOGv0.2.2 Tacotron2_PyT 1592887034.453662634 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.764660358428955
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887034.459030867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592887035.429339647 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.81769561767578
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.138817787 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.141178370 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 60713.11900485333
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.143217325 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6802444458007812
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.259515285 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.262485266 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 30091.41987448917
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.265254498 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 48863.40123158713
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.266202927 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.92220687866211
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.266754627 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 21.682060956954956
:::NVLOGv0.2.2 Tacotron2_PyT 1592887037.267289639 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592887038.564185619 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.34150695800781
:::NVLOGv0.2.2 Tacotron2_PyT 1592887038.565331221 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887038.892171383 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887040.003760576 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887041.136795282 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.32147216796875
:::NVLOGv0.2.2 Tacotron2_PyT 1592887042.675149918 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592887042.677313566 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59662.73454267928
:::NVLOGv0.2.2 Tacotron2_PyT 1592887042.678757906 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.674098014831543
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887042.692979097 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887043.694509983 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.19618225097656
:::NVLOGv0.2.2 Tacotron2_PyT 1592887045.387482643 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887045.389018297 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59619.13417900055
:::NVLOGv0.2.2 Tacotron2_PyT 1592887045.390284300 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.695829153060913
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887045.395911932 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592887046.421940327 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.608848571777344
:::NVLOGv0.2.2 Tacotron2_PyT 1592887047.927709579 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592887047.928915977 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 65851.32228261846
:::NVLOGv0.2.2 Tacotron2_PyT 1592887047.930420160 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.532280206680298
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887047.936554193 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592887048.920994043 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.02214050292969
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.721424341 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.723600626 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59444.41579986626
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.725435019 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7853078842163086
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.838400364 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.841602087 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 54624.235060392166
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.843358278 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 61144.40170104114
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.844705820 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.787160873413086
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.846098423 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 11.946931600570679
:::NVLOGv0.2.2 Tacotron2_PyT 1592887050.847431898 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887052.153705120 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.37754440307617
:::NVLOGv0.2.2 Tacotron2_PyT 1592887052.155267954 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592887052.156426668 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 66.25897359848022
:::NVLOGv0.2.2 Tacotron2_PyT 1592887052.156803131 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 66.25897359848022
:::NVLOGv0.2.2 Tacotron2_PyT 1592887052.157192945 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 74.14306116104126
:::NVLOGv0.2.2 Tacotron2_PyT 1592887052.157533646 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
