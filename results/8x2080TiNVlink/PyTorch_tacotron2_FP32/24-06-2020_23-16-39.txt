:::NVLOGv0.2.2 Tacotron2_PyT 1593040601.930388927 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593040601.955337286 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593040601.974153757 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1593040605.758335590 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1593040605.765894651 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 36, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1593040608.867846727 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1593040637.432771921 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1593040637.434548140 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040639.264572859 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040650.088000774 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.635868072509766
:::NVLOGv0.2.2 Tacotron2_PyT 1593040651.727001429 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040651.728211164 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 13116.453143506087
:::NVLOGv0.2.2 Tacotron2_PyT 1593040651.728796721 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 12.464345216751099
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040651.738865376 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040653.029530764 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.14836883544922
:::NVLOGv0.2.2 Tacotron2_PyT 1593040654.620125532 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040654.622008801 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56656.200866390995
:::NVLOGv0.2.2 Tacotron2_PyT 1593040654.623494625 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8822970390319824
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040654.630121946 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593040655.716896772 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.71152877807617
:::NVLOGv0.2.2 Tacotron2_PyT 1593040657.497921228 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593040657.499238729 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56800.22637041825
:::NVLOGv0.2.2 Tacotron2_PyT 1593040657.501464128 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8684744834899902
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040657.508670092 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593040658.498781443 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.36147689819336
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.238736868 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.241178513 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59582.594376529785
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.244338036 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7310996055603027
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.357555866 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.359133482 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 28460.90784908188
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.359802723 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 46538.868689211275
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.360322714 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.46431064605713
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.360828161 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 22.9242160320282
:::NVLOGv0.2.2 Tacotron2_PyT 1593040660.361323118 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1593040661.735874653 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.21746063232422
:::NVLOGv0.2.2 Tacotron2_PyT 1593040661.737021685 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040662.060330391 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040663.004145384 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040664.161386728 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.87187957763672
:::NVLOGv0.2.2 Tacotron2_PyT 1593040665.705151796 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1593040665.706979752 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 58999.020677816516
:::NVLOGv0.2.2 Tacotron2_PyT 1593040665.711567640 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7041804790496826
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040665.720787764 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040666.769223452 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.743350982666016
:::NVLOGv0.2.2 Tacotron2_PyT 1593040668.457471371 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040668.458783150 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 58706.18460352491
:::NVLOGv0.2.2 Tacotron2_PyT 1593040668.459999323 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7377524375915527
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040668.467480659 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593040669.514186144 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.15464401245117
:::NVLOGv0.2.2 Tacotron2_PyT 1593040671.052040100 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1593040671.053552389 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 64507.762945164475
:::NVLOGv0.2.2 Tacotron2_PyT 1593040671.054779530 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.585022211074829
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040671.062586069 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593040672.070682764 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.56523513793945
:::NVLOGv0.2.2 Tacotron2_PyT 1593040673.857333422 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1593040673.859236956 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59227.504658891914
:::NVLOGv0.2.2 Tacotron2_PyT 1593040673.860925436 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.795508623123169
:::NVLOGv0.2.2 Tacotron2_PyT 1593040674.009988308 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040674.011218071 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 54609.00506405857
:::NVLOGv0.2.2 Tacotron2_PyT 1593040674.011753559 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 60360.118221349454
:::NVLOGv0.2.2 Tacotron2_PyT 1593040674.012261629 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.33377742767334
:::NVLOGv0.2.2 Tacotron2_PyT 1593040674.012761831 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 11.950263500213623
:::NVLOGv0.2.2 Tacotron2_PyT 1593040674.013247967 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040675.369126320 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.20515060424805
:::NVLOGv0.2.2 Tacotron2_PyT 1593040675.370491743 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1593040675.371757269 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 66.50317287445068
:::NVLOGv0.2.2 Tacotron2_PyT 1593040675.372128963 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 66.50317287445068
:::NVLOGv0.2.2 Tacotron2_PyT 1593040675.372536182 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 73.53790140151978
:::NVLOGv0.2.2 Tacotron2_PyT 1593040675.372881651 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
