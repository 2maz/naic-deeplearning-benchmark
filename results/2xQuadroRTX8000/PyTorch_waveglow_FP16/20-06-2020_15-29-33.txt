:::NVLOGv0.2.2 Tacotron2_PyT 1592666976.260497570 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592666976.288955688 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592666976.306454897 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592666976.732735634 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 2, "name": ["Quadro RTX 8000", "Quadro RTX 8000"], "mem": ["48601 MiB", "48601 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592666976.737988234 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 52, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 2, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592666977.735275507 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1592666991.814135313 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592666991.815568686 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592666992.280971527 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592666996.298045397 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.001972619444131851
:::NVLOGv0.2.2 Tacotron2_PyT 1592667001.717117548 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667001.717633963 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 88142.35125113933
:::NVLOGv0.2.2 Tacotron2_PyT 1592667001.717985630 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 9.439276218414307
Batch: 1/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667001.721374750 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667002.961910486 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022612856701016426
:::NVLOGv0.2.2 Tacotron2_PyT 1592667006.328586102 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667006.329078197 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 180554.55440083693
:::NVLOGv0.2.2 Tacotron2_PyT 1592667006.329415798 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.608025550842285
Batch: 2/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667006.332802534 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592667007.580812454 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0025311443023383617
:::NVLOGv0.2.2 Tacotron2_PyT 1592667010.952833176 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592667010.953350306 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 180053.84948419363
:::NVLOGv0.2.2 Tacotron2_PyT 1592667010.953689814 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.620839834213257
Batch: 3/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667010.956934929 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592667012.217905045 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002204841235652566
:::NVLOGv0.2.2 Tacotron2_PyT 1592667015.595006466 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592667015.595514536 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 179354.1343011928
:::NVLOGv0.2.2 Tacotron2_PyT 1592667015.595847130 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.638867139816284
Batch: 4/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667015.598649502 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592667016.823498011 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018618246540427208
:::NVLOGv0.2.2 Tacotron2_PyT 1592667020.240610838 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592667020.241121531 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 179213.7886209859
:::NVLOGv0.2.2 Tacotron2_PyT 1592667020.241495371 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.642499923706055
Batch: 5/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667020.244304180 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592667021.471224070 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002264562528580427
:::NVLOGv0.2.2 Tacotron2_PyT 1592667024.860549450 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592667024.860934973 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 180216.35955469322
:::NVLOGv0.2.2 Tacotron2_PyT 1592667024.861261129 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.616672992706299
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592667025.043611765 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667025.043983221 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 150231.9911938566
:::NVLOGv0.2.2 Tacotron2_PyT 1592667025.044314861 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 164589.17293550697
:::NVLOGv0.2.2 Tacotron2_PyT 1592667025.044625759 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002182712972474595
:::NVLOGv0.2.2 Tacotron2_PyT 1592667025.044932842 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 33.22860836982727
:::NVLOGv0.2.2 Tacotron2_PyT 1592667025.045236111 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592667027.769221067 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0021610762923955917
:::NVLOGv0.2.2 Tacotron2_PyT 1592667027.770137072 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667030.502936602 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667030.826238632 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667032.086711884 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024475622922182083
:::NVLOGv0.2.2 Tacotron2_PyT 1592667035.466482878 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592667035.467184782 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 179208.62552783737
:::NVLOGv0.2.2 Tacotron2_PyT 1592667035.467523813 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.642633676528931
Batch: 1/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667035.471398592 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667036.727438927 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0017798009794205427
:::NVLOGv0.2.2 Tacotron2_PyT 1592667040.076102734 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667040.076508999 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 180638.75452540166
:::NVLOGv0.2.2 Tacotron2_PyT 1592667040.076831818 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.605877637863159
Batch: 2/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667040.079548120 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592667041.387963057 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00259246863424778
:::NVLOGv0.2.2 Tacotron2_PyT 1592667044.775607586 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592667044.776003361 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177146.59604616067
:::NVLOGv0.2.2 Tacotron2_PyT 1592667044.776329517 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.696675062179565
Batch: 3/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667044.779443026 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592667046.040123701 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002183156553655863
:::NVLOGv0.2.2 Tacotron2_PyT 1592667049.430299997 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592667049.430676460 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 178865.73079875615
:::NVLOGv0.2.2 Tacotron2_PyT 1592667049.431004047 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.651533842086792
Batch: 4/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667049.433781862 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592667050.690857172 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022925948724150658
:::NVLOGv0.2.2 Tacotron2_PyT 1592667054.082624435 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592667054.083000660 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 178953.4283064961
:::NVLOGv0.2.2 Tacotron2_PyT 1592667054.083353519 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.649254322052002
Batch: 5/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667054.085984230 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592667055.348415375 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002284705638885498
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.746918440 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.747425556 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 178485.10182285774
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.747753382 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.661453485488892
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.814535379 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.814939737 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 176320.61726229178
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.815295696 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 178883.03950458494
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.815644264 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002263381495140493
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.815992117 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 28.312060594558716
:::NVLOGv0.2.2 Tacotron2_PyT 1592667058.816346884 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667060.538347006 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0021061066072434187
:::NVLOGv0.2.2 Tacotron2_PyT 1592667060.539340496 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592667060.540371895 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 82.8045425415039
:::NVLOGv0.2.2 Tacotron2_PyT 1592667060.540729761 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 82.8045425415039
:::NVLOGv0.2.2 Tacotron2_PyT 1592667060.541117191 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 84.38727331161499
:::NVLOGv0.2.2 Tacotron2_PyT 1592667060.541452885 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
