:::NVLOGv0.2.2 Tacotron2_PyT 1592460897.169342279 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592460897.193655491 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592460897.214585304 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592460897.764459848 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 2, "name": ["Quadro RTX 8000", "Quadro RTX 8000"], "mem": ["48601 MiB", "48601 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592460897.770647764 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 52, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 2, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592460898.214831591 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1592460915.177602291 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592460915.179197788 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460915.674532413 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460923.447896719 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002069611568003893
:::NVLOGv0.2.2 Tacotron2_PyT 1592460931.220879316 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460931.221376657 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 53510.41216757931
:::NVLOGv0.2.2 Tacotron2_PyT 1592460931.221682787 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 15.548375844955444
Batch: 1/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460931.224864006 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460933.266019344 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002394821960479021
:::NVLOGv0.2.2 Tacotron2_PyT 1592460937.319352627 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460937.319867611 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 136499.56497781785
:::NVLOGv0.2.2 Tacotron2_PyT 1592460937.320216894 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.095257520675659
Batch: 2/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460937.323488235 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592460939.020617008 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020165902096778154
:::NVLOGv0.2.2 Tacotron2_PyT 1592460943.178771257 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592460943.179210663 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 142076.85109518105
:::NVLOGv0.2.2 Tacotron2_PyT 1592460943.179581165 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.855985641479492
Batch: 3/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460943.182737350 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592460944.823882103 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022316924296319485
:::NVLOGv0.2.2 Tacotron2_PyT 1592460949.028383255 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592460949.028867006 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 142311.015669639
:::NVLOGv0.2.2 Tacotron2_PyT 1592460949.029240608 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.8463499546051025
Batch: 4/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460949.031981230 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592460950.706418276 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002050842856988311
:::NVLOGv0.2.2 Tacotron2_PyT 1592460954.762202263 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592460954.762620449 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 145185.06354023868
:::NVLOGv0.2.2 Tacotron2_PyT 1592460954.762981415 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.730617046356201
Batch: 5/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460954.764932871 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592460956.453859806 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021200249902904034
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.544471979 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.544840336 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 143946.62649959317
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.545162201 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.7799201011657715
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.715833664 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.717260122 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 109623.50321302681
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.718580008 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 127254.92232500816
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.719881773 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0021472640025118985
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.721183300 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 45.53767991065979
:::NVLOGv0.2.2 Tacotron2_PyT 1592460960.722467184 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592460964.468950748 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0020046825520694256
:::NVLOGv0.2.2 Tacotron2_PyT 1592460964.471972942 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460966.994849682 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460967.297572136 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460969.170893431 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020988809410482645
:::NVLOGv0.2.2 Tacotron2_PyT 1592460973.477870941 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592460973.478404760 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 134561.01903668162
:::NVLOGv0.2.2 Tacotron2_PyT 1592460973.478727102 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.183068513870239
Batch: 1/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460973.482014418 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460975.207504272 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023299474269151688
:::NVLOGv0.2.2 Tacotron2_PyT 1592460979.374101400 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460979.374872923 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 141186.13247539927
:::NVLOGv0.2.2 Tacotron2_PyT 1592460979.375386953 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.892930030822754
Batch: 2/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460979.379554033 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592460981.023626566 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002155812457203865
:::NVLOGv0.2.2 Tacotron2_PyT 1592460985.232163429 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592460985.232666969 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 142135.96401924457
:::NVLOGv0.2.2 Tacotron2_PyT 1592460985.232998610 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.85355019569397
Batch: 3/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460985.236503363 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592460986.857755899 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00258284667506814
:::NVLOGv0.2.2 Tacotron2_PyT 1592460990.988158941 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592460990.988824844 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 144635.9920149859
:::NVLOGv0.2.2 Tacotron2_PyT 1592460990.989325762 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.752371788024902
Batch: 4/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460990.992400646 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592460992.584354401 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020771229173988104
:::NVLOGv0.2.2 Tacotron2_PyT 1592460996.706906557 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592460996.707282782 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 145579.30226864212
:::NVLOGv0.2.2 Tacotron2_PyT 1592460996.707630634 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.715098142623901
Batch: 5/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592460996.709572077 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592460998.427296877 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002296358346939087
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.675502539 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.675942898 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 139447.6699546747
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.676264763 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.966395854949951
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.718119621 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.719382524 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 139737.78991501275
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.720550299 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 141257.67996160468
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.721710682 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0022568281274288893
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.722883940 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 35.7240514755249
:::NVLOGv0.2.2 Tacotron2_PyT 1592461002.724023104 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592461004.703263521 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0020398972555994987
:::NVLOGv0.2.2 Tacotron2_PyT 1592461004.704122782 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592461004.705143452 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 106.48978924751282
:::NVLOGv0.2.2 Tacotron2_PyT 1592461004.705473423 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 106.48978924751282
:::NVLOGv0.2.2 Tacotron2_PyT 1592461004.705818176 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 107.61455655097961
:::NVLOGv0.2.2 Tacotron2_PyT 1592461004.706156731 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
