1: Collecting environment information...
3: Collecting environment information...
2: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
2: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
2: Saving results to: results/gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
2: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
3: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce GTX 1080 Ti
GPU 1: GeForce GTX 1080 Ti
GPU 2: GeForce GTX 1080 Ti
GPU 3: GeForce GTX 1080 Ti

Nvidia driver version: 440.64
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
3: Saving results to: results/gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
3: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
3: Worker 3 is using worker seed: 1323436024
2: Worker 2 is using worker seed: 3588440356
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31794
2: Size of vocabulary: 31794
3: Size of vocabulary: 31794
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Pairs before: 160078, after: 148120
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Pairs before: 160078, after: 148120
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
2: Filtering data, min len: 0, max len: 125
2: Pairs before: 5100, after: 5100
3: Filtering data, min len: 0, max len: 125
3: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
2: Filtering data, min len: 0, max len: 150
2: Pairs before: 3003, after: 3003
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Number of parameters: 159593523
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
2: Saving state of the tokenizer
2: Initializing fp32 optimizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 452
2: Scheduler decay interval: 57
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 452
0: Scheduler decay interval: 57
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
0: Starting epoch 0
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
2: Starting epoch 0
1: Scheduler remain steps: 452
1: Scheduler decay interval: 57
0: Executing preallocation
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
2: Executing preallocation
1: Starting epoch 0
1: Executing preallocation
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159593523
3: Saving state of the tokenizer
3: Initializing fp32 optimizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 452
3: Scheduler decay interval: 57
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: Starting epoch 0
3: Executing preallocation
0: Sampler for epoch 0 uses seed 2602510382
2: Sampler for epoch 0 uses seed 2602510382
3: Sampler for epoch 0 uses seed 2602510382
1: Sampler for epoch 0 uses seed 2602510382
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
3: TRAIN [0][0/340]	Time 0.979 (0.979)	Data 1.07e-01 (1.07e-01)	Tok/s 3107 (3107)	Loss/tok 10.6330 (10.6330)	LR 2.047e-05
0: TRAIN [0][0/340]	Time 0.995 (0.995)	Data 1.19e-01 (1.19e-01)	Tok/s 2918 (2918)	Loss/tok 10.5903 (10.5903)	LR 2.047e-05
2: TRAIN [0][0/340]	Time 0.987 (0.987)	Data 9.03e-02 (9.03e-02)	Tok/s 2953 (2953)	Loss/tok 10.6002 (10.6002)	LR 2.047e-05
1: TRAIN [0][0/340]	Time 0.986 (0.986)	Data 1.27e-01 (1.27e-01)	Tok/s 2894 (2894)	Loss/tok 10.5689 (10.5689)	LR 2.047e-05
0: TRAIN [0][10/340]	Time 0.721 (0.736)	Data 1.15e-04 (1.09e-02)	Tok/s 6761 (5788)	Loss/tok 9.7480 (10.1214)	LR 2.576e-05
2: TRAIN [0][10/340]	Time 0.721 (0.735)	Data 1.15e-04 (8.31e-03)	Tok/s 6939 (5853)	Loss/tok 9.6863 (10.1024)	LR 2.576e-05
3: TRAIN [0][10/340]	Time 0.721 (0.734)	Data 1.11e-04 (9.79e-03)	Tok/s 6796 (5815)	Loss/tok 9.7030 (10.1100)	LR 2.576e-05
1: TRAIN [0][10/340]	Time 0.721 (0.735)	Data 1.09e-04 (1.17e-02)	Tok/s 6735 (5820)	Loss/tok 9.6833 (10.1092)	LR 2.576e-05
0: TRAIN [0][20/340]	Time 0.610 (0.717)	Data 1.16e-04 (5.76e-03)	Tok/s 4815 (5905)	Loss/tok 9.1028 (9.7678)	LR 3.244e-05
3: TRAIN [0][20/340]	Time 0.610 (0.716)	Data 1.15e-04 (5.19e-03)	Tok/s 4865 (5881)	Loss/tok 9.0874 (9.7713)	LR 3.244e-05
1: TRAIN [0][20/340]	Time 0.610 (0.716)	Data 1.18e-04 (6.18e-03)	Tok/s 4827 (5897)	Loss/tok 8.9916 (9.7654)	LR 3.244e-05
2: TRAIN [0][20/340]	Time 0.609 (0.716)	Data 1.58e-04 (4.41e-03)	Tok/s 4893 (5889)	Loss/tok 9.0822 (9.7611)	LR 3.244e-05
0: TRAIN [0][30/340]	Time 0.859 (0.729)	Data 1.14e-04 (3.94e-03)	Tok/s 7963 (6195)	Loss/tok 8.9704 (9.5091)	LR 4.083e-05
1: TRAIN [0][30/340]	Time 0.859 (0.729)	Data 1.63e-04 (4.22e-03)	Tok/s 8001 (6196)	Loss/tok 8.9899 (9.5114)	LR 4.083e-05
2: TRAIN [0][30/340]	Time 0.859 (0.729)	Data 1.09e-04 (3.06e-03)	Tok/s 7916 (6175)	Loss/tok 8.9446 (9.5067)	LR 4.083e-05
3: TRAIN [0][30/340]	Time 0.862 (0.729)	Data 1.15e-04 (3.55e-03)	Tok/s 7850 (6175)	Loss/tok 9.0288 (9.5098)	LR 4.083e-05
3: TRAIN [0][40/340]	Time 0.612 (0.747)	Data 1.18e-04 (2.71e-03)	Tok/s 4999 (6378)	Loss/tok 8.3695 (9.3084)	LR 5.141e-05
2: TRAIN [0][40/340]	Time 0.613 (0.747)	Data 1.17e-04 (2.34e-03)	Tok/s 4633 (6374)	Loss/tok 8.3662 (9.3062)	LR 5.141e-05
0: TRAIN [0][40/340]	Time 0.612 (0.747)	Data 1.12e-04 (3.01e-03)	Tok/s 4860 (6389)	Loss/tok 8.4796 (9.3124)	LR 5.141e-05
1: TRAIN [0][40/340]	Time 0.613 (0.747)	Data 1.27e-04 (3.22e-03)	Tok/s 4813 (6389)	Loss/tok 8.4830 (9.3163)	LR 5.141e-05
1: TRAIN [0][50/340]	Time 1.040 (0.769)	Data 1.67e-04 (2.62e-03)	Tok/s 8596 (6636)	Loss/tok 8.6279 (9.1384)	LR 6.472e-05
0: TRAIN [0][50/340]	Time 1.042 (0.769)	Data 1.15e-04 (2.44e-03)	Tok/s 8431 (6631)	Loss/tok 8.6202 (9.1340)	LR 6.472e-05
3: TRAIN [0][50/340]	Time 1.041 (0.769)	Data 1.17e-04 (2.21e-03)	Tok/s 8477 (6622)	Loss/tok 8.5783 (9.1327)	LR 6.472e-05
2: TRAIN [0][50/340]	Time 1.042 (0.769)	Data 1.18e-04 (1.91e-03)	Tok/s 8410 (6616)	Loss/tok 8.6032 (9.1181)	LR 6.472e-05
0: TRAIN [0][60/340]	Time 0.872 (0.759)	Data 1.19e-04 (2.06e-03)	Tok/s 7829 (6500)	Loss/tok 8.3195 (9.0248)	LR 8.148e-05
1: TRAIN [0][60/340]	Time 0.872 (0.759)	Data 1.17e-04 (2.21e-03)	Tok/s 7825 (6501)	Loss/tok 8.3154 (9.0281)	LR 8.148e-05
3: TRAIN [0][60/340]	Time 0.872 (0.759)	Data 1.54e-04 (1.86e-03)	Tok/s 7800 (6488)	Loss/tok 8.3210 (9.0266)	LR 8.148e-05
2: TRAIN [0][60/340]	Time 0.873 (0.759)	Data 1.18e-04 (1.61e-03)	Tok/s 7727 (6481)	Loss/tok 8.2885 (9.0158)	LR 8.148e-05
2: TRAIN [0][70/340]	Time 0.725 (0.756)	Data 1.15e-04 (1.40e-03)	Tok/s 6801 (6476)	Loss/tok 7.9208 (8.8930)	LR 1.026e-04
3: TRAIN [0][70/340]	Time 0.725 (0.756)	Data 1.19e-04 (1.62e-03)	Tok/s 6825 (6485)	Loss/tok 8.0194 (8.9007)	LR 1.026e-04
0: TRAIN [0][70/340]	Time 0.726 (0.756)	Data 1.17e-04 (1.79e-03)	Tok/s 6552 (6496)	Loss/tok 8.0281 (8.8974)	LR 1.026e-04
1: TRAIN [0][70/340]	Time 0.725 (0.756)	Data 1.19e-04 (1.91e-03)	Tok/s 6762 (6496)	Loss/tok 7.9258 (8.8998)	LR 1.026e-04
1: TRAIN [0][80/340]	Time 0.617 (0.759)	Data 1.18e-04 (1.69e-03)	Tok/s 4924 (6537)	Loss/tok 7.8164 (8.7860)	LR 1.291e-04
0: TRAIN [0][80/340]	Time 0.618 (0.759)	Data 1.16e-04 (1.58e-03)	Tok/s 4814 (6534)	Loss/tok 7.6432 (8.7792)	LR 1.291e-04
2: TRAIN [0][80/340]	Time 0.617 (0.759)	Data 1.11e-04 (1.24e-03)	Tok/s 4841 (6518)	Loss/tok 7.7465 (8.7756)	LR 1.291e-04
3: TRAIN [0][80/340]	Time 0.618 (0.759)	Data 1.14e-04 (1.44e-03)	Tok/s 4656 (6525)	Loss/tok 7.7022 (8.7836)	LR 1.291e-04
3: TRAIN [0][90/340]	Time 0.882 (0.770)	Data 1.17e-04 (1.29e-03)	Tok/s 7750 (6606)	Loss/tok 7.9731 (8.6701)	LR 1.626e-04
0: TRAIN [0][90/340]	Time 0.883 (0.770)	Data 1.20e-04 (1.42e-03)	Tok/s 7713 (6615)	Loss/tok 7.8921 (8.6651)	LR 1.626e-04
1: TRAIN [0][90/340]	Time 0.883 (0.770)	Data 1.21e-04 (1.52e-03)	Tok/s 7801 (6619)	Loss/tok 7.8513 (8.6712)	LR 1.626e-04
2: TRAIN [0][90/340]	Time 0.883 (0.770)	Data 1.17e-04 (1.12e-03)	Tok/s 7721 (6600)	Loss/tok 7.8102 (8.6605)	LR 1.626e-04
3: TRAIN [0][100/340]	Time 0.739 (0.767)	Data 1.25e-04 (1.18e-03)	Tok/s 6648 (6544)	Loss/tok 7.8201 (8.6086)	LR 2.047e-04
0: TRAIN [0][100/340]	Time 0.739 (0.768)	Data 1.17e-04 (1.29e-03)	Tok/s 6612 (6548)	Loss/tok 7.7801 (8.6053)	LR 2.047e-04
2: TRAIN [0][100/340]	Time 0.740 (0.768)	Data 1.14e-04 (1.02e-03)	Tok/s 6564 (6538)	Loss/tok 7.8235 (8.6010)	LR 2.047e-04
1: TRAIN [0][100/340]	Time 0.740 (0.768)	Data 1.17e-04 (1.38e-03)	Tok/s 6618 (6554)	Loss/tok 7.8183 (8.6125)	LR 2.047e-04
0: TRAIN [0][110/340]	Time 0.617 (0.771)	Data 1.18e-04 (1.19e-03)	Tok/s 4723 (6561)	Loss/tok 7.5252 (8.5543)	LR 2.576e-04
2: TRAIN [0][110/340]	Time 0.617 (0.771)	Data 1.14e-04 (9.39e-04)	Tok/s 4715 (6555)	Loss/tok 7.5274 (8.5510)	LR 2.576e-04
1: TRAIN [0][110/340]	Time 0.619 (0.771)	Data 1.16e-04 (1.27e-03)	Tok/s 4823 (6575)	Loss/tok 7.5629 (8.5595)	LR 2.576e-04
3: TRAIN [0][110/340]	Time 0.620 (0.771)	Data 1.25e-04 (1.08e-03)	Tok/s 4666 (6556)	Loss/tok 7.4207 (8.5575)	LR 2.576e-04
3: TRAIN [0][120/340]	Time 0.628 (0.778)	Data 1.16e-04 (1.00e-03)	Tok/s 4808 (6593)	Loss/tok 7.3969 (8.4927)	LR 3.244e-04
2: TRAIN [0][120/340]	Time 0.629 (0.778)	Data 1.16e-04 (8.72e-04)	Tok/s 4538 (6589)	Loss/tok 7.3667 (8.4845)	LR 3.244e-04
0: TRAIN [0][120/340]	Time 0.632 (0.778)	Data 1.21e-04 (1.10e-03)	Tok/s 4628 (6594)	Loss/tok 7.3393 (8.4889)	LR 3.244e-04
1: TRAIN [0][120/340]	Time 0.633 (0.778)	Data 1.19e-04 (1.17e-03)	Tok/s 4615 (6608)	Loss/tok 7.3176 (8.4950)	LR 3.244e-04
3: TRAIN [0][130/340]	Time 0.620 (0.781)	Data 1.66e-04 (9.36e-04)	Tok/s 4777 (6597)	Loss/tok 7.4212 (8.4355)	LR 4.083e-04
2: TRAIN [0][130/340]	Time 0.621 (0.781)	Data 1.14e-04 (8.14e-04)	Tok/s 4550 (6595)	Loss/tok 7.5601 (8.4305)	LR 4.083e-04
0: TRAIN [0][130/340]	Time 0.621 (0.781)	Data 1.17e-04 (1.02e-03)	Tok/s 4692 (6600)	Loss/tok 7.5189 (8.4342)	LR 4.083e-04
1: TRAIN [0][130/340]	Time 0.621 (0.781)	Data 1.15e-04 (1.09e-03)	Tok/s 4782 (6612)	Loss/tok 7.4801 (8.4371)	LR 4.083e-04
1: TRAIN [0][140/340]	Time 0.873 (0.779)	Data 1.20e-04 (1.03e-03)	Tok/s 7791 (6584)	Loss/tok 7.8286 (8.3903)	LR 5.141e-04
2: TRAIN [0][140/340]	Time 0.873 (0.779)	Data 1.19e-04 (7.64e-04)	Tok/s 7696 (6565)	Loss/tok 7.7854 (8.3846)	LR 5.141e-04
0: TRAIN [0][140/340]	Time 0.873 (0.779)	Data 1.29e-04 (9.60e-04)	Tok/s 7880 (6570)	Loss/tok 7.7813 (8.3878)	LR 5.141e-04
3: TRAIN [0][140/340]	Time 0.875 (0.779)	Data 1.12e-04 (8.78e-04)	Tok/s 7680 (6565)	Loss/tok 7.7796 (8.3897)	LR 5.141e-04
0: TRAIN [0][150/340]	Time 0.885 (0.777)	Data 1.14e-04 (9.05e-04)	Tok/s 7715 (6549)	Loss/tok 7.9197 (8.3431)	LR 6.472e-04
2: TRAIN [0][150/340]	Time 0.885 (0.777)	Data 1.08e-04 (7.22e-04)	Tok/s 7495 (6542)	Loss/tok 7.9579 (8.3412)	LR 6.472e-04
3: TRAIN [0][150/340]	Time 0.885 (0.777)	Data 1.19e-04 (8.29e-04)	Tok/s 7620 (6543)	Loss/tok 7.8934 (8.3458)	LR 6.472e-04
1: TRAIN [0][150/340]	Time 0.885 (0.777)	Data 1.17e-04 (9.65e-04)	Tok/s 7647 (6560)	Loss/tok 7.9197 (8.3475)	LR 6.472e-04
2: TRAIN [0][160/340]	Time 0.738 (0.778)	Data 1.20e-04 (6.85e-04)	Tok/s 6626 (6569)	Loss/tok 7.4884 (8.2956)	LR 8.148e-04
1: TRAIN [0][160/340]	Time 0.738 (0.778)	Data 1.28e-04 (9.18e-04)	Tok/s 6671 (6587)	Loss/tok 7.5954 (8.3014)	LR 8.148e-04
0: TRAIN [0][160/340]	Time 0.737 (0.778)	Data 1.20e-04 (8.56e-04)	Tok/s 6545 (6576)	Loss/tok 7.5262 (8.2968)	LR 8.148e-04
3: TRAIN [0][160/340]	Time 0.737 (0.778)	Data 1.16e-04 (7.86e-04)	Tok/s 6737 (6574)	Loss/tok 7.5649 (8.2990)	LR 8.148e-04
0: TRAIN [0][170/340]	Time 0.866 (0.772)	Data 1.17e-04 (8.13e-04)	Tok/s 7873 (6503)	Loss/tok 7.6727 (8.2630)	LR 1.026e-03
1: TRAIN [0][170/340]	Time 0.865 (0.772)	Data 1.08e-04 (8.71e-04)	Tok/s 7817 (6512)	Loss/tok 7.6351 (8.2669)	LR 1.026e-03
2: TRAIN [0][170/340]	Time 0.866 (0.772)	Data 1.14e-04 (6.52e-04)	Tok/s 8004 (6497)	Loss/tok 7.6461 (8.2613)	LR 1.026e-03
3: TRAIN [0][170/340]	Time 0.869 (0.772)	Data 1.20e-04 (7.47e-04)	Tok/s 7836 (6501)	Loss/tok 7.5884 (8.2640)	LR 1.026e-03
3: TRAIN [0][180/340]	Time 0.632 (0.773)	Data 1.19e-04 (7.12e-04)	Tok/s 4648 (6513)	Loss/tok 7.1370 (8.2238)	LR 1.291e-03
0: TRAIN [0][180/340]	Time 0.632 (0.773)	Data 1.09e-04 (7.74e-04)	Tok/s 4548 (6515)	Loss/tok 7.1526 (8.2251)	LR 1.291e-03
1: TRAIN [0][180/340]	Time 0.633 (0.773)	Data 1.60e-04 (8.29e-04)	Tok/s 4801 (6523)	Loss/tok 7.0619 (8.2279)	LR 1.291e-03
2: TRAIN [0][180/340]	Time 0.632 (0.773)	Data 1.17e-04 (6.22e-04)	Tok/s 4669 (6507)	Loss/tok 7.1798 (8.2221)	LR 1.291e-03
3: TRAIN [0][190/340]	Time 0.741 (0.774)	Data 1.17e-04 (6.82e-04)	Tok/s 6470 (6514)	Loss/tok 7.6643 (8.1860)	LR 1.626e-03
1: TRAIN [0][190/340]	Time 0.740 (0.774)	Data 1.11e-04 (7.93e-04)	Tok/s 6601 (6524)	Loss/tok 7.5628 (8.1904)	LR 1.626e-03
0: TRAIN [0][190/340]	Time 0.741 (0.774)	Data 1.12e-04 (7.40e-04)	Tok/s 6471 (6517)	Loss/tok 7.5362 (8.1859)	LR 1.626e-03
2: TRAIN [0][190/340]	Time 0.739 (0.774)	Data 1.73e-04 (5.96e-04)	Tok/s 6590 (6509)	Loss/tok 7.6516 (8.1836)	LR 1.626e-03
1: TRAIN [0][200/340]	Time 0.758 (0.773)	Data 1.21e-04 (7.59e-04)	Tok/s 6592 (6509)	Loss/tok 7.3477 (8.1551)	LR 2.000e-03
0: TRAIN [0][200/340]	Time 0.701 (0.773)	Data 1.24e-04 (7.09e-04)	Tok/s 6994 (6500)	Loss/tok 7.1170 (8.1497)	LR 2.000e-03
3: TRAIN [0][200/340]	Time 0.762 (0.773)	Data 1.15e-04 (6.54e-04)	Tok/s 6375 (6498)	Loss/tok 7.2279 (8.1492)	LR 2.000e-03
2: TRAIN [0][200/340]	Time 0.705 (0.773)	Data 1.29e-04 (5.73e-04)	Tok/s 7002 (6494)	Loss/tok 7.2303 (8.1483)	LR 2.000e-03
0: TRAIN [0][210/340]	Time 0.627 (0.774)	Data 1.19e-04 (6.81e-04)	Tok/s 4791 (6494)	Loss/tok 6.8639 (8.1111)	LR 2.000e-03
3: TRAIN [0][210/340]	Time 0.627 (0.774)	Data 1.21e-04 (6.29e-04)	Tok/s 4690 (6491)	Loss/tok 6.9060 (8.1097)	LR 2.000e-03
1: TRAIN [0][210/340]	Time 0.627 (0.774)	Data 1.11e-04 (7.29e-04)	Tok/s 4625 (6500)	Loss/tok 7.0464 (8.1157)	LR 2.000e-03
2: TRAIN [0][210/340]	Time 0.628 (0.774)	Data 1.28e-04 (5.52e-04)	Tok/s 4718 (6486)	Loss/tok 6.9678 (8.1099)	LR 2.000e-03
0: TRAIN [0][220/340]	Time 0.625 (0.772)	Data 1.16e-04 (6.56e-04)	Tok/s 4703 (6488)	Loss/tok 6.6723 (8.0728)	LR 2.000e-03
3: TRAIN [0][220/340]	Time 0.625 (0.772)	Data 1.19e-04 (6.05e-04)	Tok/s 4744 (6486)	Loss/tok 6.8394 (8.0718)	LR 2.000e-03
2: TRAIN [0][220/340]	Time 0.625 (0.772)	Data 1.16e-04 (5.32e-04)	Tok/s 4730 (6480)	Loss/tok 6.7011 (8.0726)	LR 2.000e-03
1: TRAIN [0][220/340]	Time 0.626 (0.772)	Data 1.15e-04 (7.01e-04)	Tok/s 4742 (6493)	Loss/tok 6.8214 (8.0778)	LR 2.000e-03
0: TRAIN [0][230/340]	Time 0.896 (0.773)	Data 1.25e-04 (6.32e-04)	Tok/s 7610 (6505)	Loss/tok 7.1244 (8.0245)	LR 2.000e-03
1: TRAIN [0][230/340]	Time 0.896 (0.773)	Data 1.57e-04 (6.76e-04)	Tok/s 7557 (6510)	Loss/tok 7.0797 (8.0307)	LR 2.000e-03
3: TRAIN [0][230/340]	Time 0.896 (0.773)	Data 1.24e-04 (5.84e-04)	Tok/s 7593 (6503)	Loss/tok 7.0679 (8.0249)	LR 2.000e-03
2: TRAIN [0][230/340]	Time 0.898 (0.773)	Data 1.13e-04 (5.14e-04)	Tok/s 7527 (6499)	Loss/tok 7.1324 (8.0252)	LR 2.000e-03
3: TRAIN [0][240/340]	Time 0.892 (0.773)	Data 1.19e-04 (5.65e-04)	Tok/s 7665 (6505)	Loss/tok 7.1261 (7.9833)	LR 2.000e-03
0: TRAIN [0][240/340]	Time 0.892 (0.773)	Data 1.12e-04 (6.11e-04)	Tok/s 7498 (6504)	Loss/tok 7.0952 (7.9833)	LR 2.000e-03
2: TRAIN [0][240/340]	Time 0.892 (0.773)	Data 2.18e-04 (4.98e-04)	Tok/s 7600 (6501)	Loss/tok 7.1264 (7.9820)	LR 2.000e-03
1: TRAIN [0][240/340]	Time 0.892 (0.773)	Data 1.07e-04 (6.54e-04)	Tok/s 7605 (6513)	Loss/tok 7.1224 (7.9873)	LR 2.000e-03
0: TRAIN [0][250/340]	Time 0.734 (0.772)	Data 1.16e-04 (5.91e-04)	Tok/s 6594 (6501)	Loss/tok 6.6974 (7.9397)	LR 2.000e-03
1: TRAIN [0][250/340]	Time 0.735 (0.772)	Data 1.14e-04 (6.32e-04)	Tok/s 6682 (6508)	Loss/tok 6.7423 (7.9432)	LR 2.000e-03
3: TRAIN [0][250/340]	Time 0.734 (0.772)	Data 1.22e-04 (5.47e-04)	Tok/s 6727 (6500)	Loss/tok 6.7493 (7.9398)	LR 2.000e-03
2: TRAIN [0][250/340]	Time 0.735 (0.772)	Data 1.40e-04 (4.83e-04)	Tok/s 6576 (6497)	Loss/tok 6.7775 (7.9391)	LR 2.000e-03
0: TRAIN [0][260/340]	Time 1.046 (0.774)	Data 1.18e-04 (5.74e-04)	Tok/s 8453 (6516)	Loss/tok 7.0096 (7.8918)	LR 2.000e-03
3: TRAIN [0][260/340]	Time 1.042 (0.774)	Data 2.41e-04 (5.31e-04)	Tok/s 8376 (6516)	Loss/tok 6.9384 (7.8907)	LR 2.000e-03
1: TRAIN [0][260/340]	Time 1.046 (0.774)	Data 1.19e-04 (6.59e-04)	Tok/s 8442 (6524)	Loss/tok 6.8740 (7.8938)	LR 2.000e-03
2: TRAIN [0][260/340]	Time 1.046 (0.774)	Data 1.16e-04 (4.69e-04)	Tok/s 8442 (6514)	Loss/tok 6.8772 (7.8895)	LR 2.000e-03
0: TRAIN [0][270/340]	Time 0.868 (0.775)	Data 1.43e-04 (5.57e-04)	Tok/s 7847 (6529)	Loss/tok 6.7158 (7.8421)	LR 2.000e-03
3: TRAIN [0][270/340]	Time 0.870 (0.775)	Data 1.11e-04 (5.16e-04)	Tok/s 7734 (6528)	Loss/tok 6.7295 (7.8406)	LR 2.000e-03
2: TRAIN [0][270/340]	Time 0.870 (0.775)	Data 1.11e-04 (4.56e-04)	Tok/s 7717 (6527)	Loss/tok 6.7196 (7.8401)	LR 2.000e-03
1: TRAIN [0][270/340]	Time 0.870 (0.775)	Data 1.19e-04 (6.39e-04)	Tok/s 7727 (6536)	Loss/tok 6.7896 (7.8445)	LR 2.000e-03
0: TRAIN [0][280/340]	Time 1.045 (0.775)	Data 1.12e-04 (5.41e-04)	Tok/s 8431 (6518)	Loss/tok 6.8067 (7.8013)	LR 2.000e-03
3: TRAIN [0][280/340]	Time 1.045 (0.775)	Data 1.15e-04 (5.02e-04)	Tok/s 8435 (6518)	Loss/tok 6.8593 (7.7997)	LR 2.000e-03
2: TRAIN [0][280/340]	Time 1.044 (0.775)	Data 1.28e-04 (4.43e-04)	Tok/s 8433 (6515)	Loss/tok 6.9098 (7.7983)	LR 2.000e-03
1: TRAIN [0][280/340]	Time 1.045 (0.775)	Data 1.12e-04 (6.21e-04)	Tok/s 8306 (6523)	Loss/tok 6.8464 (7.8048)	LR 2.000e-03
0: TRAIN [0][290/340]	Time 0.733 (0.772)	Data 1.17e-04 (5.27e-04)	Tok/s 6822 (6489)	Loss/tok 6.3350 (7.7633)	LR 2.000e-03
3: TRAIN [0][290/340]	Time 0.733 (0.772)	Data 1.17e-04 (4.88e-04)	Tok/s 6630 (6487)	Loss/tok 6.4131 (7.7635)	LR 2.000e-03
2: TRAIN [0][290/340]	Time 0.733 (0.772)	Data 1.11e-04 (4.33e-04)	Tok/s 6760 (6485)	Loss/tok 6.4405 (7.7618)	LR 2.000e-03
1: TRAIN [0][290/340]	Time 0.735 (0.772)	Data 1.59e-04 (6.04e-04)	Tok/s 6587 (6494)	Loss/tok 6.3857 (7.7678)	LR 2.000e-03
0: TRAIN [0][300/340]	Time 0.761 (0.775)	Data 1.16e-04 (5.13e-04)	Tok/s 6311 (6507)	Loss/tok 6.1819 (7.7128)	LR 2.000e-03
3: TRAIN [0][300/340]	Time 0.762 (0.775)	Data 1.17e-04 (4.77e-04)	Tok/s 6473 (6506)	Loss/tok 6.3197 (7.7131)	LR 2.000e-03
1: TRAIN [0][300/340]	Time 0.761 (0.775)	Data 1.28e-04 (5.88e-04)	Tok/s 6445 (6512)	Loss/tok 6.1951 (7.7179)	LR 2.000e-03
2: TRAIN [0][300/340]	Time 0.764 (0.775)	Data 1.18e-04 (4.23e-04)	Tok/s 6478 (6503)	Loss/tok 6.3112 (7.7119)	LR 2.000e-03
0: TRAIN [0][310/340]	Time 0.729 (0.773)	Data 1.17e-04 (5.00e-04)	Tok/s 6649 (6500)	Loss/tok 6.2791 (7.6702)	LR 2.000e-03
3: TRAIN [0][310/340]	Time 0.728 (0.773)	Data 1.21e-04 (4.65e-04)	Tok/s 6675 (6497)	Loss/tok 6.2257 (7.6699)	LR 2.000e-03
2: TRAIN [0][310/340]	Time 0.727 (0.773)	Data 1.57e-04 (4.13e-04)	Tok/s 6665 (6493)	Loss/tok 6.1804 (7.6696)	LR 2.000e-03
1: TRAIN [0][310/340]	Time 0.729 (0.773)	Data 1.13e-04 (5.73e-04)	Tok/s 6614 (6504)	Loss/tok 6.3190 (7.6749)	LR 2.000e-03
0: TRAIN [0][320/340]	Time 0.885 (0.773)	Data 1.23e-04 (4.88e-04)	Tok/s 7627 (6505)	Loss/tok 6.3564 (7.6256)	LR 2.000e-03
3: TRAIN [0][320/340]	Time 0.884 (0.773)	Data 1.18e-04 (4.54e-04)	Tok/s 7613 (6501)	Loss/tok 6.3324 (7.6254)	LR 2.000e-03
1: TRAIN [0][320/340]	Time 0.885 (0.773)	Data 1.17e-04 (5.59e-04)	Tok/s 7794 (6508)	Loss/tok 6.2470 (7.6299)	LR 2.000e-03
2: TRAIN [0][320/340]	Time 0.889 (0.773)	Data 1.17e-04 (4.04e-04)	Tok/s 7733 (6498)	Loss/tok 6.3113 (7.6243)	LR 2.000e-03
3: TRAIN [0][330/340]	Time 0.734 (0.775)	Data 1.18e-04 (4.45e-04)	Tok/s 6562 (6504)	Loss/tok 6.1307 (7.5816)	LR 2.000e-03
2: TRAIN [0][330/340]	Time 0.735 (0.775)	Data 1.26e-04 (3.95e-04)	Tok/s 6513 (6500)	Loss/tok 6.1438 (7.5800)	LR 2.000e-03
0: TRAIN [0][330/340]	Time 0.735 (0.775)	Data 1.20e-04 (4.77e-04)	Tok/s 6741 (6508)	Loss/tok 6.0977 (7.5806)	LR 2.000e-03
1: TRAIN [0][330/340]	Time 0.735 (0.775)	Data 1.20e-04 (5.45e-04)	Tok/s 6673 (6511)	Loss/tok 6.0448 (7.5857)	LR 2.000e-03
1: Running validation on dev set
3: Running validation on dev set
2: Running validation on dev set
1: Executing preallocation
2: Executing preallocation
0: Running validation on dev set
3: Executing preallocation
0: Executing preallocation
3: VALIDATION [0][0/40]	Time 0.135 (0.135)	Data 1.40e-03 (1.40e-03)	Tok/s 30906 (30906)	Loss/tok 7.0726 (7.0726)
2: VALIDATION [0][0/40]	Time 0.142 (0.142)	Data 1.39e-03 (1.39e-03)	Tok/s 31123 (31123)	Loss/tok 7.1784 (7.1784)
1: VALIDATION [0][0/40]	Time 0.158 (0.158)	Data 1.57e-03 (1.57e-03)	Tok/s 29998 (29998)	Loss/tok 7.1499 (7.1499)
0: VALIDATION [0][0/40]	Time 0.215 (0.215)	Data 1.46e-03 (1.46e-03)	Tok/s 26608 (26608)	Loss/tok 7.1973 (7.1973)
2: VALIDATION [0][10/40]	Time 0.069 (0.093)	Data 1.21e-03 (1.31e-03)	Tok/s 33296 (32521)	Loss/tok 6.9890 (6.9858)
3: VALIDATION [0][10/40]	Time 0.075 (0.097)	Data 1.27e-03 (1.59e-03)	Tok/s 30318 (30416)	Loss/tok 6.9599 (7.0272)
1: VALIDATION [0][10/40]	Time 0.076 (0.100)	Data 1.25e-03 (1.31e-03)	Tok/s 30229 (30876)	Loss/tok 6.7637 (6.9969)
0: VALIDATION [0][10/40]	Time 0.071 (0.102)	Data 1.25e-03 (1.29e-03)	Tok/s 32947 (32178)	Loss/tok 6.7569 (7.0264)
2: VALIDATION [0][20/40]	Time 0.052 (0.077)	Data 1.21e-03 (1.26e-03)	Tok/s 31062 (32408)	Loss/tok 6.5760 (6.9046)
3: VALIDATION [0][20/40]	Time 0.051 (0.081)	Data 2.46e-03 (1.50e-03)	Tok/s 30568 (30238)	Loss/tok 6.7121 (6.9362)
0: VALIDATION [0][20/40]	Time 0.050 (0.082)	Data 1.22e-03 (1.27e-03)	Tok/s 32334 (32376)	Loss/tok 6.6812 (6.9238)
1: VALIDATION [0][20/40]	Time 0.055 (0.082)	Data 1.23e-03 (1.27e-03)	Tok/s 28982 (30496)	Loss/tok 6.7241 (6.9037)
2: VALIDATION [0][30/40]	Time 0.036 (0.065)	Data 1.18e-03 (1.24e-03)	Tok/s 29338 (31816)	Loss/tok 6.6054 (6.8309)
0: VALIDATION [0][30/40]	Time 0.036 (0.069)	Data 1.19e-03 (1.25e-03)	Tok/s 30260 (32008)	Loss/tok 6.6743 (6.8747)
3: VALIDATION [0][30/40]	Time 0.038 (0.069)	Data 1.20e-03 (1.43e-03)	Tok/s 27724 (29760)	Loss/tok 6.3590 (6.8764)
1: VALIDATION [0][30/40]	Time 0.038 (0.070)	Data 1.22e-03 (1.26e-03)	Tok/s 27958 (29973)	Loss/tok 6.3955 (6.8397)
0: Saving model to results/gnmt/model_best.pth
3: Running evaluation on test set
2: Running evaluation on test set
1: Running evaluation on test set
0: Running evaluation on test set
2: TEST [0][9/24]	Time 1.1224 (1.3787)	Decoder iters 149.0 (149.0)	Tok/s 1825 (2089)
0: TEST [0][9/24]	Time 1.1222 (1.3784)	Decoder iters 149.0 (149.0)	Tok/s 1762 (2182)
1: TEST [0][9/24]	Time 1.1229 (1.3787)	Decoder iters 149.0 (149.0)	Tok/s 2095 (2108)
3: TEST [0][9/24]	Time 1.1200 (1.3786)	Decoder iters 149.0 (149.0)	Tok/s 1819 (2013)
3: TEST [0][19/24]	Time 0.7036 (1.1626)	Decoder iters 149.0 (149.0)	Tok/s 1606 (1913)
2: TEST [0][19/24]	Time 0.7044 (1.1627)	Decoder iters 149.0 (149.0)	Tok/s 1854 (1925)
0: TEST [0][19/24]	Time 0.7045 (1.1626)	Decoder iters 149.0 (149.0)	Tok/s 1658 (2047)
1: TEST [0][19/24]	Time 0.7043 (1.1627)	Decoder iters 149.0 (149.0)	Tok/s 1691 (1992)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
3: Finished evaluation on test set
2: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
2: Finished epoch 0
3: Finished epoch 0
1: Starting epoch 1
2: Starting epoch 1
3: Starting epoch 1
1: Executing preallocation
2: Executing preallocation
3: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.5461	Validation Loss: 6.8006	Test BLEU: 0.48
0: Performance: Epoch: 0	Training: 25963 Tok/s	Validation: 119456 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 2606193617
3: Sampler for epoch 1 uses seed 2606193617
1: Sampler for epoch 1 uses seed 2606193617
2: Sampler for epoch 1 uses seed 2606193617
1: TRAIN [1][0/340]	Time 0.732 (0.732)	Data 1.19e-01 (1.19e-01)	Tok/s 3978 (3978)	Loss/tok 5.6332 (5.6332)	LR 2.000e-03
0: TRAIN [1][0/340]	Time 0.741 (0.741)	Data 1.04e-01 (1.04e-01)	Tok/s 3861 (3861)	Loss/tok 5.4094 (5.4094)	LR 2.000e-03
3: TRAIN [1][0/340]	Time 0.732 (0.732)	Data 1.02e-01 (1.02e-01)	Tok/s 3964 (3964)	Loss/tok 5.4731 (5.4731)	LR 2.000e-03
2: TRAIN [1][0/340]	Time 0.722 (0.722)	Data 1.11e-01 (1.11e-01)	Tok/s 3939 (3939)	Loss/tok 5.3790 (5.3790)	LR 2.000e-03
0: TRAIN [1][10/340]	Time 1.035 (0.777)	Data 1.59e-04 (9.53e-03)	Tok/s 8539 (6230)	Loss/tok 6.1880 (5.9350)	LR 2.000e-03
3: TRAIN [1][10/340]	Time 1.035 (0.776)	Data 1.16e-04 (9.35e-03)	Tok/s 8535 (6206)	Loss/tok 6.1215 (5.9535)	LR 2.000e-03
2: TRAIN [1][10/340]	Time 1.036 (0.775)	Data 1.16e-04 (1.02e-02)	Tok/s 8452 (6175)	Loss/tok 6.1946 (5.9384)	LR 2.000e-03
1: TRAIN [1][10/340]	Time 1.041 (0.777)	Data 1.21e-04 (1.10e-02)	Tok/s 8531 (6135)	Loss/tok 6.2119 (5.9810)	LR 2.000e-03
0: TRAIN [1][20/340]	Time 0.731 (0.790)	Data 1.12e-04 (5.05e-03)	Tok/s 6625 (6534)	Loss/tok 5.6252 (5.9090)	LR 2.000e-03
1: TRAIN [1][20/340]	Time 0.732 (0.789)	Data 1.10e-04 (5.81e-03)	Tok/s 6724 (6479)	Loss/tok 5.6966 (5.9277)	LR 2.000e-03
2: TRAIN [1][20/340]	Time 0.732 (0.789)	Data 9.73e-05 (5.42e-03)	Tok/s 6759 (6500)	Loss/tok 5.7681 (5.9239)	LR 2.000e-03
3: TRAIN [1][20/340]	Time 0.731 (0.789)	Data 1.19e-04 (4.95e-03)	Tok/s 6731 (6487)	Loss/tok 5.7477 (5.9318)	LR 2.000e-03
3: TRAIN [1][30/340]	Time 0.742 (0.775)	Data 1.13e-04 (3.39e-03)	Tok/s 6568 (6447)	Loss/tok 5.5981 (5.8741)	LR 2.000e-03
0: TRAIN [1][30/340]	Time 0.742 (0.776)	Data 1.13e-04 (3.46e-03)	Tok/s 6562 (6481)	Loss/tok 5.6526 (5.8564)	LR 2.000e-03
1: TRAIN [1][30/340]	Time 0.741 (0.775)	Data 1.15e-04 (3.97e-03)	Tok/s 6614 (6446)	Loss/tok 5.7012 (5.8642)	LR 2.000e-03
2: TRAIN [1][30/340]	Time 0.742 (0.775)	Data 1.18e-04 (3.71e-03)	Tok/s 6576 (6467)	Loss/tok 5.6589 (5.8632)	LR 2.000e-03
1: TRAIN [1][40/340]	Time 1.048 (0.779)	Data 1.13e-04 (3.03e-03)	Tok/s 8441 (6510)	Loss/tok 6.0405 (5.8361)	LR 2.000e-03
3: TRAIN [1][40/340]	Time 1.049 (0.779)	Data 1.16e-04 (2.60e-03)	Tok/s 8340 (6510)	Loss/tok 6.0578 (5.8422)	LR 2.000e-03
2: TRAIN [1][40/340]	Time 1.048 (0.779)	Data 1.60e-04 (2.83e-03)	Tok/s 8411 (6526)	Loss/tok 5.9251 (5.8234)	LR 2.000e-03
0: TRAIN [1][40/340]	Time 1.050 (0.779)	Data 1.21e-04 (2.64e-03)	Tok/s 8490 (6550)	Loss/tok 5.9832 (5.8332)	LR 2.000e-03
1: TRAIN [1][50/340]	Time 1.057 (0.782)	Data 1.12e-04 (2.46e-03)	Tok/s 8357 (6531)	Loss/tok 5.8773 (5.8068)	LR 2.000e-03
0: TRAIN [1][50/340]	Time 1.058 (0.782)	Data 1.19e-04 (2.15e-03)	Tok/s 8274 (6567)	Loss/tok 5.9531 (5.8087)	LR 2.000e-03
3: TRAIN [1][50/340]	Time 1.058 (0.782)	Data 1.21e-04 (2.11e-03)	Tok/s 8363 (6530)	Loss/tok 5.9379 (5.8150)	LR 2.000e-03
2: TRAIN [1][50/340]	Time 1.059 (0.782)	Data 1.21e-04 (2.30e-03)	Tok/s 8264 (6535)	Loss/tok 5.9206 (5.8101)	LR 2.000e-03
0: TRAIN [1][60/340]	Time 0.748 (0.780)	Data 1.14e-04 (1.82e-03)	Tok/s 6510 (6524)	Loss/tok 5.4650 (5.7811)	LR 2.000e-03
2: TRAIN [1][60/340]	Time 0.747 (0.779)	Data 1.11e-04 (1.94e-03)	Tok/s 6468 (6508)	Loss/tok 5.4606 (5.7832)	LR 2.000e-03
1: TRAIN [1][60/340]	Time 0.749 (0.780)	Data 1.13e-04 (2.08e-03)	Tok/s 6535 (6500)	Loss/tok 5.4167 (5.7713)	LR 2.000e-03
3: TRAIN [1][60/340]	Time 0.750 (0.780)	Data 1.24e-04 (1.78e-03)	Tok/s 6359 (6496)	Loss/tok 5.4676 (5.7850)	LR 2.000e-03
3: TRAIN [1][70/340]	Time 0.615 (0.771)	Data 1.28e-04 (1.55e-03)	Tok/s 4722 (6387)	Loss/tok 5.0853 (5.7437)	LR 2.000e-03
0: TRAIN [1][70/340]	Time 0.615 (0.771)	Data 1.17e-04 (1.58e-03)	Tok/s 4730 (6409)	Loss/tok 4.9547 (5.7377)	LR 2.000e-03
1: TRAIN [1][70/340]	Time 0.615 (0.771)	Data 1.10e-04 (1.80e-03)	Tok/s 4792 (6392)	Loss/tok 5.0825 (5.7310)	LR 2.000e-03
2: TRAIN [1][70/340]	Time 0.615 (0.771)	Data 1.13e-04 (1.68e-03)	Tok/s 4822 (6403)	Loss/tok 4.9889 (5.7399)	LR 2.000e-03
3: TRAIN [1][80/340]	Time 0.909 (0.783)	Data 1.22e-04 (1.37e-03)	Tok/s 7570 (6495)	Loss/tok 5.7198 (5.7400)	LR 2.000e-03
2: TRAIN [1][80/340]	Time 0.908 (0.782)	Data 1.60e-04 (1.49e-03)	Tok/s 7501 (6512)	Loss/tok 5.7373 (5.7305)	LR 2.000e-03
0: TRAIN [1][80/340]	Time 0.910 (0.783)	Data 1.17e-04 (1.40e-03)	Tok/s 7422 (6508)	Loss/tok 5.5999 (5.7274)	LR 2.000e-03
1: TRAIN [1][80/340]	Time 0.909 (0.783)	Data 1.18e-04 (1.62e-03)	Tok/s 7450 (6495)	Loss/tok 5.6421 (5.7217)	LR 2.000e-03
0: TRAIN [1][90/340]	Time 0.726 (0.783)	Data 1.21e-04 (1.26e-03)	Tok/s 6834 (6501)	Loss/tok 5.3309 (5.6978)	LR 2.000e-03
3: TRAIN [1][90/340]	Time 0.726 (0.783)	Data 1.60e-04 (1.24e-03)	Tok/s 6636 (6482)	Loss/tok 5.3362 (5.7153)	LR 2.000e-03
2: TRAIN [1][90/340]	Time 0.727 (0.783)	Data 1.18e-04 (1.34e-03)	Tok/s 6706 (6501)	Loss/tok 5.2022 (5.7018)	LR 2.000e-03
1: TRAIN [1][90/340]	Time 0.726 (0.783)	Data 1.14e-04 (1.46e-03)	Tok/s 6747 (6483)	Loss/tok 5.3254 (5.6953)	LR 2.000e-03
3: TRAIN [1][100/340]	Time 0.744 (0.790)	Data 1.12e-04 (1.12e-03)	Tok/s 6543 (6558)	Loss/tok 5.1980 (5.6828)	LR 2.000e-03
1: TRAIN [1][100/340]	Time 0.744 (0.790)	Data 1.13e-04 (1.32e-03)	Tok/s 6542 (6559)	Loss/tok 5.1488 (5.6683)	LR 2.000e-03
2: TRAIN [1][100/340]	Time 0.745 (0.790)	Data 1.13e-04 (1.22e-03)	Tok/s 6600 (6575)	Loss/tok 5.0847 (5.6703)	LR 2.000e-03
0: TRAIN [1][100/340]	Time 0.747 (0.790)	Data 1.14e-04 (1.15e-03)	Tok/s 6656 (6573)	Loss/tok 5.2608 (5.6662)	LR 2.000e-03
0: TRAIN [1][110/340]	Time 0.622 (0.791)	Data 1.15e-04 (1.05e-03)	Tok/s 4812 (6562)	Loss/tok 4.7868 (5.6411)	LR 2.000e-03
1: TRAIN [1][110/340]	Time 0.622 (0.791)	Data 1.13e-04 (1.22e-03)	Tok/s 4727 (6549)	Loss/tok 4.9503 (5.6452)	LR 2.000e-03
2: TRAIN [1][110/340]	Time 0.622 (0.791)	Data 1.06e-04 (1.12e-03)	Tok/s 4745 (6563)	Loss/tok 4.8178 (5.6473)	LR 2.000e-03
3: TRAIN [1][110/340]	Time 0.622 (0.791)	Data 1.28e-04 (1.04e-03)	Tok/s 4682 (6550)	Loss/tok 4.8410 (5.6542)	LR 2.000e-03
1: TRAIN [1][120/340]	Time 0.732 (0.787)	Data 1.57e-04 (1.13e-03)	Tok/s 6692 (6532)	Loss/tok 4.9338 (5.6072)	LR 1.000e-03
2: TRAIN [1][120/340]	Time 0.732 (0.787)	Data 1.15e-04 (1.04e-03)	Tok/s 6679 (6541)	Loss/tok 5.0526 (5.6096)	LR 1.000e-03
0: TRAIN [1][120/340]	Time 0.733 (0.788)	Data 1.16e-04 (9.76e-04)	Tok/s 6626 (6544)	Loss/tok 5.0927 (5.6044)	LR 1.000e-03
3: TRAIN [1][120/340]	Time 0.734 (0.787)	Data 1.20e-04 (9.60e-04)	Tok/s 6634 (6533)	Loss/tok 4.8739 (5.6145)	LR 1.000e-03
3: TRAIN [1][130/340]	Time 0.625 (0.781)	Data 1.25e-04 (8.96e-04)	Tok/s 4701 (6485)	Loss/tok 4.4786 (5.5737)	LR 1.000e-03
0: TRAIN [1][130/340]	Time 0.625 (0.781)	Data 1.10e-04 (9.10e-04)	Tok/s 4655 (6495)	Loss/tok 4.6686 (5.5679)	LR 1.000e-03
1: TRAIN [1][130/340]	Time 0.625 (0.781)	Data 1.44e-04 (1.05e-03)	Tok/s 4584 (6485)	Loss/tok 4.5579 (5.5695)	LR 1.000e-03
2: TRAIN [1][130/340]	Time 0.626 (0.781)	Data 1.19e-04 (9.67e-04)	Tok/s 4759 (6491)	Loss/tok 4.4741 (5.5691)	LR 1.000e-03
3: TRAIN [1][140/340]	Time 0.617 (0.784)	Data 1.23e-04 (8.42e-04)	Tok/s 4791 (6519)	Loss/tok 4.5363 (5.5392)	LR 1.000e-03
0: TRAIN [1][140/340]	Time 0.618 (0.784)	Data 1.25e-04 (8.55e-04)	Tok/s 4732 (6528)	Loss/tok 4.5260 (5.5331)	LR 1.000e-03
1: TRAIN [1][140/340]	Time 0.617 (0.784)	Data 1.19e-04 (9.84e-04)	Tok/s 4701 (6518)	Loss/tok 4.7552 (5.5396)	LR 1.000e-03
2: TRAIN [1][140/340]	Time 0.621 (0.784)	Data 1.18e-04 (9.06e-04)	Tok/s 4888 (6523)	Loss/tok 4.6006 (5.5364)	LR 1.000e-03
0: TRAIN [1][150/340]	Time 0.735 (0.782)	Data 1.17e-04 (8.06e-04)	Tok/s 6660 (6514)	Loss/tok 4.8320 (5.4969)	LR 1.000e-03
1: TRAIN [1][150/340]	Time 0.735 (0.782)	Data 1.60e-04 (9.26e-04)	Tok/s 6532 (6507)	Loss/tok 4.9279 (5.5039)	LR 1.000e-03
2: TRAIN [1][150/340]	Time 0.737 (0.781)	Data 1.24e-04 (8.55e-04)	Tok/s 6675 (6510)	Loss/tok 4.9401 (5.5002)	LR 1.000e-03
3: TRAIN [1][150/340]	Time 0.737 (0.782)	Data 1.41e-04 (7.93e-04)	Tok/s 6750 (6507)	Loss/tok 4.6739 (5.4985)	LR 1.000e-03
3: TRAIN [1][160/340]	Time 0.729 (0.780)	Data 1.11e-04 (7.52e-04)	Tok/s 6813 (6504)	Loss/tok 4.8360 (5.4649)	LR 1.000e-03
0: TRAIN [1][160/340]	Time 0.730 (0.780)	Data 1.12e-04 (7.63e-04)	Tok/s 6588 (6510)	Loss/tok 4.7541 (5.4608)	LR 1.000e-03
1: TRAIN [1][160/340]	Time 0.730 (0.780)	Data 1.11e-04 (8.76e-04)	Tok/s 6635 (6501)	Loss/tok 4.9210 (5.4712)	LR 1.000e-03
2: TRAIN [1][160/340]	Time 0.730 (0.780)	Data 1.08e-04 (8.10e-04)	Tok/s 6514 (6504)	Loss/tok 4.7902 (5.4655)	LR 1.000e-03
0: TRAIN [1][170/340]	Time 0.867 (0.779)	Data 1.21e-04 (7.26e-04)	Tok/s 7818 (6508)	Loss/tok 4.9761 (5.4290)	LR 5.000e-04
3: TRAIN [1][170/340]	Time 0.868 (0.779)	Data 1.16e-04 (7.15e-04)	Tok/s 7767 (6501)	Loss/tok 5.0116 (5.4325)	LR 5.000e-04
1: TRAIN [1][170/340]	Time 0.867 (0.779)	Data 1.20e-04 (8.32e-04)	Tok/s 7733 (6498)	Loss/tok 4.9900 (5.4396)	LR 5.000e-04
2: TRAIN [1][170/340]	Time 0.867 (0.779)	Data 1.75e-04 (7.69e-04)	Tok/s 7880 (6502)	Loss/tok 5.1978 (5.4335)	LR 5.000e-04
0: TRAIN [1][180/340]	Time 0.617 (0.777)	Data 1.41e-04 (6.92e-04)	Tok/s 4599 (6482)	Loss/tok 4.3981 (5.3988)	LR 5.000e-04
1: TRAIN [1][180/340]	Time 0.620 (0.777)	Data 1.13e-04 (7.93e-04)	Tok/s 4711 (6474)	Loss/tok 4.4023 (5.4095)	LR 5.000e-04
3: TRAIN [1][180/340]	Time 0.622 (0.777)	Data 1.01e-04 (6.82e-04)	Tok/s 4710 (6479)	Loss/tok 4.3490 (5.4015)	LR 5.000e-04
2: TRAIN [1][180/340]	Time 0.620 (0.777)	Data 1.46e-04 (7.33e-04)	Tok/s 4756 (6478)	Loss/tok 4.5822 (5.4042)	LR 5.000e-04
0: TRAIN [1][190/340]	Time 0.617 (0.775)	Data 1.11e-04 (6.62e-04)	Tok/s 4741 (6469)	Loss/tok 4.2111 (5.3695)	LR 5.000e-04
3: TRAIN [1][190/340]	Time 0.617 (0.775)	Data 1.13e-04 (6.53e-04)	Tok/s 4809 (6467)	Loss/tok 4.3477 (5.3698)	LR 5.000e-04
2: TRAIN [1][190/340]	Time 0.618 (0.774)	Data 1.12e-04 (7.01e-04)	Tok/s 4647 (6464)	Loss/tok 4.3242 (5.3729)	LR 5.000e-04
1: TRAIN [1][190/340]	Time 0.616 (0.775)	Data 1.18e-04 (7.57e-04)	Tok/s 4787 (6463)	Loss/tok 4.3421 (5.3778)	LR 5.000e-04
3: TRAIN [1][200/340]	Time 0.615 (0.772)	Data 9.97e-05 (6.26e-04)	Tok/s 4703 (6446)	Loss/tok 4.2869 (5.3398)	LR 5.000e-04
0: TRAIN [1][200/340]	Time 0.616 (0.772)	Data 1.17e-04 (6.35e-04)	Tok/s 4563 (6448)	Loss/tok 4.2398 (5.3413)	LR 5.000e-04
2: TRAIN [1][200/340]	Time 0.615 (0.772)	Data 1.19e-04 (6.72e-04)	Tok/s 4648 (6445)	Loss/tok 4.2981 (5.3433)	LR 5.000e-04
1: TRAIN [1][200/340]	Time 0.616 (0.772)	Data 1.11e-04 (7.25e-04)	Tok/s 4756 (6446)	Loss/tok 4.2203 (5.3468)	LR 5.000e-04
3: TRAIN [1][210/340]	Time 0.888 (0.771)	Data 1.20e-04 (6.03e-04)	Tok/s 7677 (6438)	Loss/tok 4.8817 (5.3119)	LR 5.000e-04
0: TRAIN [1][210/340]	Time 0.889 (0.771)	Data 1.24e-04 (6.10e-04)	Tok/s 7639 (6440)	Loss/tok 4.8101 (5.3132)	LR 5.000e-04
1: TRAIN [1][210/340]	Time 0.889 (0.771)	Data 1.14e-04 (6.96e-04)	Tok/s 7554 (6438)	Loss/tok 4.8778 (5.3204)	LR 5.000e-04
2: TRAIN [1][210/340]	Time 0.889 (0.771)	Data 1.14e-04 (6.65e-04)	Tok/s 7691 (6436)	Loss/tok 4.9653 (5.3172)	LR 5.000e-04
0: TRAIN [1][220/340]	Time 0.623 (0.769)	Data 1.11e-04 (5.88e-04)	Tok/s 4745 (6423)	Loss/tok 4.3502 (5.2878)	LR 5.000e-04
1: TRAIN [1][220/340]	Time 0.624 (0.769)	Data 1.29e-04 (6.70e-04)	Tok/s 4659 (6421)	Loss/tok 4.3313 (5.2947)	LR 5.000e-04
3: TRAIN [1][220/340]	Time 0.624 (0.769)	Data 1.20e-04 (5.81e-04)	Tok/s 4536 (6419)	Loss/tok 4.2833 (5.2881)	LR 5.000e-04
2: TRAIN [1][220/340]	Time 0.624 (0.769)	Data 1.02e-04 (6.40e-04)	Tok/s 4760 (6422)	Loss/tok 4.0907 (5.2908)	LR 5.000e-04
3: TRAIN [1][230/340]	Time 0.898 (0.773)	Data 1.13e-04 (5.61e-04)	Tok/s 7566 (6459)	Loss/tok 4.7475 (5.2611)	LR 2.500e-04
2: TRAIN [1][230/340]	Time 0.899 (0.773)	Data 1.17e-04 (6.18e-04)	Tok/s 7503 (6463)	Loss/tok 4.8446 (5.2664)	LR 2.500e-04
1: TRAIN [1][230/340]	Time 0.900 (0.773)	Data 1.13e-04 (6.46e-04)	Tok/s 7601 (6463)	Loss/tok 4.7973 (5.2701)	LR 2.500e-04
0: TRAIN [1][230/340]	Time 0.899 (0.773)	Data 1.57e-04 (5.67e-04)	Tok/s 7558 (6463)	Loss/tok 4.8736 (5.2633)	LR 2.500e-04
0: TRAIN [1][240/340]	Time 1.044 (0.775)	Data 1.16e-04 (5.49e-04)	Tok/s 8513 (6461)	Loss/tok 4.9499 (5.2408)	LR 2.500e-04
1: TRAIN [1][240/340]	Time 1.044 (0.775)	Data 1.13e-04 (6.24e-04)	Tok/s 8344 (6459)	Loss/tok 5.0190 (5.2481)	LR 2.500e-04
2: TRAIN [1][240/340]	Time 1.045 (0.775)	Data 1.09e-04 (6.01e-04)	Tok/s 8497 (6461)	Loss/tok 4.9079 (5.2455)	LR 2.500e-04
3: TRAIN [1][240/340]	Time 1.045 (0.775)	Data 1.18e-04 (5.42e-04)	Tok/s 8467 (6457)	Loss/tok 4.9634 (5.2412)	LR 2.500e-04
1: TRAIN [1][250/340]	Time 1.027 (0.772)	Data 1.07e-04 (6.04e-04)	Tok/s 8546 (6420)	Loss/tok 5.0121 (5.2267)	LR 2.500e-04
3: TRAIN [1][250/340]	Time 1.027 (0.772)	Data 1.16e-04 (5.26e-04)	Tok/s 8576 (6417)	Loss/tok 4.9489 (5.2208)	LR 2.500e-04
0: TRAIN [1][250/340]	Time 1.027 (0.772)	Data 1.10e-04 (5.31e-04)	Tok/s 8693 (6421)	Loss/tok 4.8830 (5.2203)	LR 2.500e-04
2: TRAIN [1][250/340]	Time 1.025 (0.772)	Data 1.23e-04 (5.82e-04)	Tok/s 8563 (6420)	Loss/tok 4.9942 (5.2258)	LR 2.500e-04
0: TRAIN [1][260/340]	Time 0.617 (0.770)	Data 1.07e-04 (5.16e-04)	Tok/s 4905 (6406)	Loss/tok 4.1846 (5.1985)	LR 2.500e-04
1: TRAIN [1][260/340]	Time 0.616 (0.770)	Data 1.13e-04 (5.85e-04)	Tok/s 4762 (6402)	Loss/tok 4.1411 (5.2056)	LR 2.500e-04
3: TRAIN [1][260/340]	Time 0.617 (0.770)	Data 1.23e-04 (5.11e-04)	Tok/s 4764 (6401)	Loss/tok 4.1449 (5.2004)	LR 2.500e-04
2: TRAIN [1][260/340]	Time 0.617 (0.770)	Data 1.18e-04 (5.64e-04)	Tok/s 4817 (6406)	Loss/tok 4.2700 (5.2033)	LR 2.500e-04
2: TRAIN [1][270/340]	Time 0.875 (0.768)	Data 1.12e-04 (5.48e-04)	Tok/s 7804 (6393)	Loss/tok 4.6405 (5.1813)	LR 2.500e-04
1: TRAIN [1][270/340]	Time 0.876 (0.768)	Data 1.08e-04 (5.67e-04)	Tok/s 7762 (6388)	Loss/tok 4.7666 (5.1849)	LR 2.500e-04
0: TRAIN [1][270/340]	Time 0.875 (0.768)	Data 1.24e-04 (5.01e-04)	Tok/s 7753 (6391)	Loss/tok 4.8544 (5.1782)	LR 2.500e-04
3: TRAIN [1][270/340]	Time 0.875 (0.768)	Data 9.92e-05 (4.96e-04)	Tok/s 7767 (6385)	Loss/tok 4.8443 (5.1808)	LR 2.500e-04
0: TRAIN [1][280/340]	Time 1.049 (0.767)	Data 1.13e-04 (4.88e-04)	Tok/s 8323 (6379)	Loss/tok 4.9490 (5.1591)	LR 2.500e-04
1: TRAIN [1][280/340]	Time 1.048 (0.767)	Data 1.63e-04 (5.52e-04)	Tok/s 8461 (6377)	Loss/tok 5.0238 (5.1662)	LR 2.500e-04
3: TRAIN [1][280/340]	Time 1.050 (0.767)	Data 1.22e-04 (4.83e-04)	Tok/s 8286 (6376)	Loss/tok 4.8389 (5.1606)	LR 2.500e-04
2: TRAIN [1][280/340]	Time 1.050 (0.767)	Data 1.11e-04 (5.33e-04)	Tok/s 8370 (6382)	Loss/tok 4.9148 (5.1619)	LR 2.500e-04
0: TRAIN [1][290/340]	Time 0.732 (0.767)	Data 1.18e-04 (4.75e-04)	Tok/s 6585 (6378)	Loss/tok 4.5043 (5.1410)	LR 1.250e-04
3: TRAIN [1][290/340]	Time 0.733 (0.767)	Data 1.34e-04 (4.71e-04)	Tok/s 6584 (6376)	Loss/tok 4.5460 (5.1424)	LR 1.250e-04
1: TRAIN [1][290/340]	Time 0.733 (0.767)	Data 1.14e-04 (5.37e-04)	Tok/s 6604 (6374)	Loss/tok 4.4289 (5.1475)	LR 1.250e-04
2: TRAIN [1][290/340]	Time 0.733 (0.767)	Data 1.15e-04 (5.19e-04)	Tok/s 6602 (6379)	Loss/tok 4.4663 (5.1410)	LR 1.250e-04
0: TRAIN [1][300/340]	Time 0.754 (0.768)	Data 1.15e-04 (4.63e-04)	Tok/s 6602 (6388)	Loss/tok 4.5297 (5.1249)	LR 1.250e-04
3: TRAIN [1][300/340]	Time 0.754 (0.768)	Data 1.24e-04 (4.59e-04)	Tok/s 6378 (6388)	Loss/tok 4.5688 (5.1242)	LR 1.250e-04
1: TRAIN [1][300/340]	Time 0.754 (0.768)	Data 1.13e-04 (5.23e-04)	Tok/s 6569 (6386)	Loss/tok 4.4008 (5.1306)	LR 1.250e-04
2: TRAIN [1][300/340]	Time 0.755 (0.768)	Data 1.06e-04 (5.05e-04)	Tok/s 6430 (6390)	Loss/tok 4.6038 (5.1255)	LR 1.250e-04
0: TRAIN [1][310/340]	Time 0.870 (0.770)	Data 1.23e-04 (4.52e-04)	Tok/s 7719 (6417)	Loss/tok 4.7027 (5.1069)	LR 1.250e-04
3: TRAIN [1][310/340]	Time 0.870 (0.770)	Data 1.18e-04 (4.49e-04)	Tok/s 7782 (6417)	Loss/tok 4.7357 (5.1080)	LR 1.250e-04
1: TRAIN [1][310/340]	Time 0.870 (0.770)	Data 1.30e-04 (5.10e-04)	Tok/s 7822 (6414)	Loss/tok 4.5422 (5.1118)	LR 1.250e-04
2: TRAIN [1][310/340]	Time 0.870 (0.770)	Data 1.18e-04 (4.93e-04)	Tok/s 7914 (6420)	Loss/tok 4.6265 (5.1076)	LR 1.250e-04
0: TRAIN [1][320/340]	Time 1.038 (0.774)	Data 1.16e-04 (4.41e-04)	Tok/s 8516 (6448)	Loss/tok 4.8751 (5.0920)	LR 1.250e-04
1: TRAIN [1][320/340]	Time 1.038 (0.774)	Data 1.15e-04 (4.98e-04)	Tok/s 8524 (6444)	Loss/tok 4.8913 (5.0986)	LR 1.250e-04
3: TRAIN [1][320/340]	Time 1.039 (0.774)	Data 1.19e-04 (4.38e-04)	Tok/s 8396 (6447)	Loss/tok 4.9397 (5.0931)	LR 1.250e-04
2: TRAIN [1][320/340]	Time 1.039 (0.774)	Data 1.54e-04 (4.82e-04)	Tok/s 8608 (6451)	Loss/tok 4.9775 (5.0939)	LR 1.250e-04
0: TRAIN [1][330/340]	Time 1.069 (0.775)	Data 1.19e-04 (4.31e-04)	Tok/s 8293 (6451)	Loss/tok 4.8376 (5.0764)	LR 1.250e-04
1: TRAIN [1][330/340]	Time 1.069 (0.775)	Data 1.17e-04 (4.87e-04)	Tok/s 8214 (6447)	Loss/tok 4.8416 (5.0819)	LR 1.250e-04
3: TRAIN [1][330/340]	Time 1.069 (0.775)	Data 1.13e-04 (4.29e-04)	Tok/s 8268 (6449)	Loss/tok 5.0696 (5.0784)	LR 1.250e-04
2: TRAIN [1][330/340]	Time 1.069 (0.775)	Data 1.65e-04 (4.71e-04)	Tok/s 8219 (6454)	Loss/tok 5.0138 (5.0798)	LR 1.250e-04
1: Running validation on dev set
2: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
3: Running validation on dev set
2: Executing preallocation
0: Executing preallocation
3: Executing preallocation
3: VALIDATION [1][0/40]	Time 0.128 (0.128)	Data 1.38e-03 (1.38e-03)	Tok/s 32527 (32527)	Loss/tok 6.0623 (6.0623)
2: VALIDATION [1][0/40]	Time 0.149 (0.149)	Data 1.41e-03 (1.41e-03)	Tok/s 29715 (29715)	Loss/tok 6.1258 (6.1258)
1: VALIDATION [1][0/40]	Time 0.162 (0.162)	Data 1.41e-03 (1.41e-03)	Tok/s 29374 (29374)	Loss/tok 6.1315 (6.1315)
0: VALIDATION [1][0/40]	Time 0.217 (0.217)	Data 1.47e-03 (1.47e-03)	Tok/s 26415 (26415)	Loss/tok 6.2674 (6.2674)
3: VALIDATION [1][10/40]	Time 0.072 (0.092)	Data 1.29e-03 (1.40e-03)	Tok/s 31506 (32007)	Loss/tok 5.6324 (5.8842)
2: VALIDATION [1][10/40]	Time 0.071 (0.098)	Data 1.26e-03 (1.28e-03)	Tok/s 31994 (30953)	Loss/tok 5.7737 (5.8363)
0: VALIDATION [1][10/40]	Time 0.071 (0.102)	Data 1.26e-03 (1.35e-03)	Tok/s 32762 (32402)	Loss/tok 5.4314 (5.9105)
1: VALIDATION [1][10/40]	Time 0.077 (0.101)	Data 1.25e-03 (1.28e-03)	Tok/s 30075 (30649)	Loss/tok 5.4949 (5.8608)
3: VALIDATION [1][20/40]	Time 0.050 (0.077)	Data 1.20e-03 (1.43e-03)	Tok/s 31689 (31393)	Loss/tok 5.5630 (5.7632)
2: VALIDATION [1][20/40]	Time 0.054 (0.081)	Data 1.23e-03 (1.26e-03)	Tok/s 29542 (30701)	Loss/tok 5.3159 (5.7315)
0: VALIDATION [1][20/40]	Time 0.051 (0.081)	Data 1.24e-03 (1.30e-03)	Tok/s 31924 (32428)	Loss/tok 5.4979 (5.7846)
1: VALIDATION [1][20/40]	Time 0.055 (0.083)	Data 1.20e-03 (1.25e-03)	Tok/s 29138 (30336)	Loss/tok 5.4877 (5.7083)
3: VALIDATION [1][30/40]	Time 0.037 (0.066)	Data 1.51e-03 (1.46e-03)	Tok/s 28337 (30773)	Loss/tok 5.1315 (5.6968)
0: VALIDATION [1][30/40]	Time 0.036 (0.069)	Data 1.20e-03 (1.27e-03)	Tok/s 29949 (31979)	Loss/tok 5.4494 (5.7160)
2: VALIDATION [1][30/40]	Time 0.037 (0.069)	Data 1.19e-03 (1.24e-03)	Tok/s 28429 (30223)	Loss/tok 5.3011 (5.6485)
1: VALIDATION [1][30/40]	Time 0.038 (0.071)	Data 1.19e-03 (1.24e-03)	Tok/s 27961 (29852)	Loss/tok 5.1643 (5.6375)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
2: Running evaluation on test set
3: Running evaluation on test set
0: Running evaluation on test set
2: TEST [1][9/24]	Time 0.5933 (0.9570)	Decoder iters 63.0 (134.1)	Tok/s 3428 (3098)
3: TEST [1][9/24]	Time 0.5933 (0.9570)	Decoder iters 55.0 (130.3)	Tok/s 3504 (3084)
0: TEST [1][9/24]	Time 0.5927 (0.9568)	Decoder iters 117.0 (145.8)	Tok/s 3563 (3400)
1: TEST [1][9/24]	Time 0.5931 (0.9570)	Decoder iters 56.0 (131.8)	Tok/s 3541 (3238)
2: TEST [1][19/24]	Time 0.2725 (0.7273)	Decoder iters 43.0 (107.0)	Tok/s 3941 (3165)
3: TEST [1][19/24]	Time 0.2726 (0.7273)	Decoder iters 43.0 (97.8)	Tok/s 3874 (3117)
1: TEST [1][19/24]	Time 0.2727 (0.7272)	Decoder iters 32.0 (94.8)	Tok/s 3976 (3266)
0: TEST [1][19/24]	Time 0.2722 (0.7273)	Decoder iters 38.0 (116.9)	Tok/s 4280 (3393)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
2: Finished evaluation on test set
3: Finished evaluation on test set
1: Finished evaluation on test set
0: Finished evaluation on test set
3: Finished epoch 1
2: Finished epoch 1
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 5.0662	Validation Loss: 5.6158	Test BLEU: 3.81
0: Performance: Epoch: 1	Training: 25879 Tok/s	Validation: 118829 Tok/s
0: Finished epoch 1
1: Total training time 609 s
2: Total training time 609 s
3: Total training time 609 s
0: Total training time 609 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       4|                 108|                      3.81|                      25921.0|                         10.15|
DONE!
