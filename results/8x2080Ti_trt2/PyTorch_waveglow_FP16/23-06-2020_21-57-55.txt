:::NVLOGv0.2.2 Tacotron2_PyT 1592949477.868315220 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592949477.890458584 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592949477.907099724 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592949482.468772411 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592949482.476549387 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 4, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592949485.352626324 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1592949545.217474222 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592949545.218883276 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949545.591114044 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949549.098092079 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020840084180235863
:::NVLOGv0.2.2 Tacotron2_PyT 1592949551.209599495 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949551.210322142 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 45551.06706971095
:::NVLOGv0.2.2 Tacotron2_PyT 1592949551.210949183 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.620065927505493
Batch: 1/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949551.214551210 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949551.998621225 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002732656430453062
:::NVLOGv0.2.2 Tacotron2_PyT 1592949553.279674053 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949553.280151844 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 123898.42916942592
:::NVLOGv0.2.2 Tacotron2_PyT 1592949553.280508757 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.066208600997925
Batch: 2/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949553.283457041 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592949553.527765989 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0016516464529559016
:::NVLOGv0.2.2 Tacotron2_PyT 1592949554.739612103 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592949554.740066290 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175697.46065942314
:::NVLOGv0.2.2 Tacotron2_PyT 1592949554.740416288 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.457050085067749
Batch: 3/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949554.743537426 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592949555.092870712 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00216087163425982
:::NVLOGv0.2.2 Tacotron2_PyT 1592949556.301832438 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592949556.302299500 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 164191.35185522732
:::NVLOGv0.2.2 Tacotron2_PyT 1592949556.302629709 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5591564178466797
Batch: 4/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949556.306196213 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592949556.703055143 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021814373321831226
:::NVLOGv0.2.2 Tacotron2_PyT 1592949557.921155214 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592949557.921602964 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 158405.86850449853
:::NVLOGv0.2.2 Tacotron2_PyT 1592949557.921930075 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.6161017417907715
Batch: 5/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949557.925057173 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592949558.139703751 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018480377038940787
:::NVLOGv0.2.2 Tacotron2_PyT 1592949559.353989601 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592949559.354384899 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 179055.89905089227
:::NVLOGv0.2.2 Tacotron2_PyT 1592949559.354730606 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4297211170196533
Batch: 6/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949559.357594252 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592949559.575254679 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002058939542621374
:::NVLOGv0.2.2 Tacotron2_PyT 1592949560.876435995 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592949560.877018213 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 168465.39510083996
:::NVLOGv0.2.2 Tacotron2_PyT 1592949560.877520561 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5195999145507812
Batch: 7/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949560.880858660 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592949561.050525188 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019425651989877224
:::NVLOGv0.2.2 Tacotron2_PyT 1592949562.260867357 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592949562.261378527 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 185381.5537949857
:::NVLOGv0.2.2 Tacotron2_PyT 1592949562.261728764 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.3809356689453125
Batch: 8/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949562.264709234 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1592949562.523171663 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023945546709001064
:::NVLOGv0.2.2 Tacotron2_PyT 1592949563.820572376 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1592949563.821160793 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 164469.03080093773
:::NVLOGv0.2.2 Tacotron2_PyT 1592949563.821664333 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5565240383148193
Batch: 9/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949563.826105595 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1592949564.034445763 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0017733534332364798
:::NVLOGv0.2.2 Tacotron2_PyT 1592949565.288715124 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1592949565.289109707 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 174876.74641921357
:::NVLOGv0.2.2 Tacotron2_PyT 1592949565.289489269 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.463888168334961
Batch: 10/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949565.292129278 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 10
:::NVLOGv0.2.2 Tacotron2_PyT 1592949565.486720562 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0025015308056026697
:::NVLOGv0.2.2 Tacotron2_PyT 1592949566.695194483 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 10
:::NVLOGv0.2.2 Tacotron2_PyT 1592949566.695637941 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 182378.42022651955
:::NVLOGv0.2.2 Tacotron2_PyT 1592949566.696025610 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.403674840927124
Batch: 11/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949566.698752403 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 11
:::NVLOGv0.2.2 Tacotron2_PyT 1592949566.982946396 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024071296211332083
:::NVLOGv0.2.2 Tacotron2_PyT 1592949568.187468052 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 11
:::NVLOGv0.2.2 Tacotron2_PyT 1592949568.187983036 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 171874.23400332915
:::NVLOGv0.2.2 Tacotron2_PyT 1592949568.188360214 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4894611835479736
Batch: 12/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949568.191296816 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 12
:::NVLOGv0.2.2 Tacotron2_PyT 1592949568.436933041 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0017736144363880157
:::NVLOGv0.2.2 Tacotron2_PyT 1592949569.732965469 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 12
:::NVLOGv0.2.2 Tacotron2_PyT 1592949569.733601332 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 165984.32725015786
:::NVLOGv0.2.2 Tacotron2_PyT 1592949569.734153509 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5423142910003662
Batch: 13/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949569.737262487 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 13
:::NVLOGv0.2.2 Tacotron2_PyT 1592949569.959445238 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021503055468201637
:::NVLOGv0.2.2 Tacotron2_PyT 1592949571.171001196 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 13
:::NVLOGv0.2.2 Tacotron2_PyT 1592949571.171433449 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 178456.43012291027
:::NVLOGv0.2.2 Tacotron2_PyT 1592949571.171843529 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4345238208770752
Batch: 14/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949571.174380064 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 14
:::NVLOGv0.2.2 Tacotron2_PyT 1592949571.428395987 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019079912453889847
:::NVLOGv0.2.2 Tacotron2_PyT 1592949572.779912233 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 14
:::NVLOGv0.2.2 Tacotron2_PyT 1592949572.780347109 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 159389.96620090547
:::NVLOGv0.2.2 Tacotron2_PyT 1592949572.780722857 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.606123685836792
Batch: 15/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949572.783539772 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 15
:::NVLOGv0.2.2 Tacotron2_PyT 1592949573.087143660 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0015691607259213924
:::NVLOGv0.2.2 Tacotron2_PyT 1592949574.323562384 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 15
:::NVLOGv0.2.2 Tacotron2_PyT 1592949574.324149847 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 166161.81831546614
:::NVLOGv0.2.2 Tacotron2_PyT 1592949574.324655771 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5406668186187744
Batch: 16/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949574.327071905 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 16
:::NVLOGv0.2.2 Tacotron2_PyT 1592949574.637061119 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021293803583830595
:::NVLOGv0.2.2 Tacotron2_PyT 1592949575.844847441 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 16
:::NVLOGv0.2.2 Tacotron2_PyT 1592949575.845295429 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 168563.69825369032
:::NVLOGv0.2.2 Tacotron2_PyT 1592949575.845626831 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5187137126922607
Batch: 17/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949575.848033905 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 17
:::NVLOGv0.2.2 Tacotron2_PyT 1592949576.156794071 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002414360176771879
:::NVLOGv0.2.2 Tacotron2_PyT 1592949577.421353102 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 17
:::NVLOGv0.2.2 Tacotron2_PyT 1592949577.421741962 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 162670.75431022074
:::NVLOGv0.2.2 Tacotron2_PyT 1592949577.422084093 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5737309455871582
Batch: 18/19 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949577.424517393 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 18
:::NVLOGv0.2.2 Tacotron2_PyT 1592949577.699107647 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023168926127254963
:::NVLOGv0.2.2 Tacotron2_PyT 1592949578.911471844 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 18
:::NVLOGv0.2.2 Tacotron2_PyT 1592949578.911884546 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 172107.02501745927
:::NVLOGv0.2.2 Tacotron2_PyT 1592949578.912265778 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4874465465545654
:::NVLOGv0.2.2 Tacotron2_PyT 1592949579.137285233 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949579.138272524 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 143399.33959571976
:::NVLOGv0.2.2 Tacotron2_PyT 1592949579.139196396 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 161451.55137504282
:::NVLOGv0.2.2 Tacotron2_PyT 1592949579.140067577 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0021051808603500064
:::NVLOGv0.2.2 Tacotron2_PyT 1592949579.140928507 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 33.9192636013031
:::NVLOGv0.2.2 Tacotron2_PyT 1592949579.141769648 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592949580.877771616 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0013357646530494094
:::NVLOGv0.2.2 Tacotron2_PyT 1592949580.878736734 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949583.824217319 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949583.920563459 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949584.128916502 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.001945606549270451
:::NVLOGv0.2.2 Tacotron2_PyT 1592949585.339135647 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592949585.339573622 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 180139.97993825114
:::NVLOGv0.2.2 Tacotron2_PyT 1592949585.339938402 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4211170673370361
Batch: 1/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949585.342640638 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949585.535797358 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022457840386778116
:::NVLOGv0.2.2 Tacotron2_PyT 1592949586.745354176 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949586.745763302 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 182418.79290590636
:::NVLOGv0.2.2 Tacotron2_PyT 1592949586.746121645 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4033641815185547
Batch: 2/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949586.748732567 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592949586.956715584 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002398925833404064
:::NVLOGv0.2.2 Tacotron2_PyT 1592949588.231757164 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592949588.232164860 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 172545.8290835983
:::NVLOGv0.2.2 Tacotron2_PyT 1592949588.232522011 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.48366379737854
Batch: 3/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949588.235312700 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592949588.426092625 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019341942388564348
:::NVLOGv0.2.2 Tacotron2_PyT 1592949589.627954483 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592949589.628342867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 183717.14308299476
:::NVLOGv0.2.2 Tacotron2_PyT 1592949589.628667831 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.393446445465088
Batch: 4/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949589.631353140 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592949589.878681660 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021243365481495857
:::NVLOGv0.2.2 Tacotron2_PyT 1592949591.087039948 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592949591.087486029 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175761.82573035223
:::NVLOGv0.2.2 Tacotron2_PyT 1592949591.087858200 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4565165042877197
Batch: 5/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949591.090514421 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592949591.365893841 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019057287136092782
:::NVLOGv0.2.2 Tacotron2_PyT 1592949592.638338804 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592949592.638744831 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 165323.41031188663
:::NVLOGv0.2.2 Tacotron2_PyT 1592949592.639113665 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.5484800338745117
Batch: 6/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949592.641689062 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592949592.822491169 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0026118126697838306
:::NVLOGv0.2.2 Tacotron2_PyT 1592949594.047387362 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1592949594.047807217 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 182028.98279480316
:::NVLOGv0.2.2 Tacotron2_PyT 1592949594.048355579 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4063694477081299
Batch: 7/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949594.051434755 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592949594.246509552 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022168257273733616
:::NVLOGv0.2.2 Tacotron2_PyT 1592949595.461939096 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1592949595.462336779 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 181386.30771697135
:::NVLOGv0.2.2 Tacotron2_PyT 1592949595.462690830 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4113523960113525
Batch: 8/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949595.465252638 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1592949595.699261189 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.001968346768990159
:::NVLOGv0.2.2 Tacotron2_PyT 1592949596.908788443 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1592949596.909210443 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177266.08853681022
:::NVLOGv0.2.2 Tacotron2_PyT 1592949596.909553766 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4441566467285156
Batch: 9/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949596.912156820 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1592949597.139229298 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0017451148014515638
:::NVLOGv0.2.2 Tacotron2_PyT 1592949598.350318909 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1592949598.350716829 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177922.52908258056
:::NVLOGv0.2.2 Tacotron2_PyT 1592949598.351065636 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.438828468322754
Batch: 10/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949598.353619099 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 10
:::NVLOGv0.2.2 Tacotron2_PyT 1592949598.546474457 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002203030278906226
:::NVLOGv0.2.2 Tacotron2_PyT 1592949599.756995916 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 10
:::NVLOGv0.2.2 Tacotron2_PyT 1592949599.757396698 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 182336.48627146514
:::NVLOGv0.2.2 Tacotron2_PyT 1592949599.757746935 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4039976596832275
Batch: 11/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949599.760425568 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 11
:::NVLOGv0.2.2 Tacotron2_PyT 1592949599.977074146 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002100070007145405
:::NVLOGv0.2.2 Tacotron2_PyT 1592949601.188270569 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 11
:::NVLOGv0.2.2 Tacotron2_PyT 1592949601.188663006 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 179211.45092698297
:::NVLOGv0.2.2 Tacotron2_PyT 1592949601.188993454 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4284801483154297
Batch: 12/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949601.191605568 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 12
:::NVLOGv0.2.2 Tacotron2_PyT 1592949601.434108734 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00203003641217947
:::NVLOGv0.2.2 Tacotron2_PyT 1592949602.644498587 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 12
:::NVLOGv0.2.2 Tacotron2_PyT 1592949602.644962549 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176120.26034012178
:::NVLOGv0.2.2 Tacotron2_PyT 1592949602.645303011 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.45355224609375
Batch: 13/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949602.648106337 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 13
:::NVLOGv0.2.2 Tacotron2_PyT 1592949602.885345459 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021478852722793818
:::NVLOGv0.2.2 Tacotron2_PyT 1592949604.093980789 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 13
:::NVLOGv0.2.2 Tacotron2_PyT 1592949604.094357967 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176977.01086148532
:::NVLOGv0.2.2 Tacotron2_PyT 1592949604.094719172 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4465155601501465
Batch: 14/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949604.097323418 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 14
:::NVLOGv0.2.2 Tacotron2_PyT 1592949604.297473907 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022743847221136093
:::NVLOGv0.2.2 Tacotron2_PyT 1592949605.502461672 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 14
:::NVLOGv0.2.2 Tacotron2_PyT 1592949605.502896547 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 182106.99679489245
:::NVLOGv0.2.2 Tacotron2_PyT 1592949605.503247023 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4057669639587402
Batch: 15/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949605.506059885 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 15
:::NVLOGv0.2.2 Tacotron2_PyT 1592949605.741880178 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.001893984037451446
:::NVLOGv0.2.2 Tacotron2_PyT 1592949606.947512865 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 15
:::NVLOGv0.2.2 Tacotron2_PyT 1592949606.947937012 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177515.33901265505
:::NVLOGv0.2.2 Tacotron2_PyT 1592949606.948283195 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4421288967132568
Batch: 16/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949606.950898409 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 16
:::NVLOGv0.2.2 Tacotron2_PyT 1592949607.157227039 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002282222267240286
:::NVLOGv0.2.2 Tacotron2_PyT 1592949608.386684656 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 16
:::NVLOGv0.2.2 Tacotron2_PyT 1592949608.387316465 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 178219.97167712453
:::NVLOGv0.2.2 Tacotron2_PyT 1592949608.387866259 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.436427116394043
Batch: 17/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949608.390680075 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 17
:::NVLOGv0.2.2 Tacotron2_PyT 1592949608.582151413 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0018207930261269212
:::NVLOGv0.2.2 Tacotron2_PyT 1592949609.839241505 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 17
:::NVLOGv0.2.2 Tacotron2_PyT 1592949609.839693546 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176653.2648815571
:::NVLOGv0.2.2 Tacotron2_PyT 1592949609.840069056 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.4491665363311768
Batch: 18/19 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949609.842533588 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 18
:::NVLOGv0.2.2 Tacotron2_PyT 1592949610.018558502 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.00246192398481071
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.232844114 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 18
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.233287573 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 184066.2825373381
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.233695269 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 1.390803337097168
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.289911509 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.290353298 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 177090.53276373196
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.290744066 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 178511.47118356722
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.291151285 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0021216318893589473
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.291537762 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 27.466177463531494
:::NVLOGv0.2.2 Tacotron2_PyT 1592949611.291913509 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949612.318967342 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.0015480373986065388
:::NVLOGv0.2.2 Tacotron2_PyT 1592949612.319671392 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592949612.320952177 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 126.96771836280823
:::NVLOGv0.2.2 Tacotron2_PyT 1592949612.321369886 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 126.96771836280823
:::NVLOGv0.2.2 Tacotron2_PyT 1592949612.321758747 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 134.56244707107544
:::NVLOGv0.2.2 Tacotron2_PyT 1592949612.322103024 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
