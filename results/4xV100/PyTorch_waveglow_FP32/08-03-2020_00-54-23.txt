:::NVLOGv0.2.2 Tacotron2_PyT 1583628866.540067673 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1583628866.568625927 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1583628866.585717916 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "754G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1583628870.710716963 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.33.01", "num": 4, "name": ["Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB", "Tesla V100-SXM2-32GB"], "mem": ["32510 MiB", "32510 MiB", "32510 MiB", "32510 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1583628870.717688084 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 15, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 4, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1583628872.505319595 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs1
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs3
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs0
libibverbs: Warning: no userspace device-specific driver found for /sys/class/infiniband_verbs/uverbs2
:::NVLOGv0.2.2 Tacotron2_PyT 1583628915.497600794 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1583628915.499366999 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628916.005129814 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628920.873344183 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002426715800538659
:::NVLOGv0.2.2 Tacotron2_PyT 1583628923.756103277 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628923.756909847 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 61917.80714064982
:::NVLOGv0.2.2 Tacotron2_PyT 1583628923.757611990 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 7.7522125244140625
Batch: 1/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628923.761448860 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628925.539386272 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002101542428135872
:::NVLOGv0.2.2 Tacotron2_PyT 1583628927.273107767 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628927.273732424 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 136648.3375290755
:::NVLOGv0.2.2 Tacotron2_PyT 1583628927.274239302 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 3.5126662254333496
Batch: 2/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628927.276864529 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583628928.348606110 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002011155476793647
:::NVLOGv0.2.2 Tacotron2_PyT 1583628930.127888203 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583628930.128553629 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 168310.90919442225
:::NVLOGv0.2.2 Tacotron2_PyT 1583628930.129045010 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.85186505317688
Batch: 3/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628930.132156134 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583628931.231380701 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002868212293833494
:::NVLOGv0.2.2 Tacotron2_PyT 1583628932.987032175 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583628932.987617493 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 168084.3346304925
:::NVLOGv0.2.2 Tacotron2_PyT 1583628932.988120794 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8557093143463135
Batch: 4/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628932.991258621 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583628934.008767843 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0025140391662716866
:::NVLOGv0.2.2 Tacotron2_PyT 1583628935.729084015 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583628935.729659557 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175269.28924153245
:::NVLOGv0.2.2 Tacotron2_PyT 1583628935.730145931 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.738642930984497
Batch: 5/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628935.733051300 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1583628936.838057518 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0024490640498697758
:::NVLOGv0.2.2 Tacotron2_PyT 1583628938.614432573 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1583628938.615180254 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 166540.52343729642
:::NVLOGv0.2.2 Tacotron2_PyT 1583628938.615844011 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.882181406021118
Batch: 6/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628938.618994236 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1583628939.693651676 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002055208431556821
:::NVLOGv0.2.2 Tacotron2_PyT 1583628941.474653959 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1583628941.475539446 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 168025.44435524516
:::NVLOGv0.2.2 Tacotron2_PyT 1583628941.476214409 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.856710195541382
Batch: 7/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628941.480087042 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1583628942.462762356 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002299786079674959
:::NVLOGv0.2.2 Tacotron2_PyT 1583628944.239968061 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1583628944.240720034 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 173854.63837830332
:::NVLOGv0.2.2 Tacotron2_PyT 1583628944.241386890 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.760927200317383
Batch: 8/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628944.244669199 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1583628945.099846125 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019746478646993637
:::NVLOGv0.2.2 Tacotron2_PyT 1583628946.878702879 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1583628946.879534483 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 182167.90259964412
:::NVLOGv0.2.2 Tacotron2_PyT 1583628946.880207062 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.634931802749634
Batch: 9/10 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628946.883481741 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1583628947.805580139 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0026004244573414326
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.579892159 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.580648899 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177962.7363606221
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.581315517 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.6971938610076904
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.818640709 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.820178509 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 139857.92880926828
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.821677208 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 157878.19228672836
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.823122501 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002330079604871571
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.824588299 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 34.32054257392883
:::NVLOGv0.2.2 Tacotron2_PyT 1583628949.826012373 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1583628953.344570160 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.00214840448461473
:::NVLOGv0.2.2 Tacotron2_PyT 1583628953.346912622 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628958.127995968 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628958.345665932 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628959.292023182 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0017207686323672533
:::NVLOGv0.2.2 Tacotron2_PyT 1583628961.094315529 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1583628961.094933510 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 174504.80249540976
:::NVLOGv0.2.2 Tacotron2_PyT 1583628961.095435619 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.750640630722046
Batch: 1/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628961.098600388 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628962.086863041 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0026655655819922686
:::NVLOGv0.2.2 Tacotron2_PyT 1583628963.857845545 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628963.858495712 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 173901.35679560012
:::NVLOGv0.2.2 Tacotron2_PyT 1583628963.859001637 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.760185480117798
Batch: 2/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628963.862197876 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583628964.931652784 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019488829420879483
:::NVLOGv0.2.2 Tacotron2_PyT 1583628966.659615278 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1583628966.660207033 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 171531.89616031453
:::NVLOGv0.2.2 Tacotron2_PyT 1583628966.660723686 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7983133792877197
Batch: 3/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628966.663400650 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583628967.837057590 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002129496308043599
:::NVLOGv0.2.2 Tacotron2_PyT 1583628969.617813110 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1583628969.618563175 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 162419.32000552135
:::NVLOGv0.2.2 Tacotron2_PyT 1583628969.619263887 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.9553134441375732
Batch: 4/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628969.622086048 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583628970.606567383 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0020778917241841555
:::NVLOGv0.2.2 Tacotron2_PyT 1583628972.377601385 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1583628972.378311396 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 174124.8437364591
:::NVLOGv0.2.2 Tacotron2_PyT 1583628972.378811598 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7566428184509277
Batch: 5/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628972.381897688 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1583628973.393891811 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0021062218584120274
:::NVLOGv0.2.2 Tacotron2_PyT 1583628975.164075851 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1583628975.164639711 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 172468.24022363598
:::NVLOGv0.2.2 Tacotron2_PyT 1583628975.165182114 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.783121109008789
Batch: 6/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628975.168201923 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1583628976.261729956 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019520791247487068
:::NVLOGv0.2.2 Tacotron2_PyT 1583628978.029512644 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 6
:::NVLOGv0.2.2 Tacotron2_PyT 1583628978.030095577 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 167703.8905412255
:::NVLOGv0.2.2 Tacotron2_PyT 1583628978.030598640 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.862187623977661
Batch: 7/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628978.032896042 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1583628979.047598362 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023244053591042757
:::NVLOGv0.2.2 Tacotron2_PyT 1583628980.810379744 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 7
:::NVLOGv0.2.2 Tacotron2_PyT 1583628980.810951233 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 172764.90745769558
:::NVLOGv0.2.2 Tacotron2_PyT 1583628980.811473846 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.7783420085906982
Batch: 8/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628980.814061642 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1583628981.908947229 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022007599472999573
:::NVLOGv0.2.2 Tacotron2_PyT 1583628983.633919477 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 8
:::NVLOGv0.2.2 Tacotron2_PyT 1583628983.634497643 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 170186.45339223108
:::NVLOGv0.2.2 Tacotron2_PyT 1583628983.634999514 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8204360008239746
Batch: 9/10 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628983.636858940 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1583628984.774695158 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023551019839942455
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.527850389 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 9
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.528465748 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 165999.68453506974
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.528983831 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8915717601776123
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.591825485 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.593478441 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 168629.2730849681
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.594978094 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 170560.53953431625
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.596453667 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0021481173462234437
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.597912788 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 28.464808702468872
:::NVLOGv0.2.2 Tacotron2_PyT 1583628986.599380016 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628988.680103779 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.002251902362331748
:::NVLOGv0.2.2 Tacotron2_PyT 1583628988.681206226 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1583628988.683072090 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 116.17685103416443
:::NVLOGv0.2.2 Tacotron2_PyT 1583628988.683767319 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 116.17685103416443
:::NVLOGv0.2.2 Tacotron2_PyT 1583628988.684509039 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 122.27487421035767
:::NVLOGv0.2.2 Tacotron2_PyT 1583628988.685159683 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
