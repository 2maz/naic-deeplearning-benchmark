Experiment dir : LM-TFM
Namespace(adaptive=False, amp='apex', apex_amp_opt_level='O2', append_dataset=False, append_time=False, attn_type=0, batch_chunk=1, batch_size=32, clamp_len=-1, clip=0.25, clip_nonemb=False, cuda=True, d_embed=1024, d_head=64, d_inner=4096, d_model=1024, data='/data/transformer-xl/wikitext-103', dataset='wt103', debug=False, decay_rate=0.5, div_val=1, dllog_file='train_log.json', dropatt=0.2, dropout=0.2, emb_init='normal', emb_init_range=0.01, eta_min=0.001, eval_batch_size=16, eval_interval=5000, eval_max_steps=-1, eval_tgt_len=128, ext_len=0, fp16=False, gpu0_bsz=-1, init='normal', init_range=0.1, init_std=0.02, local_batch_size=None, local_rank=0, log_all_ranks=False, log_interval=10, lr=0.0, lr_min=0.0, max_step=400, max_step_scheduler=None, mem_len=256, mom=0.0, multi_gpu=None, n_head=16, n_layer=18, no_env=False, no_eval=False, not_tied=False, optim='adam', patience=0, pre_lnorm=False, proj_init_std=0.01, restart='', roll=True, same_length=False, sample_softmax=-1, save_all=False, scheduler='cosine', seed=1111, swap_mem=False, target_perplexity=None, target_throughput=None, tgt_len=256, tied=True, txtlog_file='train_log.log', varlen=False, vocab='word', warmup_step=16000, weight_decay=0.0, work_dir='LM-TFM')
world size: 8
Collecting environment information...
PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3090
GPU 1: GeForce RTX 3090
GPU 2: GeForce RTX 3090
GPU 3: GeForce RTX 3090
GPU 4: GeForce RTX 3090
GPU 5: GeForce RTX 3090
GPU 6: GeForce RTX 3090
GPU 7: GeForce RTX 3090

Nvidia driver version: 460.39
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
Producing dataset wt103...
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
building vocab with min_freq=0, max_size=None
final vocab size 267735 from 267734 unique tokens
final vocab size 267735 from 267734 unique tokens
====================================================================================================
    - work_dir : LM-TFM
    - append_dataset : False
    - append_time : False
    - cuda : True
    - fp16 : False
    - restart : 
    - debug : False
    - log_all_ranks : False
    - dllog_file : train_log.json
    - txtlog_file : train_log.log
    - save_all : False
    - no_env : False
    - no_eval : False
    - log_interval : 10
    - target_throughput : None
    - target_perplexity : None
    - apex_amp_opt_level : O2
    - amp : apex
    - data : /data/transformer-xl/wikitext-103
    - dataset : wt103
    - vocab : word
    - n_layer : 18
    - n_head : 16
    - d_head : 64
    - d_embed : 1024
    - d_model : 1024
    - d_inner : 4096
    - dropout : 0.2
    - dropatt : 0.2
    - pre_lnorm : False
    - attn_type : 0
    - not_tied : False
    - clamp_len : -1
    - adaptive : False
    - div_val : 1
    - sample_softmax : -1
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.0
    - mom : 0.0
    - scheduler : cosine
    - max_step_scheduler : None
    - warmup_step : 16000
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - weight_decay : 0.0
    - clip_nonemb : False
    - patience : 0
    - eta_min : 0.001
    - max_step : 400
    - batch_size : 32
    - local_batch_size : None
    - batch_chunk : 1
    - roll : True
    - tgt_len : 256
    - ext_len : 0
    - mem_len : 256
    - seed : 1111
    - multi_gpu : None
    - gpu0_bsz : -1
    - same_length : False
    - varlen : False
    - swap_mem : False
    - eval_tgt_len : 128
    - eval_batch_size : 16
    - eval_max_steps : -1
    - eval_interval : 5000
    - local_rank : 0
    - tied : True
    - n_token : 267735
    - n_all_param : 519963095
    - n_nonemb_param : 245532672
====================================================================================================
#params = 519963095
#non emb params = 245532672
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
| epoch   1 step       10 | batches     10 / 12601 | lr 0.000e+00 | ms/batch 398.1 | tok/s   21054 | loss 12.73 | ppl 337439.41
| epoch   1 step       20 | batches     20 / 12601 | lr 0.000e+00 | ms/batch 253.2 | tok/s   32352 | loss 12.74 | ppl 339859.99
| epoch   1 step       30 | batches     30 / 12601 | lr 0.000e+00 | ms/batch 253.1 | tok/s   32362 | loss 12.73 | ppl 338146.51
| epoch   1 step       40 | batches     40 / 12601 | lr 0.000e+00 | ms/batch 253.7 | tok/s   32291 | loss 12.73 | ppl 338382.33
| epoch   1 step       50 | batches     50 / 12601 | lr 0.000e+00 | ms/batch 253.4 | tok/s   32326 | loss 12.73 | ppl 337581.67
| epoch   1 step       60 | batches     60 / 12601 | lr 0.000e+00 | ms/batch 253.9 | tok/s   32260 | loss 12.73 | ppl 338357.16
| epoch   1 step       70 | batches     70 / 12601 | lr 0.000e+00 | ms/batch 254.0 | tok/s   32255 | loss 12.73 | ppl 339050.99
| epoch   1 step       80 | batches     80 / 12601 | lr 0.000e+00 | ms/batch 254.4 | tok/s   32200 | loss 12.73 | ppl 338429.12
| epoch   1 step       90 | batches     90 / 12601 | lr 0.000e+00 | ms/batch 254.5 | tok/s   32183 | loss 12.73 | ppl 338899.05
| epoch   1 step      100 | batches    100 / 12601 | lr 0.000e+00 | ms/batch 254.8 | tok/s   32153 | loss 12.73 | ppl 339199.44
| epoch   1 step      110 | batches    110 / 12601 | lr 0.000e+00 | ms/batch 254.0 | tok/s   32249 | loss 12.73 | ppl 338190.70
| epoch   1 step      120 | batches    120 / 12601 | lr 0.000e+00 | ms/batch 254.4 | tok/s   32204 | loss 12.73 | ppl 337771.35
| epoch   1 step      130 | batches    130 / 12601 | lr 0.000e+00 | ms/batch 254.4 | tok/s   32204 | loss 12.73 | ppl 337605.82
| epoch   1 step      140 | batches    140 / 12601 | lr 0.000e+00 | ms/batch 254.0 | tok/s   32247 | loss 12.74 | ppl 339587.20
| epoch   1 step      150 | batches    150 / 12601 | lr 0.000e+00 | ms/batch 254.3 | tok/s   32213 | loss 12.73 | ppl 337836.75
| epoch   1 step      160 | batches    160 / 12601 | lr 0.000e+00 | ms/batch 253.7 | tok/s   32287 | loss 12.73 | ppl 339124.40
| epoch   1 step      170 | batches    170 / 12601 | lr 0.000e+00 | ms/batch 253.9 | tok/s   32266 | loss 12.73 | ppl 337487.36
| epoch   1 step      180 | batches    180 / 12601 | lr 0.000e+00 | ms/batch 253.8 | tok/s   32274 | loss 12.73 | ppl 338473.99
| epoch   1 step      190 | batches    190 / 12601 | lr 0.000e+00 | ms/batch 253.9 | tok/s   32269 | loss 12.73 | ppl 338471.09
| epoch   1 step      200 | batches    200 / 12601 | lr 0.000e+00 | ms/batch 254.3 | tok/s   32208 | loss 12.73 | ppl 336169.70
| epoch   1 step      210 | batches    210 / 12601 | lr 0.000e+00 | ms/batch 254.6 | tok/s   32180 | loss 12.73 | ppl 338831.83
| epoch   1 step      220 | batches    220 / 12601 | lr 0.000e+00 | ms/batch 253.8 | tok/s   32273 | loss 12.73 | ppl 338849.61
| epoch   1 step      230 | batches    230 / 12601 | lr 0.000e+00 | ms/batch 254.2 | tok/s   32222 | loss 12.73 | ppl 337830.31
| epoch   1 step      240 | batches    240 / 12601 | lr 0.000e+00 | ms/batch 254.7 | tok/s   32160 | loss 12.73 | ppl 337282.40
| epoch   1 step      250 | batches    250 / 12601 | lr 0.000e+00 | ms/batch 255.0 | tok/s   32130 | loss 12.73 | ppl 338646.73
| epoch   1 step      260 | batches    260 / 12601 | lr 0.000e+00 | ms/batch 255.0 | tok/s   32124 | loss 12.73 | ppl 338546.95
| epoch   1 step      270 | batches    270 / 12601 | lr 0.000e+00 | ms/batch 254.8 | tok/s   32155 | loss 12.73 | ppl 337397.90
| epoch   1 step      280 | batches    280 / 12601 | lr 0.000e+00 | ms/batch 254.9 | tok/s   32133 | loss 12.73 | ppl 338735.23
| epoch   1 step      290 | batches    290 / 12601 | lr 0.000e+00 | ms/batch 255.1 | tok/s   32112 | loss 12.73 | ppl 337549.16
| epoch   1 step      300 | batches    300 / 12601 | lr 0.000e+00 | ms/batch 255.0 | tok/s   32128 | loss 12.73 | ppl 338858.33
| epoch   1 step      310 | batches    310 / 12601 | lr 0.000e+00 | ms/batch 255.2 | tok/s   32100 | loss 12.73 | ppl 338199.73
| epoch   1 step      320 | batches    320 / 12601 | lr 0.000e+00 | ms/batch 255.3 | tok/s   32085 | loss 12.73 | ppl 337366.04
| epoch   1 step      330 | batches    330 / 12601 | lr 0.000e+00 | ms/batch 255.0 | tok/s   32125 | loss 12.73 | ppl 336992.07
| epoch   1 step      340 | batches    340 / 12601 | lr 0.000e+00 | ms/batch 254.9 | tok/s   32136 | loss 12.73 | ppl 337494.76
| epoch   1 step      350 | batches    350 / 12601 | lr 0.000e+00 | ms/batch 255.2 | tok/s   32101 | loss 12.73 | ppl 338534.68
| epoch   1 step      360 | batches    360 / 12601 | lr 0.000e+00 | ms/batch 254.9 | tok/s   32144 | loss 12.73 | ppl 337450.35
| epoch   1 step      370 | batches    370 / 12601 | lr 0.000e+00 | ms/batch 255.4 | tok/s   32073 | loss 12.73 | ppl 338331.67
| epoch   1 step      380 | batches    380 / 12601 | lr 0.000e+00 | ms/batch 255.1 | tok/s   32115 | loss 12.73 | ppl 338142.00
| epoch   1 step      390 | batches    390 / 12601 | lr 0.000e+00 | ms/batch 255.3 | tok/s   32094 | loss 12.73 | ppl 338282.95
| epoch   1 step      400 | batches    400 / 12601 | lr 0.000e+00 | ms/batch 255.8 | tok/s   32027 | loss 12.73 | ppl 337507.31
----------------------------------------------------------------------------------------------------
| Eval   0 at step      400 | time:  2.80s | valid loss 12.65 | valid ppl 311479.814
----------------------------------------------------------------------------------------------------
Saving checkpoint to LM-TFM/checkpoint_last.pt
Saving checkpoint to LM-TFM/checkpoint_best.pt
----------------------------------------------------------------------------------------------------
End of training
Loading checkpoint from LM-TFM/checkpoint_best.pt
====================================================================================================
| End of training | test time:  4.41s | test loss 12.66 | test ppl 313575.393
====================================================================================================
Training time: 2.04 minutes
Training throughput: 32181.20 tok/s
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
