1: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3080
GPU 1: GeForce RTX 3080

Nvidia driver version: 455.28
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
1: Saving results to: gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=112, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
1: Using master seed from command line: 2
0: PyTorch version: 1.7.0a0+7036e91
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 18.04.5 LTS (x86_64)
GCC version: (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
Clang version: Could not collect
CMake version: version 3.14.0

Python version: 3.6 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: GeForce RTX 3080
GPU 1: GeForce RTX 3080

Nvidia driver version: 455.28
cuDNN version: Probably one of the following:
/usr/lib/x86_64-linux-gnu/libcudnn.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.0.4
/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.0.4
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] numpy==1.19.1
[pip3] pytorch-transformers==1.1.0
[pip3] torch==1.7.0a0+7036e91
[pip3] torchtext==0.8.0a0
[pip3] torchvision==0.8.0a0
[conda] magma-cuda110             2.5.2                         5    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] numpy                     1.19.1           py36h30dfecb_0  
[conda] numpy-base                1.19.1           py36h75fe3a5_0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.7.0a0+7036e91          pypi_0    pypi
[conda] torchtext                 0.8.0a0                  pypi_0    pypi
[conda] torchvision               0.8.0a0                  pypi_0    pypi
0: Saving results to: gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dllog_file='train_log.json', dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, resume=None, save_all=False, save_dir='gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=24, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=112, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=24, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup=1, warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Size of vocabulary: 31800
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
1: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
1: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
1: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
1: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 876
0: Scheduler decay interval: 110
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 876
1: Scheduler decay interval: 110
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
0: Initializing amp optimizer
1: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
1: Starting epoch 0
1: Executing preallocation
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
/opt/conda/lib/python3.6/site-packages/torch/functional.py:1242: UserWarning: torch.norm is deprecated and may be removed in a future PyTorch release. Use torch.linalg.norm instead.
  "torch.norm is deprecated and may be removed in a future PyTorch release. "
0: Sampler for epoch 0 uses seed 3588440356
1: Sampler for epoch 0 uses seed 3588440356
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:127: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
1: TRAIN [0][0/658]	Time 0.177 (0.000)	Data 9.13e-02 (0.00e+00)	Tok/s 17213 (0)	Loss/tok 10.5215 (10.5215)	LR 2.047e-05
0: TRAIN [0][0/658]	Time 0.177 (0.000)	Data 8.27e-02 (0.00e+00)	Tok/s 16852 (0)	Loss/tok 10.5340 (10.5340)	LR 2.047e-05
1: TRAIN [0][10/658]	Time 0.131 (0.134)	Data 7.39e-05 (7.23e-05)	Tok/s 38686 (37642)	Loss/tok 9.6807 (10.1360)	LR 2.576e-05
0: TRAIN [0][10/658]	Time 0.131 (0.134)	Data 9.06e-05 (8.97e-05)	Tok/s 38734 (37499)	Loss/tok 9.6687 (10.1259)	LR 2.576e-05
1: TRAIN [0][20/658]	Time 0.130 (0.138)	Data 7.46e-05 (7.24e-05)	Tok/s 39075 (38711)	Loss/tok 9.1232 (9.7685)	LR 3.244e-05
0: TRAIN [0][20/658]	Time 0.131 (0.138)	Data 8.06e-05 (8.61e-05)	Tok/s 38240 (38591)	Loss/tok 9.2458 (9.7716)	LR 3.244e-05
1: TRAIN [0][30/658]	Time 0.158 (0.138)	Data 7.58e-05 (7.28e-05)	Tok/s 44316 (38539)	Loss/tok 8.9555 (9.5219)	LR 4.083e-05
0: TRAIN [0][30/658]	Time 0.158 (0.138)	Data 8.03e-05 (8.55e-05)	Tok/s 45062 (38507)	Loss/tok 8.9399 (9.5295)	LR 4.083e-05
1: TRAIN [0][40/658]	Time 0.159 (0.143)	Data 7.65e-05 (7.37e-05)	Tok/s 44422 (39828)	Loss/tok 8.7202 (9.3139)	LR 5.141e-05
0: TRAIN [0][40/658]	Time 0.158 (0.143)	Data 8.18e-05 (8.47e-05)	Tok/s 44787 (39765)	Loss/tok 8.6625 (9.3138)	LR 5.141e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
1: TRAIN [0][50/658]	Time 0.188 (0.142)	Data 7.87e-05 (7.39e-05)	Tok/s 49239 (40151)	Loss/tok 8.6107 (9.1708)	LR 6.472e-05
0: TRAIN [0][50/658]	Time 0.188 (0.142)	Data 8.70e-05 (8.35e-05)	Tok/s 48969 (40131)	Loss/tok 8.5546 (9.1704)	LR 6.472e-05
1: TRAIN [0][60/658]	Time 0.107 (0.142)	Data 7.30e-05 (7.41e-05)	Tok/s 28073 (40070)	Loss/tok 8.2176 (9.0530)	LR 8.148e-05
0: TRAIN [0][60/658]	Time 0.107 (0.142)	Data 8.54e-05 (8.27e-05)	Tok/s 28816 (40058)	Loss/tok 8.2438 (9.0557)	LR 8.148e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
1: TRAIN [0][70/658]	Time 0.107 (0.141)	Data 7.63e-05 (7.44e-05)	Tok/s 27675 (40063)	Loss/tok 7.9571 (8.9515)	LR 1.026e-04
0: TRAIN [0][70/658]	Time 0.107 (0.141)	Data 8.15e-05 (8.20e-05)	Tok/s 28962 (40082)	Loss/tok 8.1199 (8.9550)	LR 1.026e-04
1: TRAIN [0][80/658]	Time 0.158 (0.141)	Data 7.13e-05 (7.40e-05)	Tok/s 44158 (39939)	Loss/tok 8.0821 (8.8541)	LR 1.291e-04
0: TRAIN [0][80/658]	Time 0.158 (0.141)	Data 8.58e-05 (8.14e-05)	Tok/s 44138 (39892)	Loss/tok 8.0516 (8.8592)	LR 1.291e-04
1: TRAIN [0][90/658]	Time 0.131 (0.140)	Data 7.15e-05 (7.35e-05)	Tok/s 39040 (39559)	Loss/tok 7.8800 (8.7659)	LR 1.626e-04
0: TRAIN [0][90/658]	Time 0.131 (0.140)	Data 8.18e-05 (8.11e-05)	Tok/s 38399 (39509)	Loss/tok 7.7788 (8.7710)	LR 1.626e-04
1: TRAIN [0][100/658]	Time 0.131 (0.140)	Data 7.75e-05 (7.34e-05)	Tok/s 38611 (39649)	Loss/tok 7.7799 (8.6766)	LR 2.047e-04
0: TRAIN [0][100/658]	Time 0.131 (0.140)	Data 8.18e-05 (8.07e-05)	Tok/s 38148 (39603)	Loss/tok 7.7086 (8.6775)	LR 2.047e-04
1: TRAIN [0][110/658]	Time 0.088 (0.139)	Data 7.51e-05 (7.32e-05)	Tok/s 17067 (39286)	Loss/tok 7.2778 (8.6059)	LR 2.576e-04
0: TRAIN [0][110/658]	Time 0.088 (0.139)	Data 8.18e-05 (8.02e-05)	Tok/s 17251 (39217)	Loss/tok 6.9877 (8.6066)	LR 2.576e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
1: TRAIN [0][120/658]	Time 0.157 (0.139)	Data 7.30e-05 (7.31e-05)	Tok/s 45509 (39067)	Loss/tok 8.0281 (8.5439)	LR 3.244e-04
0: TRAIN [0][120/658]	Time 0.157 (0.139)	Data 8.15e-05 (8.01e-05)	Tok/s 44819 (39014)	Loss/tok 8.1052 (8.5423)	LR 3.244e-04
1: TRAIN [0][130/658]	Time 0.108 (0.138)	Data 7.32e-05 (7.30e-05)	Tok/s 28094 (39071)	Loss/tok 7.4193 (8.4875)	LR 4.083e-04
0: TRAIN [0][130/658]	Time 0.108 (0.138)	Data 8.34e-05 (8.04e-05)	Tok/s 28552 (39008)	Loss/tok 7.3467 (8.4857)	LR 4.083e-04
1: TRAIN [0][140/658]	Time 0.131 (0.139)	Data 7.34e-05 (7.30e-05)	Tok/s 38381 (39172)	Loss/tok 7.5636 (8.4358)	LR 5.141e-04
0: TRAIN [0][140/658]	Time 0.131 (0.139)	Data 8.51e-05 (8.06e-05)	Tok/s 39163 (39115)	Loss/tok 7.6799 (8.4345)	LR 5.141e-04
1: TRAIN [0][150/658]	Time 0.131 (0.139)	Data 7.53e-05 (7.30e-05)	Tok/s 38469 (39092)	Loss/tok 7.5479 (8.3856)	LR 6.472e-04
0: TRAIN [0][150/658]	Time 0.132 (0.139)	Data 8.15e-05 (8.09e-05)	Tok/s 38119 (39028)	Loss/tok 7.6856 (8.3853)	LR 6.472e-04
1: TRAIN [0][160/658]	Time 0.157 (0.139)	Data 7.51e-05 (7.31e-05)	Tok/s 44745 (39041)	Loss/tok 7.8264 (8.3443)	LR 8.148e-04
0: TRAIN [0][160/658]	Time 0.157 (0.139)	Data 9.70e-05 (8.11e-05)	Tok/s 45094 (38976)	Loss/tok 7.7596 (8.3440)	LR 8.148e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
1: TRAIN [0][170/658]	Time 0.129 (0.139)	Data 7.53e-05 (7.31e-05)	Tok/s 39829 (38889)	Loss/tok 7.8674 (8.3085)	LR 1.026e-03
0: TRAIN [0][170/658]	Time 0.128 (0.139)	Data 8.51e-05 (8.12e-05)	Tok/s 39390 (38834)	Loss/tok 7.9565 (8.3106)	LR 1.026e-03
1: TRAIN [0][180/658]	Time 0.131 (0.139)	Data 8.03e-05 (7.32e-05)	Tok/s 38155 (38814)	Loss/tok 7.4813 (8.2771)	LR 1.291e-03
0: TRAIN [0][180/658]	Time 0.131 (0.139)	Data 8.13e-05 (8.11e-05)	Tok/s 38846 (38746)	Loss/tok 7.5755 (8.2802)	LR 1.291e-03
1: TRAIN [0][190/658]	Time 0.189 (0.140)	Data 8.03e-05 (7.36e-05)	Tok/s 48999 (39092)	Loss/tok 7.6536 (8.2358)	LR 1.626e-03
0: TRAIN [0][190/658]	Time 0.189 (0.140)	Data 7.32e-05 (8.10e-05)	Tok/s 48035 (39033)	Loss/tok 7.6960 (8.2377)	LR 1.626e-03
1: TRAIN [0][200/658]	Time 0.160 (0.140)	Data 7.51e-05 (7.40e-05)	Tok/s 44627 (38896)	Loss/tok 7.6534 (8.2055)	LR 2.000e-03
0: TRAIN [0][200/658]	Time 0.160 (0.140)	Data 7.94e-05 (8.05e-05)	Tok/s 44175 (38824)	Loss/tok 7.7143 (8.2077)	LR 2.000e-03
1: TRAIN [0][210/658]	Time 0.188 (0.140)	Data 7.70e-05 (7.40e-05)	Tok/s 49026 (38820)	Loss/tok 7.5208 (8.1703)	LR 2.000e-03
0: TRAIN [0][210/658]	Time 0.188 (0.140)	Data 7.75e-05 (8.02e-05)	Tok/s 49003 (38758)	Loss/tok 7.5686 (8.1725)	LR 2.000e-03
1: TRAIN [0][220/658]	Time 0.131 (0.139)	Data 7.44e-05 (7.39e-05)	Tok/s 39511 (38754)	Loss/tok 7.2742 (8.1336)	LR 2.000e-03
0: TRAIN [0][220/658]	Time 0.131 (0.139)	Data 7.25e-05 (8.00e-05)	Tok/s 38713 (38689)	Loss/tok 7.1567 (8.1351)	LR 2.000e-03
1: TRAIN [0][230/658]	Time 0.131 (0.139)	Data 7.39e-05 (7.39e-05)	Tok/s 38813 (38773)	Loss/tok 7.0929 (8.0931)	LR 2.000e-03
0: TRAIN [0][230/658]	Time 0.131 (0.139)	Data 7.49e-05 (7.98e-05)	Tok/s 38938 (38709)	Loss/tok 7.0650 (8.0956)	LR 2.000e-03
1: TRAIN [0][240/658]	Time 0.088 (0.138)	Data 7.58e-05 (7.39e-05)	Tok/s 17224 (38358)	Loss/tok 5.9677 (8.0632)	LR 2.000e-03
0: TRAIN [0][240/658]	Time 0.088 (0.138)	Data 7.58e-05 (7.96e-05)	Tok/s 16513 (38281)	Loss/tok 6.1888 (8.0658)	LR 2.000e-03
1: TRAIN [0][250/658]	Time 0.107 (0.138)	Data 7.51e-05 (7.38e-05)	Tok/s 28415 (38236)	Loss/tok 6.7041 (8.0283)	LR 2.000e-03
0: TRAIN [0][250/658]	Time 0.108 (0.138)	Data 7.58e-05 (7.95e-05)	Tok/s 28139 (38152)	Loss/tok 6.6757 (8.0306)	LR 2.000e-03
1: TRAIN [0][260/658]	Time 0.160 (0.139)	Data 8.03e-05 (7.39e-05)	Tok/s 43322 (38429)	Loss/tok 7.0000 (7.9793)	LR 2.000e-03
0: TRAIN [0][260/658]	Time 0.160 (0.139)	Data 7.65e-05 (7.94e-05)	Tok/s 43718 (38341)	Loss/tok 6.9871 (7.9831)	LR 2.000e-03
1: TRAIN [0][270/658]	Time 0.158 (0.139)	Data 7.63e-05 (7.39e-05)	Tok/s 44102 (38494)	Loss/tok 6.8796 (7.9375)	LR 2.000e-03
0: TRAIN [0][270/658]	Time 0.158 (0.139)	Data 7.39e-05 (7.93e-05)	Tok/s 44728 (38411)	Loss/tok 6.8500 (7.9399)	LR 2.000e-03
1: TRAIN [0][280/658]	Time 0.130 (0.138)	Data 6.32e-05 (7.38e-05)	Tok/s 38853 (38371)	Loss/tok 6.6223 (7.9007)	LR 2.000e-03
0: TRAIN [0][280/658]	Time 0.130 (0.138)	Data 7.20e-05 (7.91e-05)	Tok/s 39633 (38297)	Loss/tok 6.6210 (7.9013)	LR 2.000e-03
1: TRAIN [0][290/658]	Time 0.191 (0.138)	Data 6.94e-05 (7.38e-05)	Tok/s 47870 (38359)	Loss/tok 6.8649 (7.8594)	LR 2.000e-03
0: TRAIN [0][290/658]	Time 0.191 (0.138)	Data 7.80e-05 (7.90e-05)	Tok/s 47971 (38280)	Loss/tok 6.9590 (7.8615)	LR 2.000e-03
1: TRAIN [0][300/658]	Time 0.131 (0.138)	Data 6.94e-05 (7.37e-05)	Tok/s 38273 (38257)	Loss/tok 6.5110 (7.8223)	LR 2.000e-03
0: TRAIN [0][300/658]	Time 0.131 (0.138)	Data 7.44e-05 (7.89e-05)	Tok/s 37628 (38174)	Loss/tok 6.4153 (7.8249)	LR 2.000e-03
1: TRAIN [0][310/658]	Time 0.158 (0.138)	Data 7.44e-05 (7.37e-05)	Tok/s 44114 (38253)	Loss/tok 6.5693 (7.7807)	LR 2.000e-03
0: TRAIN [0][310/658]	Time 0.158 (0.138)	Data 7.84e-05 (7.88e-05)	Tok/s 44680 (38172)	Loss/tok 6.6357 (7.7833)	LR 2.000e-03
1: TRAIN [0][320/658]	Time 0.088 (0.138)	Data 8.11e-05 (7.37e-05)	Tok/s 17037 (38116)	Loss/tok 5.7620 (7.7451)	LR 2.000e-03
0: TRAIN [0][320/658]	Time 0.088 (0.138)	Data 7.53e-05 (7.87e-05)	Tok/s 16788 (38035)	Loss/tok 5.7979 (7.7470)	LR 2.000e-03
1: TRAIN [0][330/658]	Time 0.131 (0.138)	Data 7.20e-05 (7.37e-05)	Tok/s 38873 (38217)	Loss/tok 6.3483 (7.7023)	LR 2.000e-03
0: TRAIN [0][330/658]	Time 0.131 (0.138)	Data 7.84e-05 (7.86e-05)	Tok/s 37807 (38130)	Loss/tok 6.2770 (7.7036)	LR 2.000e-03
1: TRAIN [0][340/658]	Time 0.160 (0.138)	Data 7.92e-05 (7.37e-05)	Tok/s 43766 (38125)	Loss/tok 6.4134 (7.6662)	LR 2.000e-03
0: TRAIN [0][340/658]	Time 0.160 (0.138)	Data 7.53e-05 (7.86e-05)	Tok/s 43586 (38038)	Loss/tok 6.3598 (7.6676)	LR 2.000e-03
1: TRAIN [0][350/658]	Time 0.131 (0.138)	Data 7.61e-05 (7.37e-05)	Tok/s 38440 (38181)	Loss/tok 6.1143 (7.6233)	LR 2.000e-03
0: TRAIN [0][350/658]	Time 0.131 (0.138)	Data 7.32e-05 (7.85e-05)	Tok/s 39370 (38100)	Loss/tok 6.2329 (7.6265)	LR 2.000e-03
1: TRAIN [0][360/658]	Time 0.107 (0.138)	Data 7.27e-05 (7.37e-05)	Tok/s 28409 (38193)	Loss/tok 5.7208 (7.5843)	LR 2.000e-03
0: TRAIN [0][360/658]	Time 0.107 (0.138)	Data 8.11e-05 (7.84e-05)	Tok/s 28563 (38111)	Loss/tok 5.8096 (7.5870)	LR 2.000e-03
1: TRAIN [0][370/658]	Time 0.189 (0.138)	Data 7.27e-05 (7.36e-05)	Tok/s 48738 (38265)	Loss/tok 6.4314 (7.5426)	LR 2.000e-03
0: TRAIN [0][370/658]	Time 0.188 (0.138)	Data 7.92e-05 (7.84e-05)	Tok/s 48911 (38188)	Loss/tok 6.3792 (7.5435)	LR 2.000e-03
1: TRAIN [0][380/658]	Time 0.130 (0.138)	Data 7.01e-05 (7.37e-05)	Tok/s 39228 (38224)	Loss/tok 5.9181 (7.5063)	LR 2.000e-03
0: TRAIN [0][380/658]	Time 0.131 (0.138)	Data 7.72e-05 (7.84e-05)	Tok/s 37919 (38145)	Loss/tok 5.9866 (7.5074)	LR 2.000e-03
1: TRAIN [0][390/658]	Time 0.130 (0.138)	Data 7.30e-05 (7.37e-05)	Tok/s 39370 (38219)	Loss/tok 5.8282 (7.4696)	LR 2.000e-03
0: TRAIN [0][390/658]	Time 0.130 (0.138)	Data 8.13e-05 (7.84e-05)	Tok/s 38646 (38133)	Loss/tok 5.9245 (7.4711)	LR 2.000e-03
1: TRAIN [0][400/658]	Time 0.106 (0.138)	Data 7.27e-05 (7.36e-05)	Tok/s 29067 (38137)	Loss/tok 5.5515 (7.4381)	LR 2.000e-03
0: TRAIN [0][400/658]	Time 0.107 (0.138)	Data 7.49e-05 (7.84e-05)	Tok/s 28275 (38061)	Loss/tok 5.6052 (7.4385)	LR 2.000e-03
1: TRAIN [0][410/658]	Time 0.129 (0.138)	Data 7.63e-05 (7.36e-05)	Tok/s 38622 (38163)	Loss/tok 5.8181 (7.3997)	LR 2.000e-03
0: TRAIN [0][410/658]	Time 0.129 (0.138)	Data 6.82e-05 (7.83e-05)	Tok/s 39072 (38096)	Loss/tok 5.9399 (7.4005)	LR 2.000e-03
1: TRAIN [0][420/658]	Time 0.130 (0.138)	Data 6.22e-05 (7.36e-05)	Tok/s 38783 (38202)	Loss/tok 5.7953 (7.3620)	LR 2.000e-03
0: TRAIN [0][420/658]	Time 0.130 (0.138)	Data 7.77e-05 (7.83e-05)	Tok/s 38704 (38137)	Loss/tok 5.7429 (7.3636)	LR 2.000e-03
1: TRAIN [0][430/658]	Time 0.108 (0.138)	Data 7.80e-05 (7.35e-05)	Tok/s 28532 (38209)	Loss/tok 5.5186 (7.3264)	LR 2.000e-03
0: TRAIN [0][430/658]	Time 0.108 (0.138)	Data 8.06e-05 (7.82e-05)	Tok/s 26869 (38138)	Loss/tok 5.1549 (7.3277)	LR 2.000e-03
1: TRAIN [0][440/658]	Time 0.190 (0.139)	Data 7.51e-05 (7.35e-05)	Tok/s 48105 (38282)	Loss/tok 6.0207 (7.2863)	LR 2.000e-03
0: TRAIN [0][440/658]	Time 0.190 (0.138)	Data 8.23e-05 (7.82e-05)	Tok/s 48004 (38222)	Loss/tok 6.1197 (7.2884)	LR 2.000e-03
1: TRAIN [0][450/658]	Time 0.160 (0.139)	Data 7.39e-05 (7.35e-05)	Tok/s 43767 (38324)	Loss/tok 5.8352 (7.2498)	LR 2.000e-03
0: TRAIN [0][450/658]	Time 0.160 (0.139)	Data 7.51e-05 (7.81e-05)	Tok/s 44600 (38260)	Loss/tok 5.7601 (7.2517)	LR 2.000e-03
1: TRAIN [0][460/658]	Time 0.158 (0.139)	Data 7.70e-05 (7.35e-05)	Tok/s 45169 (38295)	Loss/tok 5.7181 (7.2166)	LR 2.000e-03
0: TRAIN [0][460/658]	Time 0.158 (0.139)	Data 7.44e-05 (7.81e-05)	Tok/s 44908 (38234)	Loss/tok 5.7300 (7.2187)	LR 2.000e-03
1: TRAIN [0][470/658]	Time 0.107 (0.139)	Data 7.56e-05 (7.35e-05)	Tok/s 28216 (38276)	Loss/tok 5.1150 (7.1814)	LR 2.000e-03
0: TRAIN [0][470/658]	Time 0.107 (0.139)	Data 8.13e-05 (7.81e-05)	Tok/s 27648 (38216)	Loss/tok 5.2106 (7.1843)	LR 2.000e-03
1: TRAIN [0][480/658]	Time 0.159 (0.138)	Data 7.72e-05 (7.35e-05)	Tok/s 43842 (38219)	Loss/tok 5.6159 (7.1513)	LR 2.000e-03
0: TRAIN [0][480/658]	Time 0.159 (0.138)	Data 7.58e-05 (7.80e-05)	Tok/s 44517 (38162)	Loss/tok 5.6559 (7.1535)	LR 2.000e-03
1: TRAIN [0][490/658]	Time 0.108 (0.138)	Data 7.06e-05 (7.34e-05)	Tok/s 27161 (38183)	Loss/tok 5.0466 (7.1194)	LR 2.000e-03
0: TRAIN [0][490/658]	Time 0.108 (0.138)	Data 7.53e-05 (7.79e-05)	Tok/s 27603 (38130)	Loss/tok 5.2562 (7.1225)	LR 2.000e-03
1: TRAIN [0][500/658]	Time 0.107 (0.138)	Data 7.37e-05 (7.34e-05)	Tok/s 28240 (38064)	Loss/tok 5.0009 (7.0924)	LR 2.000e-03
0: TRAIN [0][500/658]	Time 0.107 (0.138)	Data 7.58e-05 (7.78e-05)	Tok/s 28616 (38008)	Loss/tok 4.9426 (7.0955)	LR 2.000e-03
1: TRAIN [0][510/658]	Time 0.158 (0.138)	Data 7.63e-05 (7.34e-05)	Tok/s 44517 (37969)	Loss/tok 5.6574 (7.0660)	LR 2.000e-03
0: TRAIN [0][510/658]	Time 0.158 (0.138)	Data 7.70e-05 (7.78e-05)	Tok/s 44316 (37916)	Loss/tok 5.5109 (7.0688)	LR 2.000e-03
1: TRAIN [0][520/658]	Time 0.191 (0.137)	Data 7.03e-05 (7.33e-05)	Tok/s 47515 (37886)	Loss/tok 5.6789 (7.0376)	LR 2.000e-03
0: TRAIN [0][520/658]	Time 0.191 (0.137)	Data 7.63e-05 (7.77e-05)	Tok/s 48038 (37841)	Loss/tok 5.6129 (7.0406)	LR 2.000e-03
1: TRAIN [0][530/658]	Time 0.157 (0.138)	Data 7.51e-05 (7.33e-05)	Tok/s 44404 (37938)	Loss/tok 5.4677 (7.0019)	LR 2.000e-03
0: TRAIN [0][530/658]	Time 0.157 (0.138)	Data 8.01e-05 (7.77e-05)	Tok/s 44893 (37894)	Loss/tok 5.2684 (7.0045)	LR 2.000e-03
1: TRAIN [0][540/658]	Time 0.131 (0.137)	Data 8.25e-05 (7.32e-05)	Tok/s 39924 (37883)	Loss/tok 5.0237 (6.9723)	LR 2.000e-03
0: TRAIN [0][540/658]	Time 0.131 (0.137)	Data 7.80e-05 (7.76e-05)	Tok/s 38219 (37835)	Loss/tok 5.0477 (6.9753)	LR 2.000e-03
1: TRAIN [0][550/658]	Time 0.132 (0.137)	Data 7.63e-05 (7.32e-05)	Tok/s 38266 (37866)	Loss/tok 5.0632 (6.9419)	LR 2.000e-03
0: TRAIN [0][550/658]	Time 0.132 (0.137)	Data 8.20e-05 (7.76e-05)	Tok/s 38331 (37814)	Loss/tok 5.1890 (6.9448)	LR 2.000e-03
1: TRAIN [0][560/658]	Time 0.158 (0.137)	Data 7.46e-05 (7.31e-05)	Tok/s 44512 (37857)	Loss/tok 5.2930 (6.9112)	LR 2.000e-03
0: TRAIN [0][560/658]	Time 0.158 (0.137)	Data 7.82e-05 (7.76e-05)	Tok/s 44964 (37802)	Loss/tok 5.0803 (6.9139)	LR 2.000e-03
1: TRAIN [0][570/658]	Time 0.088 (0.138)	Data 7.30e-05 (7.31e-05)	Tok/s 17189 (37904)	Loss/tok 4.2950 (6.8772)	LR 2.000e-03
0: TRAIN [0][570/658]	Time 0.088 (0.137)	Data 7.46e-05 (7.76e-05)	Tok/s 17227 (37851)	Loss/tok 4.2804 (6.8791)	LR 2.000e-03
1: TRAIN [0][580/658]	Time 0.132 (0.137)	Data 7.75e-05 (7.30e-05)	Tok/s 37978 (37898)	Loss/tok 5.0755 (6.8492)	LR 2.000e-03
0: TRAIN [0][580/658]	Time 0.132 (0.137)	Data 8.20e-05 (7.75e-05)	Tok/s 37905 (37846)	Loss/tok 5.1559 (6.8508)	LR 2.000e-03
1: TRAIN [0][590/658]	Time 0.190 (0.138)	Data 7.70e-05 (7.30e-05)	Tok/s 48920 (37930)	Loss/tok 5.3400 (6.8172)	LR 2.000e-03
0: TRAIN [0][590/658]	Time 0.190 (0.138)	Data 7.51e-05 (7.75e-05)	Tok/s 47793 (37873)	Loss/tok 5.4193 (6.8195)	LR 2.000e-03
1: TRAIN [0][600/658]	Time 0.156 (0.138)	Data 7.87e-05 (7.29e-05)	Tok/s 44900 (37937)	Loss/tok 5.1268 (6.7877)	LR 2.000e-03
0: TRAIN [0][600/658]	Time 0.158 (0.138)	Data 7.58e-05 (7.74e-05)	Tok/s 45017 (37886)	Loss/tok 5.1570 (6.7897)	LR 2.000e-03
1: TRAIN [0][610/658]	Time 0.131 (0.138)	Data 7.34e-05 (7.29e-05)	Tok/s 39043 (37932)	Loss/tok 4.8280 (6.7584)	LR 2.000e-03
0: TRAIN [0][610/658]	Time 0.131 (0.138)	Data 7.87e-05 (7.74e-05)	Tok/s 38392 (37878)	Loss/tok 4.8293 (6.7603)	LR 2.000e-03
1: TRAIN [0][620/658]	Time 0.158 (0.138)	Data 7.27e-05 (7.29e-05)	Tok/s 45209 (38011)	Loss/tok 4.9418 (6.7238)	LR 2.000e-03
0: TRAIN [0][620/658]	Time 0.158 (0.138)	Data 7.56e-05 (7.74e-05)	Tok/s 44670 (37951)	Loss/tok 4.9623 (6.7266)	LR 2.000e-03
1: TRAIN [0][630/658]	Time 0.106 (0.138)	Data 7.53e-05 (7.29e-05)	Tok/s 28754 (37937)	Loss/tok 4.5401 (6.7008)	LR 2.000e-03
0: TRAIN [0][630/658]	Time 0.106 (0.137)	Data 6.44e-05 (7.74e-05)	Tok/s 28278 (37874)	Loss/tok 4.6713 (6.7036)	LR 2.000e-03
1: TRAIN [0][640/658]	Time 0.159 (0.138)	Data 7.22e-05 (7.30e-05)	Tok/s 43867 (37932)	Loss/tok 4.9202 (6.6724)	LR 2.000e-03
0: TRAIN [0][640/658]	Time 0.159 (0.138)	Data 6.77e-05 (7.73e-05)	Tok/s 44214 (37867)	Loss/tok 4.8559 (6.6755)	LR 2.000e-03
1: TRAIN [0][650/658]	Time 0.131 (0.137)	Data 7.32e-05 (7.30e-05)	Tok/s 38915 (37916)	Loss/tok 4.6495 (6.6457)	LR 2.000e-03
0: TRAIN [0][650/658]	Time 0.131 (0.137)	Data 8.15e-05 (7.72e-05)	Tok/s 39071 (37852)	Loss/tok 4.7383 (6.6491)	LR 2.000e-03
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [0][0/106]	Time 0.034 (0.000)	Data 1.13e-03 (0.00e+00)	Tok/s 109263 (0)	Loss/tok 6.4765 (6.4765)
0: VALIDATION [0][0/107]	Time 0.047 (0.000)	Data 1.14e-03 (0.00e+00)	Tok/s 95498 (0)	Loss/tok 6.4890 (6.4890)
1: VALIDATION [0][10/106]	Time 0.021 (0.025)	Data 9.69e-04 (9.90e-04)	Tok/s 108666 (109890)	Loss/tok 6.0703 (6.1910)
0: VALIDATION [0][10/107]	Time 0.021 (0.025)	Data 9.99e-04 (9.99e-04)	Tok/s 111277 (109751)	Loss/tok 6.3278 (6.2467)
1: VALIDATION [0][20/106]	Time 0.018 (0.022)	Data 9.69e-04 (9.81e-04)	Tok/s 110923 (109640)	Loss/tok 5.8682 (6.1576)
0: VALIDATION [0][20/107]	Time 0.019 (0.022)	Data 9.72e-04 (9.87e-04)	Tok/s 105563 (109599)	Loss/tok 6.1713 (6.1296)
1: VALIDATION [0][30/106]	Time 0.016 (0.020)	Data 9.65e-04 (9.77e-04)	Tok/s 106648 (109068)	Loss/tok 5.6255 (6.0795)
0: VALIDATION [0][30/107]	Time 0.016 (0.020)	Data 9.56e-04 (9.83e-04)	Tok/s 106329 (109197)	Loss/tok 5.7124 (6.0848)
1: VALIDATION [0][40/106]	Time 0.014 (0.019)	Data 9.67e-04 (9.74e-04)	Tok/s 104419 (108215)	Loss/tok 5.6050 (6.0258)
0: VALIDATION [0][40/107]	Time 0.014 (0.019)	Data 9.68e-04 (9.79e-04)	Tok/s 103743 (108554)	Loss/tok 5.7860 (6.0256)
1: VALIDATION [0][50/106]	Time 0.013 (0.018)	Data 9.62e-04 (9.70e-04)	Tok/s 101329 (107091)	Loss/tok 5.5007 (5.9845)
0: VALIDATION [0][50/107]	Time 0.013 (0.018)	Data 9.61e-04 (9.76e-04)	Tok/s 95260 (107259)	Loss/tok 5.5441 (5.9768)
1: VALIDATION [0][60/106]	Time 0.011 (0.017)	Data 9.65e-04 (9.68e-04)	Tok/s 97143 (105874)	Loss/tok 5.4666 (5.9507)
0: VALIDATION [0][60/107]	Time 0.011 (0.017)	Data 9.48e-04 (9.73e-04)	Tok/s 100956 (106290)	Loss/tok 5.5209 (5.9442)
1: VALIDATION [0][70/106]	Time 0.009 (0.016)	Data 9.45e-04 (9.64e-04)	Tok/s 99442 (104428)	Loss/tok 5.6658 (5.9237)
0: VALIDATION [0][70/107]	Time 0.010 (0.016)	Data 9.49e-04 (9.69e-04)	Tok/s 94377 (104676)	Loss/tok 5.3397 (5.9168)
1: VALIDATION [0][80/106]	Time 0.009 (0.015)	Data 9.37e-04 (9.62e-04)	Tok/s 91454 (103038)	Loss/tok 5.6057 (5.8970)
0: VALIDATION [0][80/107]	Time 0.009 (0.015)	Data 9.42e-04 (9.67e-04)	Tok/s 90701 (103358)	Loss/tok 5.5839 (5.8946)
1: VALIDATION [0][90/106]	Time 0.008 (0.014)	Data 9.45e-04 (9.59e-04)	Tok/s 85148 (101466)	Loss/tok 4.9644 (5.8699)
0: VALIDATION [0][90/107]	Time 0.008 (0.014)	Data 9.46e-04 (9.64e-04)	Tok/s 87565 (101716)	Loss/tok 5.6989 (5.8737)
1: VALIDATION [0][100/106]	Time 0.006 (0.013)	Data 9.31e-04 (9.57e-04)	Tok/s 73318 (99322)	Loss/tok 4.9169 (5.8452)
0: VALIDATION [0][100/107]	Time 0.006 (0.014)	Data 9.44e-04 (9.62e-04)	Tok/s 76208 (99609)	Loss/tok 5.3017 (5.8487)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [0][9/63]	Time 0.2950 (0.3166)	Decoder iters 149.0 (149.0)	Tok/s 8929 (9500)
0: TEST [0][9/63]	Time 0.2950 (0.3165)	Decoder iters 149.0 (149.0)	Tok/s 8816 (9678)
1: TEST [0][19/63]	Time 0.2859 (0.3026)	Decoder iters 149.0 (149.0)	Tok/s 7908 (8699)
0: TEST [0][19/63]	Time 0.2860 (0.3026)	Decoder iters 149.0 (149.0)	Tok/s 6318 (8679)
1: TEST [0][29/63]	Time 0.2810 (0.2959)	Decoder iters 149.0 (149.0)	Tok/s 6312 (7991)
0: TEST [0][29/63]	Time 0.2810 (0.2959)	Decoder iters 149.0 (149.0)	Tok/s 5889 (7935)
1: TEST [0][39/63]	Time 0.2738 (0.2913)	Decoder iters 149.0 (149.0)	Tok/s 6436 (7352)
0: TEST [0][39/63]	Time 0.2738 (0.2913)	Decoder iters 99.0 (147.8)	Tok/s 4411 (7342)
1: TEST [0][49/63]	Time 0.2719 (0.2876)	Decoder iters 149.0 (149.0)	Tok/s 3497 (6731)
0: TEST [0][49/63]	Time 0.2719 (0.2876)	Decoder iters 149.0 (145.9)	Tok/s 4203 (6729)
1: TEST [0][59/63]	Time 0.0728 (0.2748)	Decoder iters 34.0 (141.9)	Tok/s 7639 (6405)
0: TEST [0][59/63]	Time 0.0728 (0.2748)	Decoder iters 33.0 (137.1)	Tok/s 8672 (6410)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
1: Starting epoch 1
1: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 6.6267	Validation Loss: 5.8352	Test BLEU: 2.56
0: Performance: Epoch: 0	Training: 75828 Tok/s	Validation: 195426 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
0: Sampler for epoch 1 uses seed 1323436024
1: Sampler for epoch 1 uses seed 1323436024
1: TRAIN [1][0/658]	Time 0.236 (0.000)	Data 8.84e-02 (0.00e+00)	Tok/s 29743 (0)	Loss/tok 4.7077 (4.7077)	LR 2.000e-03
0: TRAIN [1][0/658]	Time 0.236 (0.000)	Data 8.97e-02 (0.00e+00)	Tok/s 29733 (0)	Loss/tok 4.8686 (4.8686)	LR 2.000e-03
1: TRAIN [1][10/658]	Time 0.158 (0.131)	Data 8.18e-05 (8.04e-05)	Tok/s 44660 (35242)	Loss/tok 4.7383 (4.6705)	LR 2.000e-03
0: TRAIN [1][10/658]	Time 0.158 (0.131)	Data 7.51e-05 (8.13e-05)	Tok/s 44696 (35372)	Loss/tok 4.8161 (4.6685)	LR 2.000e-03
1: TRAIN [1][20/658]	Time 0.129 (0.141)	Data 8.01e-05 (7.98e-05)	Tok/s 39559 (38977)	Loss/tok 4.4242 (4.6561)	LR 2.000e-03
0: TRAIN [1][20/658]	Time 0.130 (0.141)	Data 7.46e-05 (8.07e-05)	Tok/s 38294 (38910)	Loss/tok 4.4852 (4.6516)	LR 2.000e-03
1: TRAIN [1][30/658]	Time 0.158 (0.145)	Data 8.03e-05 (7.90e-05)	Tok/s 44357 (40124)	Loss/tok 4.7333 (4.6662)	LR 2.000e-03
0: TRAIN [1][30/658]	Time 0.158 (0.145)	Data 7.53e-05 (8.03e-05)	Tok/s 44781 (40240)	Loss/tok 4.7522 (4.6641)	LR 2.000e-03
1: TRAIN [1][40/658]	Time 0.106 (0.140)	Data 7.65e-05 (7.84e-05)	Tok/s 28535 (38844)	Loss/tok 3.9069 (4.6096)	LR 2.000e-03
0: TRAIN [1][40/658]	Time 0.106 (0.140)	Data 7.70e-05 (8.02e-05)	Tok/s 28755 (38893)	Loss/tok 3.9394 (4.6076)	LR 2.000e-03
1: TRAIN [1][50/658]	Time 0.157 (0.141)	Data 7.84e-05 (7.81e-05)	Tok/s 44158 (39439)	Loss/tok 4.5744 (4.6099)	LR 2.000e-03
0: TRAIN [1][50/658]	Time 0.157 (0.141)	Data 7.49e-05 (7.99e-05)	Tok/s 44824 (39512)	Loss/tok 4.5313 (4.6070)	LR 2.000e-03
1: TRAIN [1][60/658]	Time 0.157 (0.141)	Data 7.70e-05 (7.80e-05)	Tok/s 44462 (39387)	Loss/tok 4.7381 (4.6111)	LR 2.000e-03
0: TRAIN [1][60/658]	Time 0.157 (0.141)	Data 7.80e-05 (7.97e-05)	Tok/s 44815 (39407)	Loss/tok 4.5263 (4.5935)	LR 2.000e-03
1: TRAIN [1][70/658]	Time 0.129 (0.140)	Data 8.01e-05 (7.79e-05)	Tok/s 38808 (39114)	Loss/tok 4.3746 (4.5857)	LR 2.000e-03
0: TRAIN [1][70/658]	Time 0.129 (0.140)	Data 7.44e-05 (7.94e-05)	Tok/s 39992 (39139)	Loss/tok 4.2651 (4.5741)	LR 2.000e-03
1: TRAIN [1][80/658]	Time 0.087 (0.140)	Data 8.37e-05 (7.81e-05)	Tok/s 17083 (38754)	Loss/tok 3.5364 (4.5907)	LR 2.000e-03
0: TRAIN [1][80/658]	Time 0.087 (0.140)	Data 8.08e-05 (7.97e-05)	Tok/s 17445 (38785)	Loss/tok 3.9199 (4.5847)	LR 2.000e-03
1: TRAIN [1][90/658]	Time 0.158 (0.141)	Data 8.11e-05 (7.81e-05)	Tok/s 45277 (38943)	Loss/tok 4.6910 (4.5938)	LR 2.000e-03
0: TRAIN [1][90/658]	Time 0.160 (0.141)	Data 8.13e-05 (7.98e-05)	Tok/s 44896 (38996)	Loss/tok 4.7745 (4.5865)	LR 2.000e-03
1: TRAIN [1][100/658]	Time 0.131 (0.139)	Data 8.13e-05 (7.79e-05)	Tok/s 38516 (38542)	Loss/tok 4.3571 (4.5744)	LR 2.000e-03
0: TRAIN [1][100/658]	Time 0.131 (0.139)	Data 7.56e-05 (7.96e-05)	Tok/s 38953 (38602)	Loss/tok 4.1675 (4.5617)	LR 2.000e-03
1: TRAIN [1][110/658]	Time 0.129 (0.138)	Data 6.89e-05 (7.77e-05)	Tok/s 39544 (38336)	Loss/tok 4.3755 (4.5661)	LR 2.000e-03
0: TRAIN [1][110/658]	Time 0.129 (0.138)	Data 7.49e-05 (7.95e-05)	Tok/s 38800 (38394)	Loss/tok 4.3522 (4.5523)	LR 2.000e-03
1: TRAIN [1][120/658]	Time 0.131 (0.139)	Data 8.13e-05 (7.77e-05)	Tok/s 38755 (38570)	Loss/tok 4.1821 (4.5631)	LR 2.000e-03
0: TRAIN [1][120/658]	Time 0.131 (0.139)	Data 8.13e-05 (8.01e-05)	Tok/s 39231 (38630)	Loss/tok 4.2652 (4.5497)	LR 2.000e-03
1: TRAIN [1][130/658]	Time 0.158 (0.139)	Data 7.10e-05 (7.75e-05)	Tok/s 44670 (38599)	Loss/tok 4.5771 (4.5549)	LR 2.000e-03
0: TRAIN [1][130/658]	Time 0.158 (0.139)	Data 7.39e-05 (8.01e-05)	Tok/s 44832 (38638)	Loss/tok 4.4758 (4.5419)	LR 2.000e-03
1: TRAIN [1][140/658]	Time 0.107 (0.140)	Data 8.37e-05 (7.77e-05)	Tok/s 28655 (38747)	Loss/tok 3.9994 (4.5507)	LR 2.000e-03
0: TRAIN [1][140/658]	Time 0.108 (0.140)	Data 7.94e-05 (8.01e-05)	Tok/s 28707 (38795)	Loss/tok 3.9702 (4.5387)	LR 2.000e-03
1: TRAIN [1][150/658]	Time 0.106 (0.139)	Data 7.65e-05 (7.76e-05)	Tok/s 28475 (38536)	Loss/tok 3.8441 (4.5371)	LR 2.000e-03
0: TRAIN [1][150/658]	Time 0.106 (0.139)	Data 7.63e-05 (8.00e-05)	Tok/s 28377 (38600)	Loss/tok 4.0004 (4.5261)	LR 2.000e-03
1: TRAIN [1][160/658]	Time 0.188 (0.140)	Data 7.68e-05 (7.74e-05)	Tok/s 49039 (38779)	Loss/tok 4.6941 (4.5386)	LR 2.000e-03
0: TRAIN [1][160/658]	Time 0.188 (0.140)	Data 7.75e-05 (7.99e-05)	Tok/s 48615 (38814)	Loss/tok 4.6840 (4.5266)	LR 2.000e-03
1: TRAIN [1][170/658]	Time 0.160 (0.140)	Data 8.01e-05 (7.74e-05)	Tok/s 44444 (38548)	Loss/tok 4.3511 (4.5255)	LR 2.000e-03
0: TRAIN [1][170/658]	Time 0.160 (0.140)	Data 7.68e-05 (7.99e-05)	Tok/s 43734 (38565)	Loss/tok 4.3608 (4.5159)	LR 2.000e-03
1: TRAIN [1][180/658]	Time 0.131 (0.139)	Data 7.44e-05 (7.72e-05)	Tok/s 38145 (38277)	Loss/tok 4.2397 (4.5100)	LR 2.000e-03
0: TRAIN [1][180/658]	Time 0.131 (0.139)	Data 7.72e-05 (8.01e-05)	Tok/s 38991 (38341)	Loss/tok 4.2621 (4.5003)	LR 2.000e-03
1: TRAIN [1][190/658]	Time 0.159 (0.139)	Data 7.80e-05 (7.72e-05)	Tok/s 43819 (38318)	Loss/tok 4.3696 (4.5014)	LR 2.000e-03
0: TRAIN [1][190/658]	Time 0.159 (0.139)	Data 8.32e-05 (8.00e-05)	Tok/s 43931 (38397)	Loss/tok 4.4226 (4.4936)	LR 2.000e-03
1: TRAIN [1][200/658]	Time 0.106 (0.139)	Data 8.13e-05 (7.73e-05)	Tok/s 28389 (38347)	Loss/tok 3.9100 (4.4987)	LR 2.000e-03
0: TRAIN [1][200/658]	Time 0.106 (0.139)	Data 7.99e-05 (8.01e-05)	Tok/s 28684 (38419)	Loss/tok 3.7932 (4.4868)	LR 2.000e-03
0: TRAIN [1][210/658]	Time 0.108 (0.139)	Data 7.89e-05 (8.01e-05)	Tok/s 27779 (38296)	Loss/tok 3.7798 (4.4790)	LR 2.000e-03
1: TRAIN [1][210/658]	Time 0.109 (0.139)	Data 7.82e-05 (7.72e-05)	Tok/s 28232 (38206)	Loss/tok 3.9058 (4.4896)	LR 2.000e-03
1: TRAIN [1][220/658]	Time 0.129 (0.139)	Data 7.39e-05 (7.72e-05)	Tok/s 38929 (38173)	Loss/tok 4.0569 (4.4789)	LR 1.000e-03
0: TRAIN [1][220/658]	Time 0.129 (0.139)	Data 7.49e-05 (8.01e-05)	Tok/s 38630 (38253)	Loss/tok 4.2281 (4.4707)	LR 1.000e-03
0: TRAIN [1][230/658]	Time 0.160 (0.138)	Data 8.13e-05 (8.01e-05)	Tok/s 43470 (38157)	Loss/tok 4.2797 (4.4628)	LR 1.000e-03
1: TRAIN [1][230/658]	Time 0.160 (0.138)	Data 9.30e-05 (7.73e-05)	Tok/s 44056 (38086)	Loss/tok 4.2457 (4.4694)	LR 1.000e-03
1: TRAIN [1][240/658]	Time 0.158 (0.138)	Data 8.54e-05 (7.74e-05)	Tok/s 44272 (38021)	Loss/tok 4.2867 (4.4567)	LR 1.000e-03
0: TRAIN [1][240/658]	Time 0.159 (0.138)	Data 7.99e-05 (8.01e-05)	Tok/s 43939 (38077)	Loss/tok 4.2672 (4.4499)	LR 1.000e-03
1: TRAIN [1][250/658]	Time 0.158 (0.138)	Data 8.75e-05 (7.76e-05)	Tok/s 45131 (38103)	Loss/tok 4.2847 (4.4452)	LR 1.000e-03
0: TRAIN [1][250/658]	Time 0.158 (0.138)	Data 7.89e-05 (8.03e-05)	Tok/s 45147 (38159)	Loss/tok 4.4113 (4.4396)	LR 1.000e-03
1: TRAIN [1][260/658]	Time 0.160 (0.138)	Data 8.44e-05 (7.76e-05)	Tok/s 44432 (38028)	Loss/tok 4.3527 (4.4355)	LR 1.000e-03
0: TRAIN [1][260/658]	Time 0.160 (0.138)	Data 9.42e-05 (8.04e-05)	Tok/s 44162 (38074)	Loss/tok 4.1997 (4.4285)	LR 1.000e-03
1: TRAIN [1][270/658]	Time 0.132 (0.138)	Data 8.65e-05 (7.76e-05)	Tok/s 38156 (38099)	Loss/tok 4.0040 (4.4273)	LR 1.000e-03
0: TRAIN [1][270/658]	Time 0.132 (0.138)	Data 8.15e-05 (8.06e-05)	Tok/s 38328 (38156)	Loss/tok 4.0115 (4.4212)	LR 1.000e-03
1: TRAIN [1][280/658]	Time 0.106 (0.138)	Data 6.94e-05 (7.76e-05)	Tok/s 28714 (37990)	Loss/tok 3.8901 (4.4181)	LR 1.000e-03
0: TRAIN [1][280/658]	Time 0.106 (0.138)	Data 8.44e-05 (8.08e-05)	Tok/s 28567 (38044)	Loss/tok 3.6038 (4.4125)	LR 1.000e-03
1: TRAIN [1][290/658]	Time 0.157 (0.138)	Data 8.44e-05 (7.77e-05)	Tok/s 45270 (37981)	Loss/tok 4.2879 (4.4106)	LR 1.000e-03
0: TRAIN [1][290/658]	Time 0.157 (0.138)	Data 7.99e-05 (8.08e-05)	Tok/s 44984 (38023)	Loss/tok 4.2512 (4.4053)	LR 1.000e-03
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
1: TRAIN [1][300/658]	Time 0.108 (0.138)	Data 8.54e-05 (7.77e-05)	Tok/s 28650 (37921)	Loss/tok 3.8020 (4.4027)	LR 1.000e-03
0: TRAIN [1][300/658]	Time 0.108 (0.138)	Data 8.42e-05 (8.08e-05)	Tok/s 28554 (37968)	Loss/tok 3.8117 (4.3968)	LR 1.000e-03
1: TRAIN [1][310/658]	Time 0.106 (0.138)	Data 8.54e-05 (7.77e-05)	Tok/s 27686 (37897)	Loss/tok 3.6290 (4.3924)	LR 1.000e-03
0: TRAIN [1][310/658]	Time 0.106 (0.137)	Data 7.68e-05 (8.09e-05)	Tok/s 29045 (37946)	Loss/tok 3.7537 (4.3877)	LR 1.000e-03
1: TRAIN [1][320/658]	Time 0.159 (0.137)	Data 8.30e-05 (7.77e-05)	Tok/s 44349 (37800)	Loss/tok 4.3007 (4.3850)	LR 1.000e-03
0: TRAIN [1][320/658]	Time 0.159 (0.137)	Data 7.99e-05 (8.10e-05)	Tok/s 44044 (37855)	Loss/tok 4.1086 (4.3790)	LR 1.000e-03
1: TRAIN [1][330/658]	Time 0.131 (0.137)	Data 8.87e-05 (7.78e-05)	Tok/s 38241 (37851)	Loss/tok 3.8636 (4.3738)	LR 5.000e-04
0: TRAIN [1][330/658]	Time 0.131 (0.137)	Data 8.44e-05 (8.11e-05)	Tok/s 38438 (37898)	Loss/tok 3.9697 (4.3690)	LR 5.000e-04
1: TRAIN [1][340/658]	Time 0.131 (0.138)	Data 8.46e-05 (7.78e-05)	Tok/s 39188 (37922)	Loss/tok 3.9468 (4.3651)	LR 5.000e-04
0: TRAIN [1][340/658]	Time 0.131 (0.138)	Data 8.18e-05 (8.12e-05)	Tok/s 37730 (37962)	Loss/tok 4.1219 (4.3634)	LR 5.000e-04
1: TRAIN [1][350/658]	Time 0.106 (0.137)	Data 7.06e-05 (7.78e-05)	Tok/s 28786 (37830)	Loss/tok 3.6620 (4.3558)	LR 5.000e-04
0: TRAIN [1][350/658]	Time 0.106 (0.137)	Data 6.77e-05 (8.12e-05)	Tok/s 28391 (37871)	Loss/tok 3.7152 (4.3541)	LR 5.000e-04
1: TRAIN [1][360/658]	Time 0.190 (0.137)	Data 8.68e-05 (7.78e-05)	Tok/s 48238 (37691)	Loss/tok 4.4018 (4.3485)	LR 5.000e-04
0: TRAIN [1][360/658]	Time 0.191 (0.137)	Data 8.99e-05 (8.13e-05)	Tok/s 48275 (37730)	Loss/tok 4.3028 (4.3482)	LR 5.000e-04
1: TRAIN [1][370/658]	Time 0.159 (0.137)	Data 8.73e-05 (7.79e-05)	Tok/s 44022 (37577)	Loss/tok 4.1241 (4.3413)	LR 5.000e-04
0: TRAIN [1][370/658]	Time 0.159 (0.137)	Data 8.37e-05 (8.14e-05)	Tok/s 44725 (37616)	Loss/tok 4.1820 (4.3406)	LR 5.000e-04
1: TRAIN [1][380/658]	Time 0.130 (0.136)	Data 8.44e-05 (7.79e-05)	Tok/s 39140 (37562)	Loss/tok 3.8948 (4.3300)	LR 5.000e-04
0: TRAIN [1][380/658]	Time 0.130 (0.136)	Data 8.25e-05 (8.15e-05)	Tok/s 38453 (37609)	Loss/tok 3.9135 (4.3301)	LR 5.000e-04
1: TRAIN [1][390/658]	Time 0.158 (0.136)	Data 8.51e-05 (7.79e-05)	Tok/s 44638 (37542)	Loss/tok 4.1061 (4.3201)	LR 5.000e-04
0: TRAIN [1][390/658]	Time 0.158 (0.136)	Data 7.80e-05 (8.16e-05)	Tok/s 44326 (37583)	Loss/tok 4.1588 (4.3200)	LR 5.000e-04
1: TRAIN [1][400/658]	Time 0.130 (0.136)	Data 8.32e-05 (7.79e-05)	Tok/s 38855 (37488)	Loss/tok 3.8892 (4.3114)	LR 5.000e-04
0: TRAIN [1][400/658]	Time 0.131 (0.136)	Data 8.73e-05 (8.17e-05)	Tok/s 38526 (37529)	Loss/tok 4.0043 (4.3121)	LR 5.000e-04
1: TRAIN [1][410/658]	Time 0.129 (0.136)	Data 8.56e-05 (7.80e-05)	Tok/s 39323 (37513)	Loss/tok 3.8959 (4.3044)	LR 5.000e-04
0: TRAIN [1][410/658]	Time 0.130 (0.136)	Data 8.54e-05 (8.18e-05)	Tok/s 39032 (37550)	Loss/tok 3.9367 (4.3054)	LR 5.000e-04
1: TRAIN [1][420/658]	Time 0.131 (0.136)	Data 8.37e-05 (7.80e-05)	Tok/s 39319 (37527)	Loss/tok 3.7888 (4.2948)	LR 5.000e-04
0: TRAIN [1][420/658]	Time 0.130 (0.136)	Data 8.23e-05 (8.19e-05)	Tok/s 38830 (37565)	Loss/tok 3.8273 (4.2963)	LR 5.000e-04
1: TRAIN [1][430/658]	Time 0.106 (0.136)	Data 8.54e-05 (7.80e-05)	Tok/s 28488 (37487)	Loss/tok 3.6593 (4.2865)	LR 5.000e-04
0: TRAIN [1][430/658]	Time 0.107 (0.136)	Data 7.58e-05 (8.20e-05)	Tok/s 27927 (37514)	Loss/tok 3.4493 (4.2881)	LR 5.000e-04
1: TRAIN [1][440/658]	Time 0.106 (0.136)	Data 7.68e-05 (7.80e-05)	Tok/s 28847 (37421)	Loss/tok 3.6002 (4.2791)	LR 2.500e-04
0: TRAIN [1][440/658]	Time 0.106 (0.136)	Data 8.13e-05 (8.20e-05)	Tok/s 28834 (37444)	Loss/tok 3.5588 (4.2804)	LR 2.500e-04
1: TRAIN [1][450/658]	Time 0.130 (0.136)	Data 8.46e-05 (7.81e-05)	Tok/s 38995 (37340)	Loss/tok 3.8535 (4.2716)	LR 2.500e-04
0: TRAIN [1][450/658]	Time 0.130 (0.136)	Data 7.89e-05 (8.21e-05)	Tok/s 39134 (37371)	Loss/tok 3.7702 (4.2724)	LR 2.500e-04
1: TRAIN [1][460/658]	Time 0.190 (0.136)	Data 8.56e-05 (7.81e-05)	Tok/s 48679 (37414)	Loss/tok 4.2475 (4.2659)	LR 2.500e-04
0: TRAIN [1][460/658]	Time 0.190 (0.136)	Data 8.23e-05 (8.21e-05)	Tok/s 47508 (37442)	Loss/tok 4.2242 (4.2662)	LR 2.500e-04
1: TRAIN [1][470/658]	Time 0.190 (0.136)	Data 8.27e-05 (7.82e-05)	Tok/s 48395 (37499)	Loss/tok 4.2506 (4.2615)	LR 2.500e-04
0: TRAIN [1][470/658]	Time 0.190 (0.136)	Data 8.20e-05 (8.21e-05)	Tok/s 48329 (37531)	Loss/tok 4.1012 (4.2593)	LR 2.500e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
1: TRAIN [1][480/658]	Time 0.158 (0.136)	Data 8.51e-05 (7.82e-05)	Tok/s 44310 (37557)	Loss/tok 4.0441 (4.2556)	LR 2.500e-04
0: TRAIN [1][480/658]	Time 0.158 (0.136)	Data 8.25e-05 (8.22e-05)	Tok/s 44719 (37594)	Loss/tok 4.0024 (4.2523)	LR 2.500e-04
1: TRAIN [1][490/658]	Time 0.131 (0.136)	Data 8.20e-05 (7.82e-05)	Tok/s 38877 (37565)	Loss/tok 3.6466 (4.2485)	LR 2.500e-04
0: TRAIN [1][490/658]	Time 0.130 (0.136)	Data 7.70e-05 (8.22e-05)	Tok/s 38895 (37604)	Loss/tok 3.7918 (4.2453)	LR 2.500e-04
1: TRAIN [1][500/658]	Time 0.158 (0.136)	Data 8.56e-05 (7.83e-05)	Tok/s 44358 (37617)	Loss/tok 4.0908 (4.2429)	LR 2.500e-04
0: TRAIN [1][500/658]	Time 0.158 (0.136)	Data 8.18e-05 (8.22e-05)	Tok/s 44848 (37656)	Loss/tok 4.1490 (4.2402)	LR 2.500e-04
1: TRAIN [1][510/658]	Time 0.133 (0.136)	Data 8.63e-05 (7.82e-05)	Tok/s 38460 (37635)	Loss/tok 3.8447 (4.2366)	LR 2.500e-04
0: TRAIN [1][510/658]	Time 0.133 (0.136)	Data 8.27e-05 (8.23e-05)	Tok/s 38518 (37668)	Loss/tok 3.8609 (4.2340)	LR 2.500e-04
1: TRAIN [1][520/658]	Time 0.158 (0.137)	Data 8.25e-05 (7.83e-05)	Tok/s 44161 (37681)	Loss/tok 4.0123 (4.2326)	LR 2.500e-04
0: TRAIN [1][520/658]	Time 0.159 (0.137)	Data 7.68e-05 (8.23e-05)	Tok/s 43822 (37719)	Loss/tok 3.9644 (4.2309)	LR 2.500e-04
1: TRAIN [1][530/658]	Time 0.107 (0.137)	Data 9.04e-05 (7.83e-05)	Tok/s 28349 (37650)	Loss/tok 3.5038 (4.2273)	LR 2.500e-04
0: TRAIN [1][530/658]	Time 0.108 (0.137)	Data 8.77e-05 (8.24e-05)	Tok/s 28587 (37683)	Loss/tok 3.5446 (4.2257)	LR 2.500e-04
1: TRAIN [1][540/658]	Time 0.131 (0.137)	Data 8.68e-05 (7.83e-05)	Tok/s 38764 (37673)	Loss/tok 3.8424 (4.2228)	LR 2.500e-04
0: TRAIN [1][540/658]	Time 0.131 (0.137)	Data 8.89e-05 (8.25e-05)	Tok/s 38716 (37705)	Loss/tok 3.6777 (4.2212)	LR 2.500e-04
1: TRAIN [1][550/658]	Time 0.131 (0.137)	Data 8.73e-05 (7.83e-05)	Tok/s 38506 (37699)	Loss/tok 3.7924 (4.2172)	LR 1.250e-04
0: TRAIN [1][550/658]	Time 0.130 (0.137)	Data 8.18e-05 (8.25e-05)	Tok/s 38942 (37722)	Loss/tok 3.7501 (4.2152)	LR 1.250e-04
1: TRAIN [1][560/658]	Time 0.106 (0.137)	Data 8.68e-05 (7.84e-05)	Tok/s 27759 (37721)	Loss/tok 3.4506 (4.2116)	LR 1.250e-04
0: TRAIN [1][560/658]	Time 0.106 (0.137)	Data 8.44e-05 (8.26e-05)	Tok/s 28808 (37741)	Loss/tok 3.4866 (4.2088)	LR 1.250e-04
1: TRAIN [1][570/658]	Time 0.158 (0.137)	Data 8.85e-05 (7.84e-05)	Tok/s 45026 (37737)	Loss/tok 3.9645 (4.2062)	LR 1.250e-04
0: TRAIN [1][570/658]	Time 0.158 (0.137)	Data 8.37e-05 (8.26e-05)	Tok/s 45345 (37751)	Loss/tok 3.8951 (4.2028)	LR 1.250e-04
1: TRAIN [1][580/658]	Time 0.107 (0.137)	Data 8.18e-05 (7.84e-05)	Tok/s 27096 (37771)	Loss/tok 3.5657 (4.2007)	LR 1.250e-04
0: TRAIN [1][580/658]	Time 0.107 (0.137)	Data 8.08e-05 (8.27e-05)	Tok/s 28050 (37788)	Loss/tok 3.6347 (4.1976)	LR 1.250e-04
1: TRAIN [1][590/658]	Time 0.107 (0.137)	Data 8.25e-05 (7.85e-05)	Tok/s 29322 (37767)	Loss/tok 3.6041 (4.1959)	LR 1.250e-04
0: TRAIN [1][590/658]	Time 0.107 (0.137)	Data 7.77e-05 (8.28e-05)	Tok/s 29303 (37776)	Loss/tok 3.5903 (4.1924)	LR 1.250e-04
1: TRAIN [1][600/658]	Time 0.159 (0.137)	Data 8.13e-05 (7.85e-05)	Tok/s 43911 (37748)	Loss/tok 4.1048 (4.1911)	LR 1.250e-04
0: TRAIN [1][600/658]	Time 0.158 (0.137)	Data 7.61e-05 (8.28e-05)	Tok/s 45124 (37764)	Loss/tok 4.1151 (4.1876)	LR 1.250e-04
1: TRAIN [1][610/658]	Time 0.133 (0.137)	Data 7.51e-05 (7.85e-05)	Tok/s 38286 (37720)	Loss/tok 3.7561 (4.1872)	LR 1.250e-04
0: TRAIN [1][610/658]	Time 0.132 (0.137)	Data 7.61e-05 (8.28e-05)	Tok/s 37810 (37733)	Loss/tok 3.8026 (4.1832)	LR 1.250e-04
1: TRAIN [1][620/658]	Time 0.188 (0.137)	Data 8.44e-05 (7.85e-05)	Tok/s 48486 (37767)	Loss/tok 4.3338 (4.1848)	LR 1.250e-04
0: TRAIN [1][620/658]	Time 0.188 (0.137)	Data 7.89e-05 (8.28e-05)	Tok/s 48503 (37784)	Loss/tok 4.2053 (4.1794)	LR 1.250e-04
1: TRAIN [1][630/658]	Time 0.088 (0.137)	Data 7.03e-05 (7.85e-05)	Tok/s 16951 (37739)	Loss/tok 3.5032 (4.1804)	LR 1.250e-04
0: TRAIN [1][630/658]	Time 0.088 (0.137)	Data 8.01e-05 (8.28e-05)	Tok/s 16890 (37754)	Loss/tok 3.3196 (4.1753)	LR 1.250e-04
1: TRAIN [1][640/658]	Time 0.159 (0.137)	Data 8.89e-05 (7.85e-05)	Tok/s 43468 (37808)	Loss/tok 3.8264 (4.1760)	LR 1.250e-04
0: TRAIN [1][640/658]	Time 0.159 (0.137)	Data 9.18e-05 (8.29e-05)	Tok/s 44287 (37821)	Loss/tok 3.9855 (4.1723)	LR 1.250e-04
1: TRAIN [1][650/658]	Time 0.108 (0.137)	Data 8.27e-05 (7.86e-05)	Tok/s 28811 (37844)	Loss/tok 3.5113 (4.1718)	LR 1.250e-04
0: TRAIN [1][650/658]	Time 0.108 (0.137)	Data 8.58e-05 (8.29e-05)	Tok/s 28617 (37857)	Loss/tok 3.6111 (4.1682)	LR 1.250e-04
1: Running validation on dev set
0: Running validation on dev set
1: Executing preallocation
0: Executing preallocation
1: VALIDATION [1][0/106]	Time 0.034 (0.000)	Data 1.11e-03 (0.00e+00)	Tok/s 108456 (0)	Loss/tok 5.5140 (5.5140)
0: VALIDATION [1][0/107]	Time 0.048 (0.000)	Data 1.10e-03 (0.00e+00)	Tok/s 93307 (0)	Loss/tok 5.6462 (5.6462)
1: VALIDATION [1][10/106]	Time 0.021 (0.025)	Data 9.69e-04 (9.83e-04)	Tok/s 108497 (109611)	Loss/tok 5.1141 (5.2397)
0: VALIDATION [1][10/107]	Time 0.022 (0.025)	Data 1.11e-03 (1.07e-03)	Tok/s 109286 (109365)	Loss/tok 5.2765 (5.3035)
1: VALIDATION [1][20/106]	Time 0.018 (0.022)	Data 9.62e-04 (9.77e-04)	Tok/s 110992 (109340)	Loss/tok 4.8884 (5.2126)
0: VALIDATION [1][20/107]	Time 0.019 (0.022)	Data 1.13e-03 (1.08e-03)	Tok/s 103932 (108978)	Loss/tok 5.2811 (5.1760)
1: VALIDATION [1][30/106]	Time 0.016 (0.020)	Data 9.56e-04 (9.71e-04)	Tok/s 106190 (108771)	Loss/tok 4.7940 (5.1387)
0: VALIDATION [1][30/107]	Time 0.016 (0.020)	Data 9.68e-04 (1.07e-03)	Tok/s 106230 (108563)	Loss/tok 4.5847 (5.1328)
1: VALIDATION [1][40/106]	Time 0.014 (0.019)	Data 9.55e-04 (9.67e-04)	Tok/s 104022 (107951)	Loss/tok 4.7025 (5.0918)
0: VALIDATION [1][40/107]	Time 0.014 (0.019)	Data 1.00e-03 (1.07e-03)	Tok/s 102084 (107771)	Loss/tok 4.8782 (5.0849)
1: VALIDATION [1][50/106]	Time 0.013 (0.018)	Data 9.68e-04 (9.64e-04)	Tok/s 101192 (106832)	Loss/tok 4.7948 (5.0559)
0: VALIDATION [1][50/107]	Time 0.014 (0.018)	Data 1.05e-03 (1.07e-03)	Tok/s 93560 (106381)	Loss/tok 4.5562 (5.0388)
1: VALIDATION [1][60/106]	Time 0.011 (0.017)	Data 9.45e-04 (9.62e-04)	Tok/s 97372 (105642)	Loss/tok 4.6956 (5.0290)
0: VALIDATION [1][60/107]	Time 0.011 (0.017)	Data 1.04e-03 (1.07e-03)	Tok/s 98545 (105321)	Loss/tok 4.6736 (5.0123)
1: VALIDATION [1][70/106]	Time 0.009 (0.016)	Data 9.43e-04 (9.58e-04)	Tok/s 99966 (104197)	Loss/tok 4.7436 (5.0032)
0: VALIDATION [1][70/107]	Time 0.010 (0.016)	Data 1.08e-03 (1.07e-03)	Tok/s 92217 (103622)	Loss/tok 4.5114 (4.9904)
1: VALIDATION [1][80/106]	Time 0.009 (0.015)	Data 9.36e-04 (9.57e-04)	Tok/s 91767 (102812)	Loss/tok 4.5142 (4.9786)
0: VALIDATION [1][80/107]	Time 0.009 (0.015)	Data 1.01e-03 (1.07e-03)	Tok/s 89315 (102111)	Loss/tok 4.6303 (4.9718)
1: VALIDATION [1][90/106]	Time 0.008 (0.014)	Data 9.39e-04 (9.55e-04)	Tok/s 85533 (101266)	Loss/tok 4.1079 (4.9547)
0: VALIDATION [1][90/107]	Time 0.008 (0.014)	Data 1.01e-03 (1.07e-03)	Tok/s 85748 (100469)	Loss/tok 4.8627 (4.9553)
1: VALIDATION [1][100/106]	Time 0.006 (0.014)	Data 9.43e-04 (9.55e-04)	Tok/s 72813 (99128)	Loss/tok 4.0657 (4.9313)
0: VALIDATION [1][100/107]	Time 0.006 (0.014)	Data 1.00e-03 (1.06e-03)	Tok/s 75099 (98377)	Loss/tok 4.3791 (4.9323)
0: Saving model to gnmt/model_best.pth
1: Running evaluation on test set
0: Running evaluation on test set
1: TEST [1][9/63]	Time 0.2917 (0.3105)	Decoder iters 149.0 (149.0)	Tok/s 7171 (8582)
0: TEST [1][9/63]	Time 0.2918 (0.3105)	Decoder iters 149.0 (145.4)	Tok/s 7899 (9088)
1: TEST [1][19/63]	Time 0.2748 (0.2853)	Decoder iters 149.0 (140.4)	Tok/s 5907 (8219)
0: TEST [1][19/63]	Time 0.2748 (0.2853)	Decoder iters 59.0 (130.6)	Tok/s 6409 (8484)
1: TEST [1][29/63]	Time 0.2852 (0.2802)	Decoder iters 149.0 (133.6)	Tok/s 5308 (7444)
0: TEST [1][29/63]	Time 0.2853 (0.2801)	Decoder iters 149.0 (128.0)	Tok/s 5076 (7682)
1: TEST [1][39/63]	Time 0.2686 (0.2717)	Decoder iters 149.0 (127.5)	Tok/s 4564 (7040)
0: TEST [1][39/63]	Time 0.2688 (0.2717)	Decoder iters 37.0 (122.5)	Tok/s 4152 (7253)
1: TEST [1][49/63]	Time 0.2780 (0.2607)	Decoder iters 64.0 (121.4)	Tok/s 2946 (6756)
0: TEST [1][49/63]	Time 0.2781 (0.2606)	Decoder iters 149.0 (109.9)	Tok/s 3244 (6952)
1: TEST [1][59/63]	Time 0.0678 (0.2390)	Decoder iters 33.0 (110.3)	Tok/s 7434 (6872)
0: TEST [1][59/63]	Time 0.0679 (0.2389)	Decoder iters 21.0 (98.3)	Tok/s 8055 (7066)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 4.1667	Validation Loss: 4.9209	Test BLEU: 7.29
0: Performance: Epoch: 1	Training: 75753 Tok/s	Validation: 194009 Tok/s
0: Finished epoch 1
1: Total training time 240 s
0: Total training time 240 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       2|                 112|                      7.29|                      75790.8|                         4.000|
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
