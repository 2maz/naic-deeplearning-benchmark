Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
Downloading: "https://download.pytorch.org/models/resnet50-19c8e357.pth" to /root/.cache/torch/hub/checkpoints/resnet50-19c8e357.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 1.06M/97.8M [00:00<00:09, 11.0MB/s]  1%|          | 992k/97.8M [00:00<00:13, 7.60MB/s]  6%|▌         | 5.88M/97.8M [00:00<00:06, 14.0MB/s]  6%|▌         | 5.48M/97.8M [00:00<00:09, 10.0MB/s] 10%|▉         | 9.50M/97.8M [00:00<00:05, 17.3MB/s] 10%|█         | 9.80M/97.8M [00:00<00:07, 13.0MB/s] 13%|█▎        | 13.1M/97.8M [00:00<00:04, 19.5MB/s] 14%|█▎        | 13.3M/97.8M [00:00<00:05, 15.7MB/s] 16%|█▌        | 15.4M/97.8M [00:00<00:04, 20.4MB/s] 17%|█▋        | 16.2M/97.8M [00:00<00:04, 18.4MB/s] 18%|█▊        | 18.0M/97.8M [00:00<00:03, 22.0MB/s] 19%|█▉        | 18.8M/97.8M [00:00<00:04, 20.0MB/s] 21%|██        | 20.3M/97.8M [00:00<00:04, 19.3MB/s] 22%|██▏       | 21.3M/97.8M [00:00<00:04, 17.4MB/s] 23%|██▎       | 23.0M/97.8M [00:00<00:03, 21.0MB/s] 27%|██▋       | 26.2M/97.8M [00:00<00:03, 21.6MB/s] 29%|██▉       | 28.2M/97.8M [00:01<00:02, 25.8MB/s] 31%|███       | 30.1M/97.8M [00:01<00:02, 24.5MB/s] 32%|███▏      | 31.8M/97.8M [00:01<00:02, 28.2MB/s] 35%|███▍      | 33.9M/97.8M [00:01<00:02, 27.3MB/s] 36%|███▌      | 35.0M/97.8M [00:01<00:02, 28.6MB/s] 38%|███▊      | 37.1M/97.8M [00:01<00:02, 27.2MB/s] 39%|███▉      | 38.0M/97.8M [00:01<00:02, 28.7MB/s] 42%|████▏     | 41.2M/97.8M [00:01<00:01, 30.5MB/s] 43%|████▎     | 41.7M/97.8M [00:01<00:01, 31.1MB/s] 46%|████▋     | 45.4M/97.8M [00:01<00:01, 33.7MB/s] 48%|████▊     | 46.5M/97.8M [00:01<00:01, 35.1MB/s] 51%|█████▏    | 50.1M/97.8M [00:01<00:01, 37.2MB/s] 52%|█████▏    | 50.6M/97.8M [00:01<00:01, 37.1MB/s] 55%|█████▌    | 54.1M/97.8M [00:01<00:01, 29.5MB/s] 56%|█████▌    | 54.4M/97.8M [00:01<00:01, 28.3MB/s] 60%|██████    | 58.8M/97.8M [00:01<00:01, 33.6MB/s] 61%|██████    | 59.3M/97.8M [00:01<00:01, 32.5MB/s] 64%|██████▍   | 62.8M/97.8M [00:02<00:01, 35.8MB/s] 66%|██████▌   | 64.5M/97.8M [00:02<00:00, 37.1MB/s] 70%|██████▉   | 68.0M/97.8M [00:02<00:00, 39.1MB/s] 70%|███████   | 68.7M/97.8M [00:02<00:00, 37.7MB/s] 74%|███████▍  | 72.7M/97.8M [00:02<00:00, 41.5MB/s] 76%|███████▌  | 74.1M/97.8M [00:02<00:00, 42.0MB/s] 79%|███████▊  | 76.9M/97.8M [00:02<00:00, 42.2MB/s] 80%|████████  | 78.6M/97.8M [00:02<00:00, 41.4MB/s] 83%|████████▎ | 81.2M/97.8M [00:02<00:00, 38.4MB/s] 85%|████████▍ | 82.9M/97.8M [00:02<00:00, 38.1MB/s] 87%|████████▋ | 85.1M/97.8M [00:02<00:00, 35.8MB/s] 90%|█████████ | 88.3M/97.8M [00:02<00:00, 42.3MB/s] 94%|█████████▍| 91.9M/97.8M [00:02<00:00, 42.1MB/s] 95%|█████████▍| 92.7M/97.8M [00:02<00:00, 36.3MB/s] 99%|█████████▊| 96.5M/97.8M [00:02<00:00, 37.8MB/s]100%|██████████| 97.8M/97.8M [00:02<00:00, 35.0MB/s]
100%|██████████| 97.8M/97.8M [00:02<00:00, 35.1MB/s]
DLL 2020-10-28 02:04:34.661565 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 32  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : False  precision : fp32 
DLL 2020-10-28 02:04:34.700575 - PARAMETER dataset path : /data/object_detection  epochs : 1  batch size : 32  eval batch size : 32  no cuda : False  seed : None  checkpoint path : None  mode : benchmark-training  eval on epochs : [21, 31, 37, 42, 48, 53, 59, 64]  lr decay epochs : [43, 54]  learning rate : 0.0  momentum : 0.9  weight decay : 0.0005  lr warmup : None  backbone : resnet50  backbone path : None  num workers : 4  AMP : False  precision : fp32 
Using seed = 8933
Using seed = 8787
loading annotations into memory...
loading annotations into memory...
Done (t=0.50s)
creating index...
Done (t=0.50s)
creating index...
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
index created!
/workspace/examples/ssd/src/coco_pipeline.py:63: DeprecationWarning: Argument 'output_dtype' for operator 'CropMirrorNormalize' is now deprecated. Use 'dtype' instead.
  pad_output=pad_output)
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.
  warnings.warn(warning.format(ret))
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Traceback (most recent call last):
  File "main.py", line 287, in <module>
    train(train_loop_func, logger, args)
  File "main.py", line 206, in train
    logger, args, mean, std)
  File "/workspace/examples/ssd/src/train.py", line 139, in benchmark_train_loop
    loss.backward()
  File "/opt/conda/lib/python3.6/site-packages/torch/tensor.py", line 214, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 9.78 GiB total capacity; 7.83 GiB already allocated; 37.69 MiB free; 8.16 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "main.py", line 287, in <module>
    train(train_loop_func, logger, args)
  File "main.py", line 206, in train
    logger, args, mean, std)
  File "/workspace/examples/ssd/src/train.py", line 139, in benchmark_train_loop
    loss.backward()
  File "/opt/conda/lib/python3.6/site-packages/torch/tensor.py", line 214, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/opt/conda/lib/python3.6/site-packages/torch/autograd/__init__.py", line 132, in backward
    allow_unreachable=True)  # allow_unreachable flag
RuntimeError: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 1; 9.78 GiB total capacity; 7.83 GiB already allocated; 56.56 MiB free; 8.16 GiB reserved in total by PyTorch)
Traceback (most recent call last):
  File "/opt/conda/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "__main__", mod_spec)
  File "/opt/conda/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 261, in <module>
    main()
  File "/opt/conda/lib/python3.6/site-packages/torch/distributed/launch.py", line 257, in main
    cmd=cmd)
subprocess.CalledProcessError: Command '['/opt/conda/bin/python', '-u', 'main.py', '--local_rank=1', '--mode', 'benchmark-training', '--data', '/data/object_detection', '--batch-size', '32', '--benchmark-warmup', '50', '--benchmark-iterations', '200', '--learning-rate', '0']' returned non-zero exit status 1.
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
DONE!
