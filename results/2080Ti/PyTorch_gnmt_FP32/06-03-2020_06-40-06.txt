0: Collecting environment information...
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: GPU 0: GeForce RTX 2080 Ti
Nvidia driver version: 440.44
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=1, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 911
0: Scheduler decay interval: 114
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
0: Sampler for epoch 0 uses seed 364522461
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/1369]	Time 0.586 (0.586)	Data 1.22e-01 (1.22e-01)	Tok/s 15282 (15282)	Loss/tok 10.7239 (10.7239)	LR 2.047e-05
0: TRAIN [0][10/1369]	Time 0.490 (0.359)	Data 1.24e-04 (1.12e-02)	Tok/s 17932 (16961)	Loss/tok 9.7236 (10.1573)	LR 2.576e-05
0: TRAIN [0][20/1369]	Time 0.198 (0.314)	Data 1.39e-04 (5.94e-03)	Tok/s 14489 (16345)	Loss/tok 9.1078 (9.8750)	LR 3.244e-05
0: TRAIN [0][30/1369]	Time 0.483 (0.315)	Data 1.44e-04 (4.08e-03)	Tok/s 18303 (16521)	Loss/tok 9.1230 (9.6278)	LR 4.083e-05
0: TRAIN [0][40/1369]	Time 0.381 (0.321)	Data 1.26e-04 (3.12e-03)	Tok/s 17927 (16667)	Loss/tok 8.7572 (9.4187)	LR 5.141e-05
0: TRAIN [0][50/1369]	Time 0.383 (0.310)	Data 1.15e-04 (2.53e-03)	Tok/s 18061 (16517)	Loss/tok 8.6215 (9.2911)	LR 6.472e-05
0: TRAIN [0][60/1369]	Time 0.385 (0.319)	Data 1.18e-04 (2.14e-03)	Tok/s 17658 (16639)	Loss/tok 8.4939 (9.1511)	LR 8.148e-05
0: TRAIN [0][70/1369]	Time 0.199 (0.318)	Data 1.20e-04 (1.85e-03)	Tok/s 15001 (16629)	Loss/tok 8.0870 (9.0411)	LR 1.026e-04
0: TRAIN [0][80/1369]	Time 0.295 (0.316)	Data 1.10e-04 (1.64e-03)	Tok/s 16606 (16608)	Loss/tok 8.1350 (8.9405)	LR 1.291e-04
0: TRAIN [0][90/1369]	Time 0.391 (0.316)	Data 1.25e-04 (1.47e-03)	Tok/s 17359 (16621)	Loss/tok 8.4359 (8.8563)	LR 1.626e-04
0: TRAIN [0][100/1369]	Time 0.292 (0.308)	Data 1.65e-04 (1.34e-03)	Tok/s 17046 (16466)	Loss/tok 7.8547 (8.7844)	LR 2.047e-04
0: TRAIN [0][110/1369]	Time 0.203 (0.307)	Data 1.18e-04 (1.23e-03)	Tok/s 14157 (16434)	Loss/tok 7.4126 (8.7064)	LR 2.576e-04
0: TRAIN [0][120/1369]	Time 0.203 (0.311)	Data 1.25e-04 (1.14e-03)	Tok/s 14463 (16472)	Loss/tok 7.5627 (8.6244)	LR 3.244e-04
0: TRAIN [0][130/1369]	Time 0.390 (0.317)	Data 1.90e-04 (1.06e-03)	Tok/s 17415 (16533)	Loss/tok 7.8407 (8.5488)	LR 4.083e-04
0: TRAIN [0][140/1369]	Time 0.388 (0.315)	Data 1.20e-04 (9.99e-04)	Tok/s 17512 (16498)	Loss/tok 7.9132 (8.4991)	LR 5.141e-04
0: TRAIN [0][150/1369]	Time 0.295 (0.312)	Data 1.11e-04 (9.42e-04)	Tok/s 16524 (16457)	Loss/tok 7.6499 (8.4559)	LR 6.472e-04
0: TRAIN [0][160/1369]	Time 0.205 (0.312)	Data 2.20e-04 (8.94e-04)	Tok/s 14658 (16433)	Loss/tok 7.4310 (8.4117)	LR 8.148e-04
0: TRAIN [0][170/1369]	Time 0.380 (0.312)	Data 1.81e-04 (8.52e-04)	Tok/s 17909 (16425)	Loss/tok 7.7766 (8.3720)	LR 1.026e-03
0: TRAIN [0][180/1369]	Time 0.202 (0.312)	Data 1.32e-04 (8.12e-04)	Tok/s 14155 (16408)	Loss/tok 7.3375 (8.3364)	LR 1.291e-03
0: TRAIN [0][190/1369]	Time 0.389 (0.314)	Data 9.99e-05 (7.77e-04)	Tok/s 17096 (16410)	Loss/tok 7.6158 (8.3009)	LR 1.626e-03
0: TRAIN [0][200/1369]	Time 0.385 (0.314)	Data 1.41e-04 (7.45e-04)	Tok/s 17588 (16399)	Loss/tok 7.7113 (8.2671)	LR 2.000e-03
0: TRAIN [0][210/1369]	Time 0.290 (0.315)	Data 1.79e-04 (7.17e-04)	Tok/s 16793 (16411)	Loss/tok 7.6504 (8.2309)	LR 2.000e-03
0: TRAIN [0][220/1369]	Time 0.392 (0.315)	Data 1.94e-04 (6.92e-04)	Tok/s 17623 (16401)	Loss/tok 7.4618 (8.1988)	LR 2.000e-03
0: TRAIN [0][230/1369]	Time 0.497 (0.316)	Data 1.16e-04 (6.68e-04)	Tok/s 17744 (16400)	Loss/tok 7.7557 (8.1668)	LR 2.000e-03
0: TRAIN [0][240/1369]	Time 0.387 (0.317)	Data 1.16e-04 (6.46e-04)	Tok/s 17528 (16413)	Loss/tok 7.4720 (8.1337)	LR 2.000e-03
0: TRAIN [0][250/1369]	Time 0.292 (0.315)	Data 1.20e-04 (6.25e-04)	Tok/s 16641 (16393)	Loss/tok 7.1552 (8.1029)	LR 2.000e-03
0: TRAIN [0][260/1369]	Time 0.390 (0.317)	Data 1.21e-04 (6.07e-04)	Tok/s 17441 (16423)	Loss/tok 7.2399 (8.0636)	LR 2.000e-03
0: TRAIN [0][270/1369]	Time 0.295 (0.320)	Data 1.89e-04 (5.91e-04)	Tok/s 16692 (16435)	Loss/tok 7.0891 (8.0259)	LR 2.000e-03
0: TRAIN [0][280/1369]	Time 0.217 (0.320)	Data 1.18e-04 (5.76e-04)	Tok/s 13154 (16436)	Loss/tok 6.5802 (7.9901)	LR 2.000e-03
0: TRAIN [0][290/1369]	Time 0.202 (0.319)	Data 1.06e-04 (5.60e-04)	Tok/s 14602 (16414)	Loss/tok 6.7475 (7.9595)	LR 2.000e-03
0: TRAIN [0][300/1369]	Time 0.289 (0.319)	Data 1.64e-04 (5.47e-04)	Tok/s 16548 (16399)	Loss/tok 6.8826 (7.9277)	LR 2.000e-03
0: TRAIN [0][310/1369]	Time 0.290 (0.317)	Data 1.20e-04 (5.34e-04)	Tok/s 16559 (16372)	Loss/tok 6.8920 (7.9006)	LR 2.000e-03
0: TRAIN [0][320/1369]	Time 0.203 (0.316)	Data 1.18e-04 (5.21e-04)	Tok/s 14696 (16363)	Loss/tok 6.5383 (7.8707)	LR 2.000e-03
0: TRAIN [0][330/1369]	Time 0.304 (0.318)	Data 1.19e-04 (5.09e-04)	Tok/s 15951 (16388)	Loss/tok 6.5590 (7.8331)	LR 2.000e-03
0: TRAIN [0][340/1369]	Time 0.496 (0.319)	Data 1.37e-04 (4.98e-04)	Tok/s 17682 (16392)	Loss/tok 7.0366 (7.8001)	LR 2.000e-03
0: TRAIN [0][350/1369]	Time 0.388 (0.319)	Data 1.30e-04 (4.87e-04)	Tok/s 17417 (16390)	Loss/tok 6.7688 (7.7703)	LR 2.000e-03
0: TRAIN [0][360/1369]	Time 0.388 (0.319)	Data 1.86e-04 (4.78e-04)	Tok/s 17468 (16404)	Loss/tok 6.6794 (7.7367)	LR 2.000e-03
0: TRAIN [0][370/1369]	Time 0.390 (0.320)	Data 1.22e-04 (4.69e-04)	Tok/s 17271 (16402)	Loss/tok 6.6482 (7.7057)	LR 2.000e-03
0: TRAIN [0][380/1369]	Time 0.294 (0.321)	Data 1.22e-04 (4.60e-04)	Tok/s 16632 (16417)	Loss/tok 6.4533 (7.6737)	LR 2.000e-03
0: TRAIN [0][390/1369]	Time 0.392 (0.321)	Data 1.22e-04 (4.52e-04)	Tok/s 17659 (16419)	Loss/tok 6.6604 (7.6447)	LR 2.000e-03
0: TRAIN [0][400/1369]	Time 0.202 (0.319)	Data 1.19e-04 (4.44e-04)	Tok/s 14408 (16398)	Loss/tok 6.0914 (7.6208)	LR 2.000e-03
0: TRAIN [0][410/1369]	Time 0.509 (0.319)	Data 1.26e-04 (4.36e-04)	Tok/s 17260 (16389)	Loss/tok 6.7065 (7.5954)	LR 2.000e-03
0: TRAIN [0][420/1369]	Time 0.386 (0.319)	Data 1.93e-04 (4.29e-04)	Tok/s 17545 (16384)	Loss/tok 6.4736 (7.5691)	LR 2.000e-03
0: TRAIN [0][430/1369]	Time 0.283 (0.318)	Data 1.89e-04 (4.22e-04)	Tok/s 17434 (16374)	Loss/tok 6.3567 (7.5430)	LR 2.000e-03
0: TRAIN [0][440/1369]	Time 0.295 (0.319)	Data 1.89e-04 (4.16e-04)	Tok/s 16667 (16379)	Loss/tok 6.1739 (7.5137)	LR 2.000e-03
0: TRAIN [0][450/1369]	Time 0.201 (0.318)	Data 1.88e-04 (4.11e-04)	Tok/s 14332 (16373)	Loss/tok 5.9808 (7.4882)	LR 2.000e-03
0: TRAIN [0][460/1369]	Time 0.202 (0.320)	Data 1.11e-04 (4.05e-04)	Tok/s 14565 (16389)	Loss/tok 5.8332 (7.4596)	LR 2.000e-03
0: TRAIN [0][470/1369]	Time 0.295 (0.321)	Data 1.16e-04 (4.00e-04)	Tok/s 16110 (16406)	Loss/tok 6.1619 (7.4287)	LR 2.000e-03
0: TRAIN [0][480/1369]	Time 0.285 (0.321)	Data 1.94e-04 (3.94e-04)	Tok/s 16664 (16398)	Loss/tok 6.0149 (7.4053)	LR 2.000e-03
0: TRAIN [0][490/1369]	Time 0.193 (0.321)	Data 1.86e-04 (3.89e-04)	Tok/s 15464 (16406)	Loss/tok 5.6893 (7.3800)	LR 2.000e-03
0: TRAIN [0][500/1369]	Time 0.296 (0.321)	Data 1.25e-04 (3.84e-04)	Tok/s 16348 (16402)	Loss/tok 5.9450 (7.3559)	LR 2.000e-03
0: TRAIN [0][510/1369]	Time 0.496 (0.321)	Data 1.36e-04 (3.79e-04)	Tok/s 17759 (16400)	Loss/tok 6.3334 (7.3315)	LR 2.000e-03
0: TRAIN [0][520/1369]	Time 0.294 (0.321)	Data 1.11e-04 (3.74e-04)	Tok/s 16860 (16402)	Loss/tok 5.7966 (7.3069)	LR 2.000e-03
0: TRAIN [0][530/1369]	Time 0.390 (0.321)	Data 1.24e-04 (3.69e-04)	Tok/s 17389 (16403)	Loss/tok 6.1446 (7.2835)	LR 2.000e-03
0: TRAIN [0][540/1369]	Time 0.294 (0.320)	Data 1.23e-04 (3.64e-04)	Tok/s 16629 (16393)	Loss/tok 5.9710 (7.2619)	LR 2.000e-03
0: TRAIN [0][550/1369]	Time 0.115 (0.319)	Data 1.10e-04 (3.60e-04)	Tok/s 13060 (16372)	Loss/tok 5.0918 (7.2434)	LR 2.000e-03
0: TRAIN [0][560/1369]	Time 0.295 (0.318)	Data 1.16e-04 (3.56e-04)	Tok/s 16659 (16363)	Loss/tok 5.8250 (7.2221)	LR 2.000e-03
0: TRAIN [0][570/1369]	Time 0.388 (0.318)	Data 1.25e-04 (3.52e-04)	Tok/s 17628 (16365)	Loss/tok 5.9833 (7.1996)	LR 2.000e-03
0: TRAIN [0][580/1369]	Time 0.399 (0.319)	Data 1.16e-04 (3.49e-04)	Tok/s 16972 (16372)	Loss/tok 5.9586 (7.1741)	LR 2.000e-03
0: TRAIN [0][590/1369]	Time 0.390 (0.319)	Data 1.45e-04 (3.45e-04)	Tok/s 17156 (16382)	Loss/tok 5.9504 (7.1489)	LR 2.000e-03
0: TRAIN [0][600/1369]	Time 0.291 (0.319)	Data 1.12e-04 (3.42e-04)	Tok/s 16701 (16386)	Loss/tok 5.6101 (7.1257)	LR 2.000e-03
0: TRAIN [0][610/1369]	Time 0.293 (0.320)	Data 1.95e-04 (3.39e-04)	Tok/s 16621 (16394)	Loss/tok 5.7456 (7.1013)	LR 2.000e-03
0: TRAIN [0][620/1369]	Time 0.391 (0.320)	Data 1.11e-04 (3.36e-04)	Tok/s 17644 (16398)	Loss/tok 5.8412 (7.0782)	LR 2.000e-03
0: TRAIN [0][630/1369]	Time 0.193 (0.321)	Data 1.87e-04 (3.33e-04)	Tok/s 15178 (16399)	Loss/tok 5.3492 (7.0553)	LR 2.000e-03
0: TRAIN [0][640/1369]	Time 0.217 (0.321)	Data 1.36e-04 (3.30e-04)	Tok/s 13080 (16393)	Loss/tok 5.1369 (7.0338)	LR 2.000e-03
0: TRAIN [0][650/1369]	Time 0.295 (0.320)	Data 1.40e-04 (3.27e-04)	Tok/s 16599 (16391)	Loss/tok 5.5295 (7.0134)	LR 2.000e-03
0: TRAIN [0][660/1369]	Time 0.291 (0.319)	Data 1.03e-04 (3.24e-04)	Tok/s 16398 (16382)	Loss/tok 5.5419 (6.9944)	LR 2.000e-03
0: TRAIN [0][670/1369]	Time 0.493 (0.320)	Data 1.28e-04 (3.21e-04)	Tok/s 17881 (16386)	Loss/tok 5.8322 (6.9716)	LR 2.000e-03
0: TRAIN [0][680/1369]	Time 0.292 (0.320)	Data 1.09e-04 (3.19e-04)	Tok/s 16729 (16391)	Loss/tok 5.3295 (6.9491)	LR 2.000e-03
0: TRAIN [0][690/1369]	Time 0.295 (0.320)	Data 1.18e-04 (3.16e-04)	Tok/s 16303 (16383)	Loss/tok 5.3566 (6.9300)	LR 2.000e-03
0: TRAIN [0][700/1369]	Time 0.502 (0.320)	Data 1.17e-04 (3.14e-04)	Tok/s 17706 (16386)	Loss/tok 5.7006 (6.9082)	LR 2.000e-03
0: TRAIN [0][710/1369]	Time 0.116 (0.320)	Data 1.23e-04 (3.11e-04)	Tok/s 12549 (16379)	Loss/tok 4.5949 (6.8890)	LR 2.000e-03
0: TRAIN [0][720/1369]	Time 0.202 (0.319)	Data 1.94e-04 (3.09e-04)	Tok/s 14015 (16373)	Loss/tok 5.0536 (6.8702)	LR 2.000e-03
0: TRAIN [0][730/1369]	Time 0.392 (0.320)	Data 2.05e-04 (3.07e-04)	Tok/s 17397 (16377)	Loss/tok 5.5090 (6.8494)	LR 2.000e-03
0: TRAIN [0][740/1369]	Time 0.392 (0.319)	Data 2.24e-04 (3.05e-04)	Tok/s 17205 (16377)	Loss/tok 5.3998 (6.8297)	LR 2.000e-03
0: TRAIN [0][750/1369]	Time 0.385 (0.319)	Data 1.26e-04 (3.03e-04)	Tok/s 17556 (16378)	Loss/tok 5.3808 (6.8099)	LR 2.000e-03
0: TRAIN [0][760/1369]	Time 0.381 (0.318)	Data 2.03e-04 (3.01e-04)	Tok/s 17590 (16367)	Loss/tok 5.3454 (6.7933)	LR 2.000e-03
0: TRAIN [0][770/1369]	Time 0.200 (0.318)	Data 1.19e-04 (2.99e-04)	Tok/s 14556 (16365)	Loss/tok 4.6963 (6.7743)	LR 2.000e-03
0: TRAIN [0][780/1369]	Time 0.213 (0.319)	Data 1.11e-04 (2.97e-04)	Tok/s 14147 (16366)	Loss/tok 4.8814 (6.7539)	LR 2.000e-03
0: TRAIN [0][790/1369]	Time 0.194 (0.319)	Data 2.10e-04 (2.96e-04)	Tok/s 15129 (16374)	Loss/tok 4.7973 (6.7328)	LR 2.000e-03
0: TRAIN [0][800/1369]	Time 0.508 (0.319)	Data 1.07e-04 (2.93e-04)	Tok/s 17620 (16367)	Loss/tok 5.5531 (6.7160)	LR 2.000e-03
0: TRAIN [0][810/1369]	Time 0.290 (0.319)	Data 1.22e-04 (2.91e-04)	Tok/s 16506 (16368)	Loss/tok 5.0080 (6.6977)	LR 2.000e-03
0: TRAIN [0][820/1369]	Time 0.292 (0.319)	Data 1.29e-04 (2.89e-04)	Tok/s 16760 (16368)	Loss/tok 4.9811 (6.6793)	LR 2.000e-03
0: TRAIN [0][830/1369]	Time 0.387 (0.319)	Data 1.08e-04 (2.87e-04)	Tok/s 17626 (16368)	Loss/tok 5.2144 (6.6609)	LR 2.000e-03
0: TRAIN [0][840/1369]	Time 0.385 (0.319)	Data 1.23e-04 (2.85e-04)	Tok/s 17589 (16368)	Loss/tok 5.2096 (6.6424)	LR 2.000e-03
0: TRAIN [0][850/1369]	Time 0.504 (0.319)	Data 1.18e-04 (2.83e-04)	Tok/s 17709 (16372)	Loss/tok 5.4248 (6.6227)	LR 2.000e-03
0: TRAIN [0][860/1369]	Time 0.285 (0.319)	Data 1.98e-04 (2.82e-04)	Tok/s 16916 (16372)	Loss/tok 4.9046 (6.6055)	LR 2.000e-03
0: TRAIN [0][870/1369]	Time 0.295 (0.319)	Data 1.11e-04 (2.80e-04)	Tok/s 16585 (16373)	Loss/tok 4.8885 (6.5873)	LR 2.000e-03
0: TRAIN [0][880/1369]	Time 0.113 (0.319)	Data 1.15e-04 (2.79e-04)	Tok/s 12584 (16366)	Loss/tok 4.4233 (6.5713)	LR 2.000e-03
0: TRAIN [0][890/1369]	Time 0.291 (0.318)	Data 1.23e-04 (2.77e-04)	Tok/s 16715 (16360)	Loss/tok 4.9644 (6.5563)	LR 2.000e-03
0: TRAIN [0][900/1369]	Time 0.115 (0.317)	Data 1.05e-04 (2.75e-04)	Tok/s 13001 (16346)	Loss/tok 4.2845 (6.5434)	LR 2.000e-03
0: TRAIN [0][910/1369]	Time 0.492 (0.318)	Data 1.63e-04 (2.74e-04)	Tok/s 17938 (16350)	Loss/tok 5.2614 (6.5247)	LR 1.000e-03
0: TRAIN [0][920/1369]	Time 0.487 (0.318)	Data 2.10e-04 (2.72e-04)	Tok/s 18261 (16348)	Loss/tok 5.2863 (6.5079)	LR 1.000e-03
0: TRAIN [0][930/1369]	Time 0.208 (0.318)	Data 1.20e-04 (2.71e-04)	Tok/s 14061 (16343)	Loss/tok 4.2595 (6.4926)	LR 1.000e-03
0: TRAIN [0][940/1369]	Time 0.298 (0.318)	Data 1.30e-04 (2.70e-04)	Tok/s 16199 (16347)	Loss/tok 4.8594 (6.4743)	LR 1.000e-03
0: TRAIN [0][950/1369]	Time 0.112 (0.317)	Data 1.01e-04 (2.68e-04)	Tok/s 13062 (16335)	Loss/tok 3.8060 (6.4613)	LR 1.000e-03
0: TRAIN [0][960/1369]	Time 0.200 (0.317)	Data 1.10e-04 (2.67e-04)	Tok/s 14736 (16326)	Loss/tok 4.5314 (6.4475)	LR 1.000e-03
0: TRAIN [0][970/1369]	Time 0.296 (0.317)	Data 1.11e-04 (2.66e-04)	Tok/s 16629 (16330)	Loss/tok 4.6530 (6.4287)	LR 1.000e-03
0: TRAIN [0][980/1369]	Time 0.293 (0.318)	Data 1.08e-04 (2.64e-04)	Tok/s 16258 (16332)	Loss/tok 4.7117 (6.4113)	LR 1.000e-03
0: TRAIN [0][990/1369]	Time 0.293 (0.317)	Data 1.02e-04 (2.63e-04)	Tok/s 16800 (16333)	Loss/tok 4.7185 (6.3949)	LR 1.000e-03
0: TRAIN [0][1000/1369]	Time 0.388 (0.317)	Data 1.17e-04 (2.62e-04)	Tok/s 17372 (16328)	Loss/tok 4.8320 (6.3801)	LR 1.000e-03
0: TRAIN [0][1010/1369]	Time 0.194 (0.317)	Data 1.86e-04 (2.61e-04)	Tok/s 15012 (16334)	Loss/tok 4.2602 (6.3627)	LR 1.000e-03
0: TRAIN [0][1020/1369]	Time 0.390 (0.317)	Data 1.13e-04 (2.60e-04)	Tok/s 17479 (16327)	Loss/tok 4.8036 (6.3493)	LR 1.000e-03
0: TRAIN [0][1030/1369]	Time 0.392 (0.318)	Data 2.40e-04 (2.58e-04)	Tok/s 17168 (16332)	Loss/tok 4.7834 (6.3314)	LR 5.000e-04
0: TRAIN [0][1040/1369]	Time 0.196 (0.317)	Data 2.39e-04 (2.58e-04)	Tok/s 14729 (16328)	Loss/tok 4.4363 (6.3176)	LR 5.000e-04
0: TRAIN [0][1050/1369]	Time 0.395 (0.317)	Data 1.16e-04 (2.57e-04)	Tok/s 17216 (16327)	Loss/tok 4.8041 (6.3020)	LR 5.000e-04
0: TRAIN [0][1060/1369]	Time 0.200 (0.317)	Data 9.89e-05 (2.55e-04)	Tok/s 14877 (16326)	Loss/tok 4.2478 (6.2868)	LR 5.000e-04
0: TRAIN [0][1070/1369]	Time 0.293 (0.316)	Data 1.09e-04 (2.54e-04)	Tok/s 16344 (16318)	Loss/tok 4.4957 (6.2743)	LR 5.000e-04
0: TRAIN [0][1080/1369]	Time 0.389 (0.317)	Data 1.16e-04 (2.53e-04)	Tok/s 17725 (16322)	Loss/tok 4.8982 (6.2586)	LR 5.000e-04
0: TRAIN [0][1090/1369]	Time 0.293 (0.317)	Data 1.21e-04 (2.52e-04)	Tok/s 16667 (16325)	Loss/tok 4.4654 (6.2426)	LR 5.000e-04
0: TRAIN [0][1100/1369]	Time 0.390 (0.317)	Data 1.04e-04 (2.51e-04)	Tok/s 17436 (16322)	Loss/tok 4.8201 (6.2290)	LR 5.000e-04
0: TRAIN [0][1110/1369]	Time 0.108 (0.317)	Data 2.03e-04 (2.50e-04)	Tok/s 13405 (16317)	Loss/tok 3.8809 (6.2158)	LR 5.000e-04
0: TRAIN [0][1120/1369]	Time 0.203 (0.316)	Data 1.28e-04 (2.49e-04)	Tok/s 14417 (16315)	Loss/tok 4.1363 (6.2023)	LR 5.000e-04
0: TRAIN [0][1130/1369]	Time 0.202 (0.316)	Data 1.13e-04 (2.48e-04)	Tok/s 14204 (16305)	Loss/tok 4.0801 (6.1908)	LR 5.000e-04
0: TRAIN [0][1140/1369]	Time 0.197 (0.315)	Data 1.27e-04 (2.47e-04)	Tok/s 14734 (16298)	Loss/tok 4.1266 (6.1794)	LR 2.500e-04
0: TRAIN [0][1150/1369]	Time 0.203 (0.314)	Data 1.19e-04 (2.46e-04)	Tok/s 14723 (16291)	Loss/tok 4.2694 (6.1683)	LR 2.500e-04
0: TRAIN [0][1160/1369]	Time 0.391 (0.315)	Data 1.17e-04 (2.45e-04)	Tok/s 17431 (16295)	Loss/tok 4.8180 (6.1544)	LR 2.500e-04
0: TRAIN [0][1170/1369]	Time 0.195 (0.314)	Data 1.82e-04 (2.44e-04)	Tok/s 15571 (16287)	Loss/tok 4.0914 (6.1433)	LR 2.500e-04
0: TRAIN [0][1180/1369]	Time 0.388 (0.314)	Data 1.25e-04 (2.43e-04)	Tok/s 17359 (16287)	Loss/tok 4.5634 (6.1307)	LR 2.500e-04
0: TRAIN [0][1190/1369]	Time 0.392 (0.314)	Data 1.25e-04 (2.42e-04)	Tok/s 17485 (16290)	Loss/tok 4.4583 (6.1168)	LR 2.500e-04
0: TRAIN [0][1200/1369]	Time 0.293 (0.315)	Data 1.18e-04 (2.41e-04)	Tok/s 16413 (16295)	Loss/tok 4.4282 (6.1027)	LR 2.500e-04
0: TRAIN [0][1210/1369]	Time 0.295 (0.314)	Data 2.38e-04 (2.40e-04)	Tok/s 16417 (16291)	Loss/tok 4.2795 (6.0912)	LR 2.500e-04
0: TRAIN [0][1220/1369]	Time 0.293 (0.314)	Data 1.13e-04 (2.39e-04)	Tok/s 16469 (16291)	Loss/tok 4.5724 (6.0791)	LR 2.500e-04
0: TRAIN [0][1230/1369]	Time 0.195 (0.314)	Data 2.00e-04 (2.38e-04)	Tok/s 14797 (16292)	Loss/tok 4.2055 (6.0664)	LR 2.500e-04
0: TRAIN [0][1240/1369]	Time 0.202 (0.314)	Data 1.34e-04 (2.37e-04)	Tok/s 14431 (16294)	Loss/tok 4.0034 (6.0534)	LR 2.500e-04
0: TRAIN [0][1250/1369]	Time 0.485 (0.314)	Data 1.72e-04 (2.37e-04)	Tok/s 18036 (16286)	Loss/tok 4.8667 (6.0433)	LR 2.500e-04
0: TRAIN [0][1260/1369]	Time 0.290 (0.314)	Data 1.56e-04 (2.36e-04)	Tok/s 17198 (16290)	Loss/tok 4.4553 (6.0300)	LR 1.250e-04
0: TRAIN [0][1270/1369]	Time 0.206 (0.314)	Data 1.12e-04 (2.35e-04)	Tok/s 13972 (16292)	Loss/tok 4.1478 (6.0173)	LR 1.250e-04
0: TRAIN [0][1280/1369]	Time 0.388 (0.314)	Data 1.28e-04 (2.35e-04)	Tok/s 17539 (16291)	Loss/tok 4.5226 (6.0055)	LR 1.250e-04
0: TRAIN [0][1290/1369]	Time 0.202 (0.314)	Data 1.23e-04 (2.34e-04)	Tok/s 14353 (16284)	Loss/tok 4.0580 (5.9953)	LR 1.250e-04
0: TRAIN [0][1300/1369]	Time 0.498 (0.314)	Data 1.70e-04 (2.33e-04)	Tok/s 17634 (16282)	Loss/tok 4.8194 (5.9847)	LR 1.250e-04
0: TRAIN [0][1310/1369]	Time 0.296 (0.314)	Data 1.18e-04 (2.32e-04)	Tok/s 16637 (16287)	Loss/tok 4.4678 (5.9729)	LR 1.250e-04
0: TRAIN [0][1320/1369]	Time 0.389 (0.314)	Data 1.30e-04 (2.31e-04)	Tok/s 17481 (16290)	Loss/tok 4.7419 (5.9607)	LR 1.250e-04
0: TRAIN [0][1330/1369]	Time 0.203 (0.315)	Data 1.95e-04 (2.31e-04)	Tok/s 14194 (16291)	Loss/tok 3.8921 (5.9493)	LR 1.250e-04
0: TRAIN [0][1340/1369]	Time 0.288 (0.315)	Data 2.26e-04 (2.30e-04)	Tok/s 16976 (16292)	Loss/tok 4.4257 (5.9385)	LR 1.250e-04
0: TRAIN [0][1350/1369]	Time 0.503 (0.314)	Data 1.00e-04 (2.29e-04)	Tok/s 17528 (16289)	Loss/tok 4.7922 (5.9290)	LR 1.250e-04
0: TRAIN [0][1360/1369]	Time 0.202 (0.314)	Data 1.06e-04 (2.28e-04)	Tok/s 14634 (16283)	Loss/tok 4.0764 (5.9207)	LR 1.250e-04
0: Running validation on dev set
0: Executing preallocation
0: VALIDATION [0][0/160]	Time 0.148 (0.148)	Data 1.61e-03 (1.61e-03)	Tok/s 38729 (38729)	Loss/tok 6.1373 (6.1373)
0: VALIDATION [0][10/160]	Time 0.073 (0.090)	Data 1.40e-03 (1.60e-03)	Tok/s 47272 (45732)	Loss/tok 5.6626 (5.8917)
0: VALIDATION [0][20/160]	Time 0.060 (0.079)	Data 1.35e-03 (1.49e-03)	Tok/s 48594 (46600)	Loss/tok 5.7327 (5.8429)
0: VALIDATION [0][30/160]	Time 0.057 (0.072)	Data 1.33e-03 (1.45e-03)	Tok/s 46006 (46811)	Loss/tok 5.7432 (5.7922)
0: VALIDATION [0][40/160]	Time 0.049 (0.067)	Data 1.31e-03 (1.42e-03)	Tok/s 47681 (47041)	Loss/tok 5.3480 (5.7612)
0: VALIDATION [0][50/160]	Time 0.046 (0.063)	Data 1.34e-03 (1.41e-03)	Tok/s 46919 (47169)	Loss/tok 5.6386 (5.7161)
0: VALIDATION [0][60/160]	Time 0.042 (0.060)	Data 1.28e-03 (1.39e-03)	Tok/s 46766 (47248)	Loss/tok 5.3946 (5.6818)
0: VALIDATION [0][70/160]	Time 0.039 (0.057)	Data 1.23e-03 (1.38e-03)	Tok/s 45797 (47117)	Loss/tok 5.3495 (5.6583)
0: VALIDATION [0][80/160]	Time 0.035 (0.054)	Data 1.24e-03 (1.37e-03)	Tok/s 46149 (46986)	Loss/tok 5.3727 (5.6361)
0: VALIDATION [0][90/160]	Time 0.031 (0.052)	Data 1.23e-03 (1.36e-03)	Tok/s 47307 (46942)	Loss/tok 5.1934 (5.6136)
0: VALIDATION [0][100/160]	Time 0.029 (0.050)	Data 1.23e-03 (1.36e-03)	Tok/s 46402 (46764)	Loss/tok 5.5054 (5.5992)
0: VALIDATION [0][110/160]	Time 0.027 (0.048)	Data 1.34e-03 (1.35e-03)	Tok/s 44891 (46573)	Loss/tok 5.4090 (5.5807)
0: VALIDATION [0][120/160]	Time 0.025 (0.046)	Data 1.29e-03 (1.35e-03)	Tok/s 42941 (46396)	Loss/tok 5.3045 (5.5647)
0: VALIDATION [0][130/160]	Time 0.022 (0.044)	Data 1.29e-03 (1.34e-03)	Tok/s 42646 (46112)	Loss/tok 5.0784 (5.5469)
0: VALIDATION [0][140/160]	Time 0.020 (0.043)	Data 1.29e-03 (1.34e-03)	Tok/s 40094 (45860)	Loss/tok 5.1183 (5.5345)
0: VALIDATION [0][150/160]	Time 0.016 (0.041)	Data 1.44e-03 (1.34e-03)	Tok/s 39328 (45512)	Loss/tok 4.8448 (5.5162)
0: Saving model to results/gnmt/model_best.pth
0: Running evaluation on test set
0: TEST [0][9/94]	Time 0.6731 (0.8322)	Decoder iters 149.0 (149.0)	Tok/s 5485 (5309)
0: TEST [0][19/94]	Time 0.5759 (0.7130)	Decoder iters 149.0 (146.6)	Tok/s 4994 (5266)
0: TEST [0][29/94]	Time 0.5253 (0.6597)	Decoder iters 149.0 (147.4)	Tok/s 4641 (5149)
0: TEST [0][39/94]	Time 0.3532 (0.6038)	Decoder iters 81.0 (139.9)	Tok/s 5915 (5156)
0: TEST [0][49/94]	Time 0.4713 (0.5782)	Decoder iters 149.0 (141.0)	Tok/s 4067 (4966)
0: TEST [0][59/94]	Time 0.4525 (0.5513)	Decoder iters 149.0 (139.2)	Tok/s 3381 (4836)
0: TEST [0][69/94]	Time 0.4097 (0.5208)	Decoder iters 149.0 (134.5)	Tok/s 2997 (4807)
0: TEST [0][79/94]	Time 0.4001 (0.4952)	Decoder iters 149.0 (130.7)	Tok/s 2612 (4745)
0: TEST [0][89/94]	Time 0.1230 (0.4698)	Decoder iters 30.0 (126.4)	Tok/s 5748 (4685)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
0: Finished evaluation on test set
0: Summary: Epoch: 0	Training Loss: 5.9138	Validation Loss: 5.5026	Test BLEU: 4.18
0: Performance: Epoch: 0	Training: 16279 Tok/s	Validation: 44890 Tok/s
0: Finished epoch 0
0: Total training time 499 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       1|                 108|                      4.18|                      16278.7|                         8.322|
DONE!
