1: Collecting environment information...
3: Collecting environment information...
2: Collecting environment information...
0: Collecting environment information...
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
1: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
3: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
3: Saving results to: results/gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
3: Using master seed from command line: 2
2: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
2: Saving results to: results/gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
2: Using master seed from command line: 2
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
0: Saving results to: results/gnmt
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp32', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=108, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
3: Worker 3 is using worker seed: 1323436024
2: Worker 2 is using worker seed: 3588440356
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31794
3: Size of vocabulary: 31794
2: Size of vocabulary: 31794
1: Size of vocabulary: 31794
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
0: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
3: Pairs before: 160078, after: 148120
0: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 125
2: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
2: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
3: Filtering data, min len: 0, max len: 125
3: Pairs before: 5100, after: 5100
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
0: Filtering data, min len: 0, max len: 150
2: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
2: Pairs before: 3003, after: 3003
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Filtering data, min len: 0, max len: 150
3: Pairs before: 3003, after: 3003
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159593523
3: Saving state of the tokenizer
3: Initializing fp32 optimizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 452
3: Scheduler decay interval: 57
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: Starting epoch 0
3: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159593523
0: Saving state of the tokenizer
0: Initializing fp32 optimizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 452
0: Scheduler decay interval: 57
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: Starting epoch 0
0: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159593523
1: Saving state of the tokenizer
1: Initializing fp32 optimizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 452
1: Scheduler decay interval: 57
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: Starting epoch 0
1: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31794, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31794, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31794, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159593523
2: Saving state of the tokenizer
2: Initializing fp32 optimizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 452
2: Scheduler decay interval: 57
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: Starting epoch 0
2: Executing preallocation
0: Sampler for epoch 0 uses seed 2602510382
1: Sampler for epoch 0 uses seed 2602510382
2: Sampler for epoch 0 uses seed 2602510382
3: Sampler for epoch 0 uses seed 2602510382
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:122: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
0: TRAIN [0][0/340]	Time 0.575 (0.575)	Data 1.36e-01 (1.36e-01)	Tok/s 5047 (5047)	Loss/tok 10.5910 (10.5910)	LR 2.047e-05
1: TRAIN [0][0/340]	Time 0.531 (0.531)	Data 1.69e-01 (1.69e-01)	Tok/s 5373 (5373)	Loss/tok 10.5875 (10.5875)	LR 2.047e-05
3: TRAIN [0][0/340]	Time 0.505 (0.505)	Data 1.49e-01 (1.49e-01)	Tok/s 6019 (6019)	Loss/tok 10.6276 (10.6276)	LR 2.047e-05
2: TRAIN [0][0/340]	Time 0.504 (0.504)	Data 1.46e-01 (1.46e-01)	Tok/s 5785 (5785)	Loss/tok 10.6097 (10.6097)	LR 2.047e-05
0: TRAIN [0][10/340]	Time 0.450 (0.455)	Data 2.06e-04 (1.25e-02)	Tok/s 10818 (9292)	Loss/tok 9.7564 (10.1210)	LR 2.576e-05
1: TRAIN [0][10/340]	Time 0.451 (0.451)	Data 1.07e-04 (1.55e-02)	Tok/s 10775 (9373)	Loss/tok 9.6715 (10.1062)	LR 2.576e-05
3: TRAIN [0][10/340]	Time 0.450 (0.448)	Data 1.66e-04 (1.36e-02)	Tok/s 10879 (9404)	Loss/tok 9.7142 (10.1141)	LR 2.576e-05
2: TRAIN [0][10/340]	Time 0.451 (0.448)	Data 1.05e-04 (1.34e-02)	Tok/s 11095 (9464)	Loss/tok 9.6935 (10.1036)	LR 2.576e-05
0: TRAIN [0][20/340]	Time 0.377 (0.444)	Data 1.17e-04 (6.59e-03)	Tok/s 7784 (9470)	Loss/tok 9.0856 (9.7693)	LR 3.244e-05
1: TRAIN [0][20/340]	Time 0.377 (0.442)	Data 8.89e-05 (8.18e-03)	Tok/s 7802 (9475)	Loss/tok 8.9963 (9.7619)	LR 3.244e-05
3: TRAIN [0][20/340]	Time 0.377 (0.441)	Data 1.22e-04 (7.19e-03)	Tok/s 7860 (9468)	Loss/tok 9.0920 (9.7740)	LR 3.244e-05
2: TRAIN [0][20/340]	Time 0.378 (0.441)	Data 9.61e-05 (7.07e-03)	Tok/s 7891 (9483)	Loss/tok 9.0718 (9.7599)	LR 3.244e-05
0: TRAIN [0][30/340]	Time 0.537 (0.453)	Data 1.11e-04 (4.50e-03)	Tok/s 12734 (9921)	Loss/tok 8.9686 (9.5096)	LR 4.083e-05
1: TRAIN [0][30/340]	Time 0.537 (0.452)	Data 1.54e-04 (5.58e-03)	Tok/s 12791 (9934)	Loss/tok 8.9979 (9.5090)	LR 4.083e-05
2: TRAIN [0][30/340]	Time 0.537 (0.451)	Data 1.01e-04 (4.83e-03)	Tok/s 12659 (9913)	Loss/tok 8.9504 (9.5047)	LR 4.083e-05
3: TRAIN [0][30/340]	Time 0.541 (0.451)	Data 1.10e-04 (4.91e-03)	Tok/s 12504 (9910)	Loss/tok 9.0372 (9.5110)	LR 4.083e-05
0: TRAIN [0][40/340]	Time 0.376 (0.465)	Data 9.11e-05 (3.43e-03)	Tok/s 7907 (10234)	Loss/tok 8.4798 (9.3142)	LR 5.141e-05
1: TRAIN [0][40/340]	Time 0.377 (0.464)	Data 9.51e-05 (4.24e-03)	Tok/s 7838 (10242)	Loss/tok 8.4982 (9.3156)	LR 5.141e-05
3: TRAIN [0][40/340]	Time 0.376 (0.463)	Data 1.37e-04 (3.74e-03)	Tok/s 8137 (10234)	Loss/tok 8.3722 (9.3107)	LR 5.141e-05
2: TRAIN [0][40/340]	Time 0.377 (0.463)	Data 9.87e-05 (3.68e-03)	Tok/s 7542 (10228)	Loss/tok 8.3706 (9.3050)	LR 5.141e-05
0: TRAIN [0][50/340]	Time 0.639 (0.478)	Data 1.12e-04 (2.78e-03)	Tok/s 13753 (10635)	Loss/tok 8.6030 (9.1337)	LR 6.472e-05
1: TRAIN [0][50/340]	Time 0.639 (0.478)	Data 1.29e-04 (3.43e-03)	Tok/s 14003 (10649)	Loss/tok 8.6210 (9.1362)	LR 6.472e-05
3: TRAIN [0][50/340]	Time 0.638 (0.477)	Data 1.03e-04 (3.03e-03)	Tok/s 13828 (10634)	Loss/tok 8.5594 (9.1323)	LR 6.472e-05
2: TRAIN [0][50/340]	Time 0.639 (0.477)	Data 1.03e-04 (2.98e-03)	Tok/s 13719 (10626)	Loss/tok 8.5810 (9.1145)	LR 6.472e-05
0: TRAIN [0][60/340]	Time 0.534 (0.471)	Data 9.39e-05 (2.34e-03)	Tok/s 12793 (10449)	Loss/tok 8.3177 (9.0242)	LR 8.148e-05
1: TRAIN [0][60/340]	Time 0.534 (0.470)	Data 1.09e-04 (2.89e-03)	Tok/s 12785 (10456)	Loss/tok 8.3130 (9.0261)	LR 8.148e-05
2: TRAIN [0][60/340]	Time 0.534 (0.470)	Data 9.54e-05 (2.51e-03)	Tok/s 12634 (10431)	Loss/tok 8.2908 (9.0125)	LR 8.148e-05
3: TRAIN [0][60/340]	Time 0.534 (0.470)	Data 1.06e-04 (2.55e-03)	Tok/s 12739 (10442)	Loss/tok 8.3178 (9.0263)	LR 8.148e-05
1: TRAIN [0][70/340]	Time 0.459 (0.468)	Data 1.20e-04 (2.50e-03)	Tok/s 10679 (10451)	Loss/tok 7.9514 (8.8996)	LR 1.026e-04
3: TRAIN [0][70/340]	Time 0.459 (0.468)	Data 9.92e-05 (2.21e-03)	Tok/s 10775 (10438)	Loss/tok 8.0446 (8.9014)	LR 1.026e-04
2: TRAIN [0][70/340]	Time 0.459 (0.468)	Data 1.04e-04 (2.17e-03)	Tok/s 10729 (10423)	Loss/tok 7.9515 (8.8918)	LR 1.026e-04
0: TRAIN [0][70/340]	Time 0.459 (0.469)	Data 1.48e-04 (2.03e-03)	Tok/s 10356 (10444)	Loss/tok 8.0478 (8.8979)	LR 1.026e-04
0: TRAIN [0][80/340]	Time 0.375 (0.471)	Data 1.12e-04 (1.79e-03)	Tok/s 7940 (10516)	Loss/tok 7.6346 (8.7834)	LR 1.291e-04
1: TRAIN [0][80/340]	Time 0.375 (0.470)	Data 9.51e-05 (2.20e-03)	Tok/s 8111 (10526)	Loss/tok 7.7902 (8.7893)	LR 1.291e-04
2: TRAIN [0][80/340]	Time 0.375 (0.470)	Data 1.07e-04 (1.92e-03)	Tok/s 7967 (10500)	Loss/tok 7.7438 (8.7788)	LR 1.291e-04
3: TRAIN [0][80/340]	Time 0.379 (0.470)	Data 1.04e-04 (1.95e-03)	Tok/s 7593 (10510)	Loss/tok 7.6840 (8.7880)	LR 1.291e-04
0: TRAIN [0][90/340]	Time 0.538 (0.477)	Data 9.82e-05 (1.61e-03)	Tok/s 12656 (10663)	Loss/tok 7.8646 (8.6668)	LR 1.626e-04
1: TRAIN [0][90/340]	Time 0.538 (0.476)	Data 9.51e-05 (1.97e-03)	Tok/s 12801 (10674)	Loss/tok 7.8154 (8.6722)	LR 1.626e-04
3: TRAIN [0][90/340]	Time 0.538 (0.476)	Data 1.02e-04 (1.75e-03)	Tok/s 12716 (10656)	Loss/tok 7.9367 (8.6718)	LR 1.626e-04
2: TRAIN [0][90/340]	Time 0.539 (0.476)	Data 9.63e-05 (1.72e-03)	Tok/s 12656 (10647)	Loss/tok 7.7837 (8.6618)	LR 1.626e-04
0: TRAIN [0][100/340]	Time 0.457 (0.474)	Data 1.65e-04 (1.46e-03)	Tok/s 10688 (10571)	Loss/tok 7.7894 (8.6062)	LR 2.047e-04
1: TRAIN [0][100/340]	Time 0.458 (0.474)	Data 9.87e-05 (1.79e-03)	Tok/s 10699 (10585)	Loss/tok 7.8166 (8.6130)	LR 2.047e-04
3: TRAIN [0][100/340]	Time 0.457 (0.474)	Data 1.10e-04 (1.59e-03)	Tok/s 10742 (10571)	Loss/tok 7.7950 (8.6094)	LR 2.047e-04
2: TRAIN [0][100/340]	Time 0.458 (0.474)	Data 9.58e-05 (1.56e-03)	Tok/s 10608 (10561)	Loss/tok 7.8301 (8.6011)	LR 2.047e-04
0: TRAIN [0][110/340]	Time 0.377 (0.476)	Data 1.10e-04 (1.34e-03)	Tok/s 7736 (10597)	Loss/tok 7.5158 (8.5470)	LR 2.576e-04
1: TRAIN [0][110/340]	Time 0.377 (0.476)	Data 1.04e-04 (1.64e-03)	Tok/s 7925 (10624)	Loss/tok 7.5406 (8.5512)	LR 2.576e-04
3: TRAIN [0][110/340]	Time 0.377 (0.476)	Data 9.78e-05 (1.45e-03)	Tok/s 7681 (10597)	Loss/tok 7.4023 (8.5500)	LR 2.576e-04
2: TRAIN [0][110/340]	Time 0.377 (0.476)	Data 1.11e-04 (1.43e-03)	Tok/s 7723 (10594)	Loss/tok 7.5216 (8.5422)	LR 2.576e-04
0: TRAIN [0][120/340]	Time 0.377 (0.480)	Data 9.54e-05 (1.23e-03)	Tok/s 7758 (10672)	Loss/tok 7.3295 (8.4769)	LR 3.244e-04
1: TRAIN [0][120/340]	Time 0.377 (0.480)	Data 9.39e-05 (1.51e-03)	Tok/s 7746 (10699)	Loss/tok 7.2965 (8.4822)	LR 3.244e-04
3: TRAIN [0][120/340]	Time 0.377 (0.479)	Data 1.10e-04 (1.34e-03)	Tok/s 8006 (10677)	Loss/tok 7.3643 (8.4808)	LR 3.244e-04
2: TRAIN [0][120/340]	Time 0.377 (0.479)	Data 1.24e-04 (1.32e-03)	Tok/s 7572 (10671)	Loss/tok 7.3734 (8.4721)	LR 3.244e-04
0: TRAIN [0][130/340]	Time 0.373 (0.481)	Data 1.49e-04 (1.15e-03)	Tok/s 7819 (10688)	Loss/tok 7.4647 (8.4187)	LR 4.083e-04
1: TRAIN [0][130/340]	Time 0.373 (0.481)	Data 1.01e-04 (1.40e-03)	Tok/s 7973 (10711)	Loss/tok 7.4079 (8.4208)	LR 4.083e-04
3: TRAIN [0][130/340]	Time 0.373 (0.481)	Data 1.03e-04 (1.25e-03)	Tok/s 7944 (10689)	Loss/tok 7.3737 (8.4200)	LR 4.083e-04
2: TRAIN [0][130/340]	Time 0.373 (0.481)	Data 9.54e-05 (1.23e-03)	Tok/s 7579 (10686)	Loss/tok 7.4911 (8.4145)	LR 4.083e-04
0: TRAIN [0][140/340]	Time 0.542 (0.480)	Data 8.99e-05 (1.07e-03)	Tok/s 12704 (10647)	Loss/tok 7.8111 (8.3740)	LR 5.141e-04
1: TRAIN [0][140/340]	Time 0.542 (0.479)	Data 8.63e-05 (1.31e-03)	Tok/s 12558 (10673)	Loss/tok 7.8608 (8.3765)	LR 5.141e-04
3: TRAIN [0][140/340]	Time 0.542 (0.479)	Data 1.08e-04 (1.17e-03)	Tok/s 12404 (10644)	Loss/tok 7.8060 (8.3762)	LR 5.141e-04
2: TRAIN [0][140/340]	Time 0.542 (0.479)	Data 1.09e-04 (1.15e-03)	Tok/s 12405 (10644)	Loss/tok 7.8272 (8.3711)	LR 5.141e-04
0: TRAIN [0][150/340]	Time 0.540 (0.478)	Data 1.46e-04 (1.01e-03)	Tok/s 12631 (10619)	Loss/tok 7.8372 (8.3280)	LR 6.472e-04
1: TRAIN [0][150/340]	Time 0.541 (0.478)	Data 9.27e-05 (1.23e-03)	Tok/s 12525 (10641)	Loss/tok 7.8214 (8.3325)	LR 6.472e-04
3: TRAIN [0][150/340]	Time 0.541 (0.478)	Data 1.05e-04 (1.10e-03)	Tok/s 12472 (10614)	Loss/tok 7.8023 (8.3311)	LR 6.472e-04
2: TRAIN [0][150/340]	Time 0.541 (0.478)	Data 1.16e-04 (1.08e-03)	Tok/s 12264 (10613)	Loss/tok 7.8504 (8.3264)	LR 6.472e-04
0: TRAIN [0][160/340]	Time 0.455 (0.479)	Data 9.13e-05 (9.51e-04)	Tok/s 10588 (10665)	Loss/tok 7.4622 (8.2800)	LR 8.148e-04
1: TRAIN [0][160/340]	Time 0.455 (0.479)	Data 8.87e-05 (1.16e-03)	Tok/s 10817 (10686)	Loss/tok 7.5373 (8.2848)	LR 8.148e-04
3: TRAIN [0][160/340]	Time 0.455 (0.479)	Data 1.44e-04 (1.04e-03)	Tok/s 10907 (10667)	Loss/tok 7.5220 (8.2828)	LR 8.148e-04
2: TRAIN [0][160/340]	Time 0.455 (0.479)	Data 9.78e-05 (1.02e-03)	Tok/s 10731 (10659)	Loss/tok 7.4370 (8.2791)	LR 8.148e-04
0: TRAIN [0][170/340]	Time 0.544 (0.475)	Data 1.50e-04 (9.01e-04)	Tok/s 12528 (10555)	Loss/tok 7.6813 (8.2458)	LR 1.026e-03
1: TRAIN [0][170/340]	Time 0.544 (0.475)	Data 8.56e-05 (1.10e-03)	Tok/s 12426 (10571)	Loss/tok 7.6443 (8.2504)	LR 1.026e-03
3: TRAIN [0][170/340]	Time 0.544 (0.474)	Data 1.04e-04 (9.82e-04)	Tok/s 12514 (10556)	Loss/tok 7.6109 (8.2480)	LR 1.026e-03
2: TRAIN [0][170/340]	Time 0.544 (0.474)	Data 9.23e-05 (9.62e-04)	Tok/s 12734 (10548)	Loss/tok 7.6523 (8.2446)	LR 1.026e-03
0: TRAIN [0][180/340]	Time 0.377 (0.475)	Data 9.25e-05 (8.57e-04)	Tok/s 7617 (10578)	Loss/tok 7.2111 (8.2115)	LR 1.291e-03
1: TRAIN [0][180/340]	Time 0.378 (0.475)	Data 9.94e-05 (1.04e-03)	Tok/s 8038 (10593)	Loss/tok 7.0976 (8.2150)	LR 1.291e-03
3: TRAIN [0][180/340]	Time 0.377 (0.475)	Data 1.13e-04 (9.34e-04)	Tok/s 7788 (10579)	Loss/tok 7.1854 (8.2116)	LR 1.291e-03
2: TRAIN [0][180/340]	Time 0.378 (0.475)	Data 9.82e-05 (9.15e-04)	Tok/s 7809 (10569)	Loss/tok 7.2281 (8.2087)	LR 1.291e-03
0: TRAIN [0][190/340]	Time 0.459 (0.475)	Data 9.08e-05 (8.17e-04)	Tok/s 10445 (10584)	Loss/tok 7.4820 (8.1724)	LR 1.626e-03
1: TRAIN [0][190/340]	Time 0.459 (0.475)	Data 1.48e-04 (9.93e-04)	Tok/s 10644 (10598)	Loss/tok 7.4986 (8.1774)	LR 1.626e-03
3: TRAIN [0][190/340]	Time 0.459 (0.475)	Data 1.19e-04 (8.91e-04)	Tok/s 10438 (10584)	Loss/tok 7.5848 (8.1739)	LR 1.626e-03
2: TRAIN [0][190/340]	Time 0.459 (0.475)	Data 9.04e-05 (8.72e-04)	Tok/s 10614 (10576)	Loss/tok 7.5739 (8.1702)	LR 1.626e-03
0: TRAIN [0][200/340]	Time 0.451 (0.475)	Data 9.92e-05 (7.81e-04)	Tok/s 10857 (10558)	Loss/tok 7.0576 (8.1351)	LR 2.000e-03
1: TRAIN [0][200/340]	Time 0.451 (0.475)	Data 1.15e-04 (9.49e-04)	Tok/s 11067 (10572)	Loss/tok 7.2807 (8.1407)	LR 2.000e-03
3: TRAIN [0][200/340]	Time 0.451 (0.475)	Data 1.06e-04 (8.52e-04)	Tok/s 10769 (10557)	Loss/tok 7.1693 (8.1356)	LR 2.000e-03
2: TRAIN [0][200/340]	Time 0.452 (0.475)	Data 1.10e-04 (8.33e-04)	Tok/s 10935 (10549)	Loss/tok 7.1833 (8.1333)	LR 2.000e-03
0: TRAIN [0][210/340]	Time 0.376 (0.475)	Data 9.51e-05 (7.49e-04)	Tok/s 7981 (10553)	Loss/tok 6.7748 (8.0966)	LR 2.000e-03
1: TRAIN [0][210/340]	Time 0.376 (0.475)	Data 9.47e-05 (9.08e-04)	Tok/s 7702 (10562)	Loss/tok 6.9605 (8.1016)	LR 2.000e-03
3: TRAIN [0][210/340]	Time 0.376 (0.475)	Data 1.43e-04 (8.17e-04)	Tok/s 7823 (10548)	Loss/tok 6.8459 (8.0960)	LR 2.000e-03
2: TRAIN [0][210/340]	Time 0.376 (0.475)	Data 9.61e-05 (7.99e-04)	Tok/s 7866 (10539)	Loss/tok 6.8785 (8.0949)	LR 2.000e-03
0: TRAIN [0][220/340]	Time 0.378 (0.475)	Data 9.04e-05 (7.19e-04)	Tok/s 7770 (10542)	Loss/tok 6.6306 (8.0574)	LR 2.000e-03
3: TRAIN [0][220/340]	Time 0.378 (0.474)	Data 1.12e-04 (7.85e-04)	Tok/s 7842 (10540)	Loss/tok 6.7865 (8.0572)	LR 2.000e-03
2: TRAIN [0][220/340]	Time 0.378 (0.474)	Data 9.58e-05 (7.67e-04)	Tok/s 7812 (10530)	Loss/tok 6.6595 (8.0565)	LR 2.000e-03
1: TRAIN [0][220/340]	Time 0.382 (0.475)	Data 8.65e-05 (8.72e-04)	Tok/s 7774 (10549)	Loss/tok 6.7896 (8.0630)	LR 2.000e-03
0: TRAIN [0][230/340]	Time 0.540 (0.475)	Data 1.15e-04 (6.92e-04)	Tok/s 12628 (10567)	Loss/tok 7.1972 (8.0111)	LR 2.000e-03
1: TRAIN [0][230/340]	Time 0.540 (0.475)	Data 1.06e-04 (8.39e-04)	Tok/s 12541 (10575)	Loss/tok 7.1451 (8.0179)	LR 2.000e-03
3: TRAIN [0][230/340]	Time 0.540 (0.475)	Data 1.05e-04 (7.56e-04)	Tok/s 12616 (10565)	Loss/tok 7.1245 (8.0119)	LR 2.000e-03
2: TRAIN [0][230/340]	Time 0.540 (0.475)	Data 9.35e-05 (7.38e-04)	Tok/s 12511 (10557)	Loss/tok 7.1957 (8.0114)	LR 2.000e-03
0: TRAIN [0][240/340]	Time 0.545 (0.475)	Data 9.08e-05 (6.68e-04)	Tok/s 12266 (10570)	Loss/tok 7.0466 (7.9718)	LR 2.000e-03
1: TRAIN [0][240/340]	Time 0.545 (0.475)	Data 8.61e-05 (8.08e-04)	Tok/s 12436 (10584)	Loss/tok 7.1057 (7.9766)	LR 2.000e-03
3: TRAIN [0][240/340]	Time 0.545 (0.475)	Data 1.06e-04 (7.29e-04)	Tok/s 12532 (10572)	Loss/tok 7.0691 (7.9721)	LR 2.000e-03
2: TRAIN [0][240/340]	Time 0.542 (0.475)	Data 1.39e-04 (7.12e-04)	Tok/s 12517 (10565)	Loss/tok 7.0785 (7.9700)	LR 2.000e-03
0: TRAIN [0][250/340]	Time 0.459 (0.474)	Data 9.58e-05 (6.45e-04)	Tok/s 10545 (10564)	Loss/tok 6.7235 (7.9288)	LR 2.000e-03
1: TRAIN [0][250/340]	Time 0.459 (0.474)	Data 9.54e-05 (7.80e-04)	Tok/s 10696 (10575)	Loss/tok 6.7690 (7.9333)	LR 2.000e-03
3: TRAIN [0][250/340]	Time 0.459 (0.474)	Data 1.07e-04 (7.05e-04)	Tok/s 10756 (10564)	Loss/tok 6.7601 (7.9295)	LR 2.000e-03
2: TRAIN [0][250/340]	Time 0.459 (0.474)	Data 1.00e-04 (6.88e-04)	Tok/s 10523 (10558)	Loss/tok 6.8126 (7.9281)	LR 2.000e-03
0: TRAIN [0][260/340]	Time 0.636 (0.476)	Data 9.80e-05 (6.24e-04)	Tok/s 13897 (10589)	Loss/tok 7.0989 (7.8843)	LR 2.000e-03
1: TRAIN [0][260/340]	Time 0.636 (0.475)	Data 1.03e-04 (7.54e-04)	Tok/s 13879 (10600)	Loss/tok 6.9520 (7.8868)	LR 2.000e-03
3: TRAIN [0][260/340]	Time 0.636 (0.475)	Data 1.05e-04 (6.82e-04)	Tok/s 13731 (10589)	Loss/tok 7.0316 (7.8835)	LR 2.000e-03
2: TRAIN [0][260/340]	Time 0.637 (0.475)	Data 1.03e-04 (6.66e-04)	Tok/s 13876 (10585)	Loss/tok 6.9670 (7.8815)	LR 2.000e-03
0: TRAIN [0][270/340]	Time 0.542 (0.476)	Data 8.85e-05 (6.04e-04)	Tok/s 12556 (10608)	Loss/tok 6.7013 (7.8370)	LR 2.000e-03
1: TRAIN [0][270/340]	Time 0.542 (0.476)	Data 9.20e-05 (7.30e-04)	Tok/s 12395 (10618)	Loss/tok 6.8055 (7.8406)	LR 2.000e-03
3: TRAIN [0][270/340]	Time 0.542 (0.476)	Data 1.03e-04 (6.61e-04)	Tok/s 12406 (10607)	Loss/tok 6.7337 (7.8361)	LR 2.000e-03
2: TRAIN [0][270/340]	Time 0.543 (0.476)	Data 8.89e-05 (6.45e-04)	Tok/s 12372 (10605)	Loss/tok 6.7194 (7.8347)	LR 2.000e-03
0: TRAIN [0][280/340]	Time 0.638 (0.476)	Data 8.99e-05 (5.86e-04)	Tok/s 13811 (10593)	Loss/tok 6.8665 (7.7948)	LR 2.000e-03
1: TRAIN [0][280/340]	Time 0.638 (0.476)	Data 9.66e-05 (7.07e-04)	Tok/s 13606 (10601)	Loss/tok 6.9012 (7.7994)	LR 2.000e-03
3: TRAIN [0][280/340]	Time 0.638 (0.476)	Data 9.94e-05 (6.41e-04)	Tok/s 13818 (10594)	Loss/tok 6.9168 (7.7938)	LR 2.000e-03
2: TRAIN [0][280/340]	Time 0.638 (0.476)	Data 9.54e-05 (6.25e-04)	Tok/s 13809 (10588)	Loss/tok 6.9696 (7.7922)	LR 2.000e-03
0: TRAIN [0][290/340]	Time 0.454 (0.474)	Data 1.31e-04 (5.70e-04)	Tok/s 11012 (10550)	Loss/tok 6.3435 (7.7567)	LR 2.000e-03
1: TRAIN [0][290/340]	Time 0.458 (0.474)	Data 9.97e-05 (6.86e-04)	Tok/s 10575 (10557)	Loss/tok 6.3995 (7.7622)	LR 2.000e-03
3: TRAIN [0][290/340]	Time 0.457 (0.474)	Data 1.01e-04 (6.23e-04)	Tok/s 10620 (10548)	Loss/tok 6.4323 (7.7575)	LR 2.000e-03
2: TRAIN [0][290/340]	Time 0.458 (0.474)	Data 9.06e-05 (6.07e-04)	Tok/s 10820 (10544)	Loss/tok 6.4367 (7.7554)	LR 2.000e-03
0: TRAIN [0][300/340]	Time 0.461 (0.476)	Data 9.16e-05 (5.54e-04)	Tok/s 10406 (10586)	Loss/tok 6.2187 (7.7079)	LR 2.000e-03
1: TRAIN [0][300/340]	Time 0.461 (0.475)	Data 1.72e-04 (6.67e-04)	Tok/s 10638 (10593)	Loss/tok 6.2514 (7.7141)	LR 2.000e-03
3: TRAIN [0][300/340]	Time 0.461 (0.475)	Data 1.09e-04 (6.06e-04)	Tok/s 10684 (10584)	Loss/tok 6.3410 (7.7087)	LR 2.000e-03
2: TRAIN [0][300/340]	Time 0.461 (0.475)	Data 9.25e-05 (5.90e-04)	Tok/s 10722 (10578)	Loss/tok 6.3612 (7.7071)	LR 2.000e-03
0: TRAIN [0][310/340]	Time 0.456 (0.475)	Data 8.94e-05 (5.40e-04)	Tok/s 10627 (10575)	Loss/tok 6.2425 (7.6654)	LR 2.000e-03
1: TRAIN [0][310/340]	Time 0.456 (0.475)	Data 9.11e-05 (6.49e-04)	Tok/s 10568 (10580)	Loss/tok 6.3324 (7.6714)	LR 2.000e-03
3: TRAIN [0][310/340]	Time 0.456 (0.474)	Data 1.08e-04 (5.90e-04)	Tok/s 10663 (10571)	Loss/tok 6.2087 (7.6657)	LR 2.000e-03
2: TRAIN [0][310/340]	Time 0.456 (0.474)	Data 9.63e-05 (5.74e-04)	Tok/s 10629 (10564)	Loss/tok 6.1817 (7.6650)	LR 2.000e-03
0: TRAIN [0][320/340]	Time 0.543 (0.475)	Data 9.23e-05 (5.26e-04)	Tok/s 12429 (10585)	Loss/tok 6.3665 (7.6202)	LR 2.000e-03
1: TRAIN [0][320/340]	Time 0.543 (0.475)	Data 9.42e-05 (6.31e-04)	Tok/s 12709 (10589)	Loss/tok 6.2601 (7.6256)	LR 2.000e-03
3: TRAIN [0][320/340]	Time 0.543 (0.474)	Data 1.34e-04 (5.75e-04)	Tok/s 12403 (10579)	Loss/tok 6.3488 (7.6207)	LR 2.000e-03
2: TRAIN [0][320/340]	Time 0.543 (0.474)	Data 9.75e-05 (5.60e-04)	Tok/s 12653 (10573)	Loss/tok 6.3228 (7.6192)	LR 2.000e-03
0: TRAIN [0][330/340]	Time 0.460 (0.475)	Data 1.01e-04 (5.13e-04)	Tok/s 10771 (10592)	Loss/tok 6.1158 (7.5754)	LR 2.000e-03
1: TRAIN [0][330/340]	Time 0.460 (0.475)	Data 9.89e-05 (6.15e-04)	Tok/s 10660 (10596)	Loss/tok 6.0475 (7.5816)	LR 2.000e-03
3: TRAIN [0][330/340]	Time 0.460 (0.475)	Data 1.20e-04 (5.61e-04)	Tok/s 10468 (10587)	Loss/tok 6.1458 (7.5774)	LR 2.000e-03
2: TRAIN [0][330/340]	Time 0.460 (0.475)	Data 9.51e-05 (5.46e-04)	Tok/s 10400 (10580)	Loss/tok 6.1757 (7.5754)	LR 2.000e-03
0: Running validation on dev set
0: Executing preallocation
3: Running validation on dev set
3: Executing preallocation
2: Running validation on dev set
1: Running validation on dev set
2: Executing preallocation
1: Executing preallocation
3: VALIDATION [0][0/40]	Time 0.099 (0.099)	Data 1.63e-03 (1.63e-03)	Tok/s 42176 (42176)	Loss/tok 7.0661 (7.0661)
2: VALIDATION [0][0/40]	Time 0.110 (0.110)	Data 1.70e-03 (1.70e-03)	Tok/s 40004 (40004)	Loss/tok 7.1606 (7.1606)
1: VALIDATION [0][0/40]	Time 0.119 (0.119)	Data 2.79e-03 (2.79e-03)	Tok/s 40045 (40045)	Loss/tok 7.1266 (7.1266)
0: VALIDATION [0][0/40]	Time 0.163 (0.163)	Data 1.72e-03 (1.72e-03)	Tok/s 35021 (35021)	Loss/tok 7.1934 (7.1934)
3: VALIDATION [0][10/40]	Time 0.053 (0.068)	Data 1.35e-03 (1.42e-03)	Tok/s 42763 (43233)	Loss/tok 6.9820 (7.0253)
2: VALIDATION [0][10/40]	Time 0.050 (0.070)	Data 1.41e-03 (1.46e-03)	Tok/s 45191 (43076)	Loss/tok 6.9687 (6.9785)
1: VALIDATION [0][10/40]	Time 0.054 (0.072)	Data 1.37e-03 (1.79e-03)	Tok/s 42769 (42938)	Loss/tok 6.7508 (6.9923)
0: VALIDATION [0][10/40]	Time 0.055 (0.078)	Data 1.38e-03 (1.46e-03)	Tok/s 42816 (42197)	Loss/tok 6.7605 (7.0247)
3: VALIDATION [0][20/40]	Time 0.035 (0.057)	Data 1.32e-03 (1.39e-03)	Tok/s 45293 (43095)	Loss/tok 6.7119 (6.9335)
2: VALIDATION [0][20/40]	Time 0.038 (0.058)	Data 1.35e-03 (1.42e-03)	Tok/s 42658 (43188)	Loss/tok 6.5758 (6.8959)
1: VALIDATION [0][20/40]	Time 0.038 (0.059)	Data 1.35e-03 (1.59e-03)	Tok/s 42467 (42980)	Loss/tok 6.7038 (6.9008)
0: VALIDATION [0][20/40]	Time 0.038 (0.062)	Data 1.38e-03 (1.41e-03)	Tok/s 42875 (42684)	Loss/tok 6.6977 (6.9250)
3: VALIDATION [0][30/40]	Time 0.027 (0.048)	Data 1.36e-03 (1.37e-03)	Tok/s 38712 (42420)	Loss/tok 6.3716 (6.8742)
2: VALIDATION [0][30/40]	Time 0.026 (0.049)	Data 1.36e-03 (1.41e-03)	Tok/s 41380 (42818)	Loss/tok 6.6393 (6.8252)
1: VALIDATION [0][30/40]	Time 0.026 (0.050)	Data 1.40e-03 (1.54e-03)	Tok/s 40657 (42371)	Loss/tok 6.4248 (6.8355)
0: VALIDATION [0][30/40]	Time 0.027 (0.052)	Data 1.41e-03 (1.52e-03)	Tok/s 39875 (42276)	Loss/tok 6.7203 (6.8796)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
2: Running evaluation on test set
0: Running evaluation on test set
3: Running evaluation on test set
3: TEST [0][9/24]	Time 0.8308 (1.0508)	Decoder iters 149.0 (149.0)	Tok/s 3144 (3488)
0: TEST [0][9/24]	Time 0.8294 (1.0509)	Decoder iters 149.0 (149.0)	Tok/s 3616 (3941)
2: TEST [0][9/24]	Time 0.8297 (1.0509)	Decoder iters 149.0 (149.0)	Tok/s 3224 (3656)
1: TEST [0][9/24]	Time 0.8294 (1.0509)	Decoder iters 149.0 (149.0)	Tok/s 3717 (3870)
3: TEST [0][19/24]	Time 0.5589 (0.8492)	Decoder iters 95.0 (146.3)	Tok/s 1974 (3052)
0: TEST [0][19/24]	Time 0.5598 (0.8493)	Decoder iters 149.0 (145.3)	Tok/s 2483 (3437)
1: TEST [0][19/24]	Time 0.5598 (0.8493)	Decoder iters 149.0 (149.0)	Tok/s 2583 (3454)
2: TEST [0][19/24]	Time 0.5602 (0.8493)	Decoder iters 149.0 (149.0)	Tok/s 2072 (3312)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
2: Finished evaluation on test set
1: Finished evaluation on test set
3: Finished evaluation on test set
0: Finished evaluation on test set
3: Finished epoch 0
2: Finished epoch 0
1: Finished epoch 0
2: Starting epoch 1
1: Starting epoch 1
3: Starting epoch 1
1: Executing preallocation
2: Executing preallocation
3: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 7.5419	Validation Loss: 6.7999	Test BLEU: 0.37
0: Performance: Epoch: 0	Training: 42273 Tok/s	Validation: 165703 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
3: Sampler for epoch 1 uses seed 2606193617
2: Sampler for epoch 1 uses seed 2606193617
1: Sampler for epoch 1 uses seed 2606193617
0: Sampler for epoch 1 uses seed 2606193617
1: TRAIN [1][0/340]	Time 0.520 (0.520)	Data 1.71e-01 (1.71e-01)	Tok/s 5594 (5594)	Loss/tok 5.6352 (5.6352)	LR 2.000e-03
2: TRAIN [1][0/340]	Time 0.513 (0.513)	Data 1.36e-01 (1.36e-01)	Tok/s 5546 (5546)	Loss/tok 5.3475 (5.3475)	LR 2.000e-03
3: TRAIN [1][0/340]	Time 0.520 (0.520)	Data 1.42e-01 (1.42e-01)	Tok/s 5576 (5576)	Loss/tok 5.4983 (5.4983)	LR 2.000e-03
0: TRAIN [1][0/340]	Time 0.524 (0.524)	Data 1.74e-01 (1.74e-01)	Tok/s 5461 (5461)	Loss/tok 5.4258 (5.4258)	LR 2.000e-03
0: TRAIN [1][10/340]	Time 0.639 (0.482)	Data 1.49e-04 (1.59e-02)	Tok/s 13846 (10087)	Loss/tok 6.1835 (5.9597)	LR 2.000e-03
1: TRAIN [1][10/340]	Time 0.642 (0.482)	Data 1.01e-04 (1.56e-02)	Tok/s 13826 (9926)	Loss/tok 6.2170 (6.0071)	LR 2.000e-03
3: TRAIN [1][10/340]	Time 0.642 (0.482)	Data 1.11e-04 (1.30e-02)	Tok/s 13754 (10035)	Loss/tok 6.1320 (5.9846)	LR 2.000e-03
2: TRAIN [1][10/340]	Time 0.642 (0.481)	Data 1.52e-04 (1.25e-02)	Tok/s 13623 (9980)	Loss/tok 6.2069 (5.9672)	LR 2.000e-03
0: TRAIN [1][20/340]	Time 0.460 (0.488)	Data 9.16e-05 (8.39e-03)	Tok/s 10537 (10598)	Loss/tok 5.7052 (5.9429)	LR 2.000e-03
1: TRAIN [1][20/340]	Time 0.460 (0.488)	Data 8.92e-05 (8.24e-03)	Tok/s 10706 (10502)	Loss/tok 5.7270 (5.9570)	LR 2.000e-03
3: TRAIN [1][20/340]	Time 0.460 (0.488)	Data 1.01e-04 (6.86e-03)	Tok/s 10715 (10515)	Loss/tok 5.8082 (5.9679)	LR 2.000e-03
2: TRAIN [1][20/340]	Time 0.460 (0.487)	Data 7.89e-05 (6.59e-03)	Tok/s 10758 (10533)	Loss/tok 5.8211 (5.9566)	LR 2.000e-03
0: TRAIN [1][30/340]	Time 0.454 (0.478)	Data 9.25e-05 (5.72e-03)	Tok/s 10718 (10524)	Loss/tok 5.6785 (5.8981)	LR 2.000e-03
1: TRAIN [1][30/340]	Time 0.454 (0.478)	Data 1.08e-04 (5.61e-03)	Tok/s 10795 (10463)	Loss/tok 5.7011 (5.9004)	LR 2.000e-03
3: TRAIN [1][30/340]	Time 0.454 (0.478)	Data 1.19e-04 (4.68e-03)	Tok/s 10730 (10465)	Loss/tok 5.6405 (5.9166)	LR 2.000e-03
2: TRAIN [1][30/340]	Time 0.454 (0.478)	Data 8.06e-05 (4.49e-03)	Tok/s 10736 (10495)	Loss/tok 5.6819 (5.9024)	LR 2.000e-03
0: TRAIN [1][40/340]	Time 0.643 (0.479)	Data 1.05e-04 (4.35e-03)	Tok/s 13864 (10651)	Loss/tok 6.0249 (5.8706)	LR 2.000e-03
1: TRAIN [1][40/340]	Time 0.643 (0.479)	Data 1.24e-04 (4.27e-03)	Tok/s 13762 (10583)	Loss/tok 6.0864 (5.8689)	LR 2.000e-03
3: TRAIN [1][40/340]	Time 0.643 (0.479)	Data 1.11e-04 (3.57e-03)	Tok/s 13593 (10583)	Loss/tok 6.0845 (5.8796)	LR 2.000e-03
2: TRAIN [1][40/340]	Time 0.643 (0.479)	Data 8.82e-05 (3.42e-03)	Tok/s 13703 (10608)	Loss/tok 5.9641 (5.8597)	LR 2.000e-03
0: TRAIN [1][50/340]	Time 0.639 (0.481)	Data 9.06e-05 (3.52e-03)	Tok/s 13695 (10679)	Loss/tok 5.9566 (5.8389)	LR 2.000e-03
1: TRAIN [1][50/340]	Time 0.639 (0.481)	Data 8.80e-05 (3.45e-03)	Tok/s 13822 (10619)	Loss/tok 5.8994 (5.8360)	LR 2.000e-03
3: TRAIN [1][50/340]	Time 0.639 (0.481)	Data 1.05e-04 (2.89e-03)	Tok/s 13838 (10617)	Loss/tok 5.9393 (5.8464)	LR 2.000e-03
2: TRAIN [1][50/340]	Time 0.639 (0.481)	Data 8.27e-05 (2.77e-03)	Tok/s 13684 (10624)	Loss/tok 5.9295 (5.8408)	LR 2.000e-03
0: TRAIN [1][60/340]	Time 0.452 (0.479)	Data 8.75e-05 (2.96e-03)	Tok/s 10779 (10617)	Loss/tok 5.4969 (5.8132)	LR 2.000e-03
1: TRAIN [1][60/340]	Time 0.452 (0.479)	Data 8.89e-05 (2.90e-03)	Tok/s 10831 (10576)	Loss/tok 5.4451 (5.8010)	LR 2.000e-03
3: TRAIN [1][60/340]	Time 0.452 (0.479)	Data 1.15e-04 (2.44e-03)	Tok/s 10557 (10570)	Loss/tok 5.4711 (5.8174)	LR 2.000e-03
2: TRAIN [1][60/340]	Time 0.452 (0.479)	Data 8.03e-05 (2.33e-03)	Tok/s 10696 (10588)	Loss/tok 5.5009 (5.8160)	LR 2.000e-03
0: TRAIN [1][70/340]	Time 0.375 (0.473)	Data 8.54e-05 (2.56e-03)	Tok/s 7752 (10454)	Loss/tok 4.9250 (5.7661)	LR 2.000e-03
1: TRAIN [1][70/340]	Time 0.375 (0.473)	Data 8.46e-05 (2.50e-03)	Tok/s 7851 (10424)	Loss/tok 5.0661 (5.7579)	LR 2.000e-03
3: TRAIN [1][70/340]	Time 0.371 (0.473)	Data 1.75e-04 (2.11e-03)	Tok/s 7823 (10418)	Loss/tok 5.1156 (5.7728)	LR 2.000e-03
2: TRAIN [1][70/340]	Time 0.375 (0.473)	Data 8.06e-05 (2.01e-03)	Tok/s 7905 (10442)	Loss/tok 4.9952 (5.7697)	LR 2.000e-03
0: TRAIN [1][80/340]	Time 0.542 (0.480)	Data 9.94e-05 (2.25e-03)	Tok/s 12458 (10621)	Loss/tok 5.6474 (5.7557)	LR 2.000e-03
1: TRAIN [1][80/340]	Time 0.542 (0.480)	Data 9.51e-05 (2.20e-03)	Tok/s 12490 (10597)	Loss/tok 5.6674 (5.7481)	LR 2.000e-03
3: TRAIN [1][80/340]	Time 0.542 (0.480)	Data 1.01e-04 (1.86e-03)	Tok/s 12690 (10598)	Loss/tok 5.7807 (5.7690)	LR 2.000e-03
2: TRAIN [1][80/340]	Time 0.543 (0.480)	Data 9.23e-05 (1.77e-03)	Tok/s 12552 (10625)	Loss/tok 5.7752 (5.7601)	LR 2.000e-03
0: TRAIN [1][90/340]	Time 0.462 (0.480)	Data 9.25e-05 (2.01e-03)	Tok/s 10741 (10614)	Loss/tok 5.3755 (5.7275)	LR 2.000e-03
1: TRAIN [1][90/340]	Time 0.462 (0.480)	Data 9.23e-05 (1.97e-03)	Tok/s 10605 (10584)	Loss/tok 5.3123 (5.7235)	LR 2.000e-03
3: TRAIN [1][90/340]	Time 0.462 (0.480)	Data 1.10e-04 (1.67e-03)	Tok/s 10429 (10583)	Loss/tok 5.3804 (5.7458)	LR 2.000e-03
2: TRAIN [1][90/340]	Time 0.462 (0.479)	Data 8.34e-05 (1.59e-03)	Tok/s 10550 (10613)	Loss/tok 5.2575 (5.7318)	LR 2.000e-03
0: TRAIN [1][100/340]	Time 0.455 (0.484)	Data 9.32e-05 (1.83e-03)	Tok/s 10919 (10734)	Loss/tok 5.2596 (5.6947)	LR 2.000e-03
1: TRAIN [1][100/340]	Time 0.455 (0.484)	Data 9.73e-05 (1.79e-03)	Tok/s 10689 (10709)	Loss/tok 5.1892 (5.6959)	LR 2.000e-03
2: TRAIN [1][100/340]	Time 0.455 (0.483)	Data 8.61e-05 (1.44e-03)	Tok/s 10804 (10736)	Loss/tok 5.1359 (5.7002)	LR 2.000e-03
3: TRAIN [1][100/340]	Time 0.455 (0.484)	Data 1.21e-04 (1.51e-03)	Tok/s 10699 (10709)	Loss/tok 5.2597 (5.7126)	LR 2.000e-03
0: TRAIN [1][110/340]	Time 0.379 (0.484)	Data 9.37e-05 (1.67e-03)	Tok/s 7888 (10721)	Loss/tok 4.8046 (5.6691)	LR 2.000e-03
1: TRAIN [1][110/340]	Time 0.380 (0.484)	Data 9.04e-05 (1.63e-03)	Tok/s 7747 (10697)	Loss/tok 4.9503 (5.6724)	LR 2.000e-03
3: TRAIN [1][110/340]	Time 0.380 (0.484)	Data 1.08e-04 (1.39e-03)	Tok/s 7674 (10700)	Loss/tok 4.8505 (5.6834)	LR 2.000e-03
2: TRAIN [1][110/340]	Time 0.380 (0.484)	Data 8.18e-05 (1.32e-03)	Tok/s 7776 (10722)	Loss/tok 4.8166 (5.6755)	LR 2.000e-03
0: TRAIN [1][120/340]	Time 0.455 (0.482)	Data 9.23e-05 (1.54e-03)	Tok/s 10674 (10689)	Loss/tok 5.1134 (5.6323)	LR 1.000e-03
1: TRAIN [1][120/340]	Time 0.455 (0.482)	Data 9.49e-05 (1.51e-03)	Tok/s 10763 (10667)	Loss/tok 4.9329 (5.6338)	LR 1.000e-03
3: TRAIN [1][120/340]	Time 0.455 (0.482)	Data 1.03e-04 (1.28e-03)	Tok/s 10692 (10670)	Loss/tok 4.9269 (5.6437)	LR 1.000e-03
2: TRAIN [1][120/340]	Time 0.455 (0.482)	Data 9.01e-05 (1.22e-03)	Tok/s 10743 (10683)	Loss/tok 5.0677 (5.6378)	LR 1.000e-03
0: TRAIN [1][130/340]	Time 0.375 (0.478)	Data 9.06e-05 (1.43e-03)	Tok/s 7758 (10607)	Loss/tok 4.6870 (5.5954)	LR 1.000e-03
1: TRAIN [1][130/340]	Time 0.375 (0.478)	Data 9.56e-05 (1.40e-03)	Tok/s 7640 (10589)	Loss/tok 4.6329 (5.5955)	LR 1.000e-03
3: TRAIN [1][130/340]	Time 0.375 (0.478)	Data 1.07e-04 (1.19e-03)	Tok/s 7834 (10590)	Loss/tok 4.5378 (5.6026)	LR 1.000e-03
2: TRAIN [1][130/340]	Time 0.375 (0.478)	Data 9.61e-05 (1.13e-03)	Tok/s 7934 (10599)	Loss/tok 4.4904 (5.5964)	LR 1.000e-03
0: TRAIN [1][140/340]	Time 0.379 (0.480)	Data 9.27e-05 (1.33e-03)	Tok/s 7711 (10653)	Loss/tok 4.5586 (5.5596)	LR 1.000e-03
1: TRAIN [1][140/340]	Time 0.379 (0.480)	Data 9.70e-05 (1.31e-03)	Tok/s 7654 (10636)	Loss/tok 4.7541 (5.5644)	LR 1.000e-03
3: TRAIN [1][140/340]	Time 0.379 (0.480)	Data 1.18e-04 (1.11e-03)	Tok/s 7801 (10638)	Loss/tok 4.5640 (5.5674)	LR 1.000e-03
2: TRAIN [1][140/340]	Time 0.379 (0.480)	Data 8.80e-05 (1.06e-03)	Tok/s 8006 (10645)	Loss/tok 4.6357 (5.5624)	LR 1.000e-03
0: TRAIN [1][150/340]	Time 0.458 (0.478)	Data 9.56e-05 (1.25e-03)	Tok/s 10698 (10634)	Loss/tok 4.8605 (5.5226)	LR 1.000e-03
1: TRAIN [1][150/340]	Time 0.458 (0.478)	Data 9.58e-05 (1.23e-03)	Tok/s 10492 (10621)	Loss/tok 4.9343 (5.5280)	LR 1.000e-03
3: TRAIN [1][150/340]	Time 0.458 (0.478)	Data 1.34e-04 (1.05e-03)	Tok/s 10874 (10622)	Loss/tok 4.6953 (5.5257)	LR 1.000e-03
2: TRAIN [1][150/340]	Time 0.458 (0.478)	Data 8.89e-05 (9.94e-04)	Tok/s 10734 (10627)	Loss/tok 4.9384 (5.5256)	LR 1.000e-03
0: TRAIN [1][160/340]	Time 0.461 (0.478)	Data 8.68e-05 (1.18e-03)	Tok/s 10431 (10624)	Loss/tok 4.7518 (5.4853)	LR 1.000e-03
1: TRAIN [1][160/340]	Time 0.461 (0.478)	Data 8.51e-05 (1.16e-03)	Tok/s 10501 (10608)	Loss/tok 4.9236 (5.4945)	LR 1.000e-03
3: TRAIN [1][160/340]	Time 0.461 (0.478)	Data 1.03e-04 (9.90e-04)	Tok/s 10769 (10613)	Loss/tok 4.8582 (5.4909)	LR 1.000e-03
2: TRAIN [1][160/340]	Time 0.461 (0.478)	Data 7.34e-05 (9.38e-04)	Tok/s 10308 (10614)	Loss/tok 4.7602 (5.4894)	LR 1.000e-03
0: TRAIN [1][170/340]	Time 0.541 (0.477)	Data 9.66e-05 (1.12e-03)	Tok/s 12528 (10617)	Loss/tok 4.9832 (5.4518)	LR 5.000e-04
1: TRAIN [1][170/340]	Time 0.541 (0.477)	Data 1.19e-04 (1.09e-03)	Tok/s 12389 (10599)	Loss/tok 5.0199 (5.4611)	LR 5.000e-04
3: TRAIN [1][170/340]	Time 0.541 (0.477)	Data 1.40e-04 (9.39e-04)	Tok/s 12454 (10606)	Loss/tok 5.0165 (5.4558)	LR 5.000e-04
2: TRAIN [1][170/340]	Time 0.541 (0.477)	Data 8.96e-05 (8.88e-04)	Tok/s 12621 (10606)	Loss/tok 5.2108 (5.4559)	LR 5.000e-04
0: TRAIN [1][180/340]	Time 0.378 (0.476)	Data 9.56e-05 (1.06e-03)	Tok/s 7504 (10575)	Loss/tok 4.3675 (5.4203)	LR 5.000e-04
1: TRAIN [1][180/340]	Time 0.378 (0.476)	Data 9.04e-05 (1.04e-03)	Tok/s 7729 (10560)	Loss/tok 4.4039 (5.4300)	LR 5.000e-04
3: TRAIN [1][180/340]	Time 0.378 (0.476)	Data 8.87e-05 (8.93e-04)	Tok/s 7745 (10570)	Loss/tok 4.3823 (5.4238)	LR 5.000e-04
2: TRAIN [1][180/340]	Time 0.378 (0.476)	Data 9.06e-05 (8.44e-04)	Tok/s 7790 (10567)	Loss/tok 4.6050 (5.4251)	LR 5.000e-04
0: TRAIN [1][190/340]	Time 0.371 (0.475)	Data 9.27e-05 (1.01e-03)	Tok/s 7894 (10550)	Loss/tok 4.1853 (5.3900)	LR 5.000e-04
1: TRAIN [1][190/340]	Time 0.371 (0.475)	Data 9.20e-05 (9.89e-04)	Tok/s 7957 (10539)	Loss/tok 4.2991 (5.3979)	LR 5.000e-04
3: TRAIN [1][190/340]	Time 0.371 (0.475)	Data 1.00e-04 (8.52e-04)	Tok/s 8010 (10546)	Loss/tok 4.3443 (5.3916)	LR 5.000e-04
2: TRAIN [1][190/340]	Time 0.371 (0.474)	Data 8.23e-05 (8.04e-04)	Tok/s 7735 (10541)	Loss/tok 4.3333 (5.3936)	LR 5.000e-04
0: TRAIN [1][200/340]	Time 0.374 (0.473)	Data 8.85e-05 (9.65e-04)	Tok/s 7510 (10514)	Loss/tok 4.2470 (5.3612)	LR 5.000e-04
1: TRAIN [1][200/340]	Time 0.374 (0.473)	Data 9.13e-05 (9.44e-04)	Tok/s 7823 (10509)	Loss/tok 4.2465 (5.3666)	LR 5.000e-04
3: TRAIN [1][200/340]	Time 0.374 (0.473)	Data 1.03e-04 (8.14e-04)	Tok/s 7731 (10510)	Loss/tok 4.3270 (5.3610)	LR 5.000e-04
2: TRAIN [1][200/340]	Time 0.374 (0.473)	Data 8.13e-05 (7.69e-04)	Tok/s 7634 (10510)	Loss/tok 4.2911 (5.3636)	LR 5.000e-04
0: TRAIN [1][210/340]	Time 0.539 (0.472)	Data 8.68e-05 (9.23e-04)	Tok/s 12585 (10505)	Loss/tok 4.8242 (5.3329)	LR 5.000e-04
1: TRAIN [1][210/340]	Time 0.540 (0.472)	Data 9.06e-05 (9.04e-04)	Tok/s 12442 (10500)	Loss/tok 4.8439 (5.3397)	LR 5.000e-04
3: TRAIN [1][210/340]	Time 0.539 (0.472)	Data 1.26e-04 (7.81e-04)	Tok/s 12641 (10500)	Loss/tok 4.8722 (5.3321)	LR 5.000e-04
2: TRAIN [1][210/340]	Time 0.540 (0.472)	Data 8.85e-05 (7.36e-04)	Tok/s 12669 (10498)	Loss/tok 4.9425 (5.3373)	LR 5.000e-04
0: TRAIN [1][220/340]	Time 0.374 (0.471)	Data 1.02e-04 (8.86e-04)	Tok/s 7900 (10478)	Loss/tok 4.3384 (5.3064)	LR 5.000e-04
1: TRAIN [1][220/340]	Time 0.374 (0.471)	Data 8.85e-05 (8.67e-04)	Tok/s 7772 (10473)	Loss/tok 4.3134 (5.3137)	LR 5.000e-04
3: TRAIN [1][220/340]	Time 0.374 (0.471)	Data 1.08e-04 (7.51e-04)	Tok/s 7567 (10471)	Loss/tok 4.2948 (5.3075)	LR 5.000e-04
2: TRAIN [1][220/340]	Time 0.374 (0.471)	Data 9.39e-05 (7.07e-04)	Tok/s 7940 (10476)	Loss/tok 4.0784 (5.3100)	LR 5.000e-04
0: TRAIN [1][230/340]	Time 0.541 (0.474)	Data 8.82e-05 (8.52e-04)	Tok/s 12564 (10545)	Loss/tok 4.8908 (5.2810)	LR 2.500e-04
1: TRAIN [1][230/340]	Time 0.541 (0.474)	Data 8.49e-05 (8.33e-04)	Tok/s 12638 (10543)	Loss/tok 4.7842 (5.2877)	LR 2.500e-04
3: TRAIN [1][230/340]	Time 0.541 (0.474)	Data 9.92e-05 (7.23e-04)	Tok/s 12560 (10538)	Loss/tok 4.7545 (5.2796)	LR 2.500e-04
2: TRAIN [1][230/340]	Time 0.541 (0.474)	Data 8.96e-05 (6.80e-04)	Tok/s 12464 (10543)	Loss/tok 4.8551 (5.2843)	LR 2.500e-04
0: TRAIN [1][240/340]	Time 0.648 (0.474)	Data 9.58e-05 (8.20e-04)	Tok/s 13712 (10547)	Loss/tok 4.9468 (5.2578)	LR 2.500e-04
1: TRAIN [1][240/340]	Time 0.648 (0.474)	Data 9.25e-05 (8.02e-04)	Tok/s 13438 (10543)	Loss/tok 4.9815 (5.2644)	LR 2.500e-04
3: TRAIN [1][240/340]	Time 0.648 (0.474)	Data 9.25e-05 (6.97e-04)	Tok/s 13647 (10540)	Loss/tok 4.9779 (5.2589)	LR 2.500e-04
2: TRAIN [1][240/340]	Time 0.649 (0.474)	Data 9.04e-05 (6.55e-04)	Tok/s 13691 (10546)	Loss/tok 4.8850 (5.2623)	LR 2.500e-04
0: TRAIN [1][250/340]	Time 0.641 (0.472)	Data 8.27e-05 (7.91e-04)	Tok/s 13939 (10487)	Loss/tok 4.8959 (5.2369)	LR 2.500e-04
1: TRAIN [1][250/340]	Time 0.641 (0.472)	Data 9.13e-05 (7.74e-04)	Tok/s 13699 (10484)	Loss/tok 5.0168 (5.2426)	LR 2.500e-04
3: TRAIN [1][250/340]	Time 0.641 (0.472)	Data 9.11e-05 (6.74e-04)	Tok/s 13745 (10480)	Loss/tok 4.9532 (5.2381)	LR 2.500e-04
2: TRAIN [1][250/340]	Time 0.641 (0.472)	Data 8.20e-05 (6.33e-04)	Tok/s 13697 (10485)	Loss/tok 4.9840 (5.2418)	LR 2.500e-04
0: TRAIN [1][260/340]	Time 0.371 (0.471)	Data 9.66e-05 (7.64e-04)	Tok/s 8147 (10464)	Loss/tok 4.1715 (5.2145)	LR 2.500e-04
1: TRAIN [1][260/340]	Time 0.371 (0.471)	Data 9.56e-05 (7.48e-04)	Tok/s 7902 (10456)	Loss/tok 4.1713 (5.2212)	LR 2.500e-04
3: TRAIN [1][260/340]	Time 0.371 (0.471)	Data 1.06e-04 (6.52e-04)	Tok/s 7918 (10455)	Loss/tok 4.1582 (5.2173)	LR 2.500e-04
2: TRAIN [1][260/340]	Time 0.371 (0.471)	Data 8.85e-05 (6.12e-04)	Tok/s 8008 (10463)	Loss/tok 4.2554 (5.2189)	LR 2.500e-04
0: TRAIN [1][270/340]	Time 0.540 (0.470)	Data 8.82e-05 (7.40e-04)	Tok/s 12572 (10439)	Loss/tok 4.8601 (5.1936)	LR 2.500e-04
1: TRAIN [1][270/340]	Time 0.540 (0.470)	Data 9.13e-05 (7.24e-04)	Tok/s 12587 (10433)	Loss/tok 4.7681 (5.1998)	LR 2.500e-04
3: TRAIN [1][270/340]	Time 0.540 (0.470)	Data 1.03e-04 (6.32e-04)	Tok/s 12591 (10430)	Loss/tok 4.8352 (5.1972)	LR 2.500e-04
2: TRAIN [1][270/340]	Time 0.540 (0.470)	Data 7.92e-05 (5.93e-04)	Tok/s 12648 (10441)	Loss/tok 4.6386 (5.1966)	LR 2.500e-04
0: TRAIN [1][280/340]	Time 0.633 (0.469)	Data 9.54e-05 (7.17e-04)	Tok/s 13802 (10418)	Loss/tok 4.9502 (5.1740)	LR 2.500e-04
1: TRAIN [1][280/340]	Time 0.633 (0.469)	Data 9.35e-05 (7.01e-04)	Tok/s 14012 (10414)	Loss/tok 5.0022 (5.1806)	LR 2.500e-04
3: TRAIN [1][280/340]	Time 0.633 (0.469)	Data 1.05e-04 (6.13e-04)	Tok/s 13751 (10413)	Loss/tok 4.8349 (5.1765)	LR 2.500e-04
2: TRAIN [1][280/340]	Time 0.633 (0.469)	Data 8.85e-05 (5.75e-04)	Tok/s 13889 (10422)	Loss/tok 4.9168 (5.1767)	LR 2.500e-04
0: TRAIN [1][290/340]	Time 0.455 (0.469)	Data 1.02e-04 (6.96e-04)	Tok/s 10592 (10414)	Loss/tok 4.4934 (5.1554)	LR 1.250e-04
1: TRAIN [1][290/340]	Time 0.455 (0.469)	Data 9.70e-05 (6.81e-04)	Tok/s 10623 (10408)	Loss/tok 4.4100 (5.1613)	LR 1.250e-04
3: TRAIN [1][290/340]	Time 0.456 (0.469)	Data 1.02e-04 (5.96e-04)	Tok/s 10596 (10411)	Loss/tok 4.5479 (5.1580)	LR 1.250e-04
2: TRAIN [1][290/340]	Time 0.456 (0.469)	Data 8.99e-05 (5.58e-04)	Tok/s 10625 (10415)	Loss/tok 4.4636 (5.1553)	LR 1.250e-04
0: TRAIN [1][300/340]	Time 0.461 (0.470)	Data 1.05e-04 (6.76e-04)	Tok/s 10803 (10429)	Loss/tok 4.5115 (5.1388)	LR 1.250e-04
1: TRAIN [1][300/340]	Time 0.461 (0.470)	Data 9.70e-05 (6.61e-04)	Tok/s 10744 (10424)	Loss/tok 4.4094 (5.1440)	LR 1.250e-04
3: TRAIN [1][300/340]	Time 0.461 (0.470)	Data 1.26e-04 (5.80e-04)	Tok/s 10428 (10428)	Loss/tok 4.5823 (5.1390)	LR 1.250e-04
2: TRAIN [1][300/340]	Time 0.461 (0.470)	Data 9.37e-05 (5.43e-04)	Tok/s 10528 (10431)	Loss/tok 4.5983 (5.1391)	LR 1.250e-04
0: TRAIN [1][310/340]	Time 0.544 (0.471)	Data 9.58e-05 (6.57e-04)	Tok/s 12340 (10474)	Loss/tok 4.7299 (5.1206)	LR 1.250e-04
1: TRAIN [1][310/340]	Time 0.544 (0.471)	Data 9.11e-05 (6.43e-04)	Tok/s 12504 (10468)	Loss/tok 4.5481 (5.1247)	LR 1.250e-04
3: TRAIN [1][310/340]	Time 0.544 (0.471)	Data 1.08e-04 (5.65e-04)	Tok/s 12449 (10473)	Loss/tok 4.7458 (5.1223)	LR 1.250e-04
2: TRAIN [1][310/340]	Time 0.545 (0.471)	Data 9.42e-05 (5.29e-04)	Tok/s 12648 (10478)	Loss/tok 4.6458 (5.1208)	LR 1.250e-04
0: TRAIN [1][320/340]	Time 0.641 (0.474)	Data 1.21e-04 (6.40e-04)	Tok/s 13788 (10525)	Loss/tok 4.8637 (5.1047)	LR 1.250e-04
1: TRAIN [1][320/340]	Time 0.641 (0.474)	Data 1.02e-04 (6.26e-04)	Tok/s 13795 (10518)	Loss/tok 4.8837 (5.1109)	LR 1.250e-04
3: TRAIN [1][320/340]	Time 0.642 (0.474)	Data 1.15e-04 (5.51e-04)	Tok/s 13600 (10523)	Loss/tok 4.9334 (5.1065)	LR 1.250e-04
2: TRAIN [1][320/340]	Time 0.642 (0.474)	Data 9.89e-05 (5.15e-04)	Tok/s 13937 (10530)	Loss/tok 4.9585 (5.1061)	LR 1.250e-04
0: TRAIN [1][330/340]	Time 0.638 (0.474)	Data 9.82e-05 (6.24e-04)	Tok/s 13892 (10531)	Loss/tok 4.8322 (5.0889)	LR 1.250e-04
1: TRAIN [1][330/340]	Time 0.638 (0.474)	Data 9.27e-05 (6.10e-04)	Tok/s 13748 (10524)	Loss/tok 4.8372 (5.0940)	LR 1.250e-04
3: TRAIN [1][330/340]	Time 0.638 (0.474)	Data 1.07e-04 (5.37e-04)	Tok/s 13843 (10529)	Loss/tok 5.0653 (5.0912)	LR 1.250e-04
2: TRAIN [1][330/340]	Time 0.639 (0.474)	Data 8.82e-05 (5.02e-04)	Tok/s 13758 (10537)	Loss/tok 5.0276 (5.0917)	LR 1.250e-04
1: Running validation on dev set
2: Running validation on dev set
3: Running validation on dev set
3: Executing preallocation
1: Executing preallocation
2: Executing preallocation
0: Running validation on dev set
0: Executing preallocation
3: VALIDATION [1][0/40]	Time 0.101 (0.101)	Data 2.74e-03 (2.74e-03)	Tok/s 41335 (41335)	Loss/tok 6.0553 (6.0553)
2: VALIDATION [1][0/40]	Time 0.109 (0.109)	Data 1.70e-03 (1.70e-03)	Tok/s 40354 (40354)	Loss/tok 6.1283 (6.1283)
1: VALIDATION [1][0/40]	Time 0.119 (0.119)	Data 1.63e-03 (1.63e-03)	Tok/s 39952 (39952)	Loss/tok 6.1397 (6.1397)
0: VALIDATION [1][0/40]	Time 0.163 (0.163)	Data 2.74e-03 (2.74e-03)	Tok/s 35010 (35010)	Loss/tok 6.2781 (6.2781)
3: VALIDATION [1][10/40]	Time 0.053 (0.069)	Data 1.37e-03 (1.64e-03)	Tok/s 43115 (42815)	Loss/tok 5.6124 (5.8763)
2: VALIDATION [1][10/40]	Time 0.051 (0.071)	Data 1.39e-03 (1.45e-03)	Tok/s 44855 (42945)	Loss/tok 5.7753 (5.8342)
1: VALIDATION [1][10/40]	Time 0.054 (0.073)	Data 1.38e-03 (1.43e-03)	Tok/s 42396 (42603)	Loss/tok 5.4982 (5.8566)
0: VALIDATION [1][10/40]	Time 0.054 (0.077)	Data 1.40e-03 (1.71e-03)	Tok/s 43550 (42420)	Loss/tok 5.4259 (5.9090)
3: VALIDATION [1][20/40]	Time 0.035 (0.057)	Data 1.38e-03 (1.53e-03)	Tok/s 44722 (42849)	Loss/tok 5.5528 (5.7529)
2: VALIDATION [1][20/40]	Time 0.039 (0.058)	Data 1.39e-03 (1.42e-03)	Tok/s 41231 (42991)	Loss/tok 5.3268 (5.7305)
1: VALIDATION [1][20/40]	Time 0.038 (0.059)	Data 1.38e-03 (1.40e-03)	Tok/s 42313 (42858)	Loss/tok 5.4747 (5.7086)
0: VALIDATION [1][20/40]	Time 0.037 (0.062)	Data 1.38e-03 (1.54e-03)	Tok/s 43672 (42807)	Loss/tok 5.5031 (5.7836)
3: VALIDATION [1][30/40]	Time 0.026 (0.049)	Data 1.30e-03 (1.48e-03)	Tok/s 39719 (42374)	Loss/tok 5.1059 (5.6862)
2: VALIDATION [1][30/40]	Time 0.026 (0.049)	Data 1.31e-03 (1.39e-03)	Tok/s 40781 (42670)	Loss/tok 5.2962 (5.6464)
1: VALIDATION [1][30/40]	Time 0.026 (0.050)	Data 1.33e-03 (1.38e-03)	Tok/s 41334 (42475)	Loss/tok 5.2119 (5.6380)
0: VALIDATION [1][30/40]	Time 0.026 (0.052)	Data 1.35e-03 (1.48e-03)	Tok/s 40752 (42510)	Loss/tok 5.4586 (5.7170)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
2: Running evaluation on test set
3: Running evaluation on test set
0: Running evaluation on test set
3: TEST [1][9/24]	Time 0.5595 (0.7721)	Decoder iters 149.0 (149.0)	Tok/s 3686 (3709)
2: TEST [1][9/24]	Time 0.5583 (0.7722)	Decoder iters 149.0 (149.0)	Tok/s 3908 (3907)
1: TEST [1][9/24]	Time 0.5585 (0.7723)	Decoder iters 133.0 (147.4)	Tok/s 4050 (3968)
0: TEST [1][9/24]	Time 0.5579 (0.7716)	Decoder iters 61.0 (140.2)	Tok/s 3622 (4143)
3: TEST [1][19/24]	Time 0.4487 (0.6497)	Decoder iters 46.0 (127.8)	Tok/s 2360 (3272)
0: TEST [1][19/24]	Time 0.4484 (0.6492)	Decoder iters 37.0 (123.5)	Tok/s 2632 (3595)
2: TEST [1][19/24]	Time 0.4485 (0.6497)	Decoder iters 149.0 (127.8)	Tok/s 2319 (3400)
1: TEST [1][19/24]	Time 0.4485 (0.6497)	Decoder iters 34.0 (123.1)	Tok/s 2323 (3451)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
1: Finished evaluation on test set
2: Finished evaluation on test set
3: Finished evaluation on test set
0: Finished evaluation on test set
3: Finished epoch 1
1: Finished epoch 1
2: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 5.0781	Validation Loss: 5.6136	Test BLEU: 3.63
0: Performance: Epoch: 1	Training: 42250 Tok/s	Validation: 165872 Tok/s
0: Finished epoch 1
3: Total training time 396 s
1: Total training time 396 s
0: Total training time 396 s
2: Total training time 396 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP32 (BLEU)**|**Throughput - FP32 (tok/s)**|**Time to Train - FP32 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       4|                 108|                      3.63|                      42261.9|                         6.599|
DONE!
