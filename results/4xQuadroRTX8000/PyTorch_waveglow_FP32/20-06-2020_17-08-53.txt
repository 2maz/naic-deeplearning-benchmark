:::NVLOGv0.2.2 Tacotron2_PyT 1592672935.625948429 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592672935.654385567 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592672935.680685282 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592672936.609486580 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 4, "name": ["Quadro RTX 8000", "Quadro RTX 8000", "Quadro RTX 8000", "Quadro RTX 8000"], "mem": ["48601 MiB", "48601 MiB", "48601 MiB", "48601 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592672936.614629745 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "WaveGlow", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": true, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 0.0, "grad_clip_thresh": 65504.0, "batch_size": 26, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_625_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 4, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "n_mel_channels": 80, "flows": 12, "groups": 8, "early_every": 4, "early_size": 2, "sigma": 1.0, "segment_length": 8000, "wn_kernel_size": 3, "wn_channels": 512, "wn_layers": 8}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592672937.249364376 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592672965.423432589 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592672965.426074505 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672966.140563726 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672970.361973524 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002536799293011427
:::NVLOGv0.2.2 Tacotron2_PyT 1592672974.907333136 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672974.907790422 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 94878.09761207776
:::NVLOGv0.2.2 Tacotron2_PyT 1592672974.908144712 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 8.769147157669067
Batch: 1/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672974.911230326 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592672976.335011959 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019645937718451023
:::NVLOGv0.2.2 Tacotron2_PyT 1592672979.591921806 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592672979.592444658 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177719.8129228099
:::NVLOGv0.2.2 Tacotron2_PyT 1592672979.592802286 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.68152642250061
Batch: 2/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672979.595843315 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592672980.992480993 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002297966741025448
:::NVLOGv0.2.2 Tacotron2_PyT 1592672984.260952234 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592672984.261347771 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 178324.5859605976
:::NVLOGv0.2.2 Tacotron2_PyT 1592672984.261708975 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.6656494140625
Batch: 3/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672984.264336824 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592672985.697167873 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0022841605823487043
:::NVLOGv0.2.2 Tacotron2_PyT 1592672988.969464779 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592672988.969879150 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176807.1319476691
:::NVLOGv0.2.2 Tacotron2_PyT 1592672988.970216036 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.705692529678345
Batch: 4/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672988.972891331 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592672990.384804249 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002060080412775278
:::NVLOGv0.2.2 Tacotron2_PyT 1592672993.668420553 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592672993.668805361 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177173.7037100783
:::NVLOGv0.2.2 Tacotron2_PyT 1592672993.669151783 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.695956468582153
Batch: 5/6 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672993.672095299 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592672995.083299160 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002260969951748848
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.371666908 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.372173786 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 177014.80956453728
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.372506857 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.700171709060669
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.573874474 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.574259043 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 150593.687120667
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.574599266 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 163653.0236196283
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.574944973 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.0022340951254591346
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.575305462 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 33.148799896240234
:::NVLOGv0.2.2 Tacotron2_PyT 1592672998.575642109 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592673001.601536036 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.002325573703274131
:::NVLOGv0.2.2 Tacotron2_PyT 1592673001.602480412 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_WaveGlow_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592673010.399966717 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673010.639170408 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592673012.041131735 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023062049876898527
:::NVLOGv0.2.2 Tacotron2_PyT 1592673015.371885300 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592673015.372485161 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175738.98261314307
:::NVLOGv0.2.2 Tacotron2_PyT 1592673015.372981548 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.7342939376831055
Batch: 1/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673015.376712561 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673016.762683392 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002209785860031843
:::NVLOGv0.2.2 Tacotron2_PyT 1592673020.096769333 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673020.097360849 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 176234.87391719915
:::NVLOGv0.2.2 Tacotron2_PyT 1592673020.097857237 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.720972537994385
Batch: 2/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673020.100921631 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592673021.499893665 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023137410171329975
:::NVLOGv0.2.2 Tacotron2_PyT 1592673024.841036081 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592673024.841631413 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175490.88175940016
:::NVLOGv0.2.2 Tacotron2_PyT 1592673024.842133760 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.740987062454224
Batch: 3/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673024.845146894 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592673026.236322165 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0019367870409041643
:::NVLOGv0.2.2 Tacotron2_PyT 1592673029.577289820 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592673029.577874184 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175787.4331004637
:::NVLOGv0.2.2 Tacotron2_PyT 1592673029.578370094 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.732989072799683
Batch: 4/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673029.581127882 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592673030.966804743 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.002461852505803108
:::NVLOGv0.2.2 Tacotron2_PyT 1592673034.312296867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 4
:::NVLOGv0.2.2 Tacotron2_PyT 1592673034.312738657 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 175830.6033525639
:::NVLOGv0.2.2 Tacotron2_PyT 1592673034.313069105 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.731827020645142
Batch: 5/6 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673034.315099239 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592673035.727658749 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 0.0023367186076939106
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.076314211 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 5
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.077099323 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 174721.76875264675
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.077619553 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.761856555938721
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.120403528 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.120976686 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 173809.99760103956
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.121495962 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 175634.09058256945
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.121988535 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 0.002260848336542646
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.122481346 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 28.721017599105835
:::NVLOGv0.2.2 Tacotron2_PyT 1592673039.122963190 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673040.737690449 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 0.001920113805681467
:::NVLOGv0.2.2 Tacotron2_PyT 1592673040.738661289 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592673040.739849329 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 103.4898293018341
:::NVLOGv0.2.2 Tacotron2_PyT 1592673040.740265131 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 103.4898293018341
:::NVLOGv0.2.2 Tacotron2_PyT 1592673040.740649462 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 105.19807243347168
:::NVLOGv0.2.2 Tacotron2_PyT 1592673040.740982294 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
