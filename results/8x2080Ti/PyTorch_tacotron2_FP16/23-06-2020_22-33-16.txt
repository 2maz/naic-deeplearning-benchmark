:::NVLOGv0.2.2 Tacotron2_PyT 1592951598.514951468 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592951598.539453983 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592951598.561801910 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592951603.158280373 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592951603.167035103 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 72, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_2500_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592951606.005887270 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1592951626.491897583 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592951626.494086742 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951632.883525372 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951643.774524927 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.22773742675781
:::NVLOGv0.2.2 Tacotron2_PyT 1592951646.028541327 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951646.028997421 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 24895.232456631704
:::NVLOGv0.2.2 Tacotron2_PyT 1592951646.029351950 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 13.147979259490967
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951646.036817312 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951647.689590931 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.213314056396484
:::NVLOGv0.2.2 Tacotron2_PyT 1592951651.286740065 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951651.288221598 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 62183.366386848116
:::NVLOGv0.2.2 Tacotron2_PyT 1592951651.289599419 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.25082540512085
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951651.305118799 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592951655.239868402 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.50006103515625
:::NVLOGv0.2.2 Tacotron2_PyT 1592951657.954740047 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592951657.958972454 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 48523.40244112904
:::NVLOGv0.2.2 Tacotron2_PyT 1592951657.959851265 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.650481700897217
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951657.967238426 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592951661.578183413 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.0339469909668
:::NVLOGv0.2.2 Tacotron2_PyT 1592951663.851225376 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592951663.853718519 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 55387.20810634185
:::NVLOGv0.2.2 Tacotron2_PyT 1592951663.856227398 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.8846259117126465
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592951664.011163473 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951664.012932062 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 34715.77527826672
:::NVLOGv0.2.2 Tacotron2_PyT 1592951664.014768600 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 47747.30234773768
:::NVLOGv0.2.2 Tacotron2_PyT 1592951664.015356779 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 46.993764877319336
:::NVLOGv0.2.2 Tacotron2_PyT 1592951664.015878916 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 37.5181884765625
:::NVLOGv0.2.2 Tacotron2_PyT 1592951664.016390562 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592951665.953237057 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.38835144042969
:::NVLOGv0.2.2 Tacotron2_PyT 1592951665.954456568 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951666.233355045 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951668.187379122 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951669.806718111 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.435970306396484
:::NVLOGv0.2.2 Tacotron2_PyT 1592951672.942065477 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592951672.944424629 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 67672.8122998371
:::NVLOGv0.2.2 Tacotron2_PyT 1592951672.946607828 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.757641792297363
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951672.960079908 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951676.299830675 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.46319580078125
:::NVLOGv0.2.2 Tacotron2_PyT 1592951679.121689081 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951679.124014139 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 53293.365723118346
:::NVLOGv0.2.2 Tacotron2_PyT 1592951679.126676798 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 6.162718296051025
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951679.137366295 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592951682.562900305 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.887168884277344
:::NVLOGv0.2.2 Tacotron2_PyT 1592951684.886551857 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592951684.889447927 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56431.10332245528
:::NVLOGv0.2.2 Tacotron2_PyT 1592951684.892194510 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.749807834625244
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951684.905489922 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592951686.525887728 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.750308990478516
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.417967558 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.419583559 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 71288.9850066008
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.421739101 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.5131516456604
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.588063478 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.589563370 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 55516.531081624555
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.590970039 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 62171.56658800288
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.591673374 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 46.8841609954834
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.592204571 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 23.355223655700684
:::NVLOGv0.2.2 Tacotron2_PyT 1592951689.592714071 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951691.425160170 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.41529846191406
:::NVLOGv0.2.2 Tacotron2_PyT 1592951691.428638220 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592951691.432945967 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 85.4255313873291
:::NVLOGv0.2.2 Tacotron2_PyT 1592951691.434194326 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 85.4255313873291
:::NVLOGv0.2.2 Tacotron2_PyT 1592951691.435496092 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 93.02575540542603
:::NVLOGv0.2.2 Tacotron2_PyT 1592951691.435879946 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
