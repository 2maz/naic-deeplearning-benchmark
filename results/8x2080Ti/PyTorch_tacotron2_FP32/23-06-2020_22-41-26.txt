:::NVLOGv0.2.2 Tacotron2_PyT 1592952089.209924936 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592952089.238327980 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 80, "name": "Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592952089.257297754 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "692G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1592952093.848856688 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.82", "num": 8, "name": ["GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti", "GeForce RTX 2080 Ti"], "mem": ["11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB", "11019 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1592952093.856728077 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 2, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": false, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 36, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1592952096.643642902 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1592952120.501094580 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1592952120.514449358 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952123.749528885 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952134.160962343 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.019466400146484
:::NVLOGv0.2.2 Tacotron2_PyT 1592952135.839334249 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952135.839884520 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 13521.204866438546
:::NVLOGv0.2.2 Tacotron2_PyT 1592952135.840262413 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 12.091230154037476
Batch: 1/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952135.846654177 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952136.976366520 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.5318489074707
:::NVLOGv0.2.2 Tacotron2_PyT 1592952138.576597691 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952138.578188181 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59798.263053573646
:::NVLOGv0.2.2 Tacotron2_PyT 1592952138.578931570 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.730848550796509
Batch: 2/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952138.585305452 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592952139.608013868 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.1013298034668
:::NVLOGv0.2.2 Tacotron2_PyT 1592952141.365962982 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592952141.367196321 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 58578.80614186969
:::NVLOGv0.2.2 Tacotron2_PyT 1592952141.368989229 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.781381368637085
Batch: 3/4 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952141.373955727 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592952142.547343969 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.743778228759766
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.304845333 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.306029081 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 55512.11260758018
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.309154510 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.9313602447509766
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.424301147 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.425510406 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 27273.97596382545
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.426200867 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 46852.59666736551
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.426578760 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.84910583496094
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.426947355 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 23.92185139656067
:::NVLOGv0.2.2 Tacotron2_PyT 1592952144.427322626 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1592952145.762035608 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.26586151123047
:::NVLOGv0.2.2 Tacotron2_PyT 1592952145.763220787 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952146.437461853 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952147.520128250 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952148.646320581 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.25425338745117
:::NVLOGv0.2.2 Tacotron2_PyT 1592952150.199675322 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1592952150.201771736 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 59485.92060212006
:::NVLOGv0.2.2 Tacotron2_PyT 1592952150.203398228 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.682046413421631
Batch: 1/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952150.213829517 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952151.319461584 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.128482818603516
:::NVLOGv0.2.2 Tacotron2_PyT 1592952153.038434744 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952153.041554928 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56886.06298771667
:::NVLOGv0.2.2 Tacotron2_PyT 1592952153.044307470 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.8253493309020996
Batch: 2/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952153.053721666 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592952154.185225248 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 48.535133361816406
:::NVLOGv0.2.2 Tacotron2_PyT 1592952155.704632044 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1592952155.707866430 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 62888.681032823966
:::NVLOGv0.2.2 Tacotron2_PyT 1592952155.709262133 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.65157413482666
Batch: 3/4 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952155.716330290 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592952156.771952152 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.95555877685547
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.636933804 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 3
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.640544176 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 56677.13779531351
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.642904282 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 2.9213013648986816
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.782443523 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.784045935 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 52859.91919494589
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.785311460 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 58984.450604493555
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.786512375 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.71835708618164
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.787736416 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 12.345686674118042
:::NVLOGv0.2.2 Tacotron2_PyT 1592952158.788904667 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952160.062048674 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.266719818115234
:::NVLOGv0.2.2 Tacotron2_PyT 1592952160.063401937 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1592952160.064631224 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 63.420318841934204
:::NVLOGv0.2.2 Tacotron2_PyT 1592952160.065002918 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 63.420318841934204
:::NVLOGv0.2.2 Tacotron2_PyT 1592952160.065393925 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 70.9541130065918
:::NVLOGv0.2.2 Tacotron2_PyT 1592952160.065731049 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
