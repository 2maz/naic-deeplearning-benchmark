3: Collecting environment information...
6: Collecting environment information...
2: Collecting environment information...
4: Collecting environment information...
5: Collecting environment information...
1: Collecting environment information...
7: Collecting environment information...
0: Collecting environment information...
3: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
3: Saving results to: results/gnmt
3: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=3, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=3, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
3: Using master seed from command line: 2
2: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
2: Saving results to: results/gnmt
2: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=2, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=2, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
2: Using master seed from command line: 2
4: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] magma-cuda101             2.5.2                         1    local
[conda] mkl                       2019.1                      144  
[conda] mkl-include               2019.1                      144  
[conda] nomkl                     3.0                           0  
[conda] pytorch-transformers      1.1.0                    pypi_0    pypi
[conda] torch                     1.4.0a0+a5b4d78          pypi_0    pypi
[conda] torchtext                 0.4.0                    pypi_0    pypi
[conda] torchvision               0.5.0a0                  pypi_0    pypi
4: Saving results to: results/gnmt
4: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=4, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=4, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
4: Using master seed from command line: 2
5: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] Could not collect
5: Saving results to: results/gnmt
5: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=5, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=5, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
5: Using master seed from command line: 2
1: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] Could not collect
1: Saving results to: results/gnmt
0: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] Could not collect
0: Saving results to: results/gnmt
1: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=1, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=1, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
1: Using master seed from command line: 2
0: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=0, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=0, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
0: Using master seed from command line: 2
6: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] Could not collect
6: Saving results to: results/gnmt
6: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=6, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=6, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
6: Using master seed from command line: 2
7: PyTorch version: 1.4.0a0+a5b4d78
Is debug build: No
CUDA used to build PyTorch: 10.2

OS: Ubuntu 18.04.3 LTS
GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0
CMake version: version 3.14.0

Python version: 3.6
Is CUDA available: Yes
CUDA runtime version: 10.2.89
GPU models and configuration: 
GPU 0: GeForce RTX 2080 Ti
GPU 1: GeForce RTX 2080 Ti
GPU 2: GeForce RTX 2080 Ti
GPU 3: GeForce RTX 2080 Ti
GPU 4: GeForce RTX 2080 Ti
GPU 5: GeForce RTX 2080 Ti
GPU 6: GeForce RTX 2080 Ti
GPU 7: GeForce RTX 2080 Ti

Nvidia driver version: 440.82
cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.6.5

Versions of relevant libraries:
[pip] msgpack-numpy==0.4.3.2
[pip] numpy==1.17.4
[pip] pytorch-transformers==1.1.0
[pip] torch==1.4.0a0+a5b4d78
[pip] torchtext==0.4.0
[pip] torchvision==0.5.0a0
[conda] Could not collect
7: Saving results to: results/gnmt
7: Run arguments: Namespace(batching='bucketing', beam_size=5, bpe_codes='/data/gnmt/wmt16_de_en/bpe.32000', cov_penalty_factor=0.1, cuda=True, cudnn=True, dataset_dir='/data/gnmt/wmt16_de_en', decay_factor=0.5, decay_interval=None, decay_steps=4, dropout=0.2, env=True, epochs=2, eval=True, grad_clip=5.0, hidden_size=1024, init_scale=8192, intra_epoch_eval=0, keep_checkpoints=0, lang={'src': 'en', 'tgt': 'de'}, len_norm_const=5.0, len_norm_factor=0.6, local_rank=7, log_all_ranks=True, lr=0.002, math='fp16', num_buckets=5, num_layers=4, optimizer='Adam', optimizer_extra='{}', prealloc_mode='always', print_freq=10, rank=7, remain_steps=0.666, results_dir='results', resume=None, save_all=False, save_dir='results/gnmt', save_freq=5000, seed=2, shard_size=80, share_embedding=True, smoothing=0.1, src_lang='en', start_epoch=0, target_bleu=None, target_perf=None, test_batch_size=32, test_loader_workers=0, test_max_length=150, test_min_length=0, test_src='/data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en', test_tgt='/data/gnmt/wmt16_de_en/newstest2014.de', tgt_lang='de', train_batch_size=160, train_global_batch_size=None, train_iter_size=1, train_loader_workers=2, train_max_length=50, train_max_size=None, train_min_length=0, train_src='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en', train_tgt='/data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de', upscale_interval=128, val_batch_size=32, val_loader_workers=0, val_max_length=125, val_min_length=0, val_src='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en', val_tgt='/data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de', vocab='/data/gnmt/wmt16_de_en/vocab.bpe.32000', warmup_steps=200)
7: Using master seed from command line: 2
0: Worker 0 is using worker seed: 242886303
1: Worker 1 is using worker seed: 364522461
2: Worker 2 is using worker seed: 3588440356
3: Worker 3 is using worker seed: 1323436024
6: Worker 6 is using worker seed: 4077622522
4: Worker 4 is using worker seed: 2602510382
7: Worker 7 is using worker seed: 117874757
5: Worker 5 is using worker seed: 2606193617
0: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
0: Size of vocabulary: 31800
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
3: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
1: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
5: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
7: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
2: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
6: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
4: Building vocabulary from /data/gnmt/wmt16_de_en/vocab.bpe.32000
3: Size of vocabulary: 31800
1: Size of vocabulary: 31800
5: Size of vocabulary: 31800
7: Size of vocabulary: 31800
2: Size of vocabulary: 31800
6: Size of vocabulary: 31800
4: Size of vocabulary: 31800
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
5: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/train.tok.clean.bpe.32000.de
0: Filtering data, min len: 0, max len: 50
7: Filtering data, min len: 0, max len: 50
5: Filtering data, min len: 0, max len: 50
3: Filtering data, min len: 0, max len: 50
1: Filtering data, min len: 0, max len: 50
2: Filtering data, min len: 0, max len: 50
4: Filtering data, min len: 0, max len: 50
6: Filtering data, min len: 0, max len: 50
0: Pairs before: 160078, after: 148120
5: Pairs before: 160078, after: 148120
1: Pairs before: 160078, after: 148120
7: Pairs before: 160078, after: 148120
4: Pairs before: 160078, after: 148120
6: Pairs before: 160078, after: 148120
2: Pairs before: 160078, after: 148120
3: Pairs before: 160078, after: 148120
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.en
5: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
7: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
1: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
0: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
4: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
2: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
6: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
3: Processing data from /data/gnmt/wmt16_de_en/newstest_dev.tok.clean.bpe.32000.de
5: Filtering data, min len: 0, max len: 125
7: Filtering data, min len: 0, max len: 125
5: Pairs before: 5100, after: 5100
7: Pairs before: 5100, after: 5100
1: Filtering data, min len: 0, max len: 125
0: Filtering data, min len: 0, max len: 125
1: Pairs before: 5100, after: 5100
4: Filtering data, min len: 0, max len: 125
2: Filtering data, min len: 0, max len: 125
6: Filtering data, min len: 0, max len: 125
3: Filtering data, min len: 0, max len: 125
0: Pairs before: 5100, after: 5100
4: Pairs before: 5100, after: 5100
2: Pairs before: 5100, after: 5100
6: Pairs before: 5100, after: 5100
3: Pairs before: 5100, after: 5100
5: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
5: Filtering data, min len: 0, max len: 150
5: Pairs before: 3003, after: 3003
0: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Filtering data, min len: 0, max len: 150
4: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
6: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
7: Pairs before: 3003, after: 3003
2: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
3: Processing data from /data/gnmt/wmt16_de_en/newstest2014.tok.bpe.32000.en
1: Filtering data, min len: 0, max len: 150
1: Pairs before: 3003, after: 3003
0: Filtering data, min len: 0, max len: 150
4: Filtering data, min len: 0, max len: 150
0: Pairs before: 3003, after: 3003
6: Filtering data, min len: 0, max len: 150
4: Pairs before: 3003, after: 3003
2: Filtering data, min len: 0, max len: 150
6: Pairs before: 3003, after: 3003
3: Filtering data, min len: 0, max len: 150
2: Pairs before: 3003, after: 3003
3: Pairs before: 3003, after: 3003
5: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
5: Building LabelSmoothingLoss (smoothing: 0.1)
5: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
5: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
5: Number of parameters: 159605817
5: Saving state of the tokenizer
5: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
5: Scheduler warmup steps: 200
5: Scheduler remain steps: 150
5: Scheduler decay interval: 19
5: Scheduler decay factor: 0.5
5: Scheduler max decay steps: 4
5: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
5: Initializing amp optimizer
5: Starting epoch 0
5: Executing preallocation
7: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
7: Building LabelSmoothingLoss (smoothing: 0.1)
7: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
7: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
7: Number of parameters: 159605817
7: Saving state of the tokenizer
7: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
7: Scheduler warmup steps: 200
7: Scheduler remain steps: 150
7: Scheduler decay interval: 19
7: Scheduler decay factor: 0.5
7: Scheduler max decay steps: 4
7: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
7: Initializing amp optimizer
7: Starting epoch 0
7: Executing preallocation
4: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
4: Building LabelSmoothingLoss (smoothing: 0.1)
4: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
4: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
4: Number of parameters: 159605817
4: Saving state of the tokenizer
4: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
4: Scheduler warmup steps: 200
4: Scheduler remain steps: 150
4: Scheduler decay interval: 19
4: Scheduler decay factor: 0.5
4: Scheduler max decay steps: 4
4: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
4: Initializing amp optimizer
4: Starting epoch 0
4: Executing preallocation
2: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
2: Building LabelSmoothingLoss (smoothing: 0.1)
2: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
2: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
2: Number of parameters: 159605817
2: Saving state of the tokenizer
2: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
2: Scheduler warmup steps: 200
2: Scheduler remain steps: 150
2: Scheduler decay interval: 19
2: Scheduler decay factor: 0.5
2: Scheduler max decay steps: 4
2: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
2: Initializing amp optimizer
2: Starting epoch 0
2: Executing preallocation
0: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
0: Building LabelSmoothingLoss (smoothing: 0.1)
0: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
0: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
0: Number of parameters: 159605817
0: Saving state of the tokenizer
0: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
0: Scheduler warmup steps: 200
0: Scheduler remain steps: 150
0: Scheduler decay interval: 19
0: Scheduler decay factor: 0.5
0: Scheduler max decay steps: 4
0: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.

Defaults for this optimization level are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : True
master_weights         : True
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O2
cast_model_type        : torch.float16
patch_torch_functions  : False
keep_batchnorm_fp32    : False
master_weights         : True
loss_scale             : dynamic
0: Initializing amp optimizer
0: Starting epoch 0
0: Executing preallocation
6: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
6: Building LabelSmoothingLoss (smoothing: 0.1)
6: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
6: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
6: Number of parameters: 159605817
6: Saving state of the tokenizer
6: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
6: Scheduler warmup steps: 200
6: Scheduler remain steps: 150
6: Scheduler decay interval: 19
6: Scheduler decay factor: 0.5
6: Scheduler max decay steps: 4
6: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
6: Initializing amp optimizer
6: Starting epoch 0
6: Executing preallocation
3: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
3: Building LabelSmoothingLoss (smoothing: 0.1)
3: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
3: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
3: Number of parameters: 159605817
3: Saving state of the tokenizer
3: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
3: Scheduler warmup steps: 200
3: Scheduler remain steps: 150
3: Scheduler decay interval: 19
3: Scheduler decay factor: 0.5
3: Scheduler max decay steps: 4
3: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
3: Initializing amp optimizer
3: Starting epoch 0
3: Executing preallocation
1: GNMT(
  (encoder): ResidualRecurrentEncoder(
    (rnn_layers): ModuleList(
      (0): LSTM(1024, 1024, bidirectional=True)
      (1): LSTM(2048, 1024)
      (2): LSTM(1024, 1024)
      (3): LSTM(1024, 1024)
    )
    (dropout): Dropout(p=0.2, inplace=False)
    (embedder): Embedding(31800, 1024, padding_idx=0)
  )
  (decoder): ResidualRecurrentDecoder(
    (att_rnn): RecurrentAttention(
      (rnn): LSTM(1024, 1024)
      (attn): BahdanauAttention(
        (linear_q): Linear(in_features=1024, out_features=1024, bias=False)
        (linear_k): Linear(in_features=1024, out_features=1024, bias=False)
      )
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (rnn_layers): ModuleList(
      (0): LSTM(2048, 1024)
      (1): LSTM(2048, 1024)
      (2): LSTM(2048, 1024)
    )
    (embedder): Embedding(31800, 1024, padding_idx=0)
    (classifier): Classifier(
      (classifier): Linear(in_features=1024, out_features=31800, bias=True)
    )
    (dropout): Dropout(p=0.2, inplace=False)
  )
)
1: Building LabelSmoothingLoss (smoothing: 0.1)
1: Training optimizer config: {'optimizer': 'Adam', 'lr': 0.002}
1: Training LR schedule config: {'warmup_steps': 200, 'remain_steps': 0.666, 'decay_interval': None, 'decay_steps': 4, 'decay_factor': 0.5}
1: Number of parameters: 159605817
1: Saving state of the tokenizer
1: Using optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.002
    weight_decay: 0
)
1: Scheduler warmup steps: 200
1: Scheduler remain steps: 150
1: Scheduler decay interval: 19
1: Scheduler decay factor: 0.5
1: Scheduler max decay steps: 4
1: warmup_steps should not be larger than remain_steps, setting warmup_steps=remain_steps
1: Initializing amp optimizer
1: Starting epoch 0
1: Executing preallocation
0: Sampler for epoch 0 uses seed 1632151663
1: Sampler for epoch 0 uses seed 1632151663
6: Sampler for epoch 0 uses seed 1632151663
5: Sampler for epoch 0 uses seed 1632151663
7: Sampler for epoch 0 uses seed 1632151663
2: Sampler for epoch 0 uses seed 1632151663
3: Sampler for epoch 0 uses seed 1632151663
4: Sampler for epoch 0 uses seed 1632151663
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
6: TRAIN [0][0/113]	Time 0.544 (0.544)	Data 1.67e-01 (1.67e-01)	Tok/s 13145 (13145)	Loss/tok 10.5966 (10.5966)	LR 2.062e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
/opt/conda/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  "https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate", UserWarning)
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0
2: TRAIN [0][0/113]	Time 0.545 (0.545)	Data 1.65e-01 (1.65e-01)	Tok/s 13246 (13246)	Loss/tok 10.5937 (10.5937)	LR 2.062e-05
7: TRAIN [0][0/113]	Time 0.545 (0.545)	Data 1.62e-01 (1.62e-01)	Tok/s 13087 (13087)	Loss/tok 10.6177 (10.6177)	LR 2.062e-05
5: TRAIN [0][0/113]	Time 0.546 (0.546)	Data 1.60e-01 (1.60e-01)	Tok/s 13504 (13504)	Loss/tok 10.5953 (10.5953)	LR 2.062e-05
4: TRAIN [0][0/113]	Time 0.546 (0.546)	Data 2.01e-01 (2.01e-01)	Tok/s 13390 (13390)	Loss/tok 10.6075 (10.6075)	LR 2.062e-05
0: TRAIN [0][0/113]	Time 0.545 (0.545)	Data 1.40e-01 (1.40e-01)	Tok/s 13167 (13167)	Loss/tok 10.5957 (10.5957)	LR 2.062e-05
3: TRAIN [0][0/113]	Time 0.546 (0.546)	Data 1.65e-01 (1.65e-01)	Tok/s 13289 (13289)	Loss/tok 10.6108 (10.6108)	LR 2.062e-05
1: TRAIN [0][0/113]	Time 0.546 (0.546)	Data 1.52e-01 (1.52e-01)	Tok/s 13387 (13387)	Loss/tok 10.6053 (10.6053)	LR 2.062e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0
5: TRAIN [0][10/113]	Time 0.413 (0.401)	Data 1.21e-04 (1.46e-02)	Tok/s 24531 (19005)	Loss/tok 9.7929 (10.1748)	LR 2.803e-05
3: TRAIN [0][10/113]	Time 0.414 (0.401)	Data 1.45e-04 (1.51e-02)	Tok/s 24137 (18988)	Loss/tok 9.8346 (10.1826)	LR 2.803e-05
0: TRAIN [0][10/113]	Time 0.415 (0.401)	Data 1.41e-04 (1.29e-02)	Tok/s 24237 (18892)	Loss/tok 9.8340 (10.1753)	LR 2.803e-05
4: TRAIN [0][10/113]	Time 0.414 (0.401)	Data 1.58e-04 (1.84e-02)	Tok/s 24063 (18867)	Loss/tok 9.7661 (10.1685)	LR 2.803e-05
1: TRAIN [0][10/113]	Time 0.415 (0.401)	Data 1.40e-04 (1.40e-02)	Tok/s 24285 (18910)	Loss/tok 9.8029 (10.1823)	LR 2.803e-05
6: TRAIN [0][10/113]	Time 0.414 (0.401)	Data 2.75e-04 (1.53e-02)	Tok/s 24027 (18843)	Loss/tok 9.8049 (10.1862)	LR 2.803e-05
7: TRAIN [0][10/113]	Time 0.414 (0.401)	Data 1.18e-04 (1.48e-02)	Tok/s 24315 (18906)	Loss/tok 9.8006 (10.1757)	LR 2.803e-05
2: TRAIN [0][10/113]	Time 0.415 (0.401)	Data 1.93e-04 (1.52e-02)	Tok/s 24235 (18857)	Loss/tok 9.7965 (10.1737)	LR 2.803e-05
3: TRAIN [0][20/113]	Time 0.413 (0.396)	Data 1.25e-04 (8.00e-03)	Tok/s 24668 (19487)	Loss/tok 9.2673 (9.8191)	LR 3.811e-05
1: TRAIN [0][20/113]	Time 0.414 (0.396)	Data 1.39e-04 (7.38e-03)	Tok/s 24364 (19443)	Loss/tok 9.3850 (9.8292)	LR 3.811e-05
0: TRAIN [0][20/113]	Time 0.414 (0.396)	Data 1.54e-04 (6.83e-03)	Tok/s 24028 (19413)	Loss/tok 9.2871 (9.8221)	LR 3.811e-05
5: TRAIN [0][20/113]	Time 0.413 (0.396)	Data 1.21e-04 (7.72e-03)	Tok/s 24561 (19509)	Loss/tok 9.3168 (9.8180)	LR 3.811e-05
7: TRAIN [0][20/113]	Time 0.414 (0.396)	Data 1.35e-04 (7.83e-03)	Tok/s 24275 (19418)	Loss/tok 9.2907 (9.8177)	LR 3.811e-05
4: TRAIN [0][20/113]	Time 0.413 (0.396)	Data 1.51e-04 (9.71e-03)	Tok/s 24332 (19357)	Loss/tok 9.3373 (9.8187)	LR 3.811e-05
6: TRAIN [0][20/113]	Time 0.413 (0.396)	Data 2.15e-04 (8.09e-03)	Tok/s 24357 (19361)	Loss/tok 9.3087 (9.8232)	LR 3.811e-05
2: TRAIN [0][20/113]	Time 0.414 (0.396)	Data 1.43e-04 (8.02e-03)	Tok/s 24086 (19396)	Loss/tok 9.3330 (9.8160)	LR 3.811e-05
5: TRAIN [0][30/113]	Time 0.374 (0.393)	Data 1.18e-04 (5.28e-03)	Tok/s 19317 (19556)	Loss/tok 8.8274 (9.5637)	LR 5.180e-05
3: TRAIN [0][30/113]	Time 0.375 (0.393)	Data 1.40e-04 (5.46e-03)	Tok/s 19468 (19576)	Loss/tok 8.8135 (9.5674)	LR 5.180e-05
7: TRAIN [0][30/113]	Time 0.374 (0.393)	Data 1.51e-04 (5.34e-03)	Tok/s 19465 (19481)	Loss/tok 8.7790 (9.5649)	LR 5.180e-05
4: TRAIN [0][30/113]	Time 0.375 (0.393)	Data 1.56e-04 (6.62e-03)	Tok/s 19673 (19443)	Loss/tok 8.8543 (9.5624)	LR 5.180e-05
1: TRAIN [0][30/113]	Time 0.375 (0.393)	Data 1.39e-04 (5.05e-03)	Tok/s 19026 (19507)	Loss/tok 8.8046 (9.5667)	LR 5.180e-05
0: TRAIN [0][30/113]	Time 0.375 (0.393)	Data 1.70e-04 (4.68e-03)	Tok/s 19201 (19504)	Loss/tok 8.8233 (9.5670)	LR 5.180e-05
2: TRAIN [0][30/113]	Time 0.375 (0.393)	Data 1.66e-04 (5.49e-03)	Tok/s 19209 (19442)	Loss/tok 8.8492 (9.5569)	LR 5.180e-05
6: TRAIN [0][30/113]	Time 0.374 (0.393)	Data 1.82e-04 (5.54e-03)	Tok/s 19533 (19465)	Loss/tok 8.8623 (9.5669)	LR 5.180e-05
5: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 1.17e-04 (4.02e-03)	Tok/s 18994 (19861)	Loss/tok 8.4685 (9.3355)	LR 7.042e-05
3: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 1.24e-04 (4.16e-03)	Tok/s 19462 (19889)	Loss/tok 8.4689 (9.3369)	LR 7.042e-05
6: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 1.35e-04 (4.23e-03)	Tok/s 19097 (19837)	Loss/tok 8.5465 (9.3402)	LR 7.042e-05
0: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 2.11e-04 (3.57e-03)	Tok/s 19321 (19836)	Loss/tok 8.4666 (9.3377)	LR 7.042e-05
7: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 1.12e-04 (4.07e-03)	Tok/s 19244 (19835)	Loss/tok 8.4452 (9.3382)	LR 7.042e-05
4: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 1.39e-04 (5.04e-03)	Tok/s 19229 (19815)	Loss/tok 8.4559 (9.3387)	LR 7.042e-05
1: TRAIN [0][40/113]	Time 0.377 (0.393)	Data 1.55e-04 (3.85e-03)	Tok/s 19098 (19856)	Loss/tok 8.4444 (9.3421)	LR 7.042e-05
2: TRAIN [0][40/113]	Time 0.376 (0.393)	Data 1.55e-04 (4.19e-03)	Tok/s 19127 (19783)	Loss/tok 8.4350 (9.3294)	LR 7.042e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
5: TRAIN [0][50/113]	Time 0.455 (0.388)	Data 1.72e-04 (3.26e-03)	Tok/s 28767 (19402)	Loss/tok 8.8279 (9.2643)	LR 9.573e-05
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0
7: TRAIN [0][50/113]	Time 0.459 (0.388)	Data 1.70e-04 (3.30e-03)	Tok/s 28336 (19384)	Loss/tok 8.8918 (9.2633)	LR 9.573e-05
4: TRAIN [0][50/113]	Time 0.459 (0.388)	Data 1.32e-04 (4.08e-03)	Tok/s 28578 (19383)	Loss/tok 8.8518 (9.2697)	LR 9.573e-05
6: TRAIN [0][50/113]	Time 0.459 (0.388)	Data 1.31e-04 (3.43e-03)	Tok/s 28124 (19391)	Loss/tok 8.8015 (9.2663)	LR 9.573e-05
0: TRAIN [0][50/113]	Time 0.460 (0.388)	Data 1.33e-04 (2.90e-03)	Tok/s 28633 (19397)	Loss/tok 8.8322 (9.2621)	LR 9.573e-05
3: TRAIN [0][50/113]	Time 0.461 (0.388)	Data 1.22e-04 (3.37e-03)	Tok/s 28452 (19452)	Loss/tok 8.8742 (9.2655)	LR 9.573e-05
2: TRAIN [0][50/113]	Time 0.460 (0.388)	Data 1.42e-04 (3.40e-03)	Tok/s 28642 (19366)	Loss/tok 8.8232 (9.2517)	LR 9.573e-05
1: TRAIN [0][50/113]	Time 0.461 (0.388)	Data 1.79e-04 (3.13e-03)	Tok/s 28619 (19437)	Loss/tok 8.8405 (9.2645)	LR 9.573e-05
1: TRAIN [0][60/113]	Time 0.341 (0.386)	Data 1.35e-04 (2.64e-03)	Tok/s 12730 (19375)	Loss/tok 7.9014 (9.1089)	LR 1.301e-04
3: TRAIN [0][60/113]	Time 0.340 (0.386)	Data 1.27e-04 (2.84e-03)	Tok/s 12776 (19380)	Loss/tok 7.9048 (9.1058)	LR 1.301e-04
0: TRAIN [0][60/113]	Time 0.341 (0.386)	Data 1.31e-04 (2.45e-03)	Tok/s 12825 (19325)	Loss/tok 7.8715 (9.1028)	LR 1.301e-04
2: TRAIN [0][60/113]	Time 0.340 (0.386)	Data 1.39e-04 (2.87e-03)	Tok/s 12874 (19322)	Loss/tok 7.8152 (9.0946)	LR 1.301e-04
6: TRAIN [0][60/113]	Time 0.340 (0.386)	Data 1.30e-04 (2.89e-03)	Tok/s 12501 (19314)	Loss/tok 7.9025 (9.1083)	LR 1.301e-04
7: TRAIN [0][60/113]	Time 0.341 (0.386)	Data 1.08e-04 (2.78e-03)	Tok/s 12024 (19295)	Loss/tok 7.8273 (9.1085)	LR 1.301e-04
5: TRAIN [0][60/113]	Time 0.341 (0.386)	Data 1.36e-04 (2.75e-03)	Tok/s 12666 (19338)	Loss/tok 7.8097 (9.1075)	LR 1.301e-04
4: TRAIN [0][60/113]	Time 0.341 (0.386)	Data 1.40e-04 (3.44e-03)	Tok/s 12492 (19311)	Loss/tok 7.9191 (9.1146)	LR 1.301e-04
7: TRAIN [0][70/113]	Time 0.340 (0.387)	Data 1.18e-04 (2.40e-03)	Tok/s 12772 (19415)	Loss/tok 7.6995 (8.9493)	LR 1.769e-04
0: TRAIN [0][70/113]	Time 0.341 (0.387)	Data 1.27e-04 (2.12e-03)	Tok/s 12428 (19422)	Loss/tok 7.6736 (8.9455)	LR 1.769e-04
5: TRAIN [0][70/113]	Time 0.340 (0.387)	Data 1.14e-04 (2.38e-03)	Tok/s 12835 (19460)	Loss/tok 7.7220 (8.9481)	LR 1.769e-04
3: TRAIN [0][70/113]	Time 0.341 (0.387)	Data 1.21e-04 (2.46e-03)	Tok/s 12674 (19488)	Loss/tok 7.7422 (8.9453)	LR 1.769e-04
1: TRAIN [0][70/113]	Time 0.342 (0.387)	Data 1.36e-04 (2.29e-03)	Tok/s 12622 (19483)	Loss/tok 7.5887 (8.9486)	LR 1.769e-04
6: TRAIN [0][70/113]	Time 0.340 (0.387)	Data 1.39e-04 (2.50e-03)	Tok/s 12872 (19455)	Loss/tok 7.6450 (8.9455)	LR 1.769e-04
4: TRAIN [0][70/113]	Time 0.340 (0.387)	Data 1.40e-04 (2.97e-03)	Tok/s 12581 (19434)	Loss/tok 7.6345 (8.9519)	LR 1.769e-04
2: TRAIN [0][70/113]	Time 0.342 (0.387)	Data 1.43e-04 (2.49e-03)	Tok/s 12554 (19449)	Loss/tok 7.7182 (8.9324)	LR 1.769e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 128.0
1: TRAIN [0][80/113]	Time 0.351 (0.386)	Data 1.45e-04 (2.02e-03)	Tok/s 20403 (19495)	Loss/tok 8.0166 (8.8287)	LR 2.405e-04
0: TRAIN [0][80/113]	Time 0.351 (0.386)	Data 1.53e-04 (1.88e-03)	Tok/s 20592 (19433)	Loss/tok 8.0754 (8.8221)	LR 2.405e-04
3: TRAIN [0][80/113]	Time 0.351 (0.386)	Data 1.37e-04 (2.17e-03)	Tok/s 20485 (19508)	Loss/tok 7.9790 (8.8251)	LR 2.405e-04
7: TRAIN [0][80/113]	Time 0.352 (0.386)	Data 1.29e-04 (2.12e-03)	Tok/s 20538 (19436)	Loss/tok 7.9681 (8.8263)	LR 2.405e-04
2: TRAIN [0][80/113]	Time 0.352 (0.386)	Data 1.46e-04 (2.20e-03)	Tok/s 20387 (19447)	Loss/tok 7.9928 (8.8125)	LR 2.405e-04
5: TRAIN [0][80/113]	Time 0.352 (0.386)	Data 1.68e-04 (2.10e-03)	Tok/s 20791 (19477)	Loss/tok 8.1080 (8.8238)	LR 2.405e-04
6: TRAIN [0][80/113]	Time 0.352 (0.386)	Data 1.77e-04 (2.21e-03)	Tok/s 20448 (19472)	Loss/tok 8.0819 (8.8246)	LR 2.405e-04
4: TRAIN [0][80/113]	Time 0.352 (0.386)	Data 1.37e-04 (2.62e-03)	Tok/s 20658 (19460)	Loss/tok 7.9351 (8.8273)	LR 2.405e-04
5: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.18e-04 (1.88e-03)	Tok/s 12837 (19305)	Loss/tok 7.5273 (8.7245)	LR 3.269e-04
7: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.11e-04 (1.90e-03)	Tok/s 13123 (19267)	Loss/tok 7.4789 (8.7272)	LR 3.269e-04
6: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.27e-04 (1.99e-03)	Tok/s 12729 (19307)	Loss/tok 7.4947 (8.7233)	LR 3.269e-04
2: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.34e-04 (1.98e-03)	Tok/s 12703 (19274)	Loss/tok 7.5376 (8.7154)	LR 3.269e-04
1: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.73e-04 (1.82e-03)	Tok/s 12599 (19301)	Loss/tok 7.4589 (8.7255)	LR 3.269e-04
3: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.73e-04 (1.95e-03)	Tok/s 13085 (19320)	Loss/tok 7.4205 (8.7269)	LR 3.269e-04
0: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 1.77e-04 (1.69e-03)	Tok/s 13128 (19271)	Loss/tok 7.5090 (8.7207)	LR 3.269e-04
4: TRAIN [0][90/113]	Time 0.340 (0.385)	Data 2.13e-04 (2.36e-03)	Tok/s 12910 (19292)	Loss/tok 7.4694 (8.7253)	LR 3.269e-04
3: TRAIN [0][100/113]	Time 0.337 (0.386)	Data 1.87e-04 (1.77e-03)	Tok/s 12735 (19435)	Loss/tok 7.3043 (8.6581)	LR 4.443e-04
5: TRAIN [0][100/113]	Time 0.341 (0.386)	Data 1.19e-04 (1.71e-03)	Tok/s 12735 (19412)	Loss/tok 7.3922 (8.6592)	LR 4.443e-04
7: TRAIN [0][100/113]	Time 0.341 (0.386)	Data 1.35e-04 (1.72e-03)	Tok/s 12444 (19382)	Loss/tok 7.3382 (8.6607)	LR 4.443e-04
0: TRAIN [0][100/113]	Time 0.341 (0.386)	Data 1.23e-04 (1.53e-03)	Tok/s 12879 (19382)	Loss/tok 7.4425 (8.6529)	LR 4.443e-04
6: TRAIN [0][100/113]	Time 0.342 (0.386)	Data 1.28e-04 (1.80e-03)	Tok/s 12792 (19413)	Loss/tok 7.3473 (8.6556)	LR 4.443e-04
4: TRAIN [0][100/113]	Time 0.341 (0.386)	Data 1.35e-04 (2.14e-03)	Tok/s 12747 (19409)	Loss/tok 7.3728 (8.6558)	LR 4.443e-04
2: TRAIN [0][100/113]	Time 0.342 (0.386)	Data 1.40e-04 (1.80e-03)	Tok/s 12687 (19393)	Loss/tok 7.3493 (8.6491)	LR 4.443e-04
1: TRAIN [0][100/113]	Time 0.342 (0.386)	Data 1.36e-04 (1.65e-03)	Tok/s 12455 (19418)	Loss/tok 7.4169 (8.6586)	LR 4.443e-04
3: TRAIN [0][110/113]	Time 0.419 (0.386)	Data 5.27e-05 (1.63e-03)	Tok/s 23982 (19469)	Loss/tok 7.8701 (8.5722)	LR 6.040e-04
0: TRAIN [0][110/113]	Time 0.418 (0.386)	Data 5.34e-05 (1.41e-03)	Tok/s 23625 (19420)	Loss/tok 7.7627 (8.5632)	LR 6.040e-04
2: TRAIN [0][110/113]	Time 0.418 (0.386)	Data 5.75e-05 (1.66e-03)	Tok/s 24488 (19430)	Loss/tok 7.8179 (8.5613)	LR 6.040e-04
6: TRAIN [0][110/113]	Time 0.419 (0.386)	Data 5.36e-05 (1.66e-03)	Tok/s 23877 (19454)	Loss/tok 7.8119 (8.5671)	LR 6.040e-04
5: TRAIN [0][110/113]	Time 0.420 (0.386)	Data 4.67e-05 (1.57e-03)	Tok/s 24015 (19441)	Loss/tok 7.8093 (8.5687)	LR 6.040e-04
7: TRAIN [0][110/113]	Time 0.415 (0.386)	Data 6.18e-05 (1.58e-03)	Tok/s 24271 (19429)	Loss/tok 7.7822 (8.5728)	LR 6.040e-04
4: TRAIN [0][110/113]	Time 0.420 (0.386)	Data 5.20e-05 (1.96e-03)	Tok/s 23963 (19451)	Loss/tok 7.8102 (8.5673)	LR 6.040e-04
1: TRAIN [0][110/113]	Time 0.419 (0.386)	Data 7.39e-05 (1.52e-03)	Tok/s 24138 (19464)	Loss/tok 7.7848 (8.5692)	LR 6.040e-04
3: Running validation on dev set
0: Running validation on dev set
6: Running validation on dev set
5: Running validation on dev set
7: Running validation on dev set
2: Running validation on dev set
4: Running validation on dev set
1: Running validation on dev set
3: Executing preallocation
0: Executing preallocation
5: Executing preallocation
7: Executing preallocation
6: Executing preallocation
2: Executing preallocation
4: Executing preallocation
1: Executing preallocation
2: VALIDATION [0][0/20]	Time 0.058 (0.058)	Data 2.36e-03 (2.36e-03)	Tok/s 76200 (76200)	Loss/tok 8.5770 (8.5770)
5: VALIDATION [0][0/20]	Time 0.054 (0.054)	Data 1.70e-03 (1.70e-03)	Tok/s 71870 (71870)	Loss/tok 8.5510 (8.5510)
7: VALIDATION [0][0/20]	Time 0.052 (0.052)	Data 1.71e-03 (1.71e-03)	Tok/s 70751 (70751)	Loss/tok 8.5035 (8.5035)
6: VALIDATION [0][0/20]	Time 0.054 (0.054)	Data 1.89e-03 (1.89e-03)	Tok/s 70349 (70349)	Loss/tok 8.4155 (8.4155)
3: VALIDATION [0][0/20]	Time 0.055 (0.055)	Data 1.75e-03 (1.75e-03)	Tok/s 75598 (75598)	Loss/tok 8.4881 (8.4881)
4: VALIDATION [0][0/20]	Time 0.057 (0.057)	Data 1.89e-03 (1.89e-03)	Tok/s 69711 (69711)	Loss/tok 8.4774 (8.4774)
1: VALIDATION [0][0/20]	Time 0.073 (0.073)	Data 2.87e-03 (2.87e-03)	Tok/s 64716 (64716)	Loss/tok 8.5441 (8.5441)
0: VALIDATION [0][0/20]	Time 0.096 (0.096)	Data 2.86e-03 (2.86e-03)	Tok/s 59352 (59352)	Loss/tok 8.5851 (8.5851)
7: VALIDATION [0][10/20]	Time 0.020 (0.030)	Data 1.37e-03 (1.45e-03)	Tok/s 77160 (77368)	Loss/tok 8.2983 (8.3416)
5: VALIDATION [0][10/20]	Time 0.020 (0.031)	Data 1.43e-03 (1.43e-03)	Tok/s 76305 (77247)	Loss/tok 8.0232 (8.2763)
6: VALIDATION [0][10/20]	Time 0.020 (0.032)	Data 1.57e-03 (1.70e-03)	Tok/s 77543 (75324)	Loss/tok 7.9371 (8.2649)
4: VALIDATION [0][10/20]	Time 0.020 (0.032)	Data 1.66e-03 (1.68e-03)	Tok/s 78749 (76591)	Loss/tok 8.1025 (8.3119)
2: VALIDATION [0][10/20]	Time 0.022 (0.033)	Data 1.63e-03 (1.71e-03)	Tok/s 71981 (75611)	Loss/tok 8.0515 (8.3238)
3: VALIDATION [0][10/20]	Time 0.020 (0.033)	Data 1.46e-03 (1.59e-03)	Tok/s 78889 (73943)	Loss/tok 8.0643 (8.3162)
1: VALIDATION [0][10/20]	Time 0.022 (0.036)	Data 2.31e-03 (2.42e-03)	Tok/s 71478 (72271)	Loss/tok 8.0951 (8.2990)
0: VALIDATION [0][10/20]	Time 0.021 (0.038)	Data 1.56e-03 (2.15e-03)	Tok/s 75968 (74494)	Loss/tok 8.1938 (8.3394)
0: Saving model to results/gnmt/model_best.pth
5: Running evaluation on test set
7: Running evaluation on test set
2: Running evaluation on test set
1: Running evaluation on test set
0: Running evaluation on test set
4: Running evaluation on test set
3: Running evaluation on test set
6: Running evaluation on test set
7: TEST [0][9/12]	Time 0.1374 (0.1974)	Decoder iters 28.0 (34.0)	Tok/s 8948 (9997)
5: TEST [0][9/12]	Time 0.1374 (0.1975)	Decoder iters 30.0 (33.9)	Tok/s 8840 (10153)
6: TEST [0][9/12]	Time 0.1376 (0.1976)	Decoder iters 29.0 (32.7)	Tok/s 8948 (10072)
4: TEST [0][9/12]	Time 0.1379 (0.1977)	Decoder iters 30.0 (32.6)	Tok/s 9247 (10269)
2: TEST [0][9/12]	Time 0.1376 (0.1977)	Decoder iters 30.0 (32.8)	Tok/s 9385 (10497)
3: TEST [0][9/12]	Time 0.1376 (0.1977)	Decoder iters 30.0 (44.8)	Tok/s 9184 (10363)
0: TEST [0][9/12]	Time 0.1376 (0.1978)	Decoder iters 33.0 (45.3)	Tok/s 9663 (10930)
1: TEST [0][9/12]	Time 0.1371 (0.1973)	Decoder iters 30.0 (34.2)	Tok/s 9732 (10692)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
3: Finished evaluation on test set
6: Finished evaluation on test set
1: Finished evaluation on test set
2: Finished evaluation on test set
7: Finished evaluation on test set
4: Finished evaluation on test set
5: Finished evaluation on test set
0: Finished evaluation on test set
1: Finished epoch 0
3: Finished epoch 0
7: Finished epoch 0
6: Finished epoch 0
2: Finished epoch 0
5: Finished epoch 0
4: Finished epoch 0
7: Starting epoch 1
5: Starting epoch 1
3: Starting epoch 1
2: Starting epoch 1
1: Starting epoch 1
6: Starting epoch 1
4: Starting epoch 1
7: Executing preallocation
3: Executing preallocation
2: Executing preallocation
6: Executing preallocation
1: Executing preallocation
5: Executing preallocation
4: Executing preallocation
0: Summary: Epoch: 0	Training Loss: 8.5509	Validation Loss: 8.2181	Test BLEU: 0.05
0: Performance: Epoch: 0	Training: 155886 Tok/s	Validation: 577670 Tok/s
0: Finished epoch 0
0: Starting epoch 1
0: Executing preallocation
5: Sampler for epoch 1 uses seed 2258090960
7: Sampler for epoch 1 uses seed 2258090960
1: Sampler for epoch 1 uses seed 2258090960
0: Sampler for epoch 1 uses seed 2258090960
2: Sampler for epoch 1 uses seed 2258090960
6: Sampler for epoch 1 uses seed 2258090960
4: Sampler for epoch 1 uses seed 2258090960
3: Sampler for epoch 1 uses seed 2258090960
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
3: TRAIN [1][0/113]	Time 0.567 (0.567)	Data 1.78e-01 (1.78e-01)	Tok/s 17581 (17581)	Loss/tok 7.6555 (7.6555)	LR 6.623e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
7: TRAIN [1][0/113]	Time 0.568 (0.568)	Data 1.45e-01 (1.45e-01)	Tok/s 17937 (17937)	Loss/tok 7.8010 (7.8010)	LR 6.623e-04
Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 64.0
1: TRAIN [1][0/113]	Time 0.567 (0.567)	Data 1.50e-01 (1.50e-01)	Tok/s 17710 (17710)	Loss/tok 7.7907 (7.7907)	LR 6.623e-04
2: TRAIN [1][0/113]	Time 0.568 (0.568)	Data 1.69e-01 (1.69e-01)	Tok/s 17639 (17639)	Loss/tok 7.7393 (7.7393)	LR 6.623e-04
4: TRAIN [1][0/113]	Time 0.568 (0.568)	Data 1.73e-01 (1.73e-01)	Tok/s 17654 (17654)	Loss/tok 7.6996 (7.6996)	LR 6.623e-04
0: TRAIN [1][0/113]	Time 0.567 (0.567)	Data 1.39e-01 (1.39e-01)	Tok/s 17793 (17793)	Loss/tok 7.7313 (7.7313)	LR 6.623e-04
6: TRAIN [1][0/113]	Time 0.569 (0.569)	Data 1.72e-01 (1.72e-01)	Tok/s 17754 (17754)	Loss/tok 7.7625 (7.7625)	LR 6.623e-04
5: TRAIN [1][0/113]	Time 0.569 (0.569)	Data 1.60e-01 (1.60e-01)	Tok/s 17685 (17685)	Loss/tok 7.7141 (7.7141)	LR 6.623e-04
1: TRAIN [1][10/113]	Time 0.466 (0.405)	Data 1.36e-04 (1.38e-02)	Tok/s 28243 (20276)	Loss/tok 7.8798 (7.6383)	LR 9.003e-04
3: TRAIN [1][10/113]	Time 0.463 (0.405)	Data 1.80e-04 (1.63e-02)	Tok/s 28156 (20183)	Loss/tok 7.8605 (7.6154)	LR 9.003e-04
0: TRAIN [1][10/113]	Time 0.467 (0.405)	Data 1.14e-04 (1.28e-02)	Tok/s 27729 (20158)	Loss/tok 7.8804 (7.6502)	LR 9.003e-04
5: TRAIN [1][10/113]	Time 0.467 (0.405)	Data 1.19e-04 (1.47e-02)	Tok/s 27872 (20174)	Loss/tok 7.9478 (7.6706)	LR 9.003e-04
4: TRAIN [1][10/113]	Time 0.467 (0.405)	Data 1.34e-04 (1.59e-02)	Tok/s 28044 (20245)	Loss/tok 7.9236 (7.6462)	LR 9.003e-04
6: TRAIN [1][10/113]	Time 0.467 (0.405)	Data 1.79e-04 (1.58e-02)	Tok/s 27967 (20239)	Loss/tok 7.8902 (7.6588)	LR 9.003e-04
7: TRAIN [1][10/113]	Time 0.467 (0.406)	Data 1.43e-04 (1.33e-02)	Tok/s 28005 (20160)	Loss/tok 7.8863 (7.6489)	LR 9.003e-04
2: TRAIN [1][10/113]	Time 0.467 (0.406)	Data 1.70e-04 (1.55e-02)	Tok/s 28038 (20172)	Loss/tok 7.9185 (7.6531)	LR 9.003e-04
6: TRAIN [1][20/113]	Time 0.417 (0.391)	Data 1.35e-04 (8.33e-03)	Tok/s 24195 (19078)	Loss/tok 7.8662 (7.6321)	LR 1.224e-03
5: TRAIN [1][20/113]	Time 0.418 (0.392)	Data 1.06e-04 (7.75e-03)	Tok/s 24251 (19067)	Loss/tok 7.8967 (7.6407)	LR 1.224e-03
0: TRAIN [1][20/113]	Time 0.417 (0.391)	Data 1.23e-04 (6.76e-03)	Tok/s 23877 (19055)	Loss/tok 7.8405 (7.6234)	LR 1.224e-03
7: TRAIN [1][20/113]	Time 0.417 (0.392)	Data 1.03e-04 (7.01e-03)	Tok/s 24212 (19038)	Loss/tok 7.8699 (7.6258)	LR 1.224e-03
1: TRAIN [1][20/113]	Time 0.417 (0.391)	Data 1.16e-04 (7.29e-03)	Tok/s 24204 (19064)	Loss/tok 7.8542 (7.6143)	LR 1.224e-03
3: TRAIN [1][20/113]	Time 0.414 (0.392)	Data 2.14e-04 (8.61e-03)	Tok/s 24050 (19027)	Loss/tok 7.8784 (7.6045)	LR 1.224e-03
4: TRAIN [1][20/113]	Time 0.417 (0.392)	Data 2.36e-04 (8.41e-03)	Tok/s 24204 (19088)	Loss/tok 7.9153 (7.6212)	LR 1.224e-03
2: TRAIN [1][20/113]	Time 0.418 (0.392)	Data 1.22e-04 (8.18e-03)	Tok/s 24220 (19037)	Loss/tok 7.8756 (7.6285)	LR 1.224e-03
3: TRAIN [1][30/113]	Time 0.421 (0.392)	Data 1.68e-04 (5.88e-03)	Tok/s 23756 (19414)	Loss/tok 7.5867 (7.6155)	LR 1.664e-03
4: TRAIN [1][30/113]	Time 0.421 (0.392)	Data 1.37e-04 (5.74e-03)	Tok/s 24162 (19429)	Loss/tok 7.5746 (7.6212)	LR 1.664e-03
5: TRAIN [1][30/113]	Time 0.421 (0.392)	Data 1.46e-04 (5.30e-03)	Tok/s 23564 (19394)	Loss/tok 7.5858 (7.6373)	LR 1.664e-03
1: TRAIN [1][30/113]	Time 0.422 (0.392)	Data 1.38e-04 (4.98e-03)	Tok/s 23906 (19422)	Loss/tok 7.6267 (7.6239)	LR 1.664e-03
7: TRAIN [1][30/113]	Time 0.422 (0.392)	Data 1.16e-04 (4.78e-03)	Tok/s 23508 (19416)	Loss/tok 7.6008 (7.6261)	LR 1.664e-03
6: TRAIN [1][30/113]	Time 0.422 (0.392)	Data 1.24e-04 (5.69e-03)	Tok/s 23616 (19457)	Loss/tok 7.6041 (7.6276)	LR 1.664e-03
0: TRAIN [1][30/113]	Time 0.422 (0.392)	Data 1.04e-04 (4.62e-03)	Tok/s 23856 (19418)	Loss/tok 7.6557 (7.6216)	LR 1.664e-03
2: TRAIN [1][30/113]	Time 0.421 (0.392)	Data 1.28e-04 (5.59e-03)	Tok/s 23942 (19411)	Loss/tok 7.6410 (7.6249)	LR 1.664e-03
5: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.16e-04 (4.04e-03)	Tok/s 6645 (19094)	Loss/tok 7.1248 (7.5982)	LR 1.000e-03
3: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.26e-04 (4.48e-03)	Tok/s 6942 (19128)	Loss/tok 7.0474 (7.5811)	LR 1.000e-03
7: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.08e-04 (3.64e-03)	Tok/s 7122 (19129)	Loss/tok 6.8026 (7.5843)	LR 1.000e-03
6: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.31e-04 (4.34e-03)	Tok/s 6989 (19148)	Loss/tok 7.0066 (7.5900)	LR 1.000e-03
1: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.31e-04 (3.80e-03)	Tok/s 7084 (19147)	Loss/tok 7.0173 (7.5851)	LR 1.000e-03
0: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.34e-04 (3.52e-03)	Tok/s 7079 (19119)	Loss/tok 6.9588 (7.5811)	LR 1.000e-03
4: TRAIN [1][40/113]	Time 0.310 (0.387)	Data 1.57e-04 (4.38e-03)	Tok/s 6876 (19097)	Loss/tok 6.9664 (7.5834)	LR 1.000e-03
2: TRAIN [1][40/113]	Time 0.309 (0.387)	Data 1.28e-04 (4.26e-03)	Tok/s 7014 (19123)	Loss/tok 7.1668 (7.5881)	LR 1.000e-03
3: TRAIN [1][50/113]	Time 0.419 (0.388)	Data 1.23e-04 (3.62e-03)	Tok/s 24139 (19144)	Loss/tok 7.3386 (7.5338)	LR 1.000e-03
5: TRAIN [1][50/113]	Time 0.418 (0.388)	Data 1.29e-04 (3.28e-03)	Tok/s 23837 (19122)	Loss/tok 7.3228 (7.5487)	LR 1.000e-03
7: TRAIN [1][50/113]	Time 0.418 (0.388)	Data 1.13e-04 (2.95e-03)	Tok/s 23929 (19155)	Loss/tok 7.3505 (7.5342)	LR 1.000e-03
1: TRAIN [1][50/113]	Time 0.419 (0.388)	Data 1.31e-04 (3.08e-03)	Tok/s 23953 (19168)	Loss/tok 7.4076 (7.5362)	LR 1.000e-03
0: TRAIN [1][50/113]	Time 0.419 (0.388)	Data 1.13e-04 (2.85e-03)	Tok/s 23773 (19151)	Loss/tok 7.3379 (7.5357)	LR 1.000e-03
4: TRAIN [1][50/113]	Time 0.419 (0.388)	Data 1.46e-04 (3.55e-03)	Tok/s 24213 (19138)	Loss/tok 7.3253 (7.5333)	LR 1.000e-03
2: TRAIN [1][50/113]	Time 0.419 (0.388)	Data 1.27e-04 (3.45e-03)	Tok/s 24191 (19161)	Loss/tok 7.3846 (7.5417)	LR 1.000e-03
6: TRAIN [1][50/113]	Time 0.419 (0.388)	Data 1.92e-04 (3.52e-03)	Tok/s 24119 (19183)	Loss/tok 7.3712 (7.5441)	LR 1.000e-03
3: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.36e-04 (3.05e-03)	Tok/s 19110 (19185)	Loss/tok 6.9864 (7.4761)	LR 5.000e-04
0: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.12e-04 (2.41e-03)	Tok/s 19028 (19166)	Loss/tok 7.0972 (7.4802)	LR 5.000e-04
1: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.29e-04 (2.60e-03)	Tok/s 19322 (19210)	Loss/tok 7.0696 (7.4807)	LR 5.000e-04
7: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.20e-04 (2.49e-03)	Tok/s 19282 (19210)	Loss/tok 7.1007 (7.4759)	LR 5.000e-04
4: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.57e-04 (2.99e-03)	Tok/s 19076 (19177)	Loss/tok 6.9306 (7.4750)	LR 5.000e-04
5: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.33e-04 (2.76e-03)	Tok/s 19202 (19196)	Loss/tok 7.1095 (7.4906)	LR 5.000e-04
2: TRAIN [1][60/113]	Time 0.376 (0.388)	Data 1.76e-04 (2.91e-03)	Tok/s 18906 (19200)	Loss/tok 6.9829 (7.4797)	LR 5.000e-04
6: TRAIN [1][60/113]	Time 0.377 (0.388)	Data 1.45e-04 (2.97e-03)	Tok/s 18717 (19205)	Loss/tok 7.1399 (7.4859)	LR 5.000e-04
0: TRAIN [1][70/113]	Time 0.466 (0.391)	Data 1.22e-04 (2.09e-03)	Tok/s 27846 (19468)	Loss/tok 7.1595 (7.4209)	LR 5.000e-04
7: TRAIN [1][70/113]	Time 0.466 (0.391)	Data 1.21e-04 (2.15e-03)	Tok/s 27882 (19488)	Loss/tok 7.1967 (7.4162)	LR 5.000e-04
1: TRAIN [1][70/113]	Time 0.467 (0.391)	Data 1.47e-04 (2.26e-03)	Tok/s 28036 (19496)	Loss/tok 7.2163 (7.4175)	LR 5.000e-04
3: TRAIN [1][70/113]	Time 0.467 (0.391)	Data 1.33e-04 (2.64e-03)	Tok/s 27405 (19481)	Loss/tok 7.1830 (7.4155)	LR 5.000e-04
4: TRAIN [1][70/113]	Time 0.467 (0.391)	Data 1.35e-04 (2.59e-03)	Tok/s 28180 (19474)	Loss/tok 7.1619 (7.4150)	LR 5.000e-04
2: TRAIN [1][70/113]	Time 0.467 (0.391)	Data 1.37e-04 (2.52e-03)	Tok/s 28064 (19477)	Loss/tok 7.1397 (7.4155)	LR 5.000e-04
6: TRAIN [1][70/113]	Time 0.467 (0.391)	Data 1.46e-04 (2.57e-03)	Tok/s 28021 (19504)	Loss/tok 7.1528 (7.4242)	LR 5.000e-04
5: TRAIN [1][70/113]	Time 0.474 (0.391)	Data 1.24e-04 (2.39e-03)	Tok/s 27461 (19478)	Loss/tok 7.1494 (7.4285)	LR 5.000e-04
3: TRAIN [1][80/113]	Time 0.310 (0.390)	Data 1.79e-04 (2.33e-03)	Tok/s 7131 (19453)	Loss/tok 6.2612 (7.3591)	LR 2.500e-04
1: TRAIN [1][80/113]	Time 0.309 (0.390)	Data 1.54e-04 (1.99e-03)	Tok/s 7062 (19448)	Loss/tok 6.1190 (7.3576)	LR 2.500e-04
0: TRAIN [1][80/113]	Time 0.309 (0.390)	Data 1.91e-04 (1.84e-03)	Tok/s 7060 (19424)	Loss/tok 6.0583 (7.3581)	LR 2.500e-04
2: TRAIN [1][80/113]	Time 0.309 (0.390)	Data 1.40e-04 (2.23e-03)	Tok/s 7063 (19437)	Loss/tok 6.2577 (7.3547)	LR 2.500e-04
7: TRAIN [1][80/113]	Time 0.310 (0.390)	Data 1.21e-04 (1.90e-03)	Tok/s 6787 (19447)	Loss/tok 6.3281 (7.3568)	LR 2.500e-04
6: TRAIN [1][80/113]	Time 0.309 (0.390)	Data 1.45e-04 (2.27e-03)	Tok/s 6829 (19455)	Loss/tok 6.1673 (7.3620)	LR 2.500e-04
4: TRAIN [1][80/113]	Time 0.311 (0.390)	Data 1.66e-04 (2.29e-03)	Tok/s 6763 (19423)	Loss/tok 6.1275 (7.3559)	LR 2.500e-04
5: TRAIN [1][80/113]	Time 0.311 (0.390)	Data 1.17e-04 (2.11e-03)	Tok/s 6819 (19431)	Loss/tok 6.1320 (7.3694)	LR 2.500e-04
1: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.32e-04 (1.79e-03)	Tok/s 12663 (19467)	Loss/tok 6.3694 (7.3012)	LR 2.500e-04
3: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.45e-04 (2.09e-03)	Tok/s 12604 (19489)	Loss/tok 6.4880 (7.3039)	LR 2.500e-04
5: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.19e-04 (1.89e-03)	Tok/s 13144 (19461)	Loss/tok 6.4940 (7.3112)	LR 2.500e-04
7: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.13e-04 (1.71e-03)	Tok/s 12736 (19470)	Loss/tok 6.5037 (7.3030)	LR 2.500e-04
0: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.20e-04 (1.66e-03)	Tok/s 12870 (19463)	Loss/tok 6.5111 (7.3025)	LR 2.500e-04
6: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.68e-04 (2.04e-03)	Tok/s 12659 (19476)	Loss/tok 6.4415 (7.3041)	LR 2.500e-04
2: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.62e-04 (2.00e-03)	Tok/s 12956 (19461)	Loss/tok 6.4019 (7.3008)	LR 2.500e-04
4: TRAIN [1][90/113]	Time 0.341 (0.390)	Data 1.39e-04 (2.05e-03)	Tok/s 12949 (19460)	Loss/tok 6.5585 (7.3006)	LR 2.500e-04
5: TRAIN [1][100/113]	Time 0.467 (0.390)	Data 1.33e-04 (1.72e-03)	Tok/s 27682 (19516)	Loss/tok 7.0174 (7.2654)	LR 1.250e-04
6: TRAIN [1][100/113]	Time 0.463 (0.390)	Data 1.75e-04 (1.85e-03)	Tok/s 28186 (19527)	Loss/tok 7.0070 (7.2571)	LR 1.250e-04
7: TRAIN [1][100/113]	Time 0.467 (0.390)	Data 1.16e-04 (1.55e-03)	Tok/s 28088 (19524)	Loss/tok 6.9675 (7.2566)	LR 1.250e-04
1: TRAIN [1][100/113]	Time 0.468 (0.390)	Data 1.42e-04 (1.63e-03)	Tok/s 28016 (19506)	Loss/tok 7.0265 (7.2560)	LR 1.250e-04
0: TRAIN [1][100/113]	Time 0.467 (0.390)	Data 1.20e-04 (1.51e-03)	Tok/s 28276 (19524)	Loss/tok 7.0206 (7.2544)	LR 1.250e-04
3: TRAIN [1][100/113]	Time 0.468 (0.390)	Data 1.68e-04 (1.90e-03)	Tok/s 27950 (19531)	Loss/tok 7.0493 (7.2574)	LR 1.250e-04
4: TRAIN [1][100/113]	Time 0.467 (0.390)	Data 1.40e-04 (1.87e-03)	Tok/s 28011 (19511)	Loss/tok 7.0091 (7.2559)	LR 1.250e-04
2: TRAIN [1][100/113]	Time 0.472 (0.390)	Data 1.28e-04 (1.81e-03)	Tok/s 27525 (19513)	Loss/tok 7.0522 (7.2529)	LR 1.250e-04
5: TRAIN [1][110/113]	Time 0.305 (0.388)	Data 4.63e-05 (1.58e-03)	Tok/s 7023 (19337)	Loss/tok 5.9174 (7.2239)	LR 1.250e-04
3: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 6.15e-05 (1.74e-03)	Tok/s 7287 (19339)	Loss/tok 5.8831 (7.2158)	LR 1.250e-04
1: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 5.22e-05 (1.50e-03)	Tok/s 7022 (19331)	Loss/tok 6.1583 (7.2127)	LR 1.250e-04
7: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 4.43e-05 (1.42e-03)	Tok/s 6847 (19337)	Loss/tok 6.0397 (7.2171)	LR 1.250e-04
6: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 5.36e-05 (1.70e-03)	Tok/s 7018 (19343)	Loss/tok 5.8923 (7.2139)	LR 1.250e-04
0: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 5.36e-05 (1.38e-03)	Tok/s 7037 (19334)	Loss/tok 5.9559 (7.2130)	LR 1.250e-04
4: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 5.03e-05 (1.71e-03)	Tok/s 7160 (19323)	Loss/tok 6.2117 (7.2153)	LR 1.250e-04
2: TRAIN [1][110/113]	Time 0.306 (0.388)	Data 5.65e-05 (1.67e-03)	Tok/s 6997 (19335)	Loss/tok 6.0223 (7.2127)	LR 1.250e-04
5: Running validation on dev set
4: Running validation on dev set
5: Executing preallocation
7: Running validation on dev set
4: Executing preallocation
0: Running validation on dev set
2: Running validation on dev set
3: Running validation on dev set
7: Executing preallocation
0: Executing preallocation
2: Executing preallocation
6: Running validation on dev set
3: Executing preallocation
1: Running validation on dev set
6: Executing preallocation
1: Executing preallocation
3: VALIDATION [1][0/20]	Time 0.055 (0.055)	Data 2.05e-03 (2.05e-03)	Tok/s 75354 (75354)	Loss/tok 7.6621 (7.6621)
2: VALIDATION [1][0/20]	Time 0.062 (0.062)	Data 3.23e-03 (3.23e-03)	Tok/s 71470 (71470)	Loss/tok 7.8128 (7.8128)
7: VALIDATION [1][0/20]	Time 0.050 (0.050)	Data 1.69e-03 (1.69e-03)	Tok/s 73535 (73535)	Loss/tok 7.7321 (7.7321)
4: VALIDATION [1][0/20]	Time 0.054 (0.054)	Data 1.75e-03 (1.75e-03)	Tok/s 73423 (73423)	Loss/tok 7.6621 (7.6621)
6: VALIDATION [1][0/20]	Time 0.053 (0.053)	Data 2.83e-03 (2.83e-03)	Tok/s 71399 (71399)	Loss/tok 7.5817 (7.5817)
5: VALIDATION [1][0/20]	Time 0.053 (0.053)	Data 2.81e-03 (2.81e-03)	Tok/s 72305 (72305)	Loss/tok 7.7360 (7.7360)
1: VALIDATION [1][0/20]	Time 0.073 (0.073)	Data 2.77e-03 (2.77e-03)	Tok/s 65419 (65419)	Loss/tok 7.7495 (7.7495)
0: VALIDATION [1][0/20]	Time 0.096 (0.096)	Data 1.71e-03 (1.71e-03)	Tok/s 59368 (59368)	Loss/tok 7.7991 (7.7991)
7: VALIDATION [1][10/20]	Time 0.019 (0.030)	Data 1.31e-03 (1.46e-03)	Tok/s 79155 (78838)	Loss/tok 7.5054 (7.5582)
4: VALIDATION [1][10/20]	Time 0.020 (0.031)	Data 1.48e-03 (1.60e-03)	Tok/s 78403 (79148)	Loss/tok 7.2952 (7.5192)
3: VALIDATION [1][10/20]	Time 0.020 (0.033)	Data 1.49e-03 (1.65e-03)	Tok/s 76972 (74431)	Loss/tok 7.2788 (7.5235)
2: VALIDATION [1][10/20]	Time 0.022 (0.034)	Data 1.55e-03 (2.06e-03)	Tok/s 73464 (74816)	Loss/tok 7.2535 (7.5383)
6: VALIDATION [1][10/20]	Time 0.021 (0.032)	Data 2.39e-03 (2.48e-03)	Tok/s 71715 (73788)	Loss/tok 7.1504 (7.4670)
5: VALIDATION [1][10/20]	Time 0.022 (0.032)	Data 2.43e-03 (2.46e-03)	Tok/s 71031 (74319)	Loss/tok 7.2923 (7.5028)
1: VALIDATION [1][10/20]	Time 0.022 (0.035)	Data 1.64e-03 (2.41e-03)	Tok/s 72056 (73016)	Loss/tok 7.2806 (7.4958)
0: VALIDATION [1][10/20]	Time 0.022 (0.037)	Data 1.59e-03 (1.59e-03)	Tok/s 75581 (75056)	Loss/tok 7.3621 (7.5432)
0: Saving model to results/gnmt/model_best.pth
1: Running evaluation on test set
5: Running evaluation on test set
7: Running evaluation on test set
0: Running evaluation on test set
2: Running evaluation on test set
4: Running evaluation on test set
3: Running evaluation on test set
6: Running evaluation on test set
7: TEST [1][9/12]	Time 0.4194 (0.5028)	Decoder iters 39.0 (127.3)	Tok/s 2463 (4598)
5: TEST [1][9/12]	Time 0.4195 (0.5028)	Decoder iters 149.0 (149.0)	Tok/s 3137 (5308)
4: TEST [1][9/12]	Time 0.4197 (0.5029)	Decoder iters 149.0 (148.3)	Tok/s 2957 (5146)
1: TEST [1][9/12]	Time 0.4194 (0.5029)	Decoder iters 35.0 (129.7)	Tok/s 2795 (5333)
0: TEST [1][9/12]	Time 0.4195 (0.5030)	Decoder iters 149.0 (149.0)	Tok/s 3473 (5786)
2: TEST [1][9/12]	Time 0.4193 (0.5030)	Decoder iters 149.0 (140.8)	Tok/s 3026 (5233)
3: TEST [1][9/12]	Time 0.4193 (0.5030)	Decoder iters 149.0 (149.0)	Tok/s 2647 (5310)
6: TEST [1][9/12]	Time 0.4192 (0.5030)	Decoder iters 149.0 (138.6)	Tok/s 2502 (4810)
0: Running sacrebleu (parameters: --score-only -lc --tokenize intl)
3: Finished evaluation on test set
1: Finished evaluation on test set
4: Finished evaluation on test set
2: Finished evaluation on test set
6: Finished evaluation on test set
7: Finished evaluation on test set
5: Finished evaluation on test set
0: Finished evaluation on test set
3: Finished epoch 1
5: Finished epoch 1
4: Finished epoch 1
2: Finished epoch 1
6: Finished epoch 1
7: Finished epoch 1
1: Finished epoch 1
0: Summary: Epoch: 1	Training Loss: 7.2080	Validation Loss: 7.4189	Test BLEU: 0.28
0: Performance: Epoch: 1	Training: 154839 Tok/s	Validation: 578999 Tok/s
0: Finished epoch 1
5: Total training time 133 s
7: Total training time 133 s
3: Total training time 133 s
4: Total training time 133 s
1: Total training time 133 s
6: Total training time 133 s
2: Total training time 133 s
0: Total training time 133 s
# Training Summary
|**GPUs**|**Batch Size / GPU**|**Accuracy - FP16 (BLEU)**|**Throughput - FP16 (tok/s)**|**Time to Train - FP16 (min)**|
|-------:|-------------------:|-------------------------:|----------------------------:|-----------------------------:|
|       8|                 160|                      0.28|                     155362.8|                         2.218|
DONE!
