:::NVLOGv0.2.2 Tacotron2_PyT 1586642860.101173878 (/workspace/examples/tacotron2/dllogger/logger.py:279) run_start
:::NVLOGv0.2.2 Tacotron2_PyT 1586642860.113932848 (/workspace/examples/tacotron2/dllogger/logger.py:251) cpu_info: {"num": 64, "name": "Intel(R) Xeon(R) CPU E5-2686 v4 @ 2.30GHz"}
:::NVLOGv0.2.2 Tacotron2_PyT 1586642860.121150732 (/workspace/examples/tacotron2/dllogger/logger.py:251) mem_info: {"ram": "480G"}
:::NVLOGv0.2.2 Tacotron2_PyT 1586642868.043508530 (/workspace/examples/tacotron2/dllogger/logger.py:251) gpu_info: {"driver_version": "440.64", "num": 8, "name": ["Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB", "Tesla V100-SXM2-16GB"], "mem": ["16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB", "16160 MiB"]}
:::NVLOGv0.2.2 Tacotron2_PyT 1586642868.047312498 (/workspace/examples/tacotron2/dllogger/logger.py:251) args: {"output_directory": "./", "dataset_path": "/data/tacotron2/LJSpeech-1.1", "model_name": "Tacotron2", "log_file": "nvlog.json", "anneal_steps": null, "anneal_factor": 0.1, "epochs": 3, "epochs_per_checkpoint": 50, "checkpoint_path": "", "seed": 1234, "dynamic_loss_scaling": true, "amp_run": true, "cudnn_enabled": true, "cudnn_benchmark": false, "disable_uniform_initialize_bn_weight": false, "use_saved_learning_rate": false, "learning_rate": 0.0, "weight_decay": 1e-06, "grad_clip_thresh": 1.0, "batch_size": 100, "grad_clip": 5.0, "load_mel_from_disk": false, "training_files": "filelists/ljs_audio_text_train_subset_1250_filelist.txt", "validation_files": "filelists/ljs_audio_text_val_filelist.txt", "text_cleaners": ["english_cleaners"], "max_wav_value": 32768.0, "sampling_rate": 22050, "filter_length": 1024, "hop_length": 256, "win_length": 1024, "mel_fmin": 0.0, "mel_fmax": 8000.0, "rank": 0, "world_size": 8, "dist_url": "tcp://localhost:23456", "group_name": "group_name", "dist_backend": "nccl", "mask_padding": false, "n_mel_channels": 80, "n_symbols": 148, "symbols_embedding_dim": 512, "encoder_kernel_size": 5, "encoder_n_convolutions": 3, "encoder_embedding_dim": 512, "n_frames_per_step": 1, "decoder_rnn_dim": 1024, "prenet_dim": 256, "max_decoder_steps": 2000, "gate_threshold": 0.5, "p_attention_dropout": 0.1, "p_decoder_dropout": 0.1, "decoder_no_early_stopping": false, "attention_rnn_dim": 1024, "attention_dim": 128, "attention_location_n_filters": 32, "attention_location_kernel_size": 31, "postnet_embedding_dim": 512, "postnet_kernel_size": 5, "postnet_n_convolutions": 5}
Initializing Distributed
Done initializing distributed
:::NVLOGv0.2.2 Tacotron2_PyT 1586642869.843836308 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_start
Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.

Defaults for this optimization level are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
Processing user overrides (additional kwargs that are not None)...
After processing overrides, optimization options are:
enabled                : True
opt_level              : O1
cast_model_type        : None
patch_torch_functions  : True
keep_batchnorm_fp32    : None
master_weights         : None
loss_scale             : dynamic
:::NVLOGv0.2.2 Tacotron2_PyT 1586642887.935437202 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_loop
:::NVLOGv0.2.2 Tacotron2_PyT 1586642887.943147182 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 0
Batch: 0/1 epoch 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642893.833717585 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642908.692570686 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 46.980560302734375
:::NVLOGv0.2.2 Tacotron2_PyT 1586642910.979740143 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642910.980380774 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 26332.998801907914
:::NVLOGv0.2.2 Tacotron2_PyT 1586642910.980934620 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 17.147306442260742
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586642911.064522505 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642911.065014124 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 19527.993700711148
:::NVLOGv0.2.2 Tacotron2_PyT 1586642911.065426826 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 26332.998801907914
:::NVLOGv0.2.2 Tacotron2_PyT 1586642911.065844297 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 46.980560302734375
:::NVLOGv0.2.2 Tacotron2_PyT 1586642911.066325665 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 23.122703075408936
:::NVLOGv0.2.2 Tacotron2_PyT 1586642911.066775560 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 0
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
/opt/conda/lib/python3.6/site-packages/torch/distributed/distributed_c10d.py:101: UserWarning: torch.distributed.reduce_op is deprecated, please use torch.distributed.ReduceOp instead
  warnings.warn("torch.distributed.reduce_op is deprecated, please use "
:::NVLOGv0.2.2 Tacotron2_PyT 1586642913.510014057 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.31106948852539
:::NVLOGv0.2.2 Tacotron2_PyT 1586642913.511703730 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 0
Saving model and optimizer state at epoch 0 to ./checkpoint_Tacotron2_0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642913.830418348 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 1
Batch: 0/1 epoch 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642917.100778580 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642919.550153255 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.02046203613281
:::NVLOGv0.2.2 Tacotron2_PyT 1586642921.923331499 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642921.923880816 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 93258.9105441688
:::NVLOGv0.2.2 Tacotron2_PyT 1586642921.924840212 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 4.823657035827637
:::NVLOGv0.2.2 Tacotron2_PyT 1586642922.023924589 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642922.024517775 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 54896.53340711824
:::NVLOGv0.2.2 Tacotron2_PyT 1586642922.024951696 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 93258.9105441688
:::NVLOGv0.2.2 Tacotron2_PyT 1586642922.025394201 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.02046203613281
:::NVLOGv0.2.2 Tacotron2_PyT 1586642922.025816679 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 8.194488286972046
:::NVLOGv0.2.2 Tacotron2_PyT 1586642922.026268005 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642924.534799337 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.31666946411133
:::NVLOGv0.2.2 Tacotron2_PyT 1586642924.536386251 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 1
:::NVLOGv0.2.2 Tacotron2_PyT 1586642924.537798643 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_start: 2
Batch: 0/1 epoch 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642927.823487282 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_start: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642930.482795238 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iteration_loss: 47.602264404296875
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.899765730 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_stop: 0
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.902460814 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_iter_items/sec: 89856.17153804774
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.903382778 (/workspace/examples/tacotron2/dllogger/logger.py:251) iter_time: 5.077781438827515
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.982526064 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.983088493 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_items/sec: 54026.81501461142
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.983563423 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_items/sec: 89856.17153804774
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.984034538 (/workspace/examples/tacotron2/dllogger/logger.py:251) train_epoch_avg_loss: 47.602264404296875
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.984495878 (/workspace/examples/tacotron2/dllogger/logger.py:251) epoch_time: 8.445250749588013
:::NVLOGv0.2.2 Tacotron2_PyT 1586642932.984951735 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_start: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642935.474606514 (/workspace/examples/tacotron2/dllogger/logger.py:251) val_iter_loss: 48.32192611694336
:::NVLOGv0.2.2 Tacotron2_PyT 1586642935.476167679 (/workspace/examples/tacotron2/dllogger/logger.py:251) eval_stop: 2
:::NVLOGv0.2.2 Tacotron2_PyT 1586642935.477546692 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 65.63305401802063
:::NVLOGv0.2.2 Tacotron2_PyT 1586642935.478006363 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_final
training time 65.63305401802063
:::NVLOGv0.2.2 Tacotron2_PyT 1586642935.478589773 (/workspace/examples/tacotron2/dllogger/logger.py:251) run_time: 75.49839878082275
:::NVLOGv0.2.2 Tacotron2_PyT 1586642935.479040146 (/workspace/examples/tacotron2/dllogger/logger.py:282) run_stop
DONE!
